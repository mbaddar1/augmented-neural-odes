[experiments_sandbox.py:792 -   <module>()] 2023-04-27 15:07:37,722 INFO SEED = 42
[experiments_sandbox.py:878 -   <module>()] 2023-04-27 15:07:37,723 INFO model = 
***
NN-Model 
Sequential(
  (0): Linear(in_features=3, out_features=50, bias=True)
  (1): Tanh()
  (2): Linear(in_features=50, out_features=3, bias=True)
)
numel_learnable = 353
***
[experiments_sandbox.py:879 -   <module>()] 2023-04-27 15:07:37,724 INFO optimizer  = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.05
    maximize: False
    weight_decay: 0
)
[experiments_sandbox.py:887 -   <module>()] 2023-04-27 15:07:37,724 INFO lr_scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd598e9d420>
[experiments_sandbox.py:918 -   <module>()] 2023-04-27 15:07:37,725 INFO train-dataset = 
***
Lorenz-System
N = 1100rho = 28
sigma = 10
beta = 2.6666666666666665
****

[experiments_sandbox.py:919 -   <module>()] 2023-04-27 15:07:37,725 INFO test-dataset = 
***
Lorenz-System
N = 1100rho = 28
sigma = 10
beta = 2.6666666666666665
****

[experiments_sandbox.py:920 -   <module>()] 2023-04-27 15:07:37,725 INFO train-epochs = 2000
[experiments_sandbox.py:925 -   <module>()] 2023-04-27 15:07:37,725 INFO Output Normalization = None
[experiments_sandbox.py:926 -   <module>()] 2023-04-27 15:07:37,725 INFO Gradient-clipping max-norm = 10
[experiments_sandbox.py:928 -   <module>()] 2023-04-27 15:07:37,725 INFO epochs_losses_window = 10
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:37,875 INFO epoch # 0 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 85.63285437992641
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:37,899 INFO epoch # 1 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 9.806293971197945
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:37,924 INFO epoch # 2 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 2.3578098620687213
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:37,949 INFO epoch # 3 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 1.2015221868242536
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:37,974 INFO epoch # 4 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.8949500986507961
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,000 INFO epoch # 5 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.4793214900153024
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,025 INFO epoch # 6 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.3726756383265768
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,051 INFO epoch # 7 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.45114755438906806
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,075 INFO epoch # 8 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.40703530524458204
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,100 INFO epoch # 9 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.2986515177147729
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,125 INFO epoch # 10 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.30151209958962033
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:38,125 INFO *** epoch 10, rolling-avg-loss (window=10)= 1.657091972402164
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,149 INFO epoch # 11 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.394443872996739
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,180 INFO epoch # 12 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.4008573268141065
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,207 INFO epoch # 13 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.3502462521195412
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,232 INFO epoch # 14 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.20701544476406916
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,258 INFO epoch # 15 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.22712993760194097
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,283 INFO epoch # 16 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.17012078389525415
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,307 INFO epoch # 17 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.19221366878066745
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,333 INFO epoch # 18 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.09470934027007648
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,357 INFO epoch # 19 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.08680713182049138
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,382 INFO epoch # 20 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.06995122336915562
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:38,383 INFO *** epoch 20, rolling-avg-loss (window=10)= 0.21934949824320418
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,408 INFO epoch # 21 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.07286665737628936
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,433 INFO epoch # 22 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.13509896216647965
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,458 INFO epoch # 23 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.15087568610906602
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,483 INFO epoch # 24 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.0995012598910502
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,509 INFO epoch # 25 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.07908864074519702
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,534 INFO epoch # 26 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.11550302883344037
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,559 INFO epoch # 27 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.09796552998679024
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,584 INFO epoch # 28 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.14791601417320116
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,609 INFO epoch # 29 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.13912333367126328
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,634 INFO epoch # 30 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 0.19750427646296365
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:38,634 INFO *** epoch 30, rolling-avg-loss (window=10)= 0.1235443389415741
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,659 INFO epoch # 31 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.04 -loss = 0.13150257393717765
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,684 INFO epoch # 32 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.05877906414014952
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,708 INFO epoch # 33 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.03795105309358665
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,733 INFO epoch # 34 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.0542496956884861
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,757 INFO epoch # 35 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.062275850613202365
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,781 INFO epoch # 36 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.03538230833198343
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,806 INFO epoch # 37 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.05224787781813315
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,831 INFO epoch # 38 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.0931018911834274
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,856 INFO epoch # 39 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.2645116178052766
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,881 INFO epoch # 40 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.14061863278704032
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:38,881 INFO *** epoch 40, rolling-avg-loss (window=10)= 0.09306205653984632
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,906 INFO epoch # 41 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.08244830412524087
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,931 INFO epoch # 42 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.05478425941296986
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,955 INFO epoch # 43 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.0549758216632264
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:38,980 INFO epoch # 44 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.048303497742329324
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,005 INFO epoch # 45 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.09114456054355417
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,030 INFO epoch # 46 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 0.08914373219013214
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,055 INFO epoch # 47 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.032 -loss = 0.10000816700713976
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,079 INFO epoch # 48 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.07321720580969537
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,104 INFO epoch # 49 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.04060141976390566
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,128 INFO epoch # 50 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.028249315385307585
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:39,128 INFO *** epoch 50, rolling-avg-loss (window=10)= 0.06628762836435012
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,152 INFO epoch # 51 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.01986463474375861
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,176 INFO epoch # 52 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.01562381304268326
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,201 INFO epoch # 53 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.0136129995275821
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,225 INFO epoch # 54 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.016605892910489015
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,250 INFO epoch # 55 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.02248074987104961
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,276 INFO epoch # 56 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.03417157788894006
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,300 INFO epoch # 57 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.02540341205894947
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,326 INFO epoch # 58 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.026899524299161776
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,350 INFO epoch # 59 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.023981575295329093
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,375 INFO epoch # 60 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.026788553688675165
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:39,375 INFO *** epoch 60, rolling-avg-loss (window=10)= 0.022543273332661814
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,399 INFO epoch # 61 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.016931478971881526
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,424 INFO epoch # 62 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.02852986279342856
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,449 INFO epoch # 63 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.053980998482022965
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,474 INFO epoch # 64 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.0256 -loss = 0.08089593106082507
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,501 INFO epoch # 65 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.06287354214915207
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,526 INFO epoch # 66 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.031343427965683596
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,551 INFO epoch # 67 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.021608407822038447
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,575 INFO epoch # 68 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.03367857978280101
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,599 INFO epoch # 69 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.023242526528026376
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,624 INFO epoch # 70 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.01920279834552535
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:39,625 INFO *** epoch 70, rolling-avg-loss (window=10)= 0.03722875539013849
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,650 INFO epoch # 71 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.016848528378510048
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,675 INFO epoch # 72 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.012737550080886909
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,700 INFO epoch # 73 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.01523651058253433
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,725 INFO epoch # 74 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.012742681934365204
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,749 INFO epoch # 75 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.014887901941048248
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,774 INFO epoch # 76 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.01637713663013918
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,798 INFO epoch # 77 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.015475980644779546
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,822 INFO epoch # 78 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.024250831162290914
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,847 INFO epoch # 79 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.01234817174928529
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,871 INFO epoch # 80 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.01101046109439007
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:39,872 INFO *** epoch 80, rolling-avg-loss (window=10)= 0.015191575419822976
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,897 INFO epoch # 81 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.022990104595997502
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,922 INFO epoch # 82 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.10028069950640202
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,947 INFO epoch # 83 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.10857657944517476
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,972 INFO epoch # 84 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.11664340112890516
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:39,999 INFO epoch # 85 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.08396191416042191
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,025 INFO epoch # 86 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.032277769395815475
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,049 INFO epoch # 87 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.020539526122489146
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,074 INFO epoch # 88 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.01597346430644393
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,099 INFO epoch # 89 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.021567289903759956
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,124 INFO epoch # 90 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.010189726549599851
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:40,124 INFO *** epoch 90, rolling-avg-loss (window=10)= 0.05330004751150097
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,149 INFO epoch # 91 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.012581011626337256
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,173 INFO epoch # 92 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.12167647579418761
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,198 INFO epoch # 93 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.0831849541515112
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,222 INFO epoch # 94 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.04911178781517914
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,246 INFO epoch # 95 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.03140301736337798
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,271 INFO epoch # 96 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.019549237843602896
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,296 INFO epoch # 97 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.018087342181908234
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,321 INFO epoch # 98 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.02411930835140603
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,346 INFO epoch # 99 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.03477570699261767
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,375 INFO epoch # 100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.09609814898243972
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:40,375 INFO *** epoch 100, rolling-avg-loss (window=10)= 0.04905869911025677
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,399 INFO epoch # 101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.02048 -loss = 0.11466838919690676
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,424 INFO epoch # 102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.0690726660458105
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,449 INFO epoch # 103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.024639313854277135
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,473 INFO epoch # 104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.012665446887591055
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,500 INFO epoch # 105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.007247521083003708
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,525 INFO epoch # 106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.010718448460102082
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,551 INFO epoch # 107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.017735787254891226
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,576 INFO epoch # 108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.012649015111050436
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,600 INFO epoch # 109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.010209907784259745
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,625 INFO epoch # 110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.012778156011232307
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:40,625 INFO *** epoch 110, rolling-avg-loss (window=10)= 0.029238465168912497
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,650 INFO epoch # 111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.009994205187207885
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,675 INFO epoch # 112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.014924694624330317
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,700 INFO epoch # 113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.011750030604058078
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,724 INFO epoch # 114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.006195251177996397
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,749 INFO epoch # 115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.0143146101385355
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,774 INFO epoch # 116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.010461170086637139
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,799 INFO epoch # 117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.010560174459325417
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,824 INFO epoch # 118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.011965543284480061
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,848 INFO epoch # 119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.01302447970956564
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,873 INFO epoch # 120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.00909337340188878
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:40,873 INFO *** epoch 120, rolling-avg-loss (window=10)= 0.01122835326740252
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,898 INFO epoch # 121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.006898732616433076
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,923 INFO epoch # 122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.010753813179742012
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,947 INFO epoch # 123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.01522804650345019
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,972 INFO epoch # 124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.035049847726311
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:40,997 INFO epoch # 125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.016384 -loss = 0.08423276591513838
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,022 INFO epoch # 126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.050553651686225616
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,047 INFO epoch # 127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.023804692019309317
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,071 INFO epoch # 128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.01342046819627285
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,096 INFO epoch # 129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.009471128009525792
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,121 INFO epoch # 130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.006587702109079276
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:41,121 INFO *** epoch 130, rolling-avg-loss (window=10)= 0.025600084796148753
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,146 INFO epoch # 131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.004380087524519435
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,171 INFO epoch # 132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.003980587688939912
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,196 INFO epoch # 133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.004687188831823213
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,220 INFO epoch # 134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.005370262345033032
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,245 INFO epoch # 135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.004262796376964875
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,270 INFO epoch # 136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.00470548326349152
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,294 INFO epoch # 137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.012468972050451806
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,319 INFO epoch # 138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.028417012467980386
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,344 INFO epoch # 139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.021446977608970234
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,369 INFO epoch # 140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.014458567163507853
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:41,369 INFO *** epoch 140, rolling-avg-loss (window=10)= 0.010417793532168226
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,394 INFO epoch # 141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.014967878641826766
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,419 INFO epoch # 142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.029990076566381113
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,444 INFO epoch # 143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.0131072 -loss = 0.02139158272849662
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,468 INFO epoch # 144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.008863930297749383
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,493 INFO epoch # 145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.006694651947223715
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,518 INFO epoch # 146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.006847716156127197
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,543 INFO epoch # 147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.010243635079158203
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,569 INFO epoch # 148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.009576875965909235
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,594 INFO epoch # 149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.00772153784387878
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,619 INFO epoch # 150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.008892008748703769
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:41,619 INFO *** epoch 150, rolling-avg-loss (window=10)= 0.012518989397545477
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,644 INFO epoch # 151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.006664749920102102
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,669 INFO epoch # 152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.00571108691261283
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,693 INFO epoch # 153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.0047658676813755714
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,717 INFO epoch # 154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.01048576 -loss = 0.005384146901113647
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,742 INFO epoch # 155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.005070536769926548
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,767 INFO epoch # 156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.0031031550806281823
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,792 INFO epoch # 157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.0032702232927217015
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,817 INFO epoch # 158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.0034785717226830976
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,841 INFO epoch # 159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.002659967129251787
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,866 INFO epoch # 160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.003032378400010722
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:41,866 INFO *** epoch 160, rolling-avg-loss (window=10)= 0.004314068381042619
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,891 INFO epoch # 161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.004123666380265994
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,915 INFO epoch # 162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.005348123969244106
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,939 INFO epoch # 163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.005119960399211518
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,964 INFO epoch # 164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.0034741878409736923
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:41,989 INFO epoch # 165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.0036076229132179704
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,015 INFO epoch # 166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.00491886395694954
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,041 INFO epoch # 167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.005165639213685479
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,065 INFO epoch # 168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.004732344881631434
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,090 INFO epoch # 169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.0264061942164387
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,115 INFO epoch # 170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.00838861 -loss = 0.037963091315967694
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:42,116 INFO *** epoch 170, rolling-avg-loss (window=10)= 0.010085969508758614
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,140 INFO epoch # 171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.025542395415582828
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,164 INFO epoch # 172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.010325531089412315
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,188 INFO epoch # 173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.007719496311619878
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,214 INFO epoch # 174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0035365198838657567
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,239 INFO epoch # 175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0025385651571143953
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,264 INFO epoch # 176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0030030989819871527
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,289 INFO epoch # 177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0027645054938537734
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,313 INFO epoch # 178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0020482112487245885
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,338 INFO epoch # 179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.002339345479517111
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,363 INFO epoch # 180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0022901321245756535
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:42,363 INFO *** epoch 180, rolling-avg-loss (window=10)= 0.0062107801186253465
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,387 INFO epoch # 181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.002478668451242681
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,412 INFO epoch # 182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.002794793720490166
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,437 INFO epoch # 183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0025580333113404256
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,462 INFO epoch # 184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.002454179784815226
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,487 INFO epoch # 185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0018051786181916083
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,513 INFO epoch # 186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.002452743104991636
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,537 INFO epoch # 187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0038254865777811835
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,562 INFO epoch # 188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.007851098304880517
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,587 INFO epoch # 189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.005963093840650149
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,612 INFO epoch # 190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0057326142243774875
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:42,612 INFO *** epoch 190, rolling-avg-loss (window=10)= 0.0037915889938761075
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,637 INFO epoch # 191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.003730724026848163
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,662 INFO epoch # 192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.005268126726150513
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,687 INFO epoch # 193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.004361696920490691
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,711 INFO epoch # 194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.004721176720756505
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,736 INFO epoch # 195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.005095553378175412
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,761 INFO epoch # 196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00671089 -loss = 0.003903622226789594
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,786 INFO epoch # 197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.0038467183176960264
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,810 INFO epoch # 198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.002710150122376425
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,834 INFO epoch # 199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.002684902722415115
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,859 INFO epoch # 200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.002362930723133364
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:42,859 INFO *** epoch 200, rolling-avg-loss (window=10)= 0.0038685601884831814
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,884 INFO epoch # 201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.0036759229343650596
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,909 INFO epoch # 202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.005458838143385947
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,934 INFO epoch # 203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.004273542768454978
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,959 INFO epoch # 204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.00420465980589922
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:42,984 INFO epoch # 205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.004954267542676202
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,010 INFO epoch # 206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.004299183286327337
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,035 INFO epoch # 207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00536871 -loss = 0.0024434849636496177
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,059 INFO epoch # 208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.002214944696918662
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,084 INFO epoch # 209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.0018715161131694913
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,108 INFO epoch # 210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.002029865324064823
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:43,108 INFO *** epoch 210, rolling-avg-loss (window=10)= 0.003542622557891134
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,133 INFO epoch # 211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.002032300946302712
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,158 INFO epoch # 212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.002019793990932937
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,183 INFO epoch # 213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.004440672376326152
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,208 INFO epoch # 214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.00264748705085367
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,233 INFO epoch # 215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.0025349545159510205
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,257 INFO epoch # 216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.004389952598804874
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,281 INFO epoch # 217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.002847896416538528
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,306 INFO epoch # 218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00429497 -loss = 0.0031201674602925776
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,331 INFO epoch # 219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0021308119658247702
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,356 INFO epoch # 220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0020519681224998618
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:43,356 INFO *** epoch 220, rolling-avg-loss (window=10)= 0.0028216005444327103
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,381 INFO epoch # 221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0019189080938563814
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,406 INFO epoch # 222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0019682363291004938
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,431 INFO epoch # 223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0015186159048295978
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,456 INFO epoch # 224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0014952851043614959
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,480 INFO epoch # 225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0014482392114587128
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,505 INFO epoch # 226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0014784033194051257
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,530 INFO epoch # 227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0016830744205175766
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,555 INFO epoch # 228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.001999098965565541
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,580 INFO epoch # 229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0015467284895878818
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,605 INFO epoch # 230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0014899455443290728
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:43,605 INFO *** epoch 230, rolling-avg-loss (window=10)= 0.001654653538301188
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,630 INFO epoch # 231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0015221367606760136
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,655 INFO epoch # 232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0014329473238571414
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,680 INFO epoch # 233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.00170470265937703
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,704 INFO epoch # 234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0021841952693648636
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,728 INFO epoch # 235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.00258993926857199
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,753 INFO epoch # 236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0019005644634099944
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,779 INFO epoch # 237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.001920262201955276
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,804 INFO epoch # 238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0016884058397928518
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,829 INFO epoch # 239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.001546122191939503
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,853 INFO epoch # 240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.001783460478431412
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:43,854 INFO *** epoch 240, rolling-avg-loss (window=10)= 0.0018272736457376076
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,878 INFO epoch # 241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.0015631865493820182
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,903 INFO epoch # 242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.001951386355462351
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,927 INFO epoch # 243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00343597 -loss = 0.0028315081128052304
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,951 INFO epoch # 244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.001636099038712148
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:43,976 INFO epoch # 245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.001464535335877112
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,003 INFO epoch # 246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.0020672282154139666
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,028 INFO epoch # 247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.0013756507633453502
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,053 INFO epoch # 248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.0014572314469010702
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,078 INFO epoch # 249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.001979951778360243
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,103 INFO epoch # 250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.002130160399246961
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:44,103 INFO *** epoch 250, rolling-avg-loss (window=10)= 0.0018456937995506447
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,127 INFO epoch # 251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.0022432285460776517
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,152 INFO epoch # 252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.0015128224173427693
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,176 INFO epoch # 253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.001155330496008641
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,201 INFO epoch # 254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.002212851727381349
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,226 INFO epoch # 255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.0017756257455662956
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,251 INFO epoch # 256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.0019786909771417932
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,276 INFO epoch # 257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.0023637276275881697
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,301 INFO epoch # 258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.002623424552647131
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,326 INFO epoch # 259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.002696529486482697
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,351 INFO epoch # 260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.0015640142257325352
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:44,351 INFO *** epoch 260, rolling-avg-loss (window=10)= 0.0020126245801969033
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,375 INFO epoch # 261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.001658822235185653
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,399 INFO epoch # 262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.0015424314670131674
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,424 INFO epoch # 263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.0019119116178314601
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,449 INFO epoch # 264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00274878 -loss = 0.0016624548322787243
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,475 INFO epoch # 265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.001746253459714353
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,500 INFO epoch # 266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0014207055817158627
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,525 INFO epoch # 267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0013822794218348072
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,550 INFO epoch # 268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0009646781021729111
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,575 INFO epoch # 269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0011914113386800246
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,599 INFO epoch # 270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0012026306923611889
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:44,599 INFO *** epoch 270, rolling-avg-loss (window=10)= 0.0014683578748788152
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,624 INFO epoch # 271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0012540417324219432
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,649 INFO epoch # 272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0014106720802374185
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,674 INFO epoch # 273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0012392722976593566
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,699 INFO epoch # 274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0011193208679157708
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,724 INFO epoch # 275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0013769689431813146
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,749 INFO epoch # 276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.001300705284146326
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,774 INFO epoch # 277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0012468911208478467
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,799 INFO epoch # 278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0013939580181613564
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,823 INFO epoch # 279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00219902 -loss = 0.0012448432705631214
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,847 INFO epoch # 280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0014514355288286294
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:44,847 INFO *** epoch 280, rolling-avg-loss (window=10)= 0.0013038109143963083
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,872 INFO epoch # 281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0009555367137571531
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,897 INFO epoch # 282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0009794571345472442
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,922 INFO epoch # 283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0012605986674316227
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,947 INFO epoch # 284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0009043430510376181
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,972 INFO epoch # 285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0012061336510149495
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:44,997 INFO epoch # 286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0008987772004080138
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,022 INFO epoch # 287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0012813330935646913
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,047 INFO epoch # 288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.001124151486770383
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,071 INFO epoch # 289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0010105572882041867
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,096 INFO epoch # 290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0013316805025429597
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:45,096 INFO *** epoch 290, rolling-avg-loss (window=10)= 0.0010952568789278825
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,121 INFO epoch # 291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0012026220901004438
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,146 INFO epoch # 292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0011575616696583373
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,170 INFO epoch # 293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0009633622839049037
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,195 INFO epoch # 294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.000984361703740433
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,219 INFO epoch # 295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0009219080284570477
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,244 INFO epoch # 296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.0012240456924441138
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,268 INFO epoch # 297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00175922 -loss = 0.0011060422868467868
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,292 INFO epoch # 298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.00097112704367776
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,317 INFO epoch # 299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0008776875320888524
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,342 INFO epoch # 300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0008689918006504221
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:45,342 INFO *** epoch 300, rolling-avg-loss (window=10)= 0.00102777101315691
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,367 INFO epoch # 301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0008489810088316776
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,391 INFO epoch # 302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0008255297451147011
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,416 INFO epoch # 303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0007827942583909525
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,441 INFO epoch # 304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0008404656521244241
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,465 INFO epoch # 305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0011570538751714463
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,490 INFO epoch # 306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.000779674540639722
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,515 INFO epoch # 307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0008364803531938897
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,540 INFO epoch # 308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0007968792434050036
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,565 INFO epoch # 309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0007950101995707622
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,591 INFO epoch # 310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0010028941465342152
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:45,591 INFO *** epoch 310, rolling-avg-loss (window=10)= 0.0008665763022976794
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,616 INFO epoch # 311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0010206164459564855
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,640 INFO epoch # 312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0008992932978019651
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,665 INFO epoch # 313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0008998658607846925
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,690 INFO epoch # 314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0008898183736684067
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,714 INFO epoch # 315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0008703253265204174
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,738 INFO epoch # 316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.0011390799955864038
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,763 INFO epoch # 317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00140737 -loss = 0.0010495209185007427
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,788 INFO epoch # 318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0008002787878337715
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,814 INFO epoch # 319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0008656587368542594
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,839 INFO epoch # 320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.000717467421366434
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:45,839 INFO *** epoch 320, rolling-avg-loss (window=10)= 0.0009151925164873579
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,863 INFO epoch # 321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0007883806563248591
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,888 INFO epoch # 322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0007748171504187797
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,913 INFO epoch # 323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0006915246984655303
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,937 INFO epoch # 324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0008266496788045125
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,961 INFO epoch # 325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0007518031493028892
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:45,986 INFO epoch # 326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0007795505159135375
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,013 INFO epoch # 327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0007662947480899415
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,038 INFO epoch # 328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0008595153489815337
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,064 INFO epoch # 329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0007731389262646969
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,089 INFO epoch # 330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0008978495856613985
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:46,089 INFO *** epoch 330, rolling-avg-loss (window=10)= 0.000790952445822768
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,114 INFO epoch # 331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0009758967532044543
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,139 INFO epoch # 332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0008885037120697753
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,163 INFO epoch # 333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.0007181498579614397
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,187 INFO epoch # 334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.0011259 -loss = 0.0010309738996771298
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,212 INFO epoch # 335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0008563731705570327
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,237 INFO epoch # 336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007079982032467212
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,262 INFO epoch # 337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007270594243891538
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,287 INFO epoch # 338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0006773637524539871
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,312 INFO epoch # 339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007474109621918095
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,338 INFO epoch # 340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0008639569153144423
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:46,338 INFO *** epoch 340, rolling-avg-loss (window=10)= 0.0008193686651065946
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,363 INFO epoch # 341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0008043825767734753
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,388 INFO epoch # 342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007127518724051438
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,412 INFO epoch # 343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0006838508357759565
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,438 INFO epoch # 344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007952558201005949
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,463 INFO epoch # 345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0008938100870831737
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,488 INFO epoch # 346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007417094527876803
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,513 INFO epoch # 347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0006641504321513431
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,538 INFO epoch # 348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007817949218276356
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,563 INFO epoch # 349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007535699430653559
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,588 INFO epoch # 350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0008114453538187913
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:46,588 INFO *** epoch 350, rolling-avg-loss (window=10)= 0.000764272129578915
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,613 INFO epoch # 351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007314072888610618
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,639 INFO epoch # 352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0006648173663831715
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,664 INFO epoch # 353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.000699011654692835
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,689 INFO epoch # 354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007438653639318156
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,714 INFO epoch # 355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0008492538622314376
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,739 INFO epoch # 356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007348975998216442
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,764 INFO epoch # 357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0006561055147488202
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,788 INFO epoch # 358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.000735789306262242
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,812 INFO epoch # 359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007738190021232835
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,837 INFO epoch # 360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007814752345439047
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:46,838 INFO *** epoch 360, rolling-avg-loss (window=10)= 0.0007370442193600215
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,862 INFO epoch # 361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0008601996523793787
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,887 INFO epoch # 362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0008045513931262706
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,912 INFO epoch # 363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007636319970645543
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,937 INFO epoch # 364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007696826127357781
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,962 INFO epoch # 365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007453817583154887
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:46,986 INFO epoch # 366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007229544663070036
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,012 INFO epoch # 367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0006705214568812932
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,036 INFO epoch # 368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0006460828024760953
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,061 INFO epoch # 369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007367445405439607
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,086 INFO epoch # 370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007776648205305848
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:47,086 INFO *** epoch 370, rolling-avg-loss (window=10)= 0.0007497415500360408
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,111 INFO epoch # 371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0009342278586700559
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,135 INFO epoch # 372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0009365435673056969
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,160 INFO epoch # 373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0008558704575989395
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,185 INFO epoch # 374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0008321247737122965
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,209 INFO epoch # 375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007289247886676873
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,234 INFO epoch # 376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0006054685010375189
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,259 INFO epoch # 377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007315310391797019
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,284 INFO epoch # 378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007371903180943003
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,309 INFO epoch # 379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007616498599028481
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,335 INFO epoch # 380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.000687938482068213
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:47,335 INFO *** epoch 380, rolling-avg-loss (window=10)= 0.0007811469646237258
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,360 INFO epoch # 381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007144789693744055
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,384 INFO epoch # 382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0008582124869073076
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,409 INFO epoch # 383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007293817869919751
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,433 INFO epoch # 384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0006708687138078468
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,457 INFO epoch # 385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007122463613216366
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,482 INFO epoch # 386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0007601981674919703
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,508 INFO epoch # 387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.00090072 -loss = 0.0006617243177190955
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,533 INFO epoch # 388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0006878636303424303
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,558 INFO epoch # 389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0006450289311552686
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,583 INFO epoch # 390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0006089697404864377
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:47,583 INFO *** epoch 390, rolling-avg-loss (window=10)= 0.0007048973105598375
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,608 INFO epoch # 391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0005555276264203712
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,633 INFO epoch # 392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0006506012561398425
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,657 INFO epoch # 393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0006009013483500374
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,682 INFO epoch # 394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0005585864672736664
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,707 INFO epoch # 395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0005795508847638432
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,733 INFO epoch # 396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0006363250298558602
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,757 INFO epoch # 397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0006256841834069096
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,782 INFO epoch # 398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0006671807886700012
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,807 INFO epoch # 399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0006905251556807863
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,832 INFO epoch # 400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0006290424789767712
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:47,832 INFO *** epoch 400, rolling-avg-loss (window=10)= 0.0006193925219538089
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,857 INFO epoch # 401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0006444776804918157
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,881 INFO epoch # 402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00072058 -loss = 0.0007557144272141159
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,906 INFO epoch # 403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0006467292808728026
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,931 INFO epoch # 404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0006343133423277842
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,956 INFO epoch # 405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0005533267781304727
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:47,981 INFO epoch # 406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0005989006163352835
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,007 INFO epoch # 407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0007408322300761939
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,032 INFO epoch # 408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0006370604034080836
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,057 INFO epoch # 409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0006034689398282873
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,081 INFO epoch # 410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0005996852734824643
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:48,082 INFO *** epoch 410, rolling-avg-loss (window=10)= 0.0006414508972167304
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,106 INFO epoch # 411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0005619915588925194
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,132 INFO epoch # 412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0006109247790716056
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,157 INFO epoch # 413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0007688607405205922
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,182 INFO epoch # 414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0006050961129533659
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,207 INFO epoch # 415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0006039559342233198
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,232 INFO epoch # 416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00057646 -loss = 0.0005594543630390295
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,257 INFO epoch # 417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0004933616691102673
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,281 INFO epoch # 418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.000502298044323522
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,305 INFO epoch # 419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005172387676014166
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,330 INFO epoch # 420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0004948603280354291
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:48,331 INFO *** epoch 420, rolling-avg-loss (window=10)= 0.0005718042297771067
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,356 INFO epoch # 421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0004934666025552101
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,381 INFO epoch # 422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005286183274750199
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,405 INFO epoch # 423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0004908596394151184
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,430 INFO epoch # 424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005282253517569708
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,455 INFO epoch # 425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0006181619637313165
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,479 INFO epoch # 426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005400015648254859
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,506 INFO epoch # 427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005274863776451509
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,531 INFO epoch # 428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0006189716863445937
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,556 INFO epoch # 429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005873218473945079
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,581 INFO epoch # 430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005178943721278171
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:48,581 INFO *** epoch 430, rolling-avg-loss (window=10)= 0.0005451007733271191
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,606 INFO epoch # 431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005006500465762136
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,631 INFO epoch # 432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0004892563376675493
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,655 INFO epoch # 433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0004993333585194445
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,680 INFO epoch # 434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.000549902132895243
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,704 INFO epoch # 435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.000482600655024206
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,729 INFO epoch # 436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0004987938317104376
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,753 INFO epoch # 437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005212484929610842
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,778 INFO epoch # 438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005215820290946535
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,803 INFO epoch # 439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.00046642943094151894
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,828 INFO epoch # 440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.000533251023651766
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:48,828 INFO *** epoch 440, rolling-avg-loss (window=10)= 0.0005063047339042116
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,853 INFO epoch # 441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005557065222611916
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,878 INFO epoch # 442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005178651593658807
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,902 INFO epoch # 443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.000530436119879596
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,927 INFO epoch # 444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0005500277321386551
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,951 INFO epoch # 445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0004868905667016017
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:48,976 INFO epoch # 446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.000543607905274257
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,002 INFO epoch # 447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0004981385327742569
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,027 INFO epoch # 448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0004950494297580527
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,052 INFO epoch # 449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.000491881650772744
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,077 INFO epoch # 450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00046117 -loss = 0.0005455216925059046
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:49,077 INFO *** epoch 450, rolling-avg-loss (window=10)= 0.0005215125311432141
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,101 INFO epoch # 451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.000477680452916372
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,126 INFO epoch # 452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.0004892006607925785
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,150 INFO epoch # 453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.0004328619110830394
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,175 INFO epoch # 454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.000433863972180656
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,200 INFO epoch # 455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.0004892353147235034
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,225 INFO epoch # 456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.0005267279671638139
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,249 INFO epoch # 457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.00048827652561677883
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,274 INFO epoch # 458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.0004665216285502538
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,299 INFO epoch # 459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.0004783214198791289
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,323 INFO epoch # 460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.00048696302983444186
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:49,323 INFO *** epoch 460, rolling-avg-loss (window=10)= 0.00047696528827405674
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,348 INFO epoch # 461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.00047098962407160017
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,372 INFO epoch # 462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.0004363874081588749
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,396 INFO epoch # 463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.0004606713470171339
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,422 INFO epoch # 464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00036893 -loss = 0.00044231332181620276
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,447 INFO epoch # 465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.0004567563912132755
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,472 INFO epoch # 466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.00042561349192900315
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,498 INFO epoch # 467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.00042443531274329873
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,523 INFO epoch # 468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.0004367078933033294
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,548 INFO epoch # 469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.00042622214116688284
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,572 INFO epoch # 470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.00044302291644271463
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:49,572 INFO *** epoch 470, rolling-avg-loss (window=10)= 0.00044231198478623165
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,597 INFO epoch # 471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.0004525690934055352
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,622 INFO epoch # 472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.0004238975770671719
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,647 INFO epoch # 473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.0004428263649710321
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,672 INFO epoch # 474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.0004495107531381239
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,697 INFO epoch # 475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.0004322672142214807
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,722 INFO epoch # 476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.0004642365928573002
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,747 INFO epoch # 477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.0005064786112468157
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,771 INFO epoch # 478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.00043309243129832403
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,796 INFO epoch # 479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.00042938482531878564
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,821 INFO epoch # 480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.0004368541616713628
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:49,821 INFO *** epoch 480, rolling-avg-loss (window=10)= 0.0004471117625195932
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,846 INFO epoch # 481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.00047099662645320805
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,871 INFO epoch # 482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.00048112524215996797
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,895 INFO epoch # 483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00029515 -loss = 0.00042780651004120174
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,920 INFO epoch # 484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.0004108309392384919
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,945 INFO epoch # 485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.000425836443069524
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,969 INFO epoch # 486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.0004558336472005716
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:49,993 INFO epoch # 487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.00040841996004538876
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,019 INFO epoch # 488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.00041064101387746634
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,044 INFO epoch # 489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.0004102323769724795
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,069 INFO epoch # 490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.00042215466715528495
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:50,069 INFO *** epoch 490, rolling-avg-loss (window=10)= 0.00043238774262135846
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,094 INFO epoch # 491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.0004358281788881868
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,119 INFO epoch # 492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.0004396100669899689
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,143 INFO epoch # 493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.00042679287726059555
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,168 INFO epoch # 494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.00040705263298670096
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,192 INFO epoch # 495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.000409552690689452
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,216 INFO epoch # 496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.0004011178168834054
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,241 INFO epoch # 497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.00042648635447091823
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,266 INFO epoch # 498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.00044971618460424776
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,290 INFO epoch # 499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.00044486430207533496
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,315 INFO epoch # 500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.00042686366442857047
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:50,315 INFO *** epoch 500, rolling-avg-loss (window=10)= 0.0004267884769277381
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,340 INFO epoch # 501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.0004192833763746811
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,365 INFO epoch # 502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.00040911823328185296
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,390 INFO epoch # 503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.0004307329891681937
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,414 INFO epoch # 504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.0004349985920790849
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,439 INFO epoch # 505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.0004083936002903751
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,463 INFO epoch # 506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.00042168304290888565
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,488 INFO epoch # 507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00023612 -loss = 0.0004207365235613127
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,515 INFO epoch # 508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.0004452691716973537
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,540 INFO epoch # 509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.0004061558225657791
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,564 INFO epoch # 510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.00038836334674020433
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:50,565 INFO *** epoch 510, rolling-avg-loss (window=10)= 0.0004184734698667723
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,589 INFO epoch # 511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.0003850667808105105
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,614 INFO epoch # 512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.0003950783529684746
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,639 INFO epoch # 513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.0004009820267258744
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,663 INFO epoch # 514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.0004002521458979962
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,687 INFO epoch # 515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.00039669065528349684
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,712 INFO epoch # 516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.0003934819974736976
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,737 INFO epoch # 517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.0003994086599310062
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,762 INFO epoch # 518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.00040848118036852354
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,788 INFO epoch # 519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.0004123350834041568
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,812 INFO epoch # 520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.0003922680825261133
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:50,813 INFO *** epoch 520, rolling-avg-loss (window=10)= 0.00039840449653898494
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,837 INFO epoch # 521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.00040166976936494134
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,862 INFO epoch # 522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00018889 -loss = 0.00040768656139594637
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,886 INFO epoch # 523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.000396950076018194
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,911 INFO epoch # 524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.00039387663252585166
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,936 INFO epoch # 525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.0003732865792699158
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,960 INFO epoch # 526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.0003815033148774611
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:50,985 INFO epoch # 527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.00039251255262310485
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,011 INFO epoch # 528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.00038472173244891953
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,035 INFO epoch # 529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.00036987404247546305
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,060 INFO epoch # 530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.0003929419526165085
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:51,060 INFO *** epoch 530, rolling-avg-loss (window=10)= 0.0003895023213616306
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,084 INFO epoch # 531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.00037514672897356963
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,109 INFO epoch # 532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.0003867147523643715
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,133 INFO epoch # 533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.00039147419447544963
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,158 INFO epoch # 534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.0003908943966962397
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,183 INFO epoch # 535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.0003898365531183247
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,208 INFO epoch # 536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.000381149972755728
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,233 INFO epoch # 537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.0003729287680471316
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,258 INFO epoch # 538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.0003815070106481601
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,282 INFO epoch # 539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.0003854787293156343
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,307 INFO epoch # 540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00015112 -loss = 0.0003750511145751391
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:51,307 INFO *** epoch 540, rolling-avg-loss (window=10)= 0.0003830182220969748
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,331 INFO epoch # 541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.00038543160688797275
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,356 INFO epoch # 542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.0003925259270805067
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,381 INFO epoch # 543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.00037843680183868855
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,406 INFO epoch # 544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.000377345725012544
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,431 INFO epoch # 545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.0003753130494650187
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,455 INFO epoch # 546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.0003858537275976102
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,480 INFO epoch # 547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.0003732071116766227
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,507 INFO epoch # 548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.000399404505567093
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,532 INFO epoch # 549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.00040399278846702403
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,557 INFO epoch # 550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.00036404460967917525
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:51,557 INFO *** epoch 550, rolling-avg-loss (window=10)= 0.0003835555853272256
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,582 INFO epoch # 551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.0003691182051885075
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,607 INFO epoch # 552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.00037661487835326365
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,631 INFO epoch # 553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.00036511165422520466
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,656 INFO epoch # 554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.00038197008003148115
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,681 INFO epoch # 555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.00037188635656743176
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,706 INFO epoch # 556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.0003746605058300442
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,730 INFO epoch # 557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.000365274225727522
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,754 INFO epoch # 558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.00036906956374878065
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,779 INFO epoch # 559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.00036555375513021965
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,804 INFO epoch # 560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.0003756456447133262
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:51,805 INFO *** epoch 560, rolling-avg-loss (window=10)= 0.0003714904869515781
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,829 INFO epoch # 561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00012089 -loss = 0.00038555057918918985
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,854 INFO epoch # 562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.00036224486580717244
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,879 INFO epoch # 563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.000357329285387615
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,904 INFO epoch # 564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.00035345795580984224
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,929 INFO epoch # 565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.0003516180332683559
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,953 INFO epoch # 566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.0003583854970721794
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:51,978 INFO epoch # 567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.00036197597884373475
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,003 INFO epoch # 568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.00035812093618525456
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,028 INFO epoch # 569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.0003588892315747216
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,053 INFO epoch # 570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.00036967734215847614
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:52,053 INFO *** epoch 570, rolling-avg-loss (window=10)= 0.00036172497052965416
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,078 INFO epoch # 571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.0003566204133676365
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,103 INFO epoch # 572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.0003534470984180059
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,127 INFO epoch # 573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.0003528506532477747
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,152 INFO epoch # 574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.000355833865303014
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,176 INFO epoch # 575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.00036119056998619014
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,201 INFO epoch # 576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.0001 -loss = 0.00035866999740911915
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,225 INFO epoch # 577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003592330274321804
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,250 INFO epoch # 578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003514952421288139
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,276 INFO epoch # 579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035025386480681066
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,303 INFO epoch # 580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035144450126348863
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:52,303 INFO *** epoch 580, rolling-avg-loss (window=10)= 0.0003551039233363034
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,328 INFO epoch # 581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003648279280501551
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,353 INFO epoch # 582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003679755509697965
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,377 INFO epoch # 583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003561487458812605
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,402 INFO epoch # 584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003560381712824372
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,427 INFO epoch # 585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003465749635194827
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,451 INFO epoch # 586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003437756444327533
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,477 INFO epoch # 587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034862559322001676
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,502 INFO epoch # 588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003477734858669075
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,527 INFO epoch # 589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003617431893612125
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,552 INFO epoch # 590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003526615128586335
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:52,552 INFO *** epoch 590, rolling-avg-loss (window=10)= 0.0003546144785442656
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,578 INFO epoch # 591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003533296224694433
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,602 INFO epoch # 592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00036466063091730966
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,627 INFO epoch # 593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003555390701097037
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,652 INFO epoch # 594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003608401270217395
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,676 INFO epoch # 595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035097507851397885
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,701 INFO epoch # 596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003571135802693399
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,726 INFO epoch # 597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003582759377812701
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,751 INFO epoch # 598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000360703155664461
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,776 INFO epoch # 599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003470255610799151
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,801 INFO epoch # 600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003466186864118624
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:52,801 INFO *** epoch 600, rolling-avg-loss (window=10)= 0.0003555081450239023
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,825 INFO epoch # 601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003493339297295149
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,850 INFO epoch # 602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034628126886673274
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,875 INFO epoch # 603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035668230141579574
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,900 INFO epoch # 604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003707853449408763
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,924 INFO epoch # 605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034830882463471167
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,949 INFO epoch # 606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003522461774991825
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,974 INFO epoch # 607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034793622700297937
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:52,999 INFO epoch # 608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035214471198352317
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,024 INFO epoch # 609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034510331045437074
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,048 INFO epoch # 610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003526698157656938
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:53,048 INFO *** epoch 610, rolling-avg-loss (window=10)= 0.0003521491912293381
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,073 INFO epoch # 611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035974855002548014
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,097 INFO epoch # 612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00036128603803393033
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,122 INFO epoch # 613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003612868404681129
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,147 INFO epoch # 614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003544985965293433
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,171 INFO epoch # 615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035140624552566026
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,196 INFO epoch # 616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035034798243681765
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,221 INFO epoch # 617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034744771505107306
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,246 INFO epoch # 618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035171757439716854
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,270 INFO epoch # 619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000358868483453989
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,295 INFO epoch # 620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034431101417534853
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:53,295 INFO *** epoch 620, rolling-avg-loss (window=10)= 0.0003540919040096924
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,320 INFO epoch # 621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003490326992635216
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,344 INFO epoch # 622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003792658830726785
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,369 INFO epoch # 623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00036100382068460544
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,394 INFO epoch # 624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035714770076862936
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,419 INFO epoch # 625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034721339158880123
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,443 INFO epoch # 626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034938880417030306
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,468 INFO epoch # 627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003532235597958788
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,492 INFO epoch # 628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003442384647704395
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,518 INFO epoch # 629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003476227960033741
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,543 INFO epoch # 630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035935865598730744
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:53,543 INFO *** epoch 630, rolling-avg-loss (window=10)= 0.0003547495776105539
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,568 INFO epoch # 631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003513015578002004
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,593 INFO epoch # 632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003523414123005101
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,618 INFO epoch # 633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003516791313554027
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,643 INFO epoch # 634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003413084595065032
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,668 INFO epoch # 635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034446598895426306
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,692 INFO epoch # 636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003457260604981067
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,716 INFO epoch # 637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003540459862311504
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,741 INFO epoch # 638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00036651751392387916
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,766 INFO epoch # 639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003527535379232307
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,791 INFO epoch # 640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000339968702090638
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:53,791 INFO *** epoch 640, rolling-avg-loss (window=10)= 0.0003500108350583885
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,816 INFO epoch # 641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035585678914295773
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,842 INFO epoch # 642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00036081904878041576
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,866 INFO epoch # 643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000357290670009596
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,891 INFO epoch # 644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034879066765175335
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,915 INFO epoch # 645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034892193341095533
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,940 INFO epoch # 646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034896265424322337
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,965 INFO epoch # 647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003518737162396844
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:53,989 INFO epoch # 648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035720504279847123
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,016 INFO epoch # 649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034583822582914895
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,041 INFO epoch # 650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035721616877708583
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:54,041 INFO *** epoch 650, rolling-avg-loss (window=10)= 0.0003532774916883292
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,066 INFO epoch # 651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035691014574175433
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,091 INFO epoch # 652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034268253844597246
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,115 INFO epoch # 653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003527886566839048
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,140 INFO epoch # 654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00036264699509566917
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,164 INFO epoch # 655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000349806114016766
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,189 INFO epoch # 656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003442867031221145
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,214 INFO epoch # 657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034770068720847903
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,239 INFO epoch # 658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003383260053981628
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,263 INFO epoch # 659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003464646017943908
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,288 INFO epoch # 660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034003782368797277
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:54,288 INFO *** epoch 660, rolling-avg-loss (window=10)= 0.00034816502711951865
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,313 INFO epoch # 661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003554918387505625
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,337 INFO epoch # 662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034383624442853035
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,362 INFO epoch # 663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003429306912169393
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,386 INFO epoch # 664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035338025897674796
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,411 INFO epoch # 665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003408339082462979
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,436 INFO epoch # 666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003390716351402391
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,460 INFO epoch # 667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00037441445497928986
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,486 INFO epoch # 668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003915053272586582
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,512 INFO epoch # 669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035107577277813105
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,538 INFO epoch # 670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034781435097102075
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:54,538 INFO *** epoch 670, rolling-avg-loss (window=10)= 0.0003540354482746417
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,563 INFO epoch # 671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034301807962557567
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,588 INFO epoch # 672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003489070280920714
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,613 INFO epoch # 673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003438367645555575
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,638 INFO epoch # 674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003533105839908655
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,663 INFO epoch # 675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034673181029834917
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,688 INFO epoch # 676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034528930305636357
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,713 INFO epoch # 677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000353763104898722
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,737 INFO epoch # 678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034040729515254496
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,762 INFO epoch # 679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034294936756071233
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,787 INFO epoch # 680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034376509387844376
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:54,787 INFO *** epoch 680, rolling-avg-loss (window=10)= 0.0003461978431109206
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,811 INFO epoch # 681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034516210102343136
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,837 INFO epoch # 682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033745894865465484
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,861 INFO epoch # 683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003422689145996368
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,886 INFO epoch # 684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003393895383591631
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,911 INFO epoch # 685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003409071147741218
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,936 INFO epoch # 686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003428334106242151
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,960 INFO epoch # 687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003429081436479464
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:54,985 INFO epoch # 688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033993062222309944
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,011 INFO epoch # 689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033661596244201065
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,036 INFO epoch # 690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035066068396970095
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:55,036 INFO *** epoch 690, rolling-avg-loss (window=10)= 0.0003418135440317981
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,061 INFO epoch # 691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034849662998957294
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,086 INFO epoch # 692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003442436356895736
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,111 INFO epoch # 693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003505998436594382
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,135 INFO epoch # 694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003424750561992239
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,160 INFO epoch # 695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003419163988188042
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,184 INFO epoch # 696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003413942388890843
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,209 INFO epoch # 697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003435283928411081
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,233 INFO epoch # 698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034983266211513963
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,258 INFO epoch # 699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000351065891611922
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,283 INFO epoch # 700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003303199727919751
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:55,283 INFO *** epoch 700, rolling-avg-loss (window=10)= 0.00034438727226058423
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,308 INFO epoch # 701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000339822458668745
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,333 INFO epoch # 702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034706719577245944
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,358 INFO epoch # 703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003371076438010537
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,383 INFO epoch # 704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003442762407108343
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,408 INFO epoch # 705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034731168208444225
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,432 INFO epoch # 706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034762230768267595
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,457 INFO epoch # 707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003285214105354888
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,482 INFO epoch # 708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003508863026841677
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,507 INFO epoch # 709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003403284923738933
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,533 INFO epoch # 710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033850654435809704
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:55,533 INFO *** epoch 710, rolling-avg-loss (window=10)= 0.00034214502786718574
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,559 INFO epoch # 711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035104319706858535
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,583 INFO epoch # 712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034150769123308624
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,608 INFO epoch # 713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034093580990364507
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,633 INFO epoch # 714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034433798698176233
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,658 INFO epoch # 715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034508564858697354
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,683 INFO epoch # 716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003472103306973752
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,708 INFO epoch # 717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033231524742274946
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,733 INFO epoch # 718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033925087357472095
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,757 INFO epoch # 719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003423481630826635
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,782 INFO epoch # 720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003424411801721102
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:55,782 INFO *** epoch 720, rolling-avg-loss (window=10)= 0.0003426476128723672
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,807 INFO epoch # 721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003416359586740977
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,832 INFO epoch # 722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003502957951111187
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,856 INFO epoch # 723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003406407935212233
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,880 INFO epoch # 724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003321145777590573
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,906 INFO epoch # 725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003303158321484391
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,930 INFO epoch # 726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003366825283072623
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,955 INFO epoch # 727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003407666472152674
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:55,980 INFO epoch # 728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003452810101277594
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,005 INFO epoch # 729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034279217112011143
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,030 INFO epoch # 730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003507713957722964
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:56,030 INFO *** epoch 730, rolling-avg-loss (window=10)= 0.00034112967097566323
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,055 INFO epoch # 731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034021458871263477
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,079 INFO epoch # 732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033309726589193035
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,104 INFO epoch # 733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033569927556007835
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,129 INFO epoch # 734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033575099826391256
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,154 INFO epoch # 735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003440441285160237
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,179 INFO epoch # 736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034981289604080044
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,204 INFO epoch # 737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003529162025578054
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,229 INFO epoch # 738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003431915904262236
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,253 INFO epoch # 739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034261596946245324
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,278 INFO epoch # 740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032762270794981825
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:56,278 INFO *** epoch 740, rolling-avg-loss (window=10)= 0.0003404965623381681
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,303 INFO epoch # 741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034457060392014684
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,328 INFO epoch # 742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003376841387112758
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,353 INFO epoch # 743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003405787254450843
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,377 INFO epoch # 744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032571914572534816
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,402 INFO epoch # 745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003275362026345517
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,427 INFO epoch # 746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033599811764101363
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,452 INFO epoch # 747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033615892420389823
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,476 INFO epoch # 748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003347389894770458
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,501 INFO epoch # 749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033852221011849385
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,526 INFO epoch # 750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033498906580332134
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:56,526 INFO *** epoch 750, rolling-avg-loss (window=10)= 0.000335649612368018
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,550 INFO epoch # 751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003286173059937677
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,575 INFO epoch # 752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003347399108211643
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,600 INFO epoch # 753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00036390547091806575
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,625 INFO epoch # 754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003487479685905523
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,649 INFO epoch # 755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003291544067906216
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,674 INFO epoch # 756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033738270867615937
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,698 INFO epoch # 757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003623610965275605
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,723 INFO epoch # 758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035465139247077917
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,748 INFO epoch # 759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003464561874612368
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,773 INFO epoch # 760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035279232584538734
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:56,773 INFO *** epoch 760, rolling-avg-loss (window=10)= 0.0003458808774095295
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,798 INFO epoch # 761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003389676841574588
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,823 INFO epoch # 762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033551556365897083
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,848 INFO epoch # 763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003583632151795817
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,873 INFO epoch # 764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003360252744252128
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,898 INFO epoch # 765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034009147535211275
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,923 INFO epoch # 766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003292458348109254
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,947 INFO epoch # 767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000341259193373844
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,972 INFO epoch # 768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032685264512630446
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:56,997 INFO epoch # 769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003400874465504395
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,023 INFO epoch # 770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003346218473909955
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:57,023 INFO *** epoch 770, rolling-avg-loss (window=10)= 0.0003381030180025846
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,048 INFO epoch # 771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00036621911027136125
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,073 INFO epoch # 772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003499355470661872
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,098 INFO epoch # 773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034818083222489806
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,123 INFO epoch # 774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003357560228323564
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,148 INFO epoch # 775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034028766884668067
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,173 INFO epoch # 776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032900363897039956
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,197 INFO epoch # 777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003257016825955361
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,222 INFO epoch # 778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032681985467206685
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,247 INFO epoch # 779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003373474780736225
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,272 INFO epoch # 780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033398583348441335
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:57,272 INFO *** epoch 780, rolling-avg-loss (window=10)= 0.00033932376690375214
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,297 INFO epoch # 781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003523916151607409
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,322 INFO epoch # 782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003434396544305075
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,347 INFO epoch # 783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033542335815062483
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,372 INFO epoch # 784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003313584619068674
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,397 INFO epoch # 785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032888123033834356
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,421 INFO epoch # 786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000332435618994558
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,446 INFO epoch # 787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003291514120064676
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,471 INFO epoch # 788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003347040045939918
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,496 INFO epoch # 789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003530199227887871
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,522 INFO epoch # 790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003403963957680389
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:57,523 INFO *** epoch 790, rolling-avg-loss (window=10)= 0.0003381201674138928
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,547 INFO epoch # 791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003374951463359009
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,572 INFO epoch # 792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003429933650685208
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,597 INFO epoch # 793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003396621354373305
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,621 INFO epoch # 794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003279867843957618
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,646 INFO epoch # 795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033012721313363205
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,671 INFO epoch # 796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003214231588312292
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,696 INFO epoch # 797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003243595116405881
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,721 INFO epoch # 798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000338398471441386
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,746 INFO epoch # 799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033388557993540804
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,771 INFO epoch # 800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003426310032539602
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:57,771 INFO *** epoch 800, rolling-avg-loss (window=10)= 0.0003338962369473718
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,796 INFO epoch # 801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032799552427604796
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,820 INFO epoch # 802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000329457048792392
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,845 INFO epoch # 803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000325399126242181
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,869 INFO epoch # 804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032234525507582085
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,894 INFO epoch # 805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032625740173638666
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,919 INFO epoch # 806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003261342370283923
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,944 INFO epoch # 807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003280025927649279
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,968 INFO epoch # 808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003315593265662236
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:57,993 INFO epoch # 809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003229898660460354
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,018 INFO epoch # 810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003230130595121799
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:58,019 INFO *** epoch 810, rolling-avg-loss (window=10)= 0.0003263153438040587
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,043 INFO epoch # 811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032499957222691074
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,067 INFO epoch # 812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032592474225176765
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,092 INFO epoch # 813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033721762842365676
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,117 INFO epoch # 814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033578493748791515
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,142 INFO epoch # 815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003346152258537976
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,166 INFO epoch # 816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032583132643984364
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,191 INFO epoch # 817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003245486715708726
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,215 INFO epoch # 818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032785016353175576
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,240 INFO epoch # 819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033115725798000184
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,265 INFO epoch # 820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00035552551895047405
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:58,265 INFO *** epoch 820, rolling-avg-loss (window=10)= 0.00033234550447169955
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,289 INFO epoch # 821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000349696069107657
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,314 INFO epoch # 822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003309189038450963
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,339 INFO epoch # 823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033610997509510656
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,364 INFO epoch # 824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003314404313901572
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,389 INFO epoch # 825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003272743400884792
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,415 INFO epoch # 826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003315827189778377
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,439 INFO epoch # 827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003329492319608107
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,464 INFO epoch # 828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000325364235115038
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,489 INFO epoch # 829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003268185200535559
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,514 INFO epoch # 830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033499880810268224
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:58,514 INFO *** epoch 830, rolling-avg-loss (window=10)= 0.0003327153233736421
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,538 INFO epoch # 831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003368617738098172
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,564 INFO epoch # 832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033295159186569177
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,589 INFO epoch # 833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032641205346278313
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,614 INFO epoch # 834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003362317706757624
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,639 INFO epoch # 835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003263477036463363
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,664 INFO epoch # 836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003352023773394259
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,689 INFO epoch # 837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033422763247342247
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,713 INFO epoch # 838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033351346293264737
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,738 INFO epoch # 839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032612540900507674
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,763 INFO epoch # 840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003214656051048743
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:58,763 INFO *** epoch 840, rolling-avg-loss (window=10)= 0.0003309339380315838
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,788 INFO epoch # 841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031699506944278256
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,813 INFO epoch # 842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032510907996246327
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,838 INFO epoch # 843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003290029105431001
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,864 INFO epoch # 844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034799285141551604
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,889 INFO epoch # 845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003339493539117809
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,914 INFO epoch # 846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003506247710902244
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,939 INFO epoch # 847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003375917793683974
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,964 INFO epoch # 848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003233736173050212
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:58,988 INFO epoch # 849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032051155659636215
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,014 INFO epoch # 850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003493759635603055
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:59,014 INFO *** epoch 850, rolling-avg-loss (window=10)= 0.00033345269531959534
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,039 INFO epoch # 851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003359434594001089
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,064 INFO epoch # 852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033822947921830095
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,089 INFO epoch # 853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003513921156159735
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,114 INFO epoch # 854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033902102108446084
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,138 INFO epoch # 855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032588900607411885
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,162 INFO epoch # 856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033327103446936237
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,187 INFO epoch # 857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032177800312638283
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,212 INFO epoch # 858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032665084581822156
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,237 INFO epoch # 859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000323779927774532
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,262 INFO epoch # 860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003247663881796013
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:59,262 INFO *** epoch 860, rolling-avg-loss (window=10)= 0.00033207212807610635
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,287 INFO epoch # 861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003208678589934217
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,312 INFO epoch # 862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033652672219821917
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,337 INFO epoch # 863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003230755071854219
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,361 INFO epoch # 864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003252402338798025
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,385 INFO epoch # 865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032840067890772064
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,411 INFO epoch # 866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003215845817716659
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,436 INFO epoch # 867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003215646990741204
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,461 INFO epoch # 868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032576178013446873
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,486 INFO epoch # 869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003194565579178743
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,511 INFO epoch # 870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032222658046521246
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:59,511 INFO *** epoch 870, rolling-avg-loss (window=10)= 0.00032447052005279276
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,536 INFO epoch # 871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003194782134544637
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,561 INFO epoch # 872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003234849614922756
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,586 INFO epoch # 873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033578503935132175
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,610 INFO epoch # 874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034749957308771886
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,635 INFO epoch # 875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000333078859590127
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,660 INFO epoch # 876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032047120330389587
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,685 INFO epoch # 877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032368640947554793
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,710 INFO epoch # 878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032300111197400836
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,735 INFO epoch # 879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031970706352564906
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,759 INFO epoch # 880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032121124095283447
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:07:59,760 INFO *** epoch 880, rolling-avg-loss (window=10)= 0.00032674036762078424
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,784 INFO epoch # 881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003437240529040407
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,809 INFO epoch # 882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032817266515589185
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,833 INFO epoch # 883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003269308067891481
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,858 INFO epoch # 884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031760134318444347
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,883 INFO epoch # 885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031673384364694357
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,908 INFO epoch # 886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031851209226130907
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,933 INFO epoch # 887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032764126538365547
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,958 INFO epoch # 888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032436869265179015
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:07:59,983 INFO epoch # 889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031903865019558
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,009 INFO epoch # 890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003213655785657465
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:00,009 INFO *** epoch 890, rolling-avg-loss (window=10)= 0.0003244088990738549
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,034 INFO epoch # 891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003219987149350345
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,058 INFO epoch # 892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003173731348527196
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,083 INFO epoch # 893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032354108781354235
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,109 INFO epoch # 894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003244915124793936
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,133 INFO epoch # 895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032291594876109487
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,158 INFO epoch # 896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032669507241475264
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,183 INFO epoch # 897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031672079452878927
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,208 INFO epoch # 898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031490942305286546
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,232 INFO epoch # 899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032336922512123627
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,257 INFO epoch # 900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003177193076615887
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:00,257 INFO *** epoch 900, rolling-avg-loss (window=10)= 0.0003209734221621018
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,281 INFO epoch # 901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032165897968557794
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,306 INFO epoch # 902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003231200248202575
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,331 INFO epoch # 903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003186882099336279
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,356 INFO epoch # 904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000321318645728752
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,381 INFO epoch # 905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033437589175134365
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,406 INFO epoch # 906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032659273170533457
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,431 INFO epoch # 907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003292541377179857
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,455 INFO epoch # 908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033080707196079726
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,480 INFO epoch # 909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003255962618693177
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,505 INFO epoch # 910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003133274810222377
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:00,505 INFO *** epoch 910, rolling-avg-loss (window=10)= 0.0003244739436195232
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,531 INFO epoch # 911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032783501602742554
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,556 INFO epoch # 912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031867182821900187
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,581 INFO epoch # 913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032103188610303084
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,606 INFO epoch # 914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003193361954929839
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,631 INFO epoch # 915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003307115367663625
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,656 INFO epoch # 916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033213994341037636
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,680 INFO epoch # 917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003345473503161754
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,704 INFO epoch # 918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034201418082895025
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,729 INFO epoch # 919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003202459579499971
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,754 INFO epoch # 920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003211556139701445
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:00,754 INFO *** epoch 920, rolling-avg-loss (window=10)= 0.0003267689509084448
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,779 INFO epoch # 921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003206862118012006
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,803 INFO epoch # 922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003223222048420991
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,828 INFO epoch # 923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003231856751621568
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,853 INFO epoch # 924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000322743558250035
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,878 INFO epoch # 925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003236434168814282
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,903 INFO epoch # 926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032385761837109124
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,927 INFO epoch # 927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003230917321551325
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,953 INFO epoch # 928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003237796718687085
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:00,978 INFO epoch # 929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003242429530863384
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,004 INFO epoch # 930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003095263180771976
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:01,004 INFO *** epoch 930, rolling-avg-loss (window=10)= 0.0003217079360495388
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,030 INFO epoch # 931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031016535337715013
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,055 INFO epoch # 932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003194568753575108
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,080 INFO epoch # 933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031940353177820465
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,104 INFO epoch # 934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032026141686531317
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,129 INFO epoch # 935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003139061526910934
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,153 INFO epoch # 936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033598701078777334
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,178 INFO epoch # 937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032194716124130145
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,203 INFO epoch # 938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000317263080173039
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,228 INFO epoch # 939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003253231583845003
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,253 INFO epoch # 940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032453887231115785
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:01,253 INFO *** epoch 940, rolling-avg-loss (window=10)= 0.0003208252612967044
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,278 INFO epoch # 941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003342743900637808
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,303 INFO epoch # 942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031688985514587587
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,327 INFO epoch # 943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031584817375655155
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,352 INFO epoch # 944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031823675547327314
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,376 INFO epoch # 945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003242413297162524
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,402 INFO epoch # 946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003225134432016473
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,427 INFO epoch # 947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031800143187865615
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,452 INFO epoch # 948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003114282852038741
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,477 INFO epoch # 949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031479084157451457
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,503 INFO epoch # 950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003294855634781665
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:01,503 INFO *** epoch 950, rolling-avg-loss (window=10)= 0.0003205710069492592
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,528 INFO epoch # 951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003179264288129551
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,553 INFO epoch # 952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034156452456954864
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,577 INFO epoch # 953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000334924557578883
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,602 INFO epoch # 954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003165764962821933
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,627 INFO epoch # 955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003223485205255981
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,652 INFO epoch # 956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003158334542864135
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,676 INFO epoch # 957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00034171575618009746
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,701 INFO epoch # 958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003161351034317964
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,726 INFO epoch # 959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003189477815924745
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,751 INFO epoch # 960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031358836297710823
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:01,751 INFO *** epoch 960, rolling-avg-loss (window=10)= 0.0003239560986237068
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,776 INFO epoch # 961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003221228369511664
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,800 INFO epoch # 962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003122142649122647
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,825 INFO epoch # 963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003080485255590507
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,850 INFO epoch # 964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003142059601876619
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,875 INFO epoch # 965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003331902632323493
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,900 INFO epoch # 966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003178673630048122
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,925 INFO epoch # 967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032256231560105726
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,950 INFO epoch # 968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003059405648465534
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,975 INFO epoch # 969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003193155933071726
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:01,999 INFO epoch # 970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032343288164286474
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:01,999 INFO *** epoch 970, rolling-avg-loss (window=10)= 0.0003178900569244953
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,025 INFO epoch # 971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003175009237436045
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,050 INFO epoch # 972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003076405422429421
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,075 INFO epoch # 973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003329753031721339
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,100 INFO epoch # 974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032408061940389284
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,125 INFO epoch # 975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003187894320165339
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,150 INFO epoch # 976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003213620116834396
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,175 INFO epoch # 977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003404601414721193
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,199 INFO epoch # 978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032091537182818035
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,224 INFO epoch # 979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003267506756986092
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,248 INFO epoch # 980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003148217332116993
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:02,248 INFO *** epoch 980, rolling-avg-loss (window=10)= 0.0003225296754473155
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,273 INFO epoch # 981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003134062168620793
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,299 INFO epoch # 982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031375758740718345
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,324 INFO epoch # 983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000318939023418352
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,349 INFO epoch # 984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003129037912003696
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,373 INFO epoch # 985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031280501072095444
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,398 INFO epoch # 986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003189233231491276
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,423 INFO epoch # 987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003109137693952237
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,448 INFO epoch # 988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031040631895718564
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,472 INFO epoch # 989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003125682464867298
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,497 INFO epoch # 990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003113010314077006
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:02,497 INFO *** epoch 990, rolling-avg-loss (window=10)= 0.00031359243190049065
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,524 INFO epoch # 991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003208037611329928
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,548 INFO epoch # 992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003035316794661672
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,573 INFO epoch # 993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030228387934455115
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,599 INFO epoch # 994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003131846506481192
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,623 INFO epoch # 995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003258845415465268
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,648 INFO epoch # 996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003159092947108937
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,672 INFO epoch # 997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003146805120715206
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,697 INFO epoch # 998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003036032135631623
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,722 INFO epoch # 999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030401568926338637
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,747 INFO epoch # 1000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003067509109054559
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:02,747 INFO *** epoch 1000, rolling-avg-loss (window=10)= 0.00031106481326527755
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,772 INFO epoch # 1001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003113977311711226
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,797 INFO epoch # 1002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033686891422673526
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,822 INFO epoch # 1003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032433522698868596
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,847 INFO epoch # 1004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003111461426929704
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,872 INFO epoch # 1005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003166224520620225
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,896 INFO epoch # 1006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030150574831558125
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,921 INFO epoch # 1007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003093375312995964
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,947 INFO epoch # 1008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003082473824698744
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:02,974 INFO epoch # 1009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032577361312827896
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,000 INFO epoch # 1010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000316171782573552
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:03,000 INFO *** epoch 1010, rolling-avg-loss (window=10)= 0.000316140652492842
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,025 INFO epoch # 1011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031691475929359773
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,050 INFO epoch # 1012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031451919057872145
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,074 INFO epoch # 1013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030931894684077375
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,099 INFO epoch # 1014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032123607691443925
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,123 INFO epoch # 1015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003156136201661346
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,148 INFO epoch # 1016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003136670927882993
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,173 INFO epoch # 1017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000320783159986604
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,198 INFO epoch # 1018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003118748441920616
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,223 INFO epoch # 1019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030727034235107046
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,248 INFO epoch # 1020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003189701641011717
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:03,248 INFO *** epoch 1020, rolling-avg-loss (window=10)= 0.00031501681972128737
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,272 INFO epoch # 1021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003199739140524928
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,297 INFO epoch # 1022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003104707774972277
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,322 INFO epoch # 1023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030578774333532365
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,347 INFO epoch # 1024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003232528122940234
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,372 INFO epoch # 1025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003263291948574728
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,397 INFO epoch # 1026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032582663698121905
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,422 INFO epoch # 1027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032549549442982035
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,447 INFO epoch # 1028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003067179163086361
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,471 INFO epoch # 1029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003024018057788323
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,496 INFO epoch # 1030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003032743768666738
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:03,496 INFO *** epoch 1030, rolling-avg-loss (window=10)= 0.0003149530672401722
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,520 INFO epoch # 1031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030153202512467813
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,545 INFO epoch # 1032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032068372654196406
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,570 INFO epoch # 1033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031129810584908617
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,595 INFO epoch # 1034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003071668990222471
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,620 INFO epoch # 1035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003107952066264781
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,645 INFO epoch # 1036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003039059029625995
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,669 INFO epoch # 1037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000302634244768082
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,694 INFO epoch # 1038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003097194737555193
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,719 INFO epoch # 1039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003097011341846415
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,743 INFO epoch # 1040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003044896380743012
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:03,743 INFO *** epoch 1040, rolling-avg-loss (window=10)= 0.0003081926356909597
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,767 INFO epoch # 1041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030682218057336285
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,792 INFO epoch # 1042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003085831841287602
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,817 INFO epoch # 1043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003107169094229383
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,842 INFO epoch # 1044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003072761161352641
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,867 INFO epoch # 1045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031301474830667885
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,892 INFO epoch # 1046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003156101073337985
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,916 INFO epoch # 1047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032191461234885665
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,941 INFO epoch # 1048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030429307309012593
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,966 INFO epoch # 1049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003049738929673497
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:03,990 INFO epoch # 1050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003051186847317565
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:03,990 INFO *** epoch 1050, rolling-avg-loss (window=10)= 0.0003098323509038892
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,016 INFO epoch # 1051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031007856575472815
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,041 INFO epoch # 1052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031679168085767224
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,066 INFO epoch # 1053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032194145855360797
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,092 INFO epoch # 1054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003172161606406527
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,116 INFO epoch # 1055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032107431449860866
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,141 INFO epoch # 1056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031196355038056416
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,165 INFO epoch # 1057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00033164598509236905
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,189 INFO epoch # 1058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030899689445504917
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,214 INFO epoch # 1059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029931773543856775
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,239 INFO epoch # 1060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000306260547326279
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:04,239 INFO *** epoch 1060, rolling-avg-loss (window=10)= 0.00031452868929980984
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,264 INFO epoch # 1061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030833185467469903
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,289 INFO epoch # 1062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031101932857252124
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,314 INFO epoch # 1063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031239988851926423
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,339 INFO epoch # 1064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030902821911565426
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,363 INFO epoch # 1065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030337868707387575
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,388 INFO epoch # 1066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030474057587395823
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,413 INFO epoch # 1067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003040283076448499
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,438 INFO epoch # 1068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003281546681786754
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,464 INFO epoch # 1069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031949864517498227
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,489 INFO epoch # 1070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029741935473534146
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:04,489 INFO *** epoch 1070, rolling-avg-loss (window=10)= 0.00030979995295638213
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,516 INFO epoch # 1071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030053069390955246
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,540 INFO epoch # 1072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030385394056793303
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,565 INFO epoch # 1073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003123580885585397
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,589 INFO epoch # 1074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000303479766755897
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,614 INFO epoch # 1075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030374535209765394
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,639 INFO epoch # 1076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003027415462254014
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,664 INFO epoch # 1077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031503737201481793
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,689 INFO epoch # 1078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030594897647720894
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,715 INFO epoch # 1079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030385693327324197
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,740 INFO epoch # 1080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003149627188187359
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:04,740 INFO *** epoch 1080, rolling-avg-loss (window=10)= 0.00030665153886989826
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,766 INFO epoch # 1081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003117289286040302
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,791 INFO epoch # 1082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030233416889261986
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,816 INFO epoch # 1083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003137871446337418
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,840 INFO epoch # 1084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003029378979200763
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,865 INFO epoch # 1085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002988744541653432
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,890 INFO epoch # 1086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029853362398820797
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,915 INFO epoch # 1087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029835603137533847
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,940 INFO epoch # 1088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031051760285793405
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,965 INFO epoch # 1089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003064926426824448
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:04,990 INFO epoch # 1090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003002322788233869
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:04,990 INFO *** epoch 1090, rolling-avg-loss (window=10)= 0.00030437947739431236
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,016 INFO epoch # 1091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029958992464733977
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,040 INFO epoch # 1092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003136003504291044
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,065 INFO epoch # 1093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002976229586887972
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,091 INFO epoch # 1094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030035347528090437
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,116 INFO epoch # 1095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030234469657963405
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,141 INFO epoch # 1096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030341927070237163
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,166 INFO epoch # 1097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003017334608427648
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,190 INFO epoch # 1098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031452188538553724
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,215 INFO epoch # 1099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030514422479817376
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,240 INFO epoch # 1100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030161414636365537
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:05,240 INFO *** epoch 1100, rolling-avg-loss (window=10)= 0.0003039944393718283
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,265 INFO epoch # 1101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003031148749869317
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,290 INFO epoch # 1102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003030543963957046
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,315 INFO epoch # 1103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003108485643419304
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,339 INFO epoch # 1104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003016779662825034
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,364 INFO epoch # 1105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003070244399298515
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,389 INFO epoch # 1106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002980069633589924
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,414 INFO epoch # 1107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002970206179140535
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,438 INFO epoch # 1108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003015103307136867
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,463 INFO epoch # 1109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003044460133034071
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,488 INFO epoch # 1110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003001005692307704
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:05,488 INFO *** epoch 1110, rolling-avg-loss (window=10)= 0.0003026804736457831
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,513 INFO epoch # 1111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002987199230119586
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,539 INFO epoch # 1112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003004154992855287
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,564 INFO epoch # 1113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029771967341990345
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,589 INFO epoch # 1114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003027745451877958
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,614 INFO epoch # 1115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029699986646716884
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,639 INFO epoch # 1116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003035322979225644
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,663 INFO epoch # 1117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030557200204514497
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,688 INFO epoch # 1118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002949337861666988
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,713 INFO epoch # 1119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030890832484666524
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,738 INFO epoch # 1120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030889060290064665
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:05,739 INFO *** epoch 1120, rolling-avg-loss (window=10)= 0.00030184665212540753
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,764 INFO epoch # 1121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003055554228402408
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,789 INFO epoch # 1122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030657977206699017
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,814 INFO epoch # 1123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000301026162508476
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,839 INFO epoch # 1124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002980691725887092
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,864 INFO epoch # 1125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030398859983376625
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,888 INFO epoch # 1126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003009586998294773
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,912 INFO epoch # 1127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003071942142144378
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,937 INFO epoch # 1128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002983740165031382
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,963 INFO epoch # 1129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030193245516524516
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:05,988 INFO epoch # 1130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003054134504054673
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:05,988 INFO *** epoch 1130, rolling-avg-loss (window=10)= 0.0003029091965955948
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,014 INFO epoch # 1131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030052476753813346
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,039 INFO epoch # 1132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030314896513508366
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,064 INFO epoch # 1133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002961573369767783
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,089 INFO epoch # 1134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030006481018582626
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,113 INFO epoch # 1135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003013267472852021
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,138 INFO epoch # 1136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003007261184393428
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,163 INFO epoch # 1137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030544229375664147
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,188 INFO epoch # 1138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029648630339319684
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,212 INFO epoch # 1139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030593463161494584
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,237 INFO epoch # 1140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029904345283284786
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:06,237 INFO *** epoch 1140, rolling-avg-loss (window=10)= 0.00030088554271579985
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,262 INFO epoch # 1141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002988956673237096
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,287 INFO epoch # 1142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002927486057160422
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,311 INFO epoch # 1143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003052760657737963
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,336 INFO epoch # 1144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031314548624712706
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,361 INFO epoch # 1145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00032666444770127005
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,387 INFO epoch # 1146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002978649117202232
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,412 INFO epoch # 1147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003122591629757413
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,437 INFO epoch # 1148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002986462911524411
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,462 INFO epoch # 1149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002984824692248367
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,487 INFO epoch # 1150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030689498733928693
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:06,487 INFO *** epoch 1150, rolling-avg-loss (window=10)= 0.00030508780951744743
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,512 INFO epoch # 1151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029461092469448754
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,537 INFO epoch # 1152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002943530512441482
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,562 INFO epoch # 1153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003032472949208958
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,587 INFO epoch # 1154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030434707898945946
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,612 INFO epoch # 1155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002923616316236023
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,638 INFO epoch # 1156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002994800493719855
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,663 INFO epoch # 1157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029503528369657163
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,688 INFO epoch # 1158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002942723315624919
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,712 INFO epoch # 1159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003071026157288413
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,737 INFO epoch # 1160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030111766827758404
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:06,737 INFO *** epoch 1160, rolling-avg-loss (window=10)= 0.00029859279301100677
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,761 INFO epoch # 1161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003015228029523444
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,786 INFO epoch # 1162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003016852915087449
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,812 INFO epoch # 1163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002974444407820036
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,837 INFO epoch # 1164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029656386260675
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,862 INFO epoch # 1165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029317139477435763
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,887 INFO epoch # 1166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002937886140508843
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,912 INFO epoch # 1167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029663061057882647
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,937 INFO epoch # 1168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029751453091323907
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,961 INFO epoch # 1169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029992543298119147
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:06,985 INFO epoch # 1170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031133123203679655
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:06,985 INFO *** epoch 1170, rolling-avg-loss (window=10)= 0.0002989578213185139
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,011 INFO epoch # 1171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002934852138943305
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,037 INFO epoch # 1172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000298585684088591
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,063 INFO epoch # 1173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030701034951822033
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,088 INFO epoch # 1174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031574952715475647
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,113 INFO epoch # 1175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000300798679070015
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,138 INFO epoch # 1176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029441219396955735
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,162 INFO epoch # 1177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029436967909402613
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,187 INFO epoch # 1178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002973004847133
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,212 INFO epoch # 1179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002935176956400807
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,237 INFO epoch # 1180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029566952642718595
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:07,237 INFO *** epoch 1180, rolling-avg-loss (window=10)= 0.0002990899033570063
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,263 INFO epoch # 1181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002927607395187286
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,288 INFO epoch # 1182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029523499626001075
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,313 INFO epoch # 1183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003002443816512823
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,338 INFO epoch # 1184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029743956672194014
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,363 INFO epoch # 1185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030240711099135557
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,387 INFO epoch # 1186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028875736794101874
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,411 INFO epoch # 1187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028933341382071377
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,436 INFO epoch # 1188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028889207502028774
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,462 INFO epoch # 1189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029713140892064464
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,487 INFO epoch # 1190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002925847723547901
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:07,487 INFO *** epoch 1190, rolling-avg-loss (window=10)= 0.0002944785833200773
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,514 INFO epoch # 1191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000300304959611302
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,540 INFO epoch # 1192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002960844713795398
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,565 INFO epoch # 1193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029639448704464095
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,590 INFO epoch # 1194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003041142152921696
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,615 INFO epoch # 1195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003058590626876269
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,640 INFO epoch # 1196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002947743732615241
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,665 INFO epoch # 1197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003001241969676422
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,691 INFO epoch # 1198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029369955882430077
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,716 INFO epoch # 1199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003044722763921267
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,741 INFO epoch # 1200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000293520069265339
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:07,741 INFO *** epoch 1200, rolling-avg-loss (window=10)= 0.00029893476707262124
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,766 INFO epoch # 1201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002939597751329919
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,791 INFO epoch # 1202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029134230821260387
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,816 INFO epoch # 1203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002937039821907612
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,842 INFO epoch # 1204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029797758719983644
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,868 INFO epoch # 1205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028702623655720217
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,893 INFO epoch # 1206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002917873382102698
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,919 INFO epoch # 1207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002883105615702724
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,944 INFO epoch # 1208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030653655038414786
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,969 INFO epoch # 1209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028872237682142963
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:07,994 INFO epoch # 1210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003035432139378307
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:07,994 INFO *** epoch 1210, rolling-avg-loss (window=10)= 0.0002942909930217346
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,019 INFO epoch # 1211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029922472811969264
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,044 INFO epoch # 1212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029628908066245326
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,070 INFO epoch # 1213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002914189065839829
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,095 INFO epoch # 1214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002954955902948443
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,120 INFO epoch # 1215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002863349239175607
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,145 INFO epoch # 1216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000298029077904565
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,170 INFO epoch # 1217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029016787468987915
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,194 INFO epoch # 1218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028755532985087485
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,219 INFO epoch # 1219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028517797743136594
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,244 INFO epoch # 1220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029478282626119575
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:08,244 INFO *** epoch 1220, rolling-avg-loss (window=10)= 0.00029244763157164143
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,269 INFO epoch # 1221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029327618332380164
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,295 INFO epoch # 1222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029216625553090124
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,320 INFO epoch # 1223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003002427058943015
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,345 INFO epoch # 1224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003109286481048912
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,370 INFO epoch # 1225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029695395850077536
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,395 INFO epoch # 1226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003089412273506501
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,420 INFO epoch # 1227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030659813658400837
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,445 INFO epoch # 1228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002856992634146341
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,470 INFO epoch # 1229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029812323524051215
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,496 INFO epoch # 1230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002914318731719894
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:08,496 INFO *** epoch 1230, rolling-avg-loss (window=10)= 0.00029843614871164647
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,522 INFO epoch # 1231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002864149253582582
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,548 INFO epoch # 1232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002877362396767629
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,572 INFO epoch # 1233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002919991479887228
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,597 INFO epoch # 1234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028586559201357884
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,622 INFO epoch # 1235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002818412910398495
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,647 INFO epoch # 1236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029218842537375166
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,672 INFO epoch # 1237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002862813786902864
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,697 INFO epoch # 1238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003028299684436726
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,723 INFO epoch # 1239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002877754870236718
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,747 INFO epoch # 1240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029180637377846454
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:08,748 INFO *** epoch 1240, rolling-avg-loss (window=10)= 0.00028947388293870197
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,772 INFO epoch # 1241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002890175476620373
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,797 INFO epoch # 1242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003005071102441954
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,821 INFO epoch # 1243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003005717232424234
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,847 INFO epoch # 1244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029857983796059023
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,872 INFO epoch # 1245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002890867264276104
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,897 INFO epoch # 1246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003024672881500529
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,922 INFO epoch # 1247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002921194736180561
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,946 INFO epoch # 1248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028199719048903457
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,972 INFO epoch # 1249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028572602355520107
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:08,997 INFO epoch # 1250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002875448029953986
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:08,997 INFO *** epoch 1250, rolling-avg-loss (window=10)= 0.00029276177243446
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,023 INFO epoch # 1251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028450158632559966
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,048 INFO epoch # 1252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002947255041882662
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,076 INFO epoch # 1253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002860924133398969
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,102 INFO epoch # 1254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029059038767757425
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,126 INFO epoch # 1255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002923893047929076
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,150 INFO epoch # 1256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000285455073961722
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,175 INFO epoch # 1257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029313441108180473
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,201 INFO epoch # 1258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002908031767999221
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,225 INFO epoch # 1259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002938557291469936
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,250 INFO epoch # 1260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002920061302055339
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:09,250 INFO *** epoch 1260, rolling-avg-loss (window=10)= 0.0002903553717520221
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,275 INFO epoch # 1261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002885232529869037
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,301 INFO epoch # 1262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002906292175924006
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,327 INFO epoch # 1263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003049958576281954
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,352 INFO epoch # 1264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028937850189062636
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,377 INFO epoch # 1265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002885680327640979
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,401 INFO epoch # 1266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028751684169817186
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,427 INFO epoch # 1267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028468298675891543
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,452 INFO epoch # 1268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029099069901608994
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,479 INFO epoch # 1269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002984346722119621
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,509 INFO epoch # 1270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002898099844709837
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:09,509 INFO *** epoch 1270, rolling-avg-loss (window=10)= 0.0002913530047018347
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,534 INFO epoch # 1271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002887422717841608
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,560 INFO epoch # 1272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027728881375099133
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,585 INFO epoch # 1273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028548121128031716
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,610 INFO epoch # 1274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002990908409369045
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,636 INFO epoch # 1275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029279131912127404
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,661 INFO epoch # 1276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002911446038134662
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,686 INFO epoch # 1277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000285791141297003
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,711 INFO epoch # 1278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028692333248078023
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,737 INFO epoch # 1279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029271078279374964
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,762 INFO epoch # 1280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003086404095353958
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:09,762 INFO *** epoch 1280, rolling-avg-loss (window=10)= 0.0002908604726794043
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,787 INFO epoch # 1281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000287392881208299
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,812 INFO epoch # 1282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002881895966960916
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,837 INFO epoch # 1283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028962140412269424
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,862 INFO epoch # 1284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002983589770987497
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,887 INFO epoch # 1285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029697732180855905
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,912 INFO epoch # 1286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030128804750607485
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,938 INFO epoch # 1287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002919972811858835
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,963 INFO epoch # 1288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002826870868115553
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:09,988 INFO epoch # 1289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002812172217610558
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,014 INFO epoch # 1290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029088130624066775
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:10,014 INFO *** epoch 1290, rolling-avg-loss (window=10)= 0.0002908611124439631
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,039 INFO epoch # 1291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000298700321997915
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,064 INFO epoch # 1292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003019951531314291
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,089 INFO epoch # 1293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029536870258328106
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,114 INFO epoch # 1294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029007798210451644
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,138 INFO epoch # 1295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028782768723821
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,163 INFO epoch # 1296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002783003543819567
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,189 INFO epoch # 1297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002884838216622094
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,214 INFO epoch # 1298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028255289985931346
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,238 INFO epoch # 1299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029715733544435354
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,263 INFO epoch # 1300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028837460143092485
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:10,263 INFO *** epoch 1300, rolling-avg-loss (window=10)= 0.0002908838859834109
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,288 INFO epoch # 1301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002779293219126495
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,314 INFO epoch # 1302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028045202821626195
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,340 INFO epoch # 1303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000290856281104165
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,364 INFO epoch # 1304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028421530712096553
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,389 INFO epoch # 1305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028142556340234086
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,413 INFO epoch # 1306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002886655873485974
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,438 INFO epoch # 1307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029829568583019343
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,462 INFO epoch # 1308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028542959314238815
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,486 INFO epoch # 1309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002840032526624522
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,513 INFO epoch # 1310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002925892242014275
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:10,513 INFO *** epoch 1310, rolling-avg-loss (window=10)= 0.0002863861844941442
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,538 INFO epoch # 1311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002788236000924371
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,563 INFO epoch # 1312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002821318040202771
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,587 INFO epoch # 1313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002902166173693591
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,613 INFO epoch # 1314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029968347891034294
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,637 INFO epoch # 1315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030165959698414167
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,661 INFO epoch # 1316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003004870325509858
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,686 INFO epoch # 1317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002891748781881428
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,710 INFO epoch # 1318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028790190138222116
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,735 INFO epoch # 1319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002791554615084481
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,760 INFO epoch # 1320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028289168965004916
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:10,760 INFO *** epoch 1320, rolling-avg-loss (window=10)= 0.00028921260606564055
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,786 INFO epoch # 1321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002823910646839067
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,811 INFO epoch # 1322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027827274149915736
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,836 INFO epoch # 1323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002894161780464596
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,860 INFO epoch # 1324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027796810941903716
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,885 INFO epoch # 1325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028434215803697173
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,909 INFO epoch # 1326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027999730872189894
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,934 INFO epoch # 1327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027763045696441884
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,959 INFO epoch # 1328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027762149838963526
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:10,983 INFO epoch # 1329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002837327791244856
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,010 INFO epoch # 1330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028388173252876317
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:11,010 INFO *** epoch 1330, rolling-avg-loss (window=10)= 0.00028152540274147344
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,035 INFO epoch # 1331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000283804364568953
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,060 INFO epoch # 1332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028424472922259676
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,084 INFO epoch # 1333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002911672216474212
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,109 INFO epoch # 1334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000281778397870117
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,133 INFO epoch # 1335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028114454908063636
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,157 INFO epoch # 1336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002835440087697602
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,182 INFO epoch # 1337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002989385150223305
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,207 INFO epoch # 1338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000302941127613719
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,232 INFO epoch # 1339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030332373945774247
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,257 INFO epoch # 1340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002925836599648132
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:11,257 INFO *** epoch 1340, rolling-avg-loss (window=10)= 0.00029034703132180896
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,282 INFO epoch # 1341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00031837057966705676
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,306 INFO epoch # 1342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028664050374313125
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,331 INFO epoch # 1343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028115686228764907
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,355 INFO epoch # 1344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002924371955616932
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,380 INFO epoch # 1345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027714754895506694
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,404 INFO epoch # 1346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002776382806976991
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,429 INFO epoch # 1347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002848835799211104
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,454 INFO epoch # 1348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00030743181081821345
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,479 INFO epoch # 1349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002964663481439597
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,504 INFO epoch # 1350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027796401762835945
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:11,504 INFO *** epoch 1350, rolling-avg-loss (window=10)= 0.000290013672742394
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,529 INFO epoch # 1351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002750571465834842
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,554 INFO epoch # 1352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027917503820000484
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,578 INFO epoch # 1353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028320041518392305
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,603 INFO epoch # 1354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002771708954242058
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,628 INFO epoch # 1355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027790719780438976
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,653 INFO epoch # 1356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002825996742883165
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,678 INFO epoch # 1357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002746593697728323
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,703 INFO epoch # 1358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002910271632052692
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,728 INFO epoch # 1359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002788659733986216
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,752 INFO epoch # 1360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028338718776857214
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:11,752 INFO *** epoch 1360, rolling-avg-loss (window=10)= 0.00028030500616296197
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,777 INFO epoch # 1361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002798785116673181
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,801 INFO epoch # 1362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002747094678592735
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,827 INFO epoch # 1363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028116127775449837
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,852 INFO epoch # 1364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029289433940513326
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,877 INFO epoch # 1365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00029327117497034903
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,902 INFO epoch # 1366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002753307292420816
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,928 INFO epoch # 1367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002744774799793959
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,952 INFO epoch # 1368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002802160258787418
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:11,977 INFO epoch # 1369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002746234126139565
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,002 INFO epoch # 1370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028035162166426225
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:12,002 INFO *** epoch 1370, rolling-avg-loss (window=10)= 0.000280691404103501
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,027 INFO epoch # 1371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002890342703072487
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,052 INFO epoch # 1372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002915187508084013
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,078 INFO epoch # 1373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028635796209398125
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,103 INFO epoch # 1374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002714037354702928
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,129 INFO epoch # 1375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027495191295331874
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,153 INFO epoch # 1376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002845076713128947
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,178 INFO epoch # 1377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027649191906675696
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,202 INFO epoch # 1378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028314336959738283
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,226 INFO epoch # 1379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028003808734605886
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,251 INFO epoch # 1380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028256683311026013
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:12,251 INFO *** epoch 1380, rolling-avg-loss (window=10)= 0.00028200145120665964
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,276 INFO epoch # 1381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000274690764906284
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,302 INFO epoch # 1382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028927875350096395
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,327 INFO epoch # 1383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002827115419287501
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,351 INFO epoch # 1384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027873512631880915
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,376 INFO epoch # 1385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028360364272625053
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,401 INFO epoch # 1386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028090555403780724
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,425 INFO epoch # 1387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028113344971123817
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,450 INFO epoch # 1388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027774379954540303
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,474 INFO epoch # 1389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000281977782807579
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,499 INFO epoch # 1390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028509473112145706
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:12,499 INFO *** epoch 1390, rolling-avg-loss (window=10)= 0.00028158751466045415
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,525 INFO epoch # 1391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027337921076520745
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,550 INFO epoch # 1392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002824519544706813
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,575 INFO epoch # 1393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002793040367708142
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,599 INFO epoch # 1394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028916242278812984
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,624 INFO epoch # 1395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028014223249296524
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,648 INFO epoch # 1396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002790867234580219
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,673 INFO epoch # 1397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002851858554225016
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,698 INFO epoch # 1398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002738715452973598
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,723 INFO epoch # 1399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002747661326014038
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,748 INFO epoch # 1400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002765004708115677
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:12,748 INFO *** epoch 1400, rolling-avg-loss (window=10)= 0.0002793850584878653
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,773 INFO epoch # 1401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002738167323903846
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,798 INFO epoch # 1402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002812304675379502
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,823 INFO epoch # 1403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002805509222006159
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,848 INFO epoch # 1404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000286578233603255
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,872 INFO epoch # 1405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002893712026499478
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,896 INFO epoch # 1406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002926116289537666
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,921 INFO epoch # 1407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002854215035248282
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,947 INFO epoch # 1408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002815082097575734
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,971 INFO epoch # 1409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028396239046872193
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:12,996 INFO epoch # 1410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027727661425680186
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:12,996 INFO *** epoch 1410, rolling-avg-loss (window=10)= 0.0002832327905343846
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,022 INFO epoch # 1411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027576387760096363
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,048 INFO epoch # 1412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002783649690432607
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,072 INFO epoch # 1413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027795232287774395
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,097 INFO epoch # 1414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028352288562538366
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,121 INFO epoch # 1415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028669871868000234
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,146 INFO epoch # 1416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002784315960265563
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,171 INFO epoch # 1417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027828696953032965
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,195 INFO epoch # 1418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028024757089692036
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,221 INFO epoch # 1419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002780601623401578
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,245 INFO epoch # 1420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002860288110761238
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:13,245 INFO *** epoch 1420, rolling-avg-loss (window=10)= 0.00028033578836974426
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,270 INFO epoch # 1421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0003003944967141641
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,295 INFO epoch # 1422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028436226878381733
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,319 INFO epoch # 1423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027611716442541885
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,344 INFO epoch # 1424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027529711099175206
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,369 INFO epoch # 1425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027795009913721255
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,394 INFO epoch # 1426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002774952958653947
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,419 INFO epoch # 1427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027050053280878013
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,445 INFO epoch # 1428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002694388202923749
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,470 INFO epoch # 1429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026997343479056977
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,494 INFO epoch # 1430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002773303567664698
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:13,494 INFO *** epoch 1430, rolling-avg-loss (window=10)= 0.0002778859580575954
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,520 INFO epoch # 1431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002748738984727035
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,545 INFO epoch # 1432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027405718886127164
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,570 INFO epoch # 1433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002769039985391178
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,595 INFO epoch # 1434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002691076463504162
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,621 INFO epoch # 1435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028862068595896873
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,645 INFO epoch # 1436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002958225028123707
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,671 INFO epoch # 1437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002733127642256607
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,696 INFO epoch # 1438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002842723084281066
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,720 INFO epoch # 1439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028140998786381846
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,745 INFO epoch # 1440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002740780881140381
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:13,745 INFO *** epoch 1440, rolling-avg-loss (window=10)= 0.00027924590696264725
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,771 INFO epoch # 1441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027268640426752555
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,796 INFO epoch # 1442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002715536827703805
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,821 INFO epoch # 1443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000286465317393387
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,845 INFO epoch # 1444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002867931462658037
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,870 INFO epoch # 1445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027404991851653904
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,895 INFO epoch # 1446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026893067406490444
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,920 INFO epoch # 1447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002692523258571912
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,945 INFO epoch # 1448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026832189510709475
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,970 INFO epoch # 1449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002676158893986472
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:13,995 INFO epoch # 1450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027249383669446356
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:13,995 INFO *** epoch 1450, rolling-avg-loss (window=10)= 0.0002738163090335937
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,021 INFO epoch # 1451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002764150155209271
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,047 INFO epoch # 1452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002829619180244793
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,072 INFO epoch # 1453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002768230620339247
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,097 INFO epoch # 1454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026810491690412166
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,121 INFO epoch # 1455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027335886032752956
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,145 INFO epoch # 1456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027526644823540537
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,170 INFO epoch # 1457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027457431694659004
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,194 INFO epoch # 1458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002781126878939436
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,220 INFO epoch # 1459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002753008922029819
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,246 INFO epoch # 1460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002723759986110963
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:14,247 INFO *** epoch 1460, rolling-avg-loss (window=10)= 0.00027532941167009997
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,272 INFO epoch # 1461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027384981429869575
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,297 INFO epoch # 1462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027045817883585444
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,321 INFO epoch # 1463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002795181921101175
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,346 INFO epoch # 1464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002733574346556062
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,371 INFO epoch # 1465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002799099065928853
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,396 INFO epoch # 1466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000285347541336835
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,421 INFO epoch # 1467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002770832966364521
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,446 INFO epoch # 1468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002771141828686398
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,471 INFO epoch # 1469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002679368564193802
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,496 INFO epoch # 1470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002749534440226853
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:14,496 INFO *** epoch 1470, rolling-avg-loss (window=10)= 0.00027595288477771517
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,521 INFO epoch # 1471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026751275664927174
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,546 INFO epoch # 1472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027254990335287793
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,571 INFO epoch # 1473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026288639824737664
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,595 INFO epoch # 1474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000266077379429979
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,620 INFO epoch # 1475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002704977238733721
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,645 INFO epoch # 1476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002703044347331992
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,671 INFO epoch # 1477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002676463153745447
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,696 INFO epoch # 1478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002647767968093311
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,721 INFO epoch # 1479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002666983406275644
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,746 INFO epoch # 1480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026443128923087246
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:14,746 INFO *** epoch 1480, rolling-avg-loss (window=10)= 0.00026733813383283896
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,771 INFO epoch # 1481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002735624850694356
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,795 INFO epoch # 1482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002797067032328674
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,820 INFO epoch # 1483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027005622209149544
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,845 INFO epoch # 1484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002712541047783036
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,871 INFO epoch # 1485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027367334447002834
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,896 INFO epoch # 1486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002737415492967037
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,921 INFO epoch # 1487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027101172890979796
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,945 INFO epoch # 1488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026681252139886577
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,970 INFO epoch # 1489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002649824384466878
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:14,995 INFO epoch # 1490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002713092047737778
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:14,995 INFO *** epoch 1490, rolling-avg-loss (window=10)= 0.00027161103024679633
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,021 INFO epoch # 1491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002698418210327093
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,046 INFO epoch # 1492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002698663516119788
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,071 INFO epoch # 1493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002793366729747504
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,096 INFO epoch # 1494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002691879887216991
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,121 INFO epoch # 1495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027361975517123935
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,146 INFO epoch # 1496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002761347734901522
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,170 INFO epoch # 1497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002847156874070476
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,195 INFO epoch # 1498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027611593478858205
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,220 INFO epoch # 1499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002681253669184766
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,244 INFO epoch # 1500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027245053871800855
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:15,245 INFO *** epoch 1500, rolling-avg-loss (window=10)= 0.0002739394890834644
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,270 INFO epoch # 1501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027250959371615734
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,295 INFO epoch # 1502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002674100032891147
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,319 INFO epoch # 1503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002664042977682714
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,344 INFO epoch # 1504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002787400980845892
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,369 INFO epoch # 1505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026937839450381164
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,393 INFO epoch # 1506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027773505114185224
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,418 INFO epoch # 1507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002758873753399322
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,442 INFO epoch # 1508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002707000545342453
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,467 INFO epoch # 1509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026833808057874975
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,492 INFO epoch # 1510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026344660166484704
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:15,492 INFO *** epoch 1510, rolling-avg-loss (window=10)= 0.0002710549550621571
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,519 INFO epoch # 1511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002665109061386569
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,544 INFO epoch # 1512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002673340368866255
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,569 INFO epoch # 1513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026630380681516336
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,593 INFO epoch # 1514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002679950935998932
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,618 INFO epoch # 1515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002766692568132255
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,643 INFO epoch # 1516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027904621196544864
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,668 INFO epoch # 1517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002812305552652106
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,692 INFO epoch # 1518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002611871197586879
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,717 INFO epoch # 1519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026544419961282983
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,743 INFO epoch # 1520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002611363125782061
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:15,743 INFO *** epoch 1520, rolling-avg-loss (window=10)= 0.00026928574994339473
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,769 INFO epoch # 1521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002723620761701438
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,794 INFO epoch # 1522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027752253434820364
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,819 INFO epoch # 1523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027414348691568845
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,843 INFO epoch # 1524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002607772922991509
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,868 INFO epoch # 1525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027087118983867446
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,892 INFO epoch # 1526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027454842929728327
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,917 INFO epoch # 1527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002757075211515517
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,943 INFO epoch # 1528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027471640309418687
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,968 INFO epoch # 1529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002621061226818711
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:15,993 INFO epoch # 1530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002661574582037117
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:15,993 INFO *** epoch 1530, rolling-avg-loss (window=10)= 0.00027089125140004655
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,019 INFO epoch # 1531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002668474750992443
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,043 INFO epoch # 1532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026906857009245346
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,068 INFO epoch # 1533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027944144219093557
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,092 INFO epoch # 1534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026823996304301543
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,117 INFO epoch # 1535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027514131340597355
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,141 INFO epoch # 1536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002756944217253476
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,167 INFO epoch # 1537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002661800689695935
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,191 INFO epoch # 1538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000269153415242077
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,216 INFO epoch # 1539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002691814339154267
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,240 INFO epoch # 1540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002607908964689289
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:16,240 INFO *** epoch 1540, rolling-avg-loss (window=10)= 0.00026997390001529966
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,265 INFO epoch # 1541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002651734770292283
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,290 INFO epoch # 1542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002750803415048202
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,315 INFO epoch # 1543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002642191198122289
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,340 INFO epoch # 1544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002603019367338025
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,365 INFO epoch # 1545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002640997049249043
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,389 INFO epoch # 1546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002638363090227358
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,414 INFO epoch # 1547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002862320123572967
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,439 INFO epoch # 1548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002884261347519766
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,464 INFO epoch # 1549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000280827702330758
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,489 INFO epoch # 1550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002672877287425633
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:16,489 INFO *** epoch 1550, rolling-avg-loss (window=10)= 0.00027154844672103146
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,515 INFO epoch # 1551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026036675491403525
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,540 INFO epoch # 1552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002642446426242324
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,566 INFO epoch # 1553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002651453169944164
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,590 INFO epoch # 1554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026362503142861115
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,618 INFO epoch # 1555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002703743793452824
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,650 INFO epoch # 1556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026054955586524945
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,676 INFO epoch # 1557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002615354223442929
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,701 INFO epoch # 1558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026704181717442614
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,726 INFO epoch # 1559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027540355430184197
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,751 INFO epoch # 1560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026286067025336834
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:16,751 INFO *** epoch 1560, rolling-avg-loss (window=10)= 0.0002651147145245756
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,776 INFO epoch # 1561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002644701349449211
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,803 INFO epoch # 1562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026453374095061527
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,828 INFO epoch # 1563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026375489625414566
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,854 INFO epoch # 1564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002696995827136561
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,879 INFO epoch # 1565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027112701396358064
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,904 INFO epoch # 1566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002685107669094577
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,928 INFO epoch # 1567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027673930495179127
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,953 INFO epoch # 1568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002789225030158247
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:16,978 INFO epoch # 1569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002660643619102692
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,004 INFO epoch # 1570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026076326924209884
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:17,004 INFO *** epoch 1570, rolling-avg-loss (window=10)= 0.000268458557485636
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,030 INFO epoch # 1571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002737795522469761
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,056 INFO epoch # 1572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027097128123776724
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,081 INFO epoch # 1573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002701160165348223
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,105 INFO epoch # 1574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00028793263190891595
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,130 INFO epoch # 1575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002814819774357602
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,155 INFO epoch # 1576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002654436867617603
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,180 INFO epoch # 1577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027530637037541183
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,205 INFO epoch # 1578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002657215636905416
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,231 INFO epoch # 1579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002648621311111908
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,256 INFO epoch # 1580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026456387900647573
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:17,256 INFO *** epoch 1580, rolling-avg-loss (window=10)= 0.0002720179090309622
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,282 INFO epoch # 1581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000259220102036904
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,307 INFO epoch # 1582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002666708713929568
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,331 INFO epoch # 1583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002671733473627163
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,356 INFO epoch # 1584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002553483469195531
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,381 INFO epoch # 1585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026209802350162393
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,407 INFO epoch # 1586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025869449101654547
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,432 INFO epoch # 1587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002738072886131704
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,456 INFO epoch # 1588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002614783651162205
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,481 INFO epoch # 1589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002792700962995046
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,506 INFO epoch # 1590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027819499539743575
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:17,506 INFO *** epoch 1590, rolling-avg-loss (window=10)= 0.0002661955927656631
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,531 INFO epoch # 1591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026550614233461343
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,555 INFO epoch # 1592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026491552749316076
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,580 INFO epoch # 1593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026045188112350714
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,604 INFO epoch # 1594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002638807166866692
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,629 INFO epoch # 1595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026128736956577216
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,654 INFO epoch # 1596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002598389453071702
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,679 INFO epoch # 1597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026127960362438377
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,704 INFO epoch # 1598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002686039696397659
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,729 INFO epoch # 1599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002704623176084299
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,754 INFO epoch # 1600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000263048400977693
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:17,754 INFO *** epoch 1600, rolling-avg-loss (window=10)= 0.00026392748743611654
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,779 INFO epoch # 1601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026356789665961904
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,804 INFO epoch # 1602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026126251655763814
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,829 INFO epoch # 1603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026703579434459763
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,854 INFO epoch # 1604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002706372002389149
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,879 INFO epoch # 1605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026887409538695853
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,903 INFO epoch # 1606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026384540937475064
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,928 INFO epoch # 1607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002674695547154572
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,953 INFO epoch # 1608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002605363283821914
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:17,977 INFO epoch # 1609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002617980172674704
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,001 INFO epoch # 1610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002616167804392587
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:18,002 INFO *** epoch 1610, rolling-avg-loss (window=10)= 0.00026466435933668564
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,026 INFO epoch # 1611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002572113610637773
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,051 INFO epoch # 1612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025876346044242384
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,076 INFO epoch # 1613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026010837713589093
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,101 INFO epoch # 1614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002646636479767039
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,126 INFO epoch # 1615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002629900421847456
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,152 INFO epoch # 1616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025456886547285
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,177 INFO epoch # 1617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025623744565694194
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,202 INFO epoch # 1618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002579408396351417
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,227 INFO epoch # 1619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002633791631004507
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,252 INFO epoch # 1620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026500741369090975
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:18,252 INFO *** epoch 1620, rolling-avg-loss (window=10)= 0.00026008706163598364
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,277 INFO epoch # 1621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002650212130642363
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,303 INFO epoch # 1622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002655680578235271
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,328 INFO epoch # 1623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027619479847739315
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,353 INFO epoch # 1624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026583871804177763
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,378 INFO epoch # 1625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002605546028852197
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,403 INFO epoch # 1626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025339109769057747
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,427 INFO epoch # 1627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002552844238899914
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,453 INFO epoch # 1628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026153517620904103
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,478 INFO epoch # 1629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002688840343450595
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,505 INFO epoch # 1630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025685821664020686
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:18,505 INFO *** epoch 1630, rolling-avg-loss (window=10)= 0.000262913033906703
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,531 INFO epoch # 1631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002652342348093433
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,557 INFO epoch # 1632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002713616118333968
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,582 INFO epoch # 1633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026298612356185914
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,606 INFO epoch # 1634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026035877485160847
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,631 INFO epoch # 1635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025931142819380124
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,655 INFO epoch # 1636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002582697251844885
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,679 INFO epoch # 1637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002581294353668844
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,704 INFO epoch # 1638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026105924667457916
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,728 INFO epoch # 1639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026035804725584705
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,753 INFO epoch # 1640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025766049241480814
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:18,753 INFO *** epoch 1640, rolling-avg-loss (window=10)= 0.0002614729120146616
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,778 INFO epoch # 1641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025690289330668745
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,802 INFO epoch # 1642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026163452421315017
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,827 INFO epoch # 1643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026069262239616366
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,851 INFO epoch # 1644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002680080061379288
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,876 INFO epoch # 1645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002670944626775703
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,901 INFO epoch # 1646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002635319214147915
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,926 INFO epoch # 1647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026053493534813503
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,951 INFO epoch # 1648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025294065417256204
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:18,976 INFO epoch # 1649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002607692509107957
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,001 INFO epoch # 1650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026966364551169267
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:19,001 INFO *** epoch 1650, rolling-avg-loss (window=10)= 0.00026217729160894776
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,027 INFO epoch # 1651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027401854708192073
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,052 INFO epoch # 1652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026578724438357834
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,077 INFO epoch # 1653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026231377872006436
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,103 INFO epoch # 1654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027591398268538925
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,128 INFO epoch # 1655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002589659775756965
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,153 INFO epoch # 1656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002579941570210005
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,178 INFO epoch # 1657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025629244441266306
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,202 INFO epoch # 1658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002577033781562932
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,227 INFO epoch # 1659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026277681193148184
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,252 INFO epoch # 1660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002533976780666437
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:19,252 INFO *** epoch 1660, rolling-avg-loss (window=10)= 0.0002625164000034731
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,276 INFO epoch # 1661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002634307390378256
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,300 INFO epoch # 1662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026134223278079715
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,326 INFO epoch # 1663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002557319270895927
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,352 INFO epoch # 1664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026026789335966376
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,378 INFO epoch # 1665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025868240627460184
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,403 INFO epoch # 1666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000269272290045462
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,428 INFO epoch # 1667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002554215486660334
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,452 INFO epoch # 1668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026203413581242784
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,476 INFO epoch # 1669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002605754256364889
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,501 INFO epoch # 1670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000262416222955965
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:19,501 INFO *** epoch 1670, rolling-avg-loss (window=10)= 0.0002609174821658858
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,526 INFO epoch # 1671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025299350776809403
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,551 INFO epoch # 1672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002667590212825287
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,576 INFO epoch # 1673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002718762353262199
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,601 INFO epoch # 1674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026679021289705164
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,625 INFO epoch # 1675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002545142500561529
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,650 INFO epoch # 1676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002560813362444086
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,674 INFO epoch # 1677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002679361533539902
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,699 INFO epoch # 1678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026618564172947246
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,723 INFO epoch # 1679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002568362318145643
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,747 INFO epoch # 1680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002586120814417622
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:19,748 INFO *** epoch 1680, rolling-avg-loss (window=10)= 0.00026185846719142456
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,773 INFO epoch # 1681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002705652682509806
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,798 INFO epoch # 1682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027098760287377183
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,823 INFO epoch # 1683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026498023528672224
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,848 INFO epoch # 1684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026079488889081405
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,873 INFO epoch # 1685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002564511756645516
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,899 INFO epoch # 1686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024977297268508536
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,924 INFO epoch # 1687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025824830414161883
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,949 INFO epoch # 1688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002600501842347772
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,974 INFO epoch # 1689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002560807759956723
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:19,999 INFO epoch # 1690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002612483186697188
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:20,000 INFO *** epoch 1690, rolling-avg-loss (window=10)= 0.00026091797266937126
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,027 INFO epoch # 1691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027259784172721475
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,052 INFO epoch # 1692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002515131075467382
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,077 INFO epoch # 1693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002614624600807604
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,101 INFO epoch # 1694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025456111637010636
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,125 INFO epoch # 1695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002547866459021212
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,150 INFO epoch # 1696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002502747031810161
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,174 INFO epoch # 1697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025891572378376233
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,199 INFO epoch # 1698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025617472087365706
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,223 INFO epoch # 1699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000275113049013141
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,248 INFO epoch # 1700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026594200483357004
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:20,248 INFO *** epoch 1700, rolling-avg-loss (window=10)= 0.00026013413733120877
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,272 INFO epoch # 1701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025231119153821575
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,297 INFO epoch # 1702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000251655511341856
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,321 INFO epoch # 1703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002536255566935454
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,346 INFO epoch # 1704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002552292823176166
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,372 INFO epoch # 1705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002576704240969515
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,397 INFO epoch # 1706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027238457405474035
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,422 INFO epoch # 1707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002572640219503748
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,446 INFO epoch # 1708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002509603515915972
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,471 INFO epoch # 1709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002552212841692381
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,495 INFO epoch # 1710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024988899843135314
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:20,495 INFO *** epoch 1710, rolling-avg-loss (window=10)= 0.00025562111961854883
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,520 INFO epoch # 1711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002589837344463116
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,545 INFO epoch # 1712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002504590611871598
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,569 INFO epoch # 1713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026634745299816134
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,594 INFO epoch # 1714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002582420393342285
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,619 INFO epoch # 1715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002512737112868178
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,644 INFO epoch # 1716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00027054926176788284
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,669 INFO epoch # 1717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002724302005455164
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,695 INFO epoch # 1718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002618294146876516
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,719 INFO epoch # 1719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025234051447893893
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,744 INFO epoch # 1720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002540174559856366
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:20,744 INFO *** epoch 1720, rolling-avg-loss (window=10)= 0.0002596472846718305
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,768 INFO epoch # 1721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002641510054153124
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,793 INFO epoch # 1722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002562093265753772
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,818 INFO epoch # 1723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002587610550108366
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,843 INFO epoch # 1724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002556755918444001
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,869 INFO epoch # 1725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002546766030718572
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,894 INFO epoch # 1726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025595315091777595
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,918 INFO epoch # 1727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025333128765591285
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,943 INFO epoch # 1728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002503732726576605
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,968 INFO epoch # 1729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025746328111771226
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:20,992 INFO epoch # 1730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026967755111400037
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:20,992 INFO *** epoch 1730, rolling-avg-loss (window=10)= 0.00025762721253808454
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,019 INFO epoch # 1731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025223106827719933
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,044 INFO epoch # 1732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026221746749277894
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,070 INFO epoch # 1733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024737598786097286
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,095 INFO epoch # 1734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002538098978610443
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,120 INFO epoch # 1735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002724374461519931
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,145 INFO epoch # 1736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025859576126094905
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,170 INFO epoch # 1737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025268751134197893
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,194 INFO epoch # 1738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002574162175213652
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,219 INFO epoch # 1739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025364856784498054
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,243 INFO epoch # 1740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024907736660679803
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:21,244 INFO *** epoch 1740, rolling-avg-loss (window=10)= 0.0002559497292220061
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,268 INFO epoch # 1741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002631633804412559
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,293 INFO epoch # 1742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026835879710103785
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,318 INFO epoch # 1743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026282109777509634
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,343 INFO epoch # 1744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025728100923255883
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,368 INFO epoch # 1745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025527838401363367
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,393 INFO epoch # 1746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025588232487539896
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,418 INFO epoch # 1747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002516732987714931
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,443 INFO epoch # 1748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025333044821828867
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,468 INFO epoch # 1749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002597435874382167
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,493 INFO epoch # 1750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002624289656523615
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:21,493 INFO *** epoch 1750, rolling-avg-loss (window=10)= 0.00025899612935193416
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,519 INFO epoch # 1751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000249391447661245
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,544 INFO epoch # 1752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002608623245448273
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,569 INFO epoch # 1753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025778826176454977
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,594 INFO epoch # 1754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024793562936663093
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,619 INFO epoch # 1755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002505072947574912
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,643 INFO epoch # 1756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025600951547468347
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,669 INFO epoch # 1757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025215573114110155
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,694 INFO epoch # 1758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002517653897354778
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,719 INFO epoch # 1759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002520549832427475
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,745 INFO epoch # 1760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002478889177187479
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:21,745 INFO *** epoch 1760, rolling-avg-loss (window=10)= 0.00025263594954075025
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,770 INFO epoch # 1761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002525666973919475
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,795 INFO epoch # 1762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002492045944174086
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,820 INFO epoch # 1763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002515156863540012
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,844 INFO epoch # 1764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002521695955822777
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,869 INFO epoch # 1765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002495450940581837
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,895 INFO epoch # 1766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002459759241901338
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,920 INFO epoch # 1767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002544909445402612
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,945 INFO epoch # 1768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025298434712957323
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,970 INFO epoch # 1769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025856275377529006
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:21,996 INFO epoch # 1770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002492235636704468
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:21,996 INFO *** epoch 1770, rolling-avg-loss (window=10)= 0.00025162392011095235
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,021 INFO epoch # 1771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002505404070169399
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,046 INFO epoch # 1772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025140686754769246
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,071 INFO epoch # 1773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002526278277010923
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,096 INFO epoch # 1774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025015446153702213
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,121 INFO epoch # 1775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025720152812677303
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,146 INFO epoch # 1776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002594045454835785
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,171 INFO epoch # 1777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002553095273989519
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,196 INFO epoch # 1778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002548593252348448
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,220 INFO epoch # 1779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025710793706821276
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,245 INFO epoch # 1780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025350420349111246
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:22,245 INFO *** epoch 1780, rolling-avg-loss (window=10)= 0.000254211663060622
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,270 INFO epoch # 1781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026250869725897376
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,295 INFO epoch # 1782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002543373806734702
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,320 INFO epoch # 1783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026378630386066754
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,346 INFO epoch # 1784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002519869212327259
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,371 INFO epoch # 1785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025772818917175756
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,396 INFO epoch # 1786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002539869994507171
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,421 INFO epoch # 1787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002484200335207528
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,445 INFO epoch # 1788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002564070106018335
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,470 INFO epoch # 1789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002544201979097644
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,495 INFO epoch # 1790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025389518289427673
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:22,495 INFO *** epoch 1790, rolling-avg-loss (window=10)= 0.00025574769165749394
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,522 INFO epoch # 1791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000255445485319277
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,548 INFO epoch # 1792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026152157141560956
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,573 INFO epoch # 1793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025532355814773057
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,598 INFO epoch # 1794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024854036122893116
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,624 INFO epoch # 1795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025312073204466806
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,649 INFO epoch # 1796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025713720325646656
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,674 INFO epoch # 1797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002532809737853573
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,698 INFO epoch # 1798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025887841121792525
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,723 INFO epoch # 1799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002483650190697517
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,748 INFO epoch # 1800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002540781279094517
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:22,748 INFO *** epoch 1800, rolling-avg-loss (window=10)= 0.0002545691443395169
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,774 INFO epoch # 1801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024573140253778545
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,799 INFO epoch # 1802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024762341740175285
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,824 INFO epoch # 1803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002499372419801408
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,849 INFO epoch # 1804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002564834135617795
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,875 INFO epoch # 1805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025759288089050513
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,900 INFO epoch # 1806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002451868407661095
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,924 INFO epoch # 1807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002526635744808508
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,949 INFO epoch # 1808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025409802287218295
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,974 INFO epoch # 1809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002465403636701272
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:22,999 INFO epoch # 1810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025181975174096545
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:22,999 INFO *** epoch 1810, rolling-avg-loss (window=10)= 0.00025076769099021995
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,026 INFO epoch # 1811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024686834159573273
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,050 INFO epoch # 1812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002514945328584872
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,075 INFO epoch # 1813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025012442347360775
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,100 INFO epoch # 1814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024977903272623995
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,125 INFO epoch # 1815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002462978125549853
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,149 INFO epoch # 1816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024100277015739785
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,174 INFO epoch # 1817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025017428644267577
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,200 INFO epoch # 1818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024259358760900796
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,225 INFO epoch # 1819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024413586631583583
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,250 INFO epoch # 1820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024654932640260084
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:23,250 INFO *** epoch 1820, rolling-avg-loss (window=10)= 0.0002469019980136571
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,275 INFO epoch # 1821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002472003535201241
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,300 INFO epoch # 1822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002600575075900581
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,325 INFO epoch # 1823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024405513229014885
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,350 INFO epoch # 1824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002488093153390634
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,375 INFO epoch # 1825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002467025586936091
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,400 INFO epoch # 1826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002525820174404154
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,425 INFO epoch # 1827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002512133094049724
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,450 INFO epoch # 1828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002484277622508151
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,475 INFO epoch # 1829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024717692036314734
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,499 INFO epoch # 1830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002570107886900327
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:23,500 INFO *** epoch 1830, rolling-avg-loss (window=10)= 0.00025032356655823863
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,525 INFO epoch # 1831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024368663954581798
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,550 INFO epoch # 1832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002496747549490205
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,575 INFO epoch # 1833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024550435316216733
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,600 INFO epoch # 1834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023818137664680501
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,625 INFO epoch # 1835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002443498418350438
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,650 INFO epoch # 1836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002504829870304093
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,676 INFO epoch # 1837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002477716637908348
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,701 INFO epoch # 1838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024260401842184364
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,725 INFO epoch # 1839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024416533747820984
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,749 INFO epoch # 1840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024166293047268742
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:23,750 INFO *** epoch 1840, rolling-avg-loss (window=10)= 0.0002448083903332839
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,775 INFO epoch # 1841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024327332045816417
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,800 INFO epoch # 1842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002446426128569458
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,826 INFO epoch # 1843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002515293065308859
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,850 INFO epoch # 1844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025244572544969355
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,876 INFO epoch # 1845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002497741364225346
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,901 INFO epoch # 1846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002463523778715171
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,926 INFO epoch # 1847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002650700883740293
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,950 INFO epoch # 1848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002533809687260405
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:23,975 INFO epoch # 1849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024784989280825746
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,000 INFO epoch # 1850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002452230478021582
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:24,000 INFO *** epoch 1850, rolling-avg-loss (window=10)= 0.00024995414773002264
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,026 INFO epoch # 1851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024532290844945235
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,051 INFO epoch # 1852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024634274616671195
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,076 INFO epoch # 1853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023995926811559392
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,101 INFO epoch # 1854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002566454223207464
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,125 INFO epoch # 1855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002500288184299799
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,151 INFO epoch # 1856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002615176120473604
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,176 INFO epoch # 1857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023985245290012765
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,201 INFO epoch # 1858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002665085861476005
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,226 INFO epoch # 1859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026745134777489253
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,251 INFO epoch # 1860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026109536203356196
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:24,251 INFO *** epoch 1860, rolling-avg-loss (window=10)= 0.0002534724524386027
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,276 INFO epoch # 1861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025121627446164245
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,301 INFO epoch # 1862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024472554401394777
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,327 INFO epoch # 1863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025081007624976336
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,351 INFO epoch # 1864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023958858904994226
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,376 INFO epoch # 1865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024004631912768153
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,401 INFO epoch # 1866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025285214672164457
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,426 INFO epoch # 1867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024080866590208772
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,452 INFO epoch # 1868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002412306092862439
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,477 INFO epoch # 1869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025036774833487083
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,504 INFO epoch # 1870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024678603118185753
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:24,504 INFO *** epoch 1870, rolling-avg-loss (window=10)= 0.0002458432004329682
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,529 INFO epoch # 1871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024223307118518279
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,554 INFO epoch # 1872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024368850905115584
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,579 INFO epoch # 1873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024389145941573328
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,603 INFO epoch # 1874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002455918542442045
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,629 INFO epoch # 1875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002526359137846157
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,654 INFO epoch # 1876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002486836556012609
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,679 INFO epoch # 1877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024938264624714584
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,704 INFO epoch # 1878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025440995377721263
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,729 INFO epoch # 1879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002444934775537279
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,754 INFO epoch # 1880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002556805731728673
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:24,754 INFO *** epoch 1880, rolling-avg-loss (window=10)= 0.00024806911140331063
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,778 INFO epoch # 1881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025630183039798536
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,803 INFO epoch # 1882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025813574590886543
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,828 INFO epoch # 1883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002632212033079538
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,854 INFO epoch # 1884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024306658119063027
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,879 INFO epoch # 1885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002562476681695054
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,904 INFO epoch # 1886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024943370039441756
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,930 INFO epoch # 1887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024092650128295645
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,955 INFO epoch # 1888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024261270986504054
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:24,980 INFO epoch # 1889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024974892606093946
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,004 INFO epoch # 1890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002435396217541503
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:25,004 INFO *** epoch 1890, rolling-avg-loss (window=10)= 0.00025032344883324444
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,029 INFO epoch # 1891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025296500160558417
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,054 INFO epoch # 1892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002494550987778764
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,079 INFO epoch # 1893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024313029737511117
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,104 INFO epoch # 1894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024359977147209326
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,129 INFO epoch # 1895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024635004873354254
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,153 INFO epoch # 1896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024323344737890043
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,179 INFO epoch # 1897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023988043477792028
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,203 INFO epoch # 1898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002511476946113232
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,227 INFO epoch # 1899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024394716560241899
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,252 INFO epoch # 1900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025456096877210906
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:25,252 INFO *** epoch 1900, rolling-avg-loss (window=10)= 0.0002468269929106879
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,277 INFO epoch # 1901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002490053560385214
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,302 INFO epoch # 1902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002477767918857613
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,328 INFO epoch # 1903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024300363896015499
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,353 INFO epoch # 1904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024731343624547923
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,378 INFO epoch # 1905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025142950741740476
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,402 INFO epoch # 1906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023885959297850995
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,427 INFO epoch # 1907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000247939806389955
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,451 INFO epoch # 1908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002459566861424329
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,476 INFO epoch # 1909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024212455838486287
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,501 INFO epoch # 1910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000244073843350634
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:25,501 INFO *** epoch 1910, rolling-avg-loss (window=10)= 0.00024574832177937165
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,528 INFO epoch # 1911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024343493610753546
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,553 INFO epoch # 1912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002583613182650879
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,577 INFO epoch # 1913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002457408693901795
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,602 INFO epoch # 1914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002469670775878642
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,627 INFO epoch # 1915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025525081956792356
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,651 INFO epoch # 1916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024416421594131474
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,677 INFO epoch # 1917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024536680991462035
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,702 INFO epoch # 1918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025409199068755176
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,736 INFO epoch # 1919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024537764984415843
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,766 INFO epoch # 1920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002472653928894683
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:25,767 INFO *** epoch 1920, rolling-avg-loss (window=10)= 0.0002486021080195704
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,791 INFO epoch # 1921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002573898637950021
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,816 INFO epoch # 1922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024176955817633176
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,840 INFO epoch # 1923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002423550802632235
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,865 INFO epoch # 1924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024079359094944915
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,889 INFO epoch # 1925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024910310977637503
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,914 INFO epoch # 1926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024849130235712175
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,939 INFO epoch # 1927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002379465237027034
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,963 INFO epoch # 1928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023985879233805462
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:25,988 INFO epoch # 1929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002466870723375385
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,014 INFO epoch # 1930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023938747097937657
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:26,014 INFO *** epoch 1930, rolling-avg-loss (window=10)= 0.00024437823646751767
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,039 INFO epoch # 1931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002427536391353767
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,064 INFO epoch # 1932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024451706412946804
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,088 INFO epoch # 1933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024666930777519675
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,113 INFO epoch # 1934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002411259136611729
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,138 INFO epoch # 1935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023690834683033504
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,163 INFO epoch # 1936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024226501991506666
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,188 INFO epoch # 1937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024164973146980628
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,213 INFO epoch # 1938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000246217950812674
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,238 INFO epoch # 1939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024009061183148463
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,263 INFO epoch # 1940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023973736013951046
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:26,263 INFO *** epoch 1940, rolling-avg-loss (window=10)= 0.00024219349457000914
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,289 INFO epoch # 1941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023668681886712355
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,314 INFO epoch # 1942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024101572946944673
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,340 INFO epoch # 1943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023695938892030557
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,364 INFO epoch # 1944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023654220858588816
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,389 INFO epoch # 1945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023913556526947234
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,414 INFO epoch # 1946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002455109830147454
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,438 INFO epoch # 1947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023286614783241282
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,463 INFO epoch # 1948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025532379243356577
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,488 INFO epoch # 1949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002400679488865925
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,514 INFO epoch # 1950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002442802641391089
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:26,515 INFO *** epoch 1950, rolling-avg-loss (window=10)= 0.00024083888474186617
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,539 INFO epoch # 1951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024340157313937588
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,564 INFO epoch # 1952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002472048598325013
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,589 INFO epoch # 1953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023977444631912348
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,614 INFO epoch # 1954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024399114814254322
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,639 INFO epoch # 1955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024076569804622392
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,664 INFO epoch # 1956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002509173662734351
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,689 INFO epoch # 1957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002500523459665211
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,714 INFO epoch # 1958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002483980913114335
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,739 INFO epoch # 1959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002399756367334963
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,765 INFO epoch # 1960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023314567806664854
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:26,765 INFO *** epoch 1960, rolling-avg-loss (window=10)= 0.0002437626843831302
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,790 INFO epoch # 1961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023510817118221894
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,814 INFO epoch # 1962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024039274742660512
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,839 INFO epoch # 1963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023455695170144153
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,864 INFO epoch # 1964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002420284309274783
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,889 INFO epoch # 1965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024836959284065023
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,914 INFO epoch # 1966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023531211274010794
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,938 INFO epoch # 1967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023569666019674124
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,963 INFO epoch # 1968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002354155304991374
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:26,988 INFO epoch # 1969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002403529651928693
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,014 INFO epoch # 1970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002321491589620044
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:27,014 INFO *** epoch 1970, rolling-avg-loss (window=10)= 0.00023793823216692543
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,040 INFO epoch # 1971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023548909999330394
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,065 INFO epoch # 1972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024070521716826728
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,090 INFO epoch # 1973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025220832675196497
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,115 INFO epoch # 1974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002411664236985546
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,140 INFO epoch # 1975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024115439487754236
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,164 INFO epoch # 1976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00026331165593416827
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,189 INFO epoch # 1977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002520404178150264
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,213 INFO epoch # 1978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025662312816296303
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,239 INFO epoch # 1979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024670829776109063
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,264 INFO epoch # 1980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002382906706770882
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:27,264 INFO *** epoch 1980, rolling-avg-loss (window=10)= 0.00024676976328399697
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,290 INFO epoch # 1981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024686001207945604
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,315 INFO epoch # 1982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024069566924091694
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,339 INFO epoch # 1983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025232267466240697
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,364 INFO epoch # 1984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000245243998818166
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,390 INFO epoch # 1985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00025233764754375445
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,415 INFO epoch # 1986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024598183517809957
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,439 INFO epoch # 1987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002340441006318932
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,465 INFO epoch # 1988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023487234970421663
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,489 INFO epoch # 1989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023375043466720464
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,515 INFO epoch # 1990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.000239189134611349
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:27,515 INFO *** epoch 1990, rolling-avg-loss (window=10)= 0.00024252978571374633
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,541 INFO epoch # 1991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002383335416587735
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,565 INFO epoch # 1992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023532022397765624
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,590 INFO epoch # 1993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023746840306557715
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,615 INFO epoch # 1994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024776187825149723
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,640 INFO epoch # 1995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024794465841426115
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,666 INFO epoch # 1996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00024355828583273772
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,690 INFO epoch # 1997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002450730497782518
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,715 INFO epoch # 1998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00023931668963216778
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,741 INFO epoch # 1999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00022824292204209735
[experiments_sandbox.py:955 -   <module>()] 2023-04-27 15:08:27,765 INFO epoch # 2000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0002397178308456205
[experiments_sandbox.py:967 -   <module>()] 2023-04-27 15:08:27,765 INFO *** epoch 2000, rolling-avg-loss (window=10)= 0.00024027374834986403
[experiments_sandbox.py:975 -   <module>()] 2023-04-27 15:08:27,765 INFO training time in seconds = 50
[experiments_sandbox.py:992 -   <module>()] 2023-04-27 15:08:27,882 INFO train-epochs-loss curve df :
[experiments_sandbox.py:993 -   <module>()] 2023-04-27 15:08:27,889 INFO 
     epochs      loss
0        10  1.657092
1        20  0.219349
2        30  0.123544
3        40  0.093062
4        50  0.066288
5        60  0.022543
6        70  0.037229
7        80  0.015192
8        90  0.053300
9       100  0.049059
10      110  0.029238
11      120  0.011228
12      130  0.025600
13      140  0.010418
14      150  0.012519
15      160  0.004314
16      170  0.010086
17      180  0.006211
18      190  0.003792
19      200  0.003869
20      210  0.003543
21      220  0.002822
22      230  0.001655
23      240  0.001827
24      250  0.001846
25      260  0.002013
26      270  0.001468
27      280  0.001304
28      290  0.001095
29      300  0.001028
30      310  0.000867
31      320  0.000915
32      330  0.000791
33      340  0.000819
34      350  0.000764
35      360  0.000737
36      370  0.000750
37      380  0.000781
38      390  0.000705
39      400  0.000619
40      410  0.000641
41      420  0.000572
42      430  0.000545
43      440  0.000506
44      450  0.000522
45      460  0.000477
46      470  0.000442
47      480  0.000447
48      490  0.000432
49      500  0.000427
50      510  0.000418
51      520  0.000398
52      530  0.000390
53      540  0.000383
54      550  0.000384
55      560  0.000371
56      570  0.000362
57      580  0.000355
58      590  0.000355
59      600  0.000356
60      610  0.000352
61      620  0.000354
62      630  0.000355
63      640  0.000350
64      650  0.000353
65      660  0.000348
66      670  0.000354
67      680  0.000346
68      690  0.000342
69      700  0.000344
70      710  0.000342
71      720  0.000343
72      730  0.000341
73      740  0.000340
74      750  0.000336
75      760  0.000346
76      770  0.000338
77      780  0.000339
78      790  0.000338
79      800  0.000334
80      810  0.000326
81      820  0.000332
82      830  0.000333
83      840  0.000331
84      850  0.000333
85      860  0.000332
86      870  0.000324
87      880  0.000327
88      890  0.000324
89      900  0.000321
90      910  0.000324
91      920  0.000327
92      930  0.000322
93      940  0.000321
94      950  0.000321
95      960  0.000324
96      970  0.000318
97      980  0.000323
98      990  0.000314
99     1000  0.000311
100    1010  0.000316
101    1020  0.000315
102    1030  0.000315
103    1040  0.000308
104    1050  0.000310
105    1060  0.000315
106    1070  0.000310
107    1080  0.000307
108    1090  0.000304
109    1100  0.000304
110    1110  0.000303
111    1120  0.000302
112    1130  0.000303
113    1140  0.000301
114    1150  0.000305
115    1160  0.000299
116    1170  0.000299
117    1180  0.000299
118    1190  0.000294
119    1200  0.000299
120    1210  0.000294
121    1220  0.000292
122    1230  0.000298
123    1240  0.000289
124    1250  0.000293
125    1260  0.000290
126    1270  0.000291
127    1280  0.000291
128    1290  0.000291
129    1300  0.000291
130    1310  0.000286
131    1320  0.000289
132    1330  0.000282
133    1340  0.000290
134    1350  0.000290
135    1360  0.000280
136    1370  0.000281
137    1380  0.000282
138    1390  0.000282
139    1400  0.000279
140    1410  0.000283
141    1420  0.000280
142    1430  0.000278
143    1440  0.000279
144    1450  0.000274
145    1460  0.000275
146    1470  0.000276
147    1480  0.000267
148    1490  0.000272
149    1500  0.000274
150    1510  0.000271
151    1520  0.000269
152    1530  0.000271
153    1540  0.000270
154    1550  0.000272
155    1560  0.000265
156    1570  0.000268
157    1580  0.000272
158    1590  0.000266
159    1600  0.000264
160    1610  0.000265
161    1620  0.000260
162    1630  0.000263
163    1640  0.000261
164    1650  0.000262
165    1660  0.000263
166    1670  0.000261
167    1680  0.000262
168    1690  0.000261
169    1700  0.000260
170    1710  0.000256
171    1720  0.000260
172    1730  0.000258
173    1740  0.000256
174    1750  0.000259
175    1760  0.000253
176    1770  0.000252
177    1780  0.000254
178    1790  0.000256
179    1800  0.000255
180    1810  0.000251
181    1820  0.000247
182    1830  0.000250
183    1840  0.000245
184    1850  0.000250
185    1860  0.000253
186    1870  0.000246
187    1880  0.000248
188    1890  0.000250
189    1900  0.000247
190    1910  0.000246
191    1920  0.000249
192    1930  0.000244
193    1940  0.000242
194    1950  0.000241
195    1960  0.000244
196    1970  0.000238
197    1980  0.000247
198    1990  0.000243
199    2000  0.000240
[experiments_sandbox.py:995 -   <module>()] 2023-04-27 15:08:27,889 INFO Model parameters after training
[experiments_sandbox.py:996 -   <module>()] 2023-04-27 15:08:27,889 INFO Model = NNmodel
[experiments_sandbox.py:998 -   <module>()] 2023-04-27 15:08:27,890 INFO net.0.weight = Parameter containing:
tensor([[ 0.7419, -0.5192,  0.1207],
        [ 0.7980, -0.5112, -0.1283],
        [ 1.1300, -0.2436,  0.4188],
        [ 0.7223, -0.5352, -0.0936],
        [ 0.2453, -0.5220,  1.7652],
        [-1.1011, -0.7599,  0.1945],
        [ 1.2128, -0.1082,  0.2437],
        [-1.0040,  0.6873,  0.0076],
        [-1.0594, -0.7508,  0.1658],
        [ 0.3716,  0.0447, -0.4023],
        [ 0.5947, -0.5585,  0.1566],
        [-0.6976, -0.1880,  0.0460],
        [-0.5688,  0.4814, -0.1006],
        [-1.1499, -0.7890,  0.1924],
        [ 0.7737,  0.2110, -0.3222],
        [ 0.2683, -0.5310,  1.7211],
        [-0.8480, -0.7072,  0.1671],
        [ 0.7859, -0.1378, -0.2347],
        [ 0.8731, -0.5886,  0.0114],
        [-0.4515,  0.5527,  0.1662],
        [ 0.4240,  0.5192,  0.1255],
        [ 0.7229, -0.2116, -0.1263],
        [-0.0299, -0.4268, -0.0979],
        [ 0.8448, -0.3212,  0.2303],
        [-0.8713,  0.1256,  0.2502],
        [ 0.8180, -0.5333, -0.1227],
        [ 0.8488,  0.6480, -0.1288],
        [-0.7942,  0.0366,  0.2657],
        [ 0.8106, -0.2344,  0.2366],
        [-0.0909, -0.5398, -0.0563],
        [-0.9250,  0.6053,  0.2081],
        [ 0.8144, -0.0951,  0.3644],
        [ 0.8661,  0.5928, -0.1293],
        [-0.9152,  0.0326, -0.4896],
        [ 1.1009,  0.7634, -0.1960],
        [ 0.4196, -0.5672, -0.0646],
        [-1.0406,  0.0156,  0.3341],
        [-0.9488,  0.2140, -0.3446],
        [ 0.7155,  0.0278,  0.3384],
        [-0.8181, -0.0253,  0.1380],
        [-0.8956,  0.6189,  0.0413],
        [ 0.8133,  2.3457,  1.9124],
        [-0.1644, -0.5932,  0.0032],
        [ 0.7814, -0.4270,  0.0297],
        [-0.5401, -0.6107,  0.1332],
        [-0.9542,  0.8210,  0.0579],
        [ 0.4973,  0.1070,  0.2459],
        [ 0.1558,  0.1900, -0.4409],
        [ 0.8408, -0.2334,  0.3023],
        [-0.9562,  0.1771, -0.1838]], requires_grad=True)
[experiments_sandbox.py:998 -   <module>()] 2023-04-27 15:08:27,891 INFO net.0.bias = Parameter containing:
tensor([-2.6183, -1.2827, -0.0770,  1.7943, -0.1467,  0.7626,  3.8556,  0.1058,
         3.4753,  1.6244, -2.0201, -1.4031,  0.7192, -0.2794, -2.3107, -0.1275,
        -2.7559,  0.8630,  0.8378,  0.1584,  2.0637, -2.4294, -0.5533,  2.2325,
         0.1913, -2.1073, -2.2069,  0.3194,  1.3171,  1.5527,  3.1260, -2.1830,
         1.1234, -2.9581, -1.8380,  0.9705,  1.5344,  1.1104,  1.0800,  2.7001,
         1.1054,  0.9859,  0.1191,  1.5158, -1.1697, -3.6329, -0.2060,  0.0337,
         0.7559, -3.1301], requires_grad=True)
[experiments_sandbox.py:998 -   <module>()] 2023-04-27 15:08:27,891 INFO net.2.weight = Parameter containing:
tensor([[-3.4423e+00, -2.5089e+00, -9.4155e-01, -2.7618e+00,  1.7169e-01,
         -8.9585e-01, -5.1689e-01,  2.2786e+00, -1.1748e+00, -1.3366e+00,
         -2.7563e+00,  1.3506e-01,  3.3834e+00, -6.4639e-01, -1.7236e-01,
         -1.8211e-01, -1.1044e+00, -1.2145e+00, -3.0034e+00,  1.2856e+00,
          2.0971e+00, -1.1267e+00, -1.7515e+00, -2.4887e+00,  9.6038e-01,
         -1.7894e+00,  7.6022e-01,  4.9821e-01, -1.6621e+00, -2.3702e+00,
          3.3272e+00, -1.2142e+00,  1.0253e+00, -7.8422e-02,  8.3536e-01,
         -3.2616e+00,  6.5099e-01,  1.7072e+00,  4.8479e-01,  1.0294e+00,
          1.6402e+00, -9.6917e-03, -9.5457e-01, -2.0680e+00, -8.7579e-01,
          4.3732e+00,  1.1098e+00, -2.7317e-01, -1.8447e+00,  1.8906e+00],
        [ 2.8003e+00,  2.2821e+00,  1.8870e+00,  2.8128e+00,  8.4018e-02,
         -3.5912e+00,  3.4237e+00, -2.5841e+00, -3.6993e+00,  4.5065e-01,
          1.4163e+00, -3.3598e+00, -1.8875e+00, -2.6069e+00,  5.0835e+00,
         -8.1616e-02, -4.1174e+00,  2.5658e+00,  3.5070e+00, -8.2602e-01,
          2.9956e+00,  3.6526e+00, -4.8221e-01,  3.7534e+00, -2.7295e+00,
          3.1883e+00,  3.0938e+00, -1.0596e+00,  2.9568e+00, -5.3025e-01,
         -3.8297e+00,  3.1912e+00,  4.3633e+00, -3.2251e+00,  3.0933e+00,
          1.2275e+00, -2.4830e+00, -3.1677e+00,  3.4210e+00, -3.1672e+00,
         -2.6115e+00,  4.0480e-03, -8.0584e-01,  2.4818e+00, -1.8930e+00,
         -3.0324e+00,  2.6570e+00,  2.9371e-01,  2.6573e+00, -4.1066e+00],
        [-8.9674e-01,  4.9277e-01, -8.0506e-01,  1.3097e+00,  1.7687e-01,
         -5.8787e-01,  6.6277e-01, -1.6078e-01, -1.6174e+00,  1.7196e+00,
         -1.6568e+00, -7.0316e-01,  2.7952e-01, -3.5960e-01,  1.9601e+00,
         -1.7095e-01,  6.6722e-01,  1.0596e+00,  2.1355e-01, -4.2911e-01,
         -1.1293e+00, -1.1574e+00,  1.1964e+00, -5.5235e-01, -8.0311e-01,
          2.7270e-01,  5.7035e-01, -3.8070e-01, -3.3693e-01, -8.6662e-01,
         -5.5309e-01, -2.2851e+00,  1.4845e-01,  2.3844e+00,  8.3031e-01,
          1.4622e+00, -5.2180e-01,  1.5963e+00, -2.0274e+00,  4.0083e-01,
          1.6458e-01,  6.7887e-03, -2.5284e-01,  4.4508e-01, -4.8860e-02,
         -2.5912e+00, -1.5452e+00,  6.2397e-01, -1.1114e+00,  1.0344e+00]],
       requires_grad=True)
[experiments_sandbox.py:998 -   <module>()] 2023-04-27 15:08:27,892 INFO net.2.bias = Parameter containing:
tensor([-0.3436, -0.4759, -0.5269], requires_grad=True)
[experiments_sandbox.py:1001 -   <module>()] 2023-04-27 15:08:27,892 INFO Out-of sample batch-test
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,892 INFO test-batch  # 0 => test-loss = 0.0027358047664165497
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,893 INFO test-batch  # 1 => test-loss = 0.00018255547911394387
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,893 INFO test-batch  # 2 => test-loss = 0.0014116066740825772
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,893 INFO test-batch  # 3 => test-loss = 0.0002669728419277817
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,893 INFO test-batch  # 4 => test-loss = 0.00014449442096520215
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,894 INFO test-batch  # 5 => test-loss = 0.003188030794262886
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,894 INFO test-batch  # 6 => test-loss = 0.001348761492408812
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,895 INFO test-batch  # 7 => test-loss = 0.0006002577138133347
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,895 INFO test-batch  # 8 => test-loss = 0.0008942871354520321
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,895 INFO test-batch  # 9 => test-loss = 0.0002971666108351201
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,895 INFO test-batch  # 10 => test-loss = 0.005947931203991175
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,896 INFO test-batch  # 11 => test-loss = 0.0001372965198243037
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,896 INFO test-batch  # 12 => test-loss = 0.0017574606463313103
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,896 INFO test-batch  # 13 => test-loss = 0.00031753836083225906
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,897 INFO test-batch  # 14 => test-loss = 0.0003710965102072805
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,897 INFO test-batch  # 15 => test-loss = 0.0003950512036681175
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,897 INFO test-batch  # 16 => test-loss = 0.00022117649496067315
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,898 INFO test-batch  # 17 => test-loss = 0.0002826692652888596
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,898 INFO test-batch  # 18 => test-loss = 0.000604187254793942
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,898 INFO test-batch  # 19 => test-loss = 0.0005101212882436812
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,898 INFO test-batch  # 20 => test-loss = 0.000430923217209056
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,899 INFO test-batch  # 21 => test-loss = 0.001693926751613617
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,899 INFO test-batch  # 22 => test-loss = 0.0001039645285345614
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,899 INFO test-batch  # 23 => test-loss = 0.00016904713993426412
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,900 INFO test-batch  # 24 => test-loss = 0.02730087749660015
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,900 INFO test-batch  # 25 => test-loss = 0.000363069586455822
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,900 INFO test-batch  # 26 => test-loss = 0.002759694354608655
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,901 INFO test-batch  # 27 => test-loss = 0.0004073298769071698
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,901 INFO test-batch  # 28 => test-loss = 0.007580562960356474
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,901 INFO test-batch  # 29 => test-loss = 0.00017333438154309988
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,901 INFO test-batch  # 30 => test-loss = 0.0008893458289094269
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,902 INFO test-batch  # 31 => test-loss = 0.00011010918387910351
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,902 INFO test-batch  # 32 => test-loss = 0.004586616065353155
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,902 INFO test-batch  # 33 => test-loss = 0.00025790699874050915
[experiments_sandbox.py:1006 -   <module>()] 2023-04-27 15:08:27,903 INFO test-batch  # 34 => test-loss = 0.0004195085493847728
