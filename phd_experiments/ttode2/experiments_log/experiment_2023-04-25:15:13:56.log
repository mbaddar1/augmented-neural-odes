[experiments_sandbox.py:736 -   <module>()] 2023-04-25 15:13:56,225 INFO SEED = 42
[experiments_sandbox.py:817 -   <module>()] 2023-04-25 15:13:56,226 INFO model = ***
TTRBF
order = 4
num_rbf_centers= 16
tt_rank = 3
dim = 4
learnable_numel = 184
***

[experiments_sandbox.py:818 -   <module>()] 2023-04-25 15:13:56,226 INFO optimizer  = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.2
    maximize: False
    weight_decay: 0
)
[experiments_sandbox.py:826 -   <module>()] 2023-04-25 15:13:56,226 INFO lr_scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd8113987f0>
[experiments_sandbox.py:829 -   <module>()] 2023-04-25 15:13:56,226 INFO Normalize-Data-source-X-train = False
[experiments_sandbox.py:830 -   <module>()] 2023-04-25 15:13:56,226 INFO Normalize-Data-source-Y-train = False
[experiments_sandbox.py:831 -   <module>()] 2023-04-25 15:13:56,226 INFO Normalize-Data-source-X-test = False
[experiments_sandbox.py:832 -   <module>()] 2023-04-25 15:13:56,226 INFO Normalize-Data-source-Y-test = False
[experiments_sandbox.py:861 -   <module>()] 2023-04-25 15:13:56,228 INFO train-dataset = 
***
Lorenz-System
N = 1000rho = 28
sigma = 10
beta = 2.6666666666666665
normalize_X = Falsenormalize_Y = False****

[experiments_sandbox.py:862 -   <module>()] 2023-04-25 15:13:56,228 INFO test-dataset = 
***
Lorenz-System
N = 1000rho = 28
sigma = 10
beta = 2.6666666666666665
normalize_X = Falsenormalize_Y = False****

[experiments_sandbox.py:863 -   <module>()] 2023-04-25 15:13:56,228 INFO train-epochs = 10000
[experiments_sandbox.py:867 -   <module>()] 2023-04-25 15:13:56,228 INFO Input batch normalization = False
[experiments_sandbox.py:868 -   <module>()] 2023-04-25 15:13:56,228 INFO Output Normalization = None
[experiments_sandbox.py:869 -   <module>()] 2023-04-25 15:13:56,228 INFO Gradient-clipping max-norm = 10
[experiments_sandbox.py:871 -   <module>()] 2023-04-25 15:13:56,228 INFO epochs_losses_window = 10
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:57,886 INFO epoch # 0 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 263.3439893722534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:57,945 INFO epoch # 1 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 149.17440235614777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,006 INFO epoch # 2 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 80.08605694770813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,074 INFO epoch # 3 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 39.2865432202816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,136 INFO epoch # 4 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 37.75939255952835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,198 INFO epoch # 5 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 26.5242567807436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,259 INFO epoch # 6 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 37.67802952229977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,327 INFO epoch # 7 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 23.356761634349823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,391 INFO epoch # 8 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 18.118511341512203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,455 INFO epoch # 9 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 19.20671136677265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,518 INFO epoch # 10 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 20.7045526355505
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:13:58,518 INFO *** epoch 10, rolling-avg-loss (window=10)= 45.18952183648944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,589 INFO epoch # 11 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 24.938966408371925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,654 INFO epoch # 12 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 20.269248634576797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,716 INFO epoch # 13 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 28.30510865151882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,778 INFO epoch # 14 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 19.237145572900772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,845 INFO epoch # 15 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 30.631050765514374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,906 INFO epoch # 16 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 21.939633294939995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:58,967 INFO epoch # 17 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 17.914427019655704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,029 INFO epoch # 18 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 29.510752335190773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,095 INFO epoch # 19 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 16.479061514139175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,157 INFO epoch # 20 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 17.932330161333084
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:13:59,157 INFO *** epoch 20, rolling-avg-loss (window=10)= 22.71577243581414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,218 INFO epoch # 21 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 21.946411192417145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,279 INFO epoch # 22 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 25.227585345506668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,346 INFO epoch # 23 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 21.396420776844025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,408 INFO epoch # 24 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 38.06232011318207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,469 INFO epoch # 25 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 25.257411509752274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,530 INFO epoch # 26 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 25.85709074139595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,601 INFO epoch # 27 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 24.863126933574677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,662 INFO epoch # 28 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 16.88442227244377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,723 INFO epoch # 29 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 20.67984788119793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,784 INFO epoch # 30 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.16 -loss = 23.936391074210405
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:13:59,784 INFO *** epoch 30, rolling-avg-loss (window=10)= 24.411102784052492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,854 INFO epoch # 31 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 20.68055672198534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,915 INFO epoch # 32 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 14.258046064525843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:13:59,977 INFO epoch # 33 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 18.80564009398222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,037 INFO epoch # 34 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 14.472274757921696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,106 INFO epoch # 35 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 10.676246337592602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,167 INFO epoch # 36 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 18.461586594581604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,229 INFO epoch # 37 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 13.54172445833683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,289 INFO epoch # 38 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 13.389322608709335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,357 INFO epoch # 39 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 13.558671094477177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,419 INFO epoch # 40 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 10.778470322489738
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:00,419 INFO *** epoch 40, rolling-avg-loss (window=10)= 14.86225390546024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,481 INFO epoch # 41 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 10.486613251268864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,542 INFO epoch # 42 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 13.716488726437092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,620 INFO epoch # 43 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 14.27602869272232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,682 INFO epoch # 44 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 19.133734077215195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,743 INFO epoch # 45 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 19.135213792324066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,805 INFO epoch # 46 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 12.596610054373741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,878 INFO epoch # 47 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 13.979683972895145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:00,940 INFO epoch # 48 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 11.96633118391037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,002 INFO epoch # 49 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 9.545938178896904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,064 INFO epoch # 50 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 17.947517216205597
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:01,064 INFO *** epoch 50, rolling-avg-loss (window=10)= 14.27841591462493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,141 INFO epoch # 51 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 9.945469446480274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,202 INFO epoch # 52 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 18.904602527618408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,264 INFO epoch # 53 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 16.572835609316826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,326 INFO epoch # 54 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 12.969123370945454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,393 INFO epoch # 55 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 12.054825067520142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,454 INFO epoch # 56 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 12.324611462652683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,516 INFO epoch # 57 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 18.095659479498863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,579 INFO epoch # 58 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 13.289030864834785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,647 INFO epoch # 59 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 11.465814426541328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,711 INFO epoch # 60 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.128 -loss = 11.92705599218607
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:01,711 INFO *** epoch 60, rolling-avg-loss (window=10)= 13.754902824759483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,773 INFO epoch # 61 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 10.695442534983158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,835 INFO epoch # 62 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 21.33452118933201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,906 INFO epoch # 63 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 11.848514847457409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:01,968 INFO epoch # 64 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 6.62610724568367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,032 INFO epoch # 65 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 10.28332781791687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,096 INFO epoch # 66 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 11.535161592066288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,167 INFO epoch # 67 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 12.684692308306694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,229 INFO epoch # 68 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 11.488030634820461
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,292 INFO epoch # 69 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 12.075167998671532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,355 INFO epoch # 70 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 8.688257589936256
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:02,355 INFO *** epoch 70, rolling-avg-loss (window=10)= 11.725922375917435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,432 INFO epoch # 71 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 10.754121705889702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,494 INFO epoch # 72 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 16.324689500033855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,557 INFO epoch # 73 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 16.833222061395645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,621 INFO epoch # 74 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 8.243569124490023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,697 INFO epoch # 75 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.1024 -loss = 12.76284395903349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,760 INFO epoch # 76 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 11.297752149403095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,822 INFO epoch # 77 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 7.013610273599625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,885 INFO epoch # 78 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 6.617639541625977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:02,951 INFO epoch # 79 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 4.641673449426889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,013 INFO epoch # 80 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 6.415369026362896
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:03,014 INFO *** epoch 80, rolling-avg-loss (window=10)= 10.09044907912612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,080 INFO epoch # 81 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 12.230003878474236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,142 INFO epoch # 82 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 10.486541856080294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,210 INFO epoch # 83 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 10.015097305178642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,272 INFO epoch # 84 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 7.0135829001665115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,335 INFO epoch # 85 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 6.804282154887915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,398 INFO epoch # 86 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 6.822145529091358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,467 INFO epoch # 87 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 6.977527063339949
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,530 INFO epoch # 88 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 8.864777989685535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,599 INFO epoch # 89 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 8.621303044259548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,664 INFO epoch # 90 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.08192 -loss = 5.319364503026009
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:03,664 INFO *** epoch 90, rolling-avg-loss (window=10)= 8.315462622419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,741 INFO epoch # 91 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 6.774970170110464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,804 INFO epoch # 92 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 5.632687110453844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,866 INFO epoch # 93 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 5.432926349341869
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:03,928 INFO epoch # 94 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 5.062381448224187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,002 INFO epoch # 95 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 4.900692164897919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,069 INFO epoch # 96 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 5.413375087082386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,134 INFO epoch # 97 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 5.393233697861433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,198 INFO epoch # 98 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 3.7639059349894524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,267 INFO epoch # 99 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 6.213270675390959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,329 INFO epoch # 100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 3.9965339340269566
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:04,329 INFO *** epoch 100, rolling-avg-loss (window=10)= 5.258397657237947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,394 INFO epoch # 101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 4.796304423362017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,458 INFO epoch # 102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 8.784808792173862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,528 INFO epoch # 103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 6.258004918694496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,595 INFO epoch # 104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 5.721479531377554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,662 INFO epoch # 105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 5.163920506834984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,727 INFO epoch # 106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 10.24876993149519
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,796 INFO epoch # 107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 4.005325093865395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,858 INFO epoch # 108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 6.624049015343189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,922 INFO epoch # 109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.065536 -loss = 5.464742787182331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:04,989 INFO epoch # 110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.2914819419384003
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:04,989 INFO *** epoch 110, rolling-avg-loss (window=10)= 6.035888694226742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,054 INFO epoch # 111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.0244647096842527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,125 INFO epoch # 112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.3599494956433773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,191 INFO epoch # 113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.010227844119072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,259 INFO epoch # 114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.6467107385396957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,322 INFO epoch # 115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.384266745299101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,390 INFO epoch # 116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 4.5269127786159515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,459 INFO epoch # 117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 4.562169706448913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,523 INFO epoch # 118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 4.832196190953255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,588 INFO epoch # 119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 4.179146379232407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,654 INFO epoch # 120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.8035971894860268
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:05,654 INFO *** epoch 120, rolling-avg-loss (window=10)= 3.832964177802205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,725 INFO epoch # 121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.8150871619582176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,791 INFO epoch # 122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.664994118735194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,854 INFO epoch # 123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.9516293480992317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,918 INFO epoch # 124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 2.9210117757320404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:05,985 INFO epoch # 125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.603151874616742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,060 INFO epoch # 126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 5.307989161461592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,134 INFO epoch # 127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.158609177917242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,202 INFO epoch # 128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.015271198004484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,270 INFO epoch # 129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.3329685013741255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,335 INFO epoch # 130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.522204753011465
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:06,335 INFO *** epoch 130, rolling-avg-loss (window=10)= 3.6292917070910335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,401 INFO epoch # 131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.5588020645081997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,469 INFO epoch # 132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 2.9665833096951246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,543 INFO epoch # 133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.6358029171824455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,612 INFO epoch # 134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 3.7638645842671394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,683 INFO epoch # 135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.0524288 -loss = 3.3143376037478447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,752 INFO epoch # 136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 2.979295315220952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,820 INFO epoch # 137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 1.8198848310858011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,888 INFO epoch # 138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 3.074414672330022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:06,958 INFO epoch # 139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 3.146712301298976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,025 INFO epoch # 140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 2.3491271222010255
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:07,026 INFO *** epoch 140, rolling-avg-loss (window=10)= 3.060882472153753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,095 INFO epoch # 141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 3.297541406005621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,163 INFO epoch # 142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 3.7705077081918716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,231 INFO epoch # 143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 2.705303205177188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,297 INFO epoch # 144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 2.3075504805892706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,369 INFO epoch # 145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 2.4069264251738787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,437 INFO epoch # 146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 3.0222762282937765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,503 INFO epoch # 147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 2.6604425590485334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,573 INFO epoch # 148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.04194304 -loss = 3.616449736058712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,639 INFO epoch # 149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.9536192128434777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,702 INFO epoch # 150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 2.2905115876346827
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:07,702 INFO *** epoch 150, rolling-avg-loss (window=10)= 2.803112854901701
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,772 INFO epoch # 151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.7659817729145288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,839 INFO epoch # 152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.8039005734026432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,902 INFO epoch # 153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 2.4621133906766772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:07,971 INFO epoch # 154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.4936559852212667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,036 INFO epoch # 155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.78496328368783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,103 INFO epoch # 156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.9495847653597593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,172 INFO epoch # 157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.7353968396782875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,236 INFO epoch # 158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.7673968989402056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,302 INFO epoch # 159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 2.475085273385048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,373 INFO epoch # 160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.9874034160748124
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:08,373 INFO *** epoch 160, rolling-avg-loss (window=10)= 1.922548219934106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,440 INFO epoch # 161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.6339967362582684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,507 INFO epoch # 162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.807503373362124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,572 INFO epoch # 163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.9084849450737238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,642 INFO epoch # 164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 1.7854958325624466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,706 INFO epoch # 165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.03355443 -loss = 1.5302791651338339
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,769 INFO epoch # 166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.6533899400383234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,833 INFO epoch # 167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.4553317530080676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,900 INFO epoch # 168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.3533612797036767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:08,964 INFO epoch # 169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.483301293104887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,027 INFO epoch # 170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.8631922453641891
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:09,027 INFO *** epoch 170, rolling-avg-loss (window=10)= 1.647433656360954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,092 INFO epoch # 171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.9850972862914205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,160 INFO epoch # 172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.6236959919333458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,226 INFO epoch # 173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.3259022179991007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,289 INFO epoch # 174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.8302921410650015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,353 INFO epoch # 175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.2161851497367024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,419 INFO epoch # 176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.6968352608382702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,482 INFO epoch # 177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.89232343621552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,545 INFO epoch # 178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.3212844915688038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,610 INFO epoch # 179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.4202986909076571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,676 INFO epoch # 180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.3903645323589444
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:09,677 INFO *** epoch 180, rolling-avg-loss (window=10)= 1.5702279198914766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,741 INFO epoch # 181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.4094689525663853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,804 INFO epoch # 182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.3166436273604631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,867 INFO epoch # 183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.7133141020312905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,934 INFO epoch # 184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.209307854063809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:09,998 INFO epoch # 185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.1207783063873649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,060 INFO epoch # 186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.1269465377554297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,126 INFO epoch # 187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.7160352803766727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,193 INFO epoch # 188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.953074125573039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,256 INFO epoch # 189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.8364157378673553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,319 INFO epoch # 190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.5022884150967002
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:10,319 INFO *** epoch 190, rolling-avg-loss (window=10)= 1.490427293907851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,383 INFO epoch # 191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.7498643323779106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,450 INFO epoch # 192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.3684257352724671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,514 INFO epoch # 193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.8563004918396473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,580 INFO epoch # 194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.1986203342676163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,644 INFO epoch # 195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 1.3902069488540292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,711 INFO epoch # 196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.02684355 -loss = 1.5369812920689583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,775 INFO epoch # 197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.1179353334009647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,840 INFO epoch # 198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.1394177936017513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,904 INFO epoch # 199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.3777375598438084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:10,970 INFO epoch # 200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.2903248891234398
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:10,970 INFO *** epoch 200, rolling-avg-loss (window=10)= 1.4025814710650593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,034 INFO epoch # 201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.1302607962861657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,102 INFO epoch # 202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.3876626836135983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,181 INFO epoch # 203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.4204101236537099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,250 INFO epoch # 204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.9761676043272018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,317 INFO epoch # 205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.9514640746638179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,381 INFO epoch # 206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.2519070906564593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,447 INFO epoch # 207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.9038131162524223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,511 INFO epoch # 208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.147784125059843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,575 INFO epoch # 209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.4037379324436188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,644 INFO epoch # 210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.112046130001545
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:11,644 INFO *** epoch 210, rolling-avg-loss (window=10)= 1.1685253676958383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,708 INFO epoch # 211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.4295632410794497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,771 INFO epoch # 212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.1930311564356089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,834 INFO epoch # 213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.526286544278264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,903 INFO epoch # 214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.1603815276175737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:11,969 INFO epoch # 215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.0120176319032907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,032 INFO epoch # 216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.2105548353865743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,096 INFO epoch # 217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 1.117778042331338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,162 INFO epoch # 218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02147484 -loss = 1.3189687454141676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,225 INFO epoch # 219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 1.2771320305764675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,287 INFO epoch # 220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8908956618979573
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:12,287 INFO *** epoch 220, rolling-avg-loss (window=10)= 1.2136609416920692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,350 INFO epoch # 221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8801535507664084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,416 INFO epoch # 222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8084009131416678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,481 INFO epoch # 223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8743435610085726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,544 INFO epoch # 224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.9167577531188726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,609 INFO epoch # 225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 1.0481220576912165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,678 INFO epoch # 226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8089540731161833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,741 INFO epoch # 227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.876244924031198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,804 INFO epoch # 228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8265406843274832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,867 INFO epoch # 229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8902436997741461
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,935 INFO epoch # 230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.773748588282615
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:12,935 INFO *** epoch 230, rolling-avg-loss (window=10)= 0.8703509805258364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:12,999 INFO epoch # 231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.7970911022275686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,062 INFO epoch # 232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8037695460952818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,128 INFO epoch # 233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8792218510061502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,196 INFO epoch # 234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.9617095738649368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,260 INFO epoch # 235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.9916624967008829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,323 INFO epoch # 236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.6692542568780482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,387 INFO epoch # 237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.7178588169626892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,452 INFO epoch # 238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8079167837277055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,515 INFO epoch # 239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.7065244084224105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,583 INFO epoch # 240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8450902309268713
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:13,583 INFO *** epoch 240, rolling-avg-loss (window=10)= 0.8180099066812545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,647 INFO epoch # 241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.8204125883057714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,714 INFO epoch # 242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.7513628760352731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,778 INFO epoch # 243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.9492841400206089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,841 INFO epoch # 244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 1.1097989492118359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,903 INFO epoch # 245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.7770319683477283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:13,970 INFO epoch # 246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.7801048262044787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,034 INFO epoch # 247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.01717987 -loss = 0.8348008543252945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,096 INFO epoch # 248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6343423207290471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,161 INFO epoch # 249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6188659239560366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,228 INFO epoch # 250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6689107259735465
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:14,228 INFO *** epoch 250, rolling-avg-loss (window=10)= 0.7944915173109621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,292 INFO epoch # 251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.77199930511415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,354 INFO epoch # 252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6987759014591575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,418 INFO epoch # 253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.7950469478964806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,484 INFO epoch # 254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.9891228107735515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,547 INFO epoch # 255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.5815696050412953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,612 INFO epoch # 256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.8272546818479896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,678 INFO epoch # 257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6518375100567937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,744 INFO epoch # 258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6564646167680621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,808 INFO epoch # 259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6252754013985395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,872 INFO epoch # 260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.8447698284871876
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:14,872 INFO *** epoch 260, rolling-avg-loss (window=10)= 0.7442116608843208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:14,936 INFO epoch # 261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6891793231479824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,002 INFO epoch # 262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.5626000529155135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,072 INFO epoch # 263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.670879390090704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,139 INFO epoch # 264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.8182504298165441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,205 INFO epoch # 265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.7692678291350603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,273 INFO epoch # 266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.8200273821130395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,339 INFO epoch # 267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.5874215709045529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,405 INFO epoch # 268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6724673323333263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,472 INFO epoch # 269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.7134653860703111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,538 INFO epoch # 270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.5728981141000986
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:15,538 INFO *** epoch 270, rolling-avg-loss (window=10)= 0.6876456810627133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,607 INFO epoch # 271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6397863044403493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,675 INFO epoch # 272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.47846989682875574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,745 INFO epoch # 273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.586109041236341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,812 INFO epoch # 274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.7517471611499786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,878 INFO epoch # 275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.7932637957856059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:15,948 INFO epoch # 276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.5785575998015702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,018 INFO epoch # 277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.728609106503427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,087 INFO epoch # 278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6822640364989638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,155 INFO epoch # 279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6718914741650224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,221 INFO epoch # 280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.7254406614229083
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:16,221 INFO *** epoch 280, rolling-avg-loss (window=10)= 0.6636139077832922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,287 INFO epoch # 281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.690050529781729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,357 INFO epoch # 282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.6010384317487478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,424 INFO epoch # 283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.0137439 -loss = 0.6517347041517496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,490 INFO epoch # 284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.6028088354505599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,557 INFO epoch # 285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.5513455187901855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,639 INFO epoch # 286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.5354377133771777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,719 INFO epoch # 287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.5072330208495259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,787 INFO epoch # 288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.5675988998264074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,853 INFO epoch # 289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.4927875907160342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,917 INFO epoch # 290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.5411052163690329
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:16,917 INFO *** epoch 290, rolling-avg-loss (window=10)= 0.5741140461061149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:16,991 INFO epoch # 291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.6048198696225882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,062 INFO epoch # 292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.5434658103622496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,129 INFO epoch # 293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.46343871857970953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,197 INFO epoch # 294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.6569139882922173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,264 INFO epoch # 295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.6124411625787616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,329 INFO epoch # 296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.5380869139917195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,395 INFO epoch # 297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.5120916804298759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,460 INFO epoch # 298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.6850003562867641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,526 INFO epoch # 299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.6246100226417184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,600 INFO epoch # 300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.5855764660518616
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:17,600 INFO *** epoch 300, rolling-avg-loss (window=10)= 0.5826444988837466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,667 INFO epoch # 301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.6442420589737594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,731 INFO epoch # 302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.5870386739261448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,798 INFO epoch # 303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.5656717387028039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,865 INFO epoch # 304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.01099512 -loss = 0.5503725493326783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,936 INFO epoch # 305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.45328523917123675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:17,999 INFO epoch # 306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.4164615999907255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,073 INFO epoch # 307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.5330043425783515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,139 INFO epoch # 308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.5407090489752591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,203 INFO epoch # 309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.46696538059040904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,269 INFO epoch # 310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.5106591540388763
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:18,270 INFO *** epoch 310, rolling-avg-loss (window=10)= 0.5268409786280245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,336 INFO epoch # 311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.4952195999212563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,407 INFO epoch # 312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.4358325586654246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,485 INFO epoch # 313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.5054311519488692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,550 INFO epoch # 314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.43173438031226397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,618 INFO epoch # 315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.42624594224616885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,684 INFO epoch # 316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.496956467628479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,750 INFO epoch # 317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.00879609 -loss = 0.4570594569668174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,815 INFO epoch # 318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.42755367141216993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,883 INFO epoch # 319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.45961120165884495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:18,950 INFO epoch # 320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.4259287496097386
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:18,950 INFO *** epoch 320, rolling-avg-loss (window=10)= 0.45615731803700327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,014 INFO epoch # 321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.5486251185648143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,081 INFO epoch # 322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.4289032300002873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,153 INFO epoch # 323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.4206532237585634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,218 INFO epoch # 324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.4046586463227868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,283 INFO epoch # 325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.3896760316565633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,352 INFO epoch # 326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.42955901799723506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,418 INFO epoch # 327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.45001161517575383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,482 INFO epoch # 328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.41060923552140594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,553 INFO epoch # 329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.3929877656046301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,622 INFO epoch # 330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.4145261854864657
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:19,622 INFO *** epoch 330, rolling-avg-loss (window=10)= 0.4290210070088506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,688 INFO epoch # 331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.40853655990213156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,755 INFO epoch # 332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.3739154702052474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,821 INFO epoch # 333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.3942903825081885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,886 INFO epoch # 334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.41135621117427945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:19,955 INFO epoch # 335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.3887039814144373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,021 INFO epoch # 336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.5474278382025659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,086 INFO epoch # 337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.4613905497826636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,152 INFO epoch # 338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.4832577658817172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,220 INFO epoch # 339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.4721266208216548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,286 INFO epoch # 340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.4085839265026152
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:20,286 INFO *** epoch 340, rolling-avg-loss (window=10)= 0.4349589306395501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,357 INFO epoch # 341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.4033959126099944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,424 INFO epoch # 342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.417077190708369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,489 INFO epoch # 343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00703687 -loss = 0.40539243910461664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,554 INFO epoch # 344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3640972897410393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,625 INFO epoch # 345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3682475979439914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,690 INFO epoch # 346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.4341164818033576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,755 INFO epoch # 347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3912785556167364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,820 INFO epoch # 348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3555328715592623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,887 INFO epoch # 349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3877358539029956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:20,952 INFO epoch # 350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.381269003264606
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:20,952 INFO *** epoch 350, rolling-avg-loss (window=10)= 0.39081431962549684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,018 INFO epoch # 351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3827569717541337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,085 INFO epoch # 352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3663332078140229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,153 INFO epoch # 353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.34982859529554844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,219 INFO epoch # 354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.4191129645332694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,284 INFO epoch # 355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.34775827499106526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,353 INFO epoch # 356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3603916182182729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,418 INFO epoch # 357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.400601917412132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,483 INFO epoch # 358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.33028479339554906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,553 INFO epoch # 359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3614545175805688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,622 INFO epoch # 360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.34605179145000875
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:21,622 INFO *** epoch 360, rolling-avg-loss (window=10)= 0.3664574652444571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,690 INFO epoch # 361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.36453352263197303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,758 INFO epoch # 362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.38317151833325624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,827 INFO epoch # 363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.35207474837079644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,894 INFO epoch # 364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.4188467958010733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:21,961 INFO epoch # 365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.331137013155967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,029 INFO epoch # 366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3705466277897358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,095 INFO epoch # 367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3877798868343234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,160 INFO epoch # 368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.3398567591793835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,230 INFO epoch # 369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.0056295 -loss = 0.35468021035194397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,295 INFO epoch # 370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.3274931765627116
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:22,295 INFO *** epoch 370, rolling-avg-loss (window=10)= 0.3630120259011164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,361 INFO epoch # 371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.31480373977683485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,431 INFO epoch # 372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.2965034036897123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,497 INFO epoch # 373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.31099238339811563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,567 INFO epoch # 374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.33026233222335577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,640 INFO epoch # 375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.3258324293419719
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,708 INFO epoch # 376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.33901766827329993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,777 INFO epoch # 377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.3241821867413819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,847 INFO epoch # 378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.30053511494770646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,917 INFO epoch # 379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.3756457488052547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:22,985 INFO epoch # 380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.2992354779271409
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:22,985 INFO *** epoch 380, rolling-avg-loss (window=10)= 0.32170104851247744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,056 INFO epoch # 381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.3259374871850014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,128 INFO epoch # 382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.3312139136251062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,198 INFO epoch # 383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0045036 -loss = 0.3108897383790463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,271 INFO epoch # 384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.28595112380571663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,341 INFO epoch # 385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.3165551507845521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,407 INFO epoch # 386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.32674742909148335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,474 INFO epoch # 387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.30708215991035104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,540 INFO epoch # 388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.3003193042241037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,609 INFO epoch # 389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.32682011486031115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,687 INFO epoch # 390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.3044210337102413
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:23,687 INFO *** epoch 390, rolling-avg-loss (window=10)= 0.31359374555759134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,754 INFO epoch # 391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.33628397644497454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,820 INFO epoch # 392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.28349846438504755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,891 INFO epoch # 393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.3336547249928117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:23,982 INFO epoch # 394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.3494802871719003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,046 INFO epoch # 395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.28459432255476713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,108 INFO epoch # 396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.33126721042208374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,173 INFO epoch # 397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.31005125236697495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,245 INFO epoch # 398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.2887677261605859
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,363 INFO epoch # 399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.3003861710894853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,451 INFO epoch # 400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.3063827631995082
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:24,451 INFO *** epoch 400, rolling-avg-loss (window=10)= 0.3124366898788139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,534 INFO epoch # 401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.296142514096573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,612 INFO epoch # 402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.2890707431361079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,691 INFO epoch # 403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.00360288 -loss = 0.298211365705356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,768 INFO epoch # 404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.2824305444955826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,837 INFO epoch # 405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.2840219303034246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,903 INFO epoch # 406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.2994991010054946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:24,973 INFO epoch # 407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.30442097736522555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,040 INFO epoch # 408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.3052725619636476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,104 INFO epoch # 409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.27400873741135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,170 INFO epoch # 410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.28347126254811883
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:25,170 INFO *** epoch 410, rolling-avg-loss (window=10)= 0.2916549738030881
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,234 INFO epoch # 411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.27637293841689825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,300 INFO epoch # 412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.27358225476928055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,366 INFO epoch # 413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.2947170860134065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,431 INFO epoch # 414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.29326636530458927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,495 INFO epoch # 415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.2747852667234838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,561 INFO epoch # 416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.28670066408813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,626 INFO epoch # 417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.2840555270668119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,693 INFO epoch # 418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.2766799151431769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,757 INFO epoch # 419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.30244765616953373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,821 INFO epoch # 420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.27144916728138924
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:25,821 INFO *** epoch 420, rolling-avg-loss (window=10)= 0.28340568409767003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,883 INFO epoch # 421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.2792118866927922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:25,946 INFO epoch # 422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.29700792813673615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,008 INFO epoch # 423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.2756597064435482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,071 INFO epoch # 424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.3110919902101159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,134 INFO epoch # 425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.285012747393921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,196 INFO epoch # 426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.31713345739990473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,259 INFO epoch # 427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.28822375112213194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,323 INFO epoch # 428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.2879275097511709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,386 INFO epoch # 429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.31120471144095063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,450 INFO epoch # 430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.2827491541393101
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:26,450 INFO *** epoch 430, rolling-avg-loss (window=10)= 0.29352228427305815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,512 INFO epoch # 431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.0028823 -loss = 0.2733042708132416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,577 INFO epoch # 432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.272379340371117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,643 INFO epoch # 433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.25299856159836054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,708 INFO epoch # 434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.26645291107706726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,772 INFO epoch # 435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.26361895096488297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,837 INFO epoch # 436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.26645716885104775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,905 INFO epoch # 437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.2902197469957173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:26,969 INFO epoch # 438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.2689596340060234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,031 INFO epoch # 439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.2585255962330848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,103 INFO epoch # 440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.26476160576567054
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:27,103 INFO *** epoch 440, rolling-avg-loss (window=10)= 0.2677677786676213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,175 INFO epoch # 441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.2623942664358765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,238 INFO epoch # 442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.25184882967732847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,300 INFO epoch # 443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.259481486864388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,364 INFO epoch # 444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.26899044355377555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,428 INFO epoch # 445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.2537649169098586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,495 INFO epoch # 446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.2776214824989438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,557 INFO epoch # 447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.2576959445141256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,624 INFO epoch # 448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.2737797927111387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,686 INFO epoch # 449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.2710186231415719
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,751 INFO epoch # 450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.2740097928326577
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:27,751 INFO *** epoch 450, rolling-avg-loss (window=10)= 0.2650605579139665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,813 INFO epoch # 451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.2651408379897475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,878 INFO epoch # 452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.26646027620881796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:27,940 INFO epoch # 453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.00230584 -loss = 0.26389486249536276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,005 INFO epoch # 454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.2513916911557317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,067 INFO epoch # 455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.2527372764889151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,131 INFO epoch # 456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.2491682495456189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,193 INFO epoch # 457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.24057371425442398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,258 INFO epoch # 458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.2501536167692393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,320 INFO epoch # 459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.2463517391588539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,384 INFO epoch # 460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.24809254007413983
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:28,385 INFO *** epoch 460, rolling-avg-loss (window=10)= 0.2533964804140851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,447 INFO epoch # 461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.25590685941278934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,511 INFO epoch # 462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.26138634723611176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,573 INFO epoch # 463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.2571701416745782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,639 INFO epoch # 464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.2565044097136706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,701 INFO epoch # 465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.25067938887514174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,763 INFO epoch # 466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.246352625079453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,825 INFO epoch # 467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.25129367830231786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,888 INFO epoch # 468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00184467 -loss = 0.2641718762461096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:28,950 INFO epoch # 469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.2342859678901732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,012 INFO epoch # 470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.24627455323934555
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:29,012 INFO *** epoch 470, rolling-avg-loss (window=10)= 0.2524025847669691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,076 INFO epoch # 471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.2495794885326177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,141 INFO epoch # 472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.22944954875856638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,203 INFO epoch # 473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.23703927895985544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,266 INFO epoch # 474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.25226455461233854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,329 INFO epoch # 475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.24555141455493867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,395 INFO epoch # 476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.23232250311411917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,457 INFO epoch # 477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.2385303487535566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,520 INFO epoch # 478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.23367482551839203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,583 INFO epoch # 479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.2550257712136954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,646 INFO epoch # 480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.22601831750944257
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:29,646 INFO *** epoch 480, rolling-avg-loss (window=10)= 0.23994560515275226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,708 INFO epoch # 481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.23425159766338766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,770 INFO epoch # 482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.2340051515493542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,832 INFO epoch # 483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.23730389610864222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,895 INFO epoch # 484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.23090054956264794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:29,957 INFO epoch # 485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.24637903412804008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,019 INFO epoch # 486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.23574993526563048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,081 INFO epoch # 487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.23228216683492064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,144 INFO epoch # 488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.23014494054950774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,207 INFO epoch # 489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.24755734065547585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,269 INFO epoch # 490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.2510484200902283
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:30,269 INFO *** epoch 490, rolling-avg-loss (window=10)= 0.23796230324078352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,332 INFO epoch # 491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00147574 -loss = 0.23808342753909528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,394 INFO epoch # 492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.24163925438188016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,457 INFO epoch # 493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.22360290680080652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,519 INFO epoch # 494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.22189944796264172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,582 INFO epoch # 495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.24366131285205483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,647 INFO epoch # 496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.2328945235349238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,728 INFO epoch # 497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.2302109010051936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,793 INFO epoch # 498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.22562550962902606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,856 INFO epoch # 499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.23395154904574156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,920 INFO epoch # 500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.23121144622564316
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:30,921 INFO *** epoch 500, rolling-avg-loss (window=10)= 0.23227802789770066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:30,983 INFO epoch # 501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.2209378480911255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,048 INFO epoch # 502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.228087815688923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,110 INFO epoch # 503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.2255339426919818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,173 INFO epoch # 504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.22760540526360273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,236 INFO epoch # 505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.22999983024783432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,298 INFO epoch # 506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.22472689463756979
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,363 INFO epoch # 507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.22762733860872686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,425 INFO epoch # 508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.22627135110087693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,489 INFO epoch # 509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.2322796806693077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,551 INFO epoch # 510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.22798463143408298
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:31,551 INFO *** epoch 510, rolling-avg-loss (window=10)= 0.22710547384340316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,616 INFO epoch # 511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.22184342378750443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,682 INFO epoch # 512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00118059 -loss = 0.22272988758049905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,745 INFO epoch # 513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.21839823806658387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,808 INFO epoch # 514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.22049057437106967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,872 INFO epoch # 515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.22289574705064297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,935 INFO epoch # 516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.22691432479768991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:31,998 INFO epoch # 517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.2153472532518208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,061 INFO epoch # 518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.21752289589494467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,127 INFO epoch # 519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.23887653602287173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,191 INFO epoch # 520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.2165450886823237
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:32,191 INFO *** epoch 520, rolling-avg-loss (window=10)= 0.22215639695059508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,254 INFO epoch # 521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.21787363989278674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,316 INFO epoch # 522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.21837304788641632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,381 INFO epoch # 523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.22906222427263856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,444 INFO epoch # 524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.22232881980016828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,506 INFO epoch # 525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.21829948271624744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,570 INFO epoch # 526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.22782160411588848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,635 INFO epoch # 527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.22770875319838524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,697 INFO epoch # 528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.001 -loss = 0.23660213989205658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,760 INFO epoch # 529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2217777823098004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,824 INFO epoch # 530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21636869106441736
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:32,824 INFO *** epoch 530, rolling-avg-loss (window=10)= 0.22362161851488055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,892 INFO epoch # 531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21574752405285835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:32,955 INFO epoch # 532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21696718805469573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,017 INFO epoch # 533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21885655168443918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,080 INFO epoch # 534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.22498928708955646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,149 INFO epoch # 535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2219789542723447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,212 INFO epoch # 536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2183912934269756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,276 INFO epoch # 537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.22873490722849965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,339 INFO epoch # 538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.22204481484368443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,404 INFO epoch # 539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21497365646064281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,468 INFO epoch # 540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2137858639471233
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:33,468 INFO *** epoch 540, rolling-avg-loss (window=10)= 0.219647004106082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,531 INFO epoch # 541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2202090765349567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,594 INFO epoch # 542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2123498423025012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,660 INFO epoch # 543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21137872396502644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,724 INFO epoch # 544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21237627905793488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,832 INFO epoch # 545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2182000612374395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,896 INFO epoch # 546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21163387736305594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:33,958 INFO epoch # 547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21914907451719046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,021 INFO epoch # 548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.22013899986632168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,086 INFO epoch # 549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21881999331526458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,150 INFO epoch # 550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2249963313806802
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:34,150 INFO *** epoch 550, rolling-avg-loss (window=10)= 0.21692522595403715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,216 INFO epoch # 551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2152996687218547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,282 INFO epoch # 552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.22380342613905668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,347 INFO epoch # 553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21398541028611362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,414 INFO epoch # 554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2144119192380458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,480 INFO epoch # 555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21885783225297928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,545 INFO epoch # 556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21998688927851617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,612 INFO epoch # 557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.23123731929808855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,678 INFO epoch # 558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2113324839156121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,743 INFO epoch # 559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21186260785907507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,808 INFO epoch # 560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21726245060563087
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:34,808 INFO *** epoch 560, rolling-avg-loss (window=10)= 0.2178040007594973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,873 INFO epoch # 561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2327678503934294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:34,939 INFO epoch # 562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.22048145858570933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,006 INFO epoch # 563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21417414373718202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,073 INFO epoch # 564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20996161946095526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,137 INFO epoch # 565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2153874901123345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,201 INFO epoch # 566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.22635769401676953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,265 INFO epoch # 567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21401203610002995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,329 INFO epoch # 568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21234999666921794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,393 INFO epoch # 569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2108119826298207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,457 INFO epoch # 570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21755635412409902
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:35,457 INFO *** epoch 570, rolling-avg-loss (window=10)= 0.21738606258295476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,520 INFO epoch # 571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2136422572657466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,598 INFO epoch # 572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21451206062920392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,673 INFO epoch # 573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20896061439998448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,736 INFO epoch # 574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20990711613558233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,800 INFO epoch # 575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21422876766882837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,879 INFO epoch # 576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2201650319620967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:35,945 INFO epoch # 577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21014273073524237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,009 INFO epoch # 578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21392845083028078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,072 INFO epoch # 579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20998977916315198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,145 INFO epoch # 580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21797481551766396
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:36,145 INFO *** epoch 580, rolling-avg-loss (window=10)= 0.21334516243077814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,214 INFO epoch # 581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2172546051442623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,277 INFO epoch # 582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21635315706953406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,341 INFO epoch # 583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2156991590745747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,405 INFO epoch # 584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20507589471526444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,472 INFO epoch # 585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21865352778695524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,536 INFO epoch # 586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21352166845463216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,601 INFO epoch # 587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20749359286855906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,667 INFO epoch # 588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21320905582979321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,734 INFO epoch # 589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2152408913243562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,798 INFO epoch # 590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21121160476468503
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:36,799 INFO *** epoch 590, rolling-avg-loss (window=10)= 0.21337131570326165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,863 INFO epoch # 591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2023285087198019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,927 INFO epoch # 592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21815409883856773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:36,991 INFO epoch # 593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20771924033761024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,055 INFO epoch # 594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21405091043561697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,135 INFO epoch # 595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2082860916852951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,206 INFO epoch # 596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21766468498390168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,270 INFO epoch # 597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21253986307419837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,349 INFO epoch # 598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21800710097886622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,416 INFO epoch # 599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21148819173686206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,482 INFO epoch # 600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20832260884344578
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:37,482 INFO *** epoch 600, rolling-avg-loss (window=10)= 0.2118561299634166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,548 INFO epoch # 601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20929822884500027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,613 INFO epoch # 602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21600198722444475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,696 INFO epoch # 603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2121223588474095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,772 INFO epoch # 604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21123686502687633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,836 INFO epoch # 605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.22384184156544507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,901 INFO epoch # 606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20164544018916786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:37,970 INFO epoch # 607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20551329664885998
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,072 INFO epoch # 608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21124502271413803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,145 INFO epoch # 609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2155382060445845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,210 INFO epoch # 610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2196067206095904
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:38,210 INFO *** epoch 610, rolling-avg-loss (window=10)= 0.21260499677155167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,278 INFO epoch # 611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2115562225226313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,353 INFO epoch # 612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21568807703442872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,426 INFO epoch # 613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21046158974058926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,493 INFO epoch # 614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2029653104254976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,566 INFO epoch # 615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2085090558975935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,635 INFO epoch # 616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21196397370658815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,704 INFO epoch # 617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20229752082377672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,768 INFO epoch # 618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20721083506941795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,837 INFO epoch # 619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20712735271081328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,906 INFO epoch # 620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20632286020554602
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:38,906 INFO *** epoch 620, rolling-avg-loss (window=10)= 0.20841027981368826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:38,973 INFO epoch # 621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2069494054885581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,041 INFO epoch # 622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20691070263274014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,109 INFO epoch # 623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20719338627532125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,174 INFO epoch # 624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.213365817675367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,242 INFO epoch # 625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20876118238084018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,306 INFO epoch # 626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20566748431883752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,376 INFO epoch # 627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20373183663468808
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,459 INFO epoch # 628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2049410871695727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,540 INFO epoch # 629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20936006610281765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,613 INFO epoch # 630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19959125691093504
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:39,614 INFO *** epoch 630, rolling-avg-loss (window=10)= 0.20664722255896778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,697 INFO epoch # 631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20268736826255918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,764 INFO epoch # 632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2096509076654911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,833 INFO epoch # 633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20974564272910357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,904 INFO epoch # 634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2117341326083988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:39,970 INFO epoch # 635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.210742813302204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,035 INFO epoch # 636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20354170352220535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,109 INFO epoch # 637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20139416167512536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,179 INFO epoch # 638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20646959939040244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,245 INFO epoch # 639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20832499559037387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,310 INFO epoch # 640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2012883743736893
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:40,310 INFO *** epoch 640, rolling-avg-loss (window=10)= 0.2065579699119553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,375 INFO epoch # 641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20482822973281145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,438 INFO epoch # 642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2039886130951345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,506 INFO epoch # 643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20167917548678815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,570 INFO epoch # 644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20570349553599954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,637 INFO epoch # 645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20407174318097532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,701 INFO epoch # 646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20262728701345623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,772 INFO epoch # 647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20764008606784046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,838 INFO epoch # 648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20131888706237078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,908 INFO epoch # 649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20423916913568974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:40,973 INFO epoch # 650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20642088213935494
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:40,973 INFO *** epoch 650, rolling-avg-loss (window=10)= 0.2042517568450421
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,039 INFO epoch # 651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20401079882867634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,108 INFO epoch # 652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2050578633788973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,176 INFO epoch # 653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2030641871970147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,240 INFO epoch # 654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20840551611036062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,315 INFO epoch # 655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.213132883887738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,383 INFO epoch # 656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19966095266863704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,452 INFO epoch # 657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20566424285061657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,521 INFO epoch # 658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20744172134436667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,591 INFO epoch # 659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19742029649205506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,657 INFO epoch # 660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2040622199419886
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:41,657 INFO *** epoch 660, rolling-avg-loss (window=10)= 0.2047920682700351
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,729 INFO epoch # 661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20493313833139837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,795 INFO epoch # 662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2034561769105494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,862 INFO epoch # 663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19885451183654368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,930 INFO epoch # 664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19668781151995063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:41,998 INFO epoch # 665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20185415935702622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,070 INFO epoch # 666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20263856486417353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,138 INFO epoch # 667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2037483926396817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,204 INFO epoch # 668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20224009011872113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,283 INFO epoch # 669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20496695465408266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,347 INFO epoch # 670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.21411282499320805
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:42,347 INFO *** epoch 670, rolling-avg-loss (window=10)= 0.20334926252253355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,416 INFO epoch # 671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20008568791672587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,483 INFO epoch # 672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1998507382813841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,552 INFO epoch # 673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2060635059606284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,622 INFO epoch # 674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20138933369889855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,691 INFO epoch # 675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20302143623121083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,757 INFO epoch # 676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20229067467153072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,826 INFO epoch # 677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19990647141821682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:42,936 INFO epoch # 678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19719088968122378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,040 INFO epoch # 679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2008432054426521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,104 INFO epoch # 680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20627039531245828
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:43,104 INFO *** epoch 680, rolling-avg-loss (window=10)= 0.20169123386149296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,168 INFO epoch # 681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20370343211106956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,234 INFO epoch # 682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2016165996901691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,299 INFO epoch # 683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19740173523314297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,365 INFO epoch # 684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2025829202029854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,432 INFO epoch # 685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20402315235696733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,496 INFO epoch # 686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20179748558439314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,566 INFO epoch # 687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20858397334814072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,631 INFO epoch # 688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19792816252447665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,697 INFO epoch # 689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20545739703811705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,762 INFO epoch # 690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19873546971939504
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:43,762 INFO *** epoch 690, rolling-avg-loss (window=10)= 0.2021830327808857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,830 INFO epoch # 691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.200673327955883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,898 INFO epoch # 692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19477005884982646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:43,969 INFO epoch # 693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2137984219007194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,040 INFO epoch # 694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20858539105392992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,109 INFO epoch # 695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2038292766083032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,183 INFO epoch # 696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20065942266955972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,268 INFO epoch # 697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19837275170721114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,336 INFO epoch # 698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20122747402638197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,402 INFO epoch # 699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20273088803514838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,468 INFO epoch # 700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19493006053380668
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:44,468 INFO *** epoch 700, rolling-avg-loss (window=10)= 0.20195770733407697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,533 INFO epoch # 701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19713900098577142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,599 INFO epoch # 702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2066713566891849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,664 INFO epoch # 703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19583195843733847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,732 INFO epoch # 704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19384789536707103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,798 INFO epoch # 705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19578177365474403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,865 INFO epoch # 706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20107429230120033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,933 INFO epoch # 707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19727198383770883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:44,999 INFO epoch # 708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20265111653134227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,066 INFO epoch # 709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20005929679609835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,131 INFO epoch # 710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19717206410132349
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:45,131 INFO *** epoch 710, rolling-avg-loss (window=10)= 0.19875007387017832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,199 INFO epoch # 711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2150489913765341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,264 INFO epoch # 712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20102687156759202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,330 INFO epoch # 713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1987845259718597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,399 INFO epoch # 714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19783368473872542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,465 INFO epoch # 715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19590976531617343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,533 INFO epoch # 716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20007035112939775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,599 INFO epoch # 717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19371967506594956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,666 INFO epoch # 718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19169755675829947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,734 INFO epoch # 719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19311842450406402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,800 INFO epoch # 720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19742222665809095
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:45,800 INFO *** epoch 720, rolling-avg-loss (window=10)= 0.19846320730866865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,866 INFO epoch # 721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19733996060676873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:45,934 INFO epoch # 722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1971832603449002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,009 INFO epoch # 723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20872482389677316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,107 INFO epoch # 724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2066341578029096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,177 INFO epoch # 725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19915584102272987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,245 INFO epoch # 726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20352305402047932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,320 INFO epoch # 727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2091240412555635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,394 INFO epoch # 728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20027910126373172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,461 INFO epoch # 729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20103529817424715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,526 INFO epoch # 730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20060281874611974
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:46,526 INFO *** epoch 730, rolling-avg-loss (window=10)= 0.2023602357134223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,591 INFO epoch # 731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20962080475874245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,665 INFO epoch # 732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20072903321124613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,736 INFO epoch # 733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20324453595094383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,803 INFO epoch # 734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1998115610331297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,879 INFO epoch # 735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2000705525279045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:46,955 INFO epoch # 736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20019541471265256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,028 INFO epoch # 737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1961981812492013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,113 INFO epoch # 738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19624999433290213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,194 INFO epoch # 739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19611159525811672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,271 INFO epoch # 740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20093168271705508
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:47,271 INFO *** epoch 740, rolling-avg-loss (window=10)= 0.20031633557518944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,341 INFO epoch # 741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19553866423666477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,416 INFO epoch # 742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19961843034252524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,487 INFO epoch # 743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19556527654640377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,560 INFO epoch # 744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19937023730017245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,639 INFO epoch # 745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1893416086677462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,711 INFO epoch # 746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1906687372829765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,781 INFO epoch # 747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18933964299503714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,852 INFO epoch # 748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19138649269007146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,923 INFO epoch # 749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.22136577020864934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:47,995 INFO epoch # 750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1968803487252444
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:47,995 INFO *** epoch 750, rolling-avg-loss (window=10)= 0.19690752089954913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,071 INFO epoch # 751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19300489453598857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,153 INFO epoch # 752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1947373595321551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,235 INFO epoch # 753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1963618837762624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,300 INFO epoch # 754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19987318152561784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,366 INFO epoch # 755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1965595306828618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,433 INFO epoch # 756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19043544447049499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,500 INFO epoch # 757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20537374238483608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,566 INFO epoch # 758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18772822292521596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,632 INFO epoch # 759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1917857681401074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,700 INFO epoch # 760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19160266988910735
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:48,700 INFO *** epoch 760, rolling-avg-loss (window=10)= 0.19474626978626475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,765 INFO epoch # 761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19118198449723423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,832 INFO epoch # 762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19126210024114698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,898 INFO epoch # 763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19913208158686757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:48,965 INFO epoch # 764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1980805853381753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,031 INFO epoch # 765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19439230766147375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,102 INFO epoch # 766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1876828600652516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,170 INFO epoch # 767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19027781812474132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,238 INFO epoch # 768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1951245474629104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,303 INFO epoch # 769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19967060233466327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,369 INFO epoch # 770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19590811734087765
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:49,369 INFO *** epoch 770, rolling-avg-loss (window=10)= 0.1942713004653342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,438 INFO epoch # 771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1891864505596459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,504 INFO epoch # 772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19050299609079957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,568 INFO epoch # 773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19664155249483883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,634 INFO epoch # 774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.2056936516892165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,700 INFO epoch # 775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19516764034051448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,766 INFO epoch # 776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1898112092167139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,832 INFO epoch # 777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.189189636381343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,902 INFO epoch # 778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1910626528551802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:49,969 INFO epoch # 779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19024059968069196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,038 INFO epoch # 780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18800758267752826
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:50,038 INFO *** epoch 780, rolling-avg-loss (window=10)= 0.19255039719864725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,104 INFO epoch # 781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1935696837026626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,171 INFO epoch # 782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1943673666100949
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,237 INFO epoch # 783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18910217424854636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,322 INFO epoch # 784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19746330357156694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,388 INFO epoch # 785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1908401339314878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,455 INFO epoch # 786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1907171142520383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,521 INFO epoch # 787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19829888129606843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,588 INFO epoch # 788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19025419186800718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,654 INFO epoch # 789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19228134246077389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,723 INFO epoch # 790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.20101513131521642
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:50,723 INFO *** epoch 790, rolling-avg-loss (window=10)= 0.1937909323256463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,791 INFO epoch # 791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1905598877929151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,861 INFO epoch # 792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18865498481318355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,927 INFO epoch # 793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19592423783615232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:50,996 INFO epoch # 794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19006960396654904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,063 INFO epoch # 795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1875138245522976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,131 INFO epoch # 796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18647470930591226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,196 INFO epoch # 797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19333817798178643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,267 INFO epoch # 798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18808993557468057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,334 INFO epoch # 799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1901763000059873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,419 INFO epoch # 800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19039921346120536
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:51,419 INFO *** epoch 800, rolling-avg-loss (window=10)= 0.19012008752906695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,499 INFO epoch # 801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18476575706154108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,573 INFO epoch # 802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18967282236553729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,639 INFO epoch # 803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19175434741191566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,705 INFO epoch # 804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18910285085439682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,771 INFO epoch # 805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19389692996628582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,840 INFO epoch # 806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1942491722293198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,908 INFO epoch # 807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18761056230869144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:51,976 INFO epoch # 808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1891670438926667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,046 INFO epoch # 809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19443494291044772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,115 INFO epoch # 810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18803487857803702
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:52,115 INFO *** epoch 810, rolling-avg-loss (window=10)= 0.19026893075788393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,187 INFO epoch # 811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18718378408811986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,258 INFO epoch # 812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1850723046809435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,327 INFO epoch # 813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1863789118360728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,395 INFO epoch # 814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1899527320638299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,462 INFO epoch # 815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19481632974930108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,532 INFO epoch # 816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19917320110835135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,607 INFO epoch # 817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19367805309593678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,679 INFO epoch # 818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1850603148341179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,748 INFO epoch # 819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.188963444205001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,827 INFO epoch # 820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1862536200787872
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:52,828 INFO *** epoch 820, rolling-avg-loss (window=10)= 0.18965326957404613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,895 INFO epoch # 821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19095065374858677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:52,962 INFO epoch # 822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18149535660631955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,029 INFO epoch # 823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19361128797754645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,097 INFO epoch # 824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19305808562785387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,163 INFO epoch # 825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1926241519395262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,230 INFO epoch # 826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1999776754528284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,299 INFO epoch # 827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1963112170342356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,369 INFO epoch # 828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19116285676136613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,438 INFO epoch # 829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19044348131865263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,507 INFO epoch # 830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18739060824736953
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:53,507 INFO *** epoch 830, rolling-avg-loss (window=10)= 0.1917025374714285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,576 INFO epoch # 831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1971193109638989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,644 INFO epoch # 832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1939411386847496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,709 INFO epoch # 833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1915667715948075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,777 INFO epoch # 834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19498384092003107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,847 INFO epoch # 835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19504593079909682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,915 INFO epoch # 836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18988961330614984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:53,984 INFO epoch # 837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1842814520932734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,054 INFO epoch # 838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1856903585139662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,123 INFO epoch # 839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1826206597033888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,191 INFO epoch # 840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19395084539428353
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:54,191 INFO *** epoch 840, rolling-avg-loss (window=10)= 0.19090899219736457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,266 INFO epoch # 841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18036047206260264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,339 INFO epoch # 842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18839254253543913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,407 INFO epoch # 843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18743881420232356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,473 INFO epoch # 844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19387140311300755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,540 INFO epoch # 845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.193514151731506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,608 INFO epoch # 846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1940658832900226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,676 INFO epoch # 847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18395866686478257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,742 INFO epoch # 848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18339324765838683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,810 INFO epoch # 849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19159601046703756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,881 INFO epoch # 850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18413520196918398
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:54,881 INFO *** epoch 850, rolling-avg-loss (window=10)= 0.18807263938942925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:54,950 INFO epoch # 851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18251330684870481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,018 INFO epoch # 852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17978246812708676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,086 INFO epoch # 853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18413576844614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,152 INFO epoch # 854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19233155925758183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,217 INFO epoch # 855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18053128547035158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,288 INFO epoch # 856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19156759313773364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,357 INFO epoch # 857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18494463455863297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,424 INFO epoch # 858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18104821140877903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,491 INFO epoch # 859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18767804466187954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,557 INFO epoch # 860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1815816960297525
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:55,558 INFO *** epoch 860, rolling-avg-loss (window=10)= 0.18461145679466426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,623 INFO epoch # 861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.184474024688825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,690 INFO epoch # 862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18269607913680375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,756 INFO epoch # 863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18855083757080138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,822 INFO epoch # 864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18214807845652103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,911 INFO epoch # 865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1874744393862784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:55,986 INFO epoch # 866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18323944637086242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,056 INFO epoch # 867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.203641525702551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,123 INFO epoch # 868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18489672848954797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,189 INFO epoch # 869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18229889660142362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,256 INFO epoch # 870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1840290396939963
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:56,256 INFO *** epoch 870, rolling-avg-loss (window=10)= 0.18634490960976108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,322 INFO epoch # 871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18373598204925656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,389 INFO epoch # 872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18465134094003588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,455 INFO epoch # 873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1824056042241864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,521 INFO epoch # 874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18780789920128882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,594 INFO epoch # 875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17949148663319647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,662 INFO epoch # 876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17702989815734327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,729 INFO epoch # 877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1814378306735307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,795 INFO epoch # 878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19987832801416516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,863 INFO epoch # 879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1828526461031288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,930 INFO epoch # 880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1838880383875221
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:56,930 INFO *** epoch 880, rolling-avg-loss (window=10)= 0.1843179054383654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:56,996 INFO epoch # 881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19173649582080543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,066 INFO epoch # 882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1731325564906001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,132 INFO epoch # 883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18350480939261615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,198 INFO epoch # 884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18047629506327212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,264 INFO epoch # 885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18880572193302214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,331 INFO epoch # 886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18063082150183618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,399 INFO epoch # 887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17572392290458083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,466 INFO epoch # 888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17851810506545007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,532 INFO epoch # 889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.19133299845270813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,598 INFO epoch # 890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17726555652916431
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:57,598 INFO *** epoch 890, rolling-avg-loss (window=10)= 0.18211272831540554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,665 INFO epoch # 891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1781403326895088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,731 INFO epoch # 892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18107907404191792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,797 INFO epoch # 893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18019480141811073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,863 INFO epoch # 894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18493123492226005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,929 INFO epoch # 895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1801256382605061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:57,995 INFO epoch # 896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1827485605608672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,061 INFO epoch # 897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17673116095829755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,127 INFO epoch # 898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17849444365128875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,193 INFO epoch # 899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18016450013965368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,259 INFO epoch # 900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17368980473838747
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:58,259 INFO *** epoch 900, rolling-avg-loss (window=10)= 0.17962995513807983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,327 INFO epoch # 901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18120572715997696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,394 INFO epoch # 902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17906375625170767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,460 INFO epoch # 903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1834048160817474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,528 INFO epoch # 904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1753844510531053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,594 INFO epoch # 905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18042815360240638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,660 INFO epoch # 906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18049269216135144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,728 INFO epoch # 907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1842177715152502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,795 INFO epoch # 908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16993800387717783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,861 INFO epoch # 909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1805097283795476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,928 INFO epoch # 910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1810400513932109
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:58,929 INFO *** epoch 910, rolling-avg-loss (window=10)= 0.17956851514754818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:58,995 INFO epoch # 911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1760549249011092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,061 INFO epoch # 912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17606596648693085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,136 INFO epoch # 913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16947827016701922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,201 INFO epoch # 914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18654477200470865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,269 INFO epoch # 915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17455294751562178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,336 INFO epoch # 916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17269613686949015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,402 INFO epoch # 917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17507133423350751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,469 INFO epoch # 918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17657634243369102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,538 INFO epoch # 919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17617003666236997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,605 INFO epoch # 920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17738921963609755
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:14:59,606 INFO *** epoch 920, rolling-avg-loss (window=10)= 0.17605999509105458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,674 INFO epoch # 921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17127766949124634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,742 INFO epoch # 922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1752288960851729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,811 INFO epoch # 923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1787055510794744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,877 INFO epoch # 924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17744272865820676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:14:59,943 INFO epoch # 925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16942065372131765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,010 INFO epoch # 926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17407419602386653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,077 INFO epoch # 927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1739680371247232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,145 INFO epoch # 928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18272271449677646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,212 INFO epoch # 929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17625735537149012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,278 INFO epoch # 930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.170538067817688
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:00,278 INFO *** epoch 930, rolling-avg-loss (window=10)= 0.17496358698699624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,348 INFO epoch # 931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17667023884132504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,415 INFO epoch # 932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1745753934374079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,482 INFO epoch # 933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17181546078063548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,549 INFO epoch # 934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18018885795027018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,618 INFO epoch # 935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18070054182317108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,687 INFO epoch # 936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1760703674517572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,755 INFO epoch # 937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17070459155365825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,824 INFO epoch # 938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.18529645004309714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,890 INFO epoch # 939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1671358176972717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:00,958 INFO epoch # 940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1689854918513447
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:00,958 INFO *** epoch 940, rolling-avg-loss (window=10)= 0.17521432114299387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,025 INFO epoch # 941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1670791341457516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,096 INFO epoch # 942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16536531888414174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,165 INFO epoch # 943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16673732036724687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,234 INFO epoch # 944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17486565525177866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,303 INFO epoch # 945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16658079077024013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,396 INFO epoch # 946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16707744868472219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,512 INFO epoch # 947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1714290410745889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,622 INFO epoch # 948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16432726569473743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,719 INFO epoch # 949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16549067804589868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,803 INFO epoch # 950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1647463843692094
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:01,803 INFO *** epoch 950, rolling-avg-loss (window=10)= 0.16736990372883156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,877 INFO epoch # 951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16066699917428195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:01,951 INFO epoch # 952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16496669710613787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,025 INFO epoch # 953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16193257621489465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,124 INFO epoch # 954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15835365210659802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,204 INFO epoch # 955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.17200681241229177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,271 INFO epoch # 956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16949599445797503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,349 INFO epoch # 957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1588725782930851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,429 INFO epoch # 958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16384455317165703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,499 INFO epoch # 959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15726967254886404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,573 INFO epoch # 960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16401290870271623
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:02,573 INFO *** epoch 960, rolling-avg-loss (window=10)= 0.16314224441885017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,641 INFO epoch # 961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15918070869520307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,716 INFO epoch # 962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.16354248113930225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,786 INFO epoch # 963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15325342025607824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,856 INFO epoch # 964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1599692669697106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,923 INFO epoch # 965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1561912428587675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:02,989 INFO epoch # 966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15191570110619068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,056 INFO epoch # 967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15478199906647205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,123 INFO epoch # 968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15724481572397053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,195 INFO epoch # 969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15677149989642203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,265 INFO epoch # 970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15848871902562678
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:03,265 INFO *** epoch 970, rolling-avg-loss (window=10)= 0.15713398547377438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,335 INFO epoch # 971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1578055154532194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,412 INFO epoch # 972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15073635964654386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,486 INFO epoch # 973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14908958377782255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,553 INFO epoch # 974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1498620288912207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,620 INFO epoch # 975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15265271603129804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,686 INFO epoch # 976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15364064136520028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,755 INFO epoch # 977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14991640928201377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,821 INFO epoch # 978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14806068839970976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,889 INFO epoch # 979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1505456161685288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:03,955 INFO epoch # 980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1536030564457178
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:03,955 INFO *** epoch 980, rolling-avg-loss (window=10)= 0.1515912615461275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,022 INFO epoch # 981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14336841803742573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,090 INFO epoch # 982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14741026714909822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,158 INFO epoch # 983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15021906793117523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,228 INFO epoch # 984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1507624895311892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,295 INFO epoch # 985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1510750507004559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,364 INFO epoch # 986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14690686529502273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,432 INFO epoch # 987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15328531700652093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,499 INFO epoch # 988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14159577654208988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,570 INFO epoch # 989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14581027382519096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,640 INFO epoch # 990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14817365375347435
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:04,640 INFO *** epoch 990, rolling-avg-loss (window=10)= 0.14786071797716432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,706 INFO epoch # 991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14196129445917904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,773 INFO epoch # 992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1417488087899983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,862 INFO epoch # 993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14709091372787952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:04,938 INFO epoch # 994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14208669867366552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,013 INFO epoch # 995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14568631793372333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,093 INFO epoch # 996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14645158837083727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,163 INFO epoch # 997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15147949615493417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,229 INFO epoch # 998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14396679773926735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,296 INFO epoch # 999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14148649759590626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,361 INFO epoch # 1000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14309783733915538
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:05,361 INFO *** epoch 1000, rolling-avg-loss (window=10)= 0.1445056250784546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,429 INFO epoch # 1001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14584696991369128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,496 INFO epoch # 1002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14284655987285078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,563 INFO epoch # 1003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13903035433031619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,631 INFO epoch # 1004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14006636291742325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,698 INFO epoch # 1005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1414287897059694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,765 INFO epoch # 1006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13768401416018605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,835 INFO epoch # 1007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14500203425996006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,902 INFO epoch # 1008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14099471678491682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:05,969 INFO epoch # 1009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13938005222007632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,037 INFO epoch # 1010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13984204945154488
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:06,037 INFO *** epoch 1010, rolling-avg-loss (window=10)= 0.1412121903616935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,104 INFO epoch # 1011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14767741470132023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,173 INFO epoch # 1012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.138273332035169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,241 INFO epoch # 1013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.15538164554163814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,310 INFO epoch # 1014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14219538739416748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,377 INFO epoch # 1015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14562132628634572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,445 INFO epoch # 1016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14093486731871963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,511 INFO epoch # 1017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14125189546030015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,577 INFO epoch # 1018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13549440959468484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,646 INFO epoch # 1019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14002431381959468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,730 INFO epoch # 1020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13806165533605963
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:06,730 INFO *** epoch 1020, rolling-avg-loss (window=10)= 0.14249162474879995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,814 INFO epoch # 1021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1329388888552785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,882 INFO epoch # 1022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1365978317335248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:06,953 INFO epoch # 1023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1426806440576911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,023 INFO epoch # 1024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13255893148016185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,095 INFO epoch # 1025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13137134222779423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,165 INFO epoch # 1026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1379580171778798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,232 INFO epoch # 1027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14141873107291758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,300 INFO epoch # 1028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1353539222618565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,370 INFO epoch # 1029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1342490412062034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,437 INFO epoch # 1030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1307317924220115
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:07,438 INFO *** epoch 1030, rolling-avg-loss (window=10)= 0.13558591424953192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,506 INFO epoch # 1031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1341269778786227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,573 INFO epoch # 1032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12907103437464684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,640 INFO epoch # 1033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1336934904102236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,719 INFO epoch # 1034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13505363720469177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,799 INFO epoch # 1035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12888334062881768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,881 INFO epoch # 1036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13438589917495847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:07,953 INFO epoch # 1037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1366018899716437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:08,023 INFO epoch # 1038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1365669067017734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:08,102 INFO epoch # 1039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13242569868452847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:08,260 INFO epoch # 1040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13274950941558927
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:08,260 INFO *** epoch 1040, rolling-avg-loss (window=10)= 0.1333558384445496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:08,351 INFO epoch # 1041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13130457419902086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:08,442 INFO epoch # 1042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13419982127379626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:08,533 INFO epoch # 1043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14149857824668288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:08,606 INFO epoch # 1044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13074206560850143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:08,724 INFO epoch # 1045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13401663582772017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:08,841 INFO epoch # 1046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13026171934325248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:08,921 INFO epoch # 1047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13204058015253395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:08,995 INFO epoch # 1048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1351004452444613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,065 INFO epoch # 1049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.14404135278891772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,136 INFO epoch # 1050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13266731495968997
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:09,136 INFO *** epoch 1050, rolling-avg-loss (window=10)= 0.1345873087644577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,205 INFO epoch # 1051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1282929270528257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,275 INFO epoch # 1052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.128497825586237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,346 INFO epoch # 1053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12651484261732548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,417 INFO epoch # 1054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1309455861337483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,485 INFO epoch # 1055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13488753931596875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,578 INFO epoch # 1056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13075222668703645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,647 INFO epoch # 1057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13727250159718096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,717 INFO epoch # 1058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12787062884308398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,787 INFO epoch # 1059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13708188582677394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,856 INFO epoch # 1060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12797603337094188
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:09,856 INFO *** epoch 1060, rolling-avg-loss (window=10)= 0.13100919970311226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,925 INFO epoch # 1061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1277542756870389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:09,994 INFO epoch # 1062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1369030405767262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,071 INFO epoch # 1063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12741925800219178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,141 INFO epoch # 1064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1344125682953745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,212 INFO epoch # 1065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1266195985954255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,281 INFO epoch # 1066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1300925831310451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,351 INFO epoch # 1067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13012799888383597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,420 INFO epoch # 1068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13418458099476993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,489 INFO epoch # 1069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1249059506226331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,558 INFO epoch # 1070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1325826954562217
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:10,559 INFO *** epoch 1070, rolling-avg-loss (window=10)= 0.13050025502452628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,628 INFO epoch # 1071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13308031461201608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,696 INFO epoch # 1072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12561057566199452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,765 INFO epoch # 1073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12503654405009001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,842 INFO epoch # 1074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12759291648399085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,911 INFO epoch # 1075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13167476851958781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:10,980 INFO epoch # 1076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1298532304354012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,050 INFO epoch # 1077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12601032317616045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,120 INFO epoch # 1078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12386305222753435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,190 INFO epoch # 1079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12839215656276792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,261 INFO epoch # 1080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12580051482655108
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:11,261 INFO *** epoch 1080, rolling-avg-loss (window=10)= 0.12769143965560942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,334 INFO epoch # 1081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12476791930384934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,402 INFO epoch # 1082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1239861969370395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,470 INFO epoch # 1083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1286305651301518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,539 INFO epoch # 1084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12813527195248753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,609 INFO epoch # 1085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12217827967833728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,680 INFO epoch # 1086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13099330244585872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,761 INFO epoch # 1087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12157966068480164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,831 INFO epoch # 1088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12595467758364975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,903 INFO epoch # 1089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12589952209964395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:11,971 INFO epoch # 1090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1250492432154715
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:11,971 INFO *** epoch 1090, rolling-avg-loss (window=10)= 0.1257174639031291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,042 INFO epoch # 1091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1373744059819728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,111 INFO epoch # 1092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.119721835013479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,182 INFO epoch # 1093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12362359720282257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,252 INFO epoch # 1094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12698097294196486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,324 INFO epoch # 1095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1232628826983273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,393 INFO epoch # 1096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12064244388602674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,468 INFO epoch # 1097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11987394304014742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,537 INFO epoch # 1098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12031720287632197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,616 INFO epoch # 1099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12605518440250307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,688 INFO epoch # 1100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12676083866972476
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:12,688 INFO *** epoch 1100, rolling-avg-loss (window=10)= 0.12446133067132906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,759 INFO epoch # 1101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12730802001897246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,830 INFO epoch # 1102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1230137764941901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,901 INFO epoch # 1103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12034812464844435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:12,975 INFO epoch # 1104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12974325369577855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,053 INFO epoch # 1105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12663301383145154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,122 INFO epoch # 1106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11878218816127628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,193 INFO epoch # 1107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.13372060633264482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,270 INFO epoch # 1108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12205379398074001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,352 INFO epoch # 1109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11798757780343294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,438 INFO epoch # 1110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11879861750639975
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:13,438 INFO *** epoch 1110, rolling-avg-loss (window=10)= 0.12383889724733307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,522 INFO epoch # 1111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12335573916789144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,608 INFO epoch # 1112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1258781849173829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,692 INFO epoch # 1113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11889936274383217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,775 INFO epoch # 1114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12374917767010629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,860 INFO epoch # 1115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12070905917789787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:13,945 INFO epoch # 1116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1212022720137611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,026 INFO epoch # 1117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12253357097506523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,110 INFO epoch # 1118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1169889570446685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,192 INFO epoch # 1119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12231846479699016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,275 INFO epoch # 1120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11724584246985614
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:14,275 INFO *** epoch 1120, rolling-avg-loss (window=10)= 0.12128806309774517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,356 INFO epoch # 1121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1196633162908256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,439 INFO epoch # 1122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1255710907280445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,522 INFO epoch # 1123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11758302163798362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,603 INFO epoch # 1124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11683944705873728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,685 INFO epoch # 1125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11712419940158725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,768 INFO epoch # 1126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11316167935729027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,850 INFO epoch # 1127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1153470118297264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:14,932 INFO epoch # 1128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.109777465055231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,014 INFO epoch # 1129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11908254527952522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,097 INFO epoch # 1130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11520917364396155
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:15,097 INFO *** epoch 1130, rolling-avg-loss (window=10)= 0.11693589502829127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,177 INFO epoch # 1131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11807404970750213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,266 INFO epoch # 1132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11780072667170316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,356 INFO epoch # 1133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11578514135908335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,441 INFO epoch # 1134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11531973979435861
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,525 INFO epoch # 1135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11998624680563807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,606 INFO epoch # 1136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11815183889120817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,688 INFO epoch # 1137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11861687083728611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,770 INFO epoch # 1138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11711529945023358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,855 INFO epoch # 1139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12087798351421952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:15,938 INFO epoch # 1140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11588070204015821
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:15,938 INFO *** epoch 1140, rolling-avg-loss (window=10)= 0.1177608599071391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,021 INFO epoch # 1141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11980418220628053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,100 INFO epoch # 1142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11899483960587531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,182 INFO epoch # 1143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12194599956274033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,265 INFO epoch # 1144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12809582671616226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,350 INFO epoch # 1145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12033640162553638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,432 INFO epoch # 1146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11435875110328197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,514 INFO epoch # 1147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11725247232243419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,598 INFO epoch # 1148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11054835293907672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,712 INFO epoch # 1149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11615139129571617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,801 INFO epoch # 1150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11497084773145616
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:16,801 INFO *** epoch 1150, rolling-avg-loss (window=10)= 0.11824590651085601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,887 INFO epoch # 1151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1127575421705842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:16,969 INFO epoch # 1152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11082793609239161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,052 INFO epoch # 1153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11037530575413257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,135 INFO epoch # 1154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10911332117393613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,218 INFO epoch # 1155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11574424814898521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,301 INFO epoch # 1156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11142824997659773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,383 INFO epoch # 1157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11123216128908098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,465 INFO epoch # 1158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11645783542189747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,546 INFO epoch # 1159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10841903369873762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,626 INFO epoch # 1160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10614168643951416
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:17,626 INFO *** epoch 1160, rolling-avg-loss (window=10)= 0.11124973201658576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,709 INFO epoch # 1161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11744684656150639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,804 INFO epoch # 1162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11335497465915978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,887 INFO epoch # 1163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10997586150187999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:17,969 INFO epoch # 1164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11611733806785196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,053 INFO epoch # 1165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11660372198093683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,134 INFO epoch # 1166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11444999650120735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,217 INFO epoch # 1167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11360228422563523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,301 INFO epoch # 1168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11018156528007239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,384 INFO epoch # 1169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10767227038741112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,466 INFO epoch # 1170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10763367544859648
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:18,466 INFO *** epoch 1170, rolling-avg-loss (window=10)= 0.11270385346142575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,549 INFO epoch # 1171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10722126963082701
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,632 INFO epoch # 1172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10875975317321718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,714 INFO epoch # 1173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10795927315484732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,799 INFO epoch # 1174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10926098073832691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,882 INFO epoch # 1175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10704414546489716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:18,964 INFO epoch # 1176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11334483197424561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,048 INFO epoch # 1177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12018829095177352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,133 INFO epoch # 1178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1025155836250633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,215 INFO epoch # 1179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10905382060445845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,301 INFO epoch # 1180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10534371878020465
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:19,301 INFO *** epoch 1180, rolling-avg-loss (window=10)= 0.10906916680978611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,384 INFO epoch # 1181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11166737938765436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,465 INFO epoch # 1182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10794770997017622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,549 INFO epoch # 1183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10327215935103595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,629 INFO epoch # 1184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1033081627683714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,712 INFO epoch # 1185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10611990874167532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,795 INFO epoch # 1186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09965879237279296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,878 INFO epoch # 1187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10343484301120043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:19,959 INFO epoch # 1188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1021188247250393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:20,042 INFO epoch # 1189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10638957226183265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:20,174 INFO epoch # 1190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10541432444006205
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:20,174 INFO *** epoch 1190, rolling-avg-loss (window=10)= 0.10493316770298407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:20,258 INFO epoch # 1191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11145375145133585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:20,341 INFO epoch # 1192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10303373250644654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:20,423 INFO epoch # 1193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10675872431602329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:20,506 INFO epoch # 1194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10672922641970217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:20,589 INFO epoch # 1195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10293477703817189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:20,682 INFO epoch # 1196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11087524192407727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:20,764 INFO epoch # 1197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09594972833292559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:20,847 INFO epoch # 1198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09912665211595595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:20,929 INFO epoch # 1199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10657061147503555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,014 INFO epoch # 1200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10237086948473006
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:21,014 INFO *** epoch 1200, rolling-avg-loss (window=10)= 0.10458033150644042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,095 INFO epoch # 1201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10007685516029596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,173 INFO epoch # 1202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10202740912791342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,257 INFO epoch # 1203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10536683641839772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,340 INFO epoch # 1204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10199816583190113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,425 INFO epoch # 1205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10323175555095077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,507 INFO epoch # 1206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1000812704442069
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,589 INFO epoch # 1207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09618787537328899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,670 INFO epoch # 1208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10187718854285777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,753 INFO epoch # 1209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09839291498064995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,835 INFO epoch # 1210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09926221566274762
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:21,835 INFO *** epoch 1210, rolling-avg-loss (window=10)= 0.10085024870932102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,917 INFO epoch # 1211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09669124573701993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:21,996 INFO epoch # 1212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09651446982752532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,077 INFO epoch # 1213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10714620887301862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,160 INFO epoch # 1214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10352858516853303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,243 INFO epoch # 1215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1006045623216778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,325 INFO epoch # 1216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.096873160044197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,409 INFO epoch # 1217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09977625065948814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,492 INFO epoch # 1218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0920092168962583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,571 INFO epoch # 1219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0928769587771967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,652 INFO epoch # 1220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0952550598885864
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:22,652 INFO *** epoch 1220, rolling-avg-loss (window=10)= 0.09812757181935013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,731 INFO epoch # 1221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09195199853274971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,812 INFO epoch # 1222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0930521679110825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,893 INFO epoch # 1223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09685271186754107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:22,975 INFO epoch # 1224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09437792387325317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,055 INFO epoch # 1225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0981389157823287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,134 INFO epoch # 1226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09536346601089463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,214 INFO epoch # 1227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09059969848021865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,294 INFO epoch # 1228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09162522217957303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,378 INFO epoch # 1229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0922147047240287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,458 INFO epoch # 1230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09654285246506333
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:23,459 INFO *** epoch 1230, rolling-avg-loss (window=10)= 0.09407196618267336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,539 INFO epoch # 1231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09044664463726804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,620 INFO epoch # 1232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09691868990194052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,700 INFO epoch # 1233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0935913467546925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,780 INFO epoch # 1234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09533642255701125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,862 INFO epoch # 1235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09877143101766706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:23,942 INFO epoch # 1236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08963056514039636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,022 INFO epoch # 1237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0864877937710844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,102 INFO epoch # 1238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08917809958802536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,185 INFO epoch # 1239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0914228594629094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,267 INFO epoch # 1240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08820743148680776
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:24,267 INFO *** epoch 1240, rolling-avg-loss (window=10)= 0.09199912843178026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,352 INFO epoch # 1241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08591959369368851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,435 INFO epoch # 1242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08506805391516536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,519 INFO epoch # 1243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08585119672352448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,599 INFO epoch # 1244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08625466463854536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,684 INFO epoch # 1245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0851500891149044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,765 INFO epoch # 1246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08647504210239276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,849 INFO epoch # 1247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0845659578917548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:24,933 INFO epoch # 1248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08315639605280012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,016 INFO epoch # 1249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08523237332701683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,103 INFO epoch # 1250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0854641095502302
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:25,103 INFO *** epoch 1250, rolling-avg-loss (window=10)= 0.08531374770100228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,192 INFO epoch # 1251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09047238517086953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,278 INFO epoch # 1252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08150388131616637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,361 INFO epoch # 1253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08553442230913788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,444 INFO epoch # 1254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08326622482854873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,525 INFO epoch # 1255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08393807231914252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,604 INFO epoch # 1256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08881163294427097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,688 INFO epoch # 1257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07738931226776913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,769 INFO epoch # 1258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07791324477875605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,852 INFO epoch # 1259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0797627589199692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:25,933 INFO epoch # 1260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07922324258834124
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:25,933 INFO *** epoch 1260, rolling-avg-loss (window=10)= 0.08278151774429716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,015 INFO epoch # 1261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0802085476461798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,096 INFO epoch # 1262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07711478346027434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,176 INFO epoch # 1263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08426578529179096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,257 INFO epoch # 1264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07929816679097712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,338 INFO epoch # 1265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07962930569192395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,419 INFO epoch # 1266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07714125997154042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,499 INFO epoch # 1267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.082487509585917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,577 INFO epoch # 1268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07527944591129199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,661 INFO epoch # 1269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0776948090060614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,742 INFO epoch # 1270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07903282722691074
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:26,742 INFO *** epoch 1270, rolling-avg-loss (window=10)= 0.07921524405828677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,822 INFO epoch # 1271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0740116429515183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,905 INFO epoch # 1272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07540125877130777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:26,984 INFO epoch # 1273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07636471441946924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:27,063 INFO epoch # 1274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07128239469602704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:27,145 INFO epoch # 1275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07384164934046566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:27,245 INFO epoch # 1276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07713652023812756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:27,329 INFO epoch # 1277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07773223257390782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:27,415 INFO epoch # 1278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0741226397221908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:27,498 INFO epoch # 1279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07447660673642531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:27,580 INFO epoch # 1280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07374879287090153
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:27,581 INFO *** epoch 1280, rolling-avg-loss (window=10)= 0.0748118452320341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:27,668 INFO epoch # 1281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07347294461214915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:27,751 INFO epoch # 1282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07352055161027238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:27,846 INFO epoch # 1283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07343903137370944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:27,937 INFO epoch # 1284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0726941644679755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,023 INFO epoch # 1285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08319582790136337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,109 INFO epoch # 1286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08153803140157834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,197 INFO epoch # 1287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.071849072992336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,285 INFO epoch # 1288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07385747486841865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,373 INFO epoch # 1289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0739964212407358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,456 INFO epoch # 1290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06869028869550675
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:28,456 INFO *** epoch 1290, rolling-avg-loss (window=10)= 0.07462538091640454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,537 INFO epoch # 1291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07588980114087462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,617 INFO epoch # 1292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07155034365132451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,696 INFO epoch # 1293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07525203557452187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,777 INFO epoch # 1294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06817906838841736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,859 INFO epoch # 1295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06995592277962714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:28,937 INFO epoch # 1296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06618439313024282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,018 INFO epoch # 1297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06729838612955064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,098 INFO epoch # 1298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07039871928282082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,181 INFO epoch # 1299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07226027391152456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,261 INFO epoch # 1300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06460414445609786
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:29,261 INFO *** epoch 1300, rolling-avg-loss (window=10)= 0.07015730884450022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,358 INFO epoch # 1301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06713235244387761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,443 INFO epoch # 1302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06704506417736411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,525 INFO epoch # 1303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06855750613613054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,607 INFO epoch # 1304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.066971427120734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,687 INFO epoch # 1305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06835732172476128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,768 INFO epoch # 1306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06464414775837213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,853 INFO epoch # 1307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0637233771267347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:29,936 INFO epoch # 1308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06312841147882864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,019 INFO epoch # 1309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06507925171172246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,100 INFO epoch # 1310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06403700046939775
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:30,100 INFO *** epoch 1310, rolling-avg-loss (window=10)= 0.06586758601479233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,178 INFO epoch # 1311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06909027183428407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,258 INFO epoch # 1312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.061408421548549086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,341 INFO epoch # 1313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06580892234342173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,421 INFO epoch # 1314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06365005462430418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,503 INFO epoch # 1315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06768430897500366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,583 INFO epoch # 1316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.064114811248146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,666 INFO epoch # 1317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06216533086262643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,749 INFO epoch # 1318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06896499678259715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,833 INFO epoch # 1319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06099625868955627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,916 INFO epoch # 1320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06542420608457178
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:30,917 INFO *** epoch 1320, rolling-avg-loss (window=10)= 0.06493075829930603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:30,996 INFO epoch # 1321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07043071155203506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,076 INFO epoch # 1322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06508802680764347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,156 INFO epoch # 1323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06128757761325687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,237 INFO epoch # 1324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06043505866546184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,329 INFO epoch # 1325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06122923281509429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,412 INFO epoch # 1326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06345394684467465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,492 INFO epoch # 1327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06542815611464903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,572 INFO epoch # 1328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06156889075646177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,652 INFO epoch # 1329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.061765047430526465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,732 INFO epoch # 1330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06760045577539131
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:31,732 INFO *** epoch 1330, rolling-avg-loss (window=10)= 0.06382871043751948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,814 INFO epoch # 1331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06073750098585151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,896 INFO epoch # 1332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06133285694522783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:31,978 INFO epoch # 1333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0664977275300771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:32,058 INFO epoch # 1334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.059955733246169984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:32,139 INFO epoch # 1335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06045314413495362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:32,223 INFO epoch # 1336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06447792387916707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:32,326 INFO epoch # 1337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06907942803809419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:32,415 INFO epoch # 1338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06006047083064914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:32,499 INFO epoch # 1339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05946249369299039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:32,590 INFO epoch # 1340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.057948665984440595
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:32,590 INFO *** epoch 1340, rolling-avg-loss (window=10)= 0.06200059452676214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:32,673 INFO epoch # 1341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05891961505403742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:32,757 INFO epoch # 1342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05612587375799194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:32,856 INFO epoch # 1343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05766807071631774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:32,938 INFO epoch # 1344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05895787262124941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,020 INFO epoch # 1345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06397797295358032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,127 INFO epoch # 1346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06040289957309142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,209 INFO epoch # 1347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.060704784002155066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,294 INFO epoch # 1348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0586348467040807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,379 INFO epoch # 1349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06106632889714092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,458 INFO epoch # 1350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.057159185758791864
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:33,459 INFO *** epoch 1350, rolling-avg-loss (window=10)= 0.05936174500384368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,540 INFO epoch # 1351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06440400035353377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,620 INFO epoch # 1352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05607268126914278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,701 INFO epoch # 1353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.060531453404109925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,783 INFO epoch # 1354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05870108166709542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,869 INFO epoch # 1355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06752604624489322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:33,955 INFO epoch # 1356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05876561635523103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,043 INFO epoch # 1357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05896597617538646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,130 INFO epoch # 1358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.062155519146472216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,217 INFO epoch # 1359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06052500562509522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,300 INFO epoch # 1360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05958573205862194
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:34,301 INFO *** epoch 1360, rolling-avg-loss (window=10)= 0.0607233112299582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,384 INFO epoch # 1361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.060162611945997924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,464 INFO epoch # 1362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05772220145445317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,545 INFO epoch # 1363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05780757969478145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,623 INFO epoch # 1364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.059393305506091565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,704 INFO epoch # 1365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.055762684292858467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,782 INFO epoch # 1366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05786047695437446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,863 INFO epoch # 1367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05887256871210411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:34,942 INFO epoch # 1368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.058814958727452904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,022 INFO epoch # 1369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05311156460084021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,102 INFO epoch # 1370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05564651370514184
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:35,102 INFO *** epoch 1370, rolling-avg-loss (window=10)= 0.057515446559409614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,183 INFO epoch # 1371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05377002840396017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,262 INFO epoch # 1372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.059819200658239424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,341 INFO epoch # 1373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05660298973089084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,421 INFO epoch # 1374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05919450172223151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,499 INFO epoch # 1375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0567693431803491
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,582 INFO epoch # 1376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05563012481434271
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,662 INFO epoch # 1377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05966196197550744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,744 INFO epoch # 1378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05769110657274723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,825 INFO epoch # 1379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05719570501241833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,906 INFO epoch # 1380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05654582165880129
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:35,907 INFO *** epoch 1380, rolling-avg-loss (window=10)= 0.0572880783729488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:35,993 INFO epoch # 1381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06058740190928802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,105 INFO epoch # 1382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05544052377808839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,185 INFO epoch # 1383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.054054509004345164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,264 INFO epoch # 1384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.055994464026298374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,348 INFO epoch # 1385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05547737289452925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,428 INFO epoch # 1386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05417437705909833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,510 INFO epoch # 1387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05523142957827076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,591 INFO epoch # 1388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05628690909361467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,672 INFO epoch # 1389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.056702095782384276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,752 INFO epoch # 1390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05697266454808414
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:36,752 INFO *** epoch 1390, rolling-avg-loss (window=10)= 0.05609217476740014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,834 INFO epoch # 1391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.059291797573678195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,915 INFO epoch # 1392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.060535467811860144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:36,992 INFO epoch # 1393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05163613834884018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,072 INFO epoch # 1394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05413112003589049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,149 INFO epoch # 1395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.053810487559530884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,231 INFO epoch # 1396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05451374698895961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,310 INFO epoch # 1397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05418832792202011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,391 INFO epoch # 1398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05980961536988616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,471 INFO epoch # 1399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.054075629566796124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,550 INFO epoch # 1400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.054468789807287976
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:37,550 INFO *** epoch 1400, rolling-avg-loss (window=10)= 0.05564611209847499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,631 INFO epoch # 1401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05323364841751754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,710 INFO epoch # 1402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05896358977770433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,792 INFO epoch # 1403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.058135413157287985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,872 INFO epoch # 1404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05414882974582724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:37,953 INFO epoch # 1405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05564259988022968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,032 INFO epoch # 1406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05511917709372938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,114 INFO epoch # 1407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.06118310987949371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,192 INFO epoch # 1408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05281141720479354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,271 INFO epoch # 1409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.054155083489604294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,371 INFO epoch # 1410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05662205989938229
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:38,372 INFO *** epoch 1410, rolling-avg-loss (window=10)= 0.056001492854557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,451 INFO epoch # 1411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05383062333567068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,529 INFO epoch # 1412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05585565016372129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,610 INFO epoch # 1413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.051969863125123084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,688 INFO epoch # 1414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.061709659290499985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,770 INFO epoch # 1415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05373281840002164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,852 INFO epoch # 1416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.052891062397975475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:38,933 INFO epoch # 1417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05758976086508483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,012 INFO epoch # 1418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05384264004533179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,093 INFO epoch # 1419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.053302834334317595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,171 INFO epoch # 1420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0517754249740392
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:39,171 INFO *** epoch 1420, rolling-avg-loss (window=10)= 0.05465003369317856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,250 INFO epoch # 1421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05471153446706012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,331 INFO epoch # 1422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05426064133644104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,410 INFO epoch # 1423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.052005068107973784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,488 INFO epoch # 1424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05386167182587087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,570 INFO epoch # 1425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.052650829282356426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,650 INFO epoch # 1426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05300606644595973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,730 INFO epoch # 1427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05333024187711999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,809 INFO epoch # 1428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05228262089076452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,889 INFO epoch # 1429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.055515515559818596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:39,968 INFO epoch # 1430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05215495522134006
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:39,968 INFO *** epoch 1430, rolling-avg-loss (window=10)= 0.053377914501470516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,050 INFO epoch # 1431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05531967239221558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,129 INFO epoch # 1432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.052482587547274306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,207 INFO epoch # 1433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.050985711481189355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,287 INFO epoch # 1434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05662786914035678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,369 INFO epoch # 1435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05633189872605726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,450 INFO epoch # 1436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.051564801891800016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,531 INFO epoch # 1437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05263753229519352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,611 INFO epoch # 1438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.054216349904891104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,690 INFO epoch # 1439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0513574440847151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,769 INFO epoch # 1440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.051239822583738714
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:40,769 INFO *** epoch 1440, rolling-avg-loss (window=10)= 0.053276369004743175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,860 INFO epoch # 1441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05062638456001878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:40,944 INFO epoch # 1442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05727003014180809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,024 INFO epoch # 1443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05217911780346185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,102 INFO epoch # 1444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05216725927311927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,180 INFO epoch # 1445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.051074460498057306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,258 INFO epoch # 1446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04924420814495534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,339 INFO epoch # 1447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.050156653043814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,419 INFO epoch # 1448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05179653147934005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,499 INFO epoch # 1449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05257267918204889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,577 INFO epoch # 1450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05381667462643236
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:41,577 INFO *** epoch 1450, rolling-avg-loss (window=10)= 0.052090399875305594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,657 INFO epoch # 1451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05043141540954821
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,736 INFO epoch # 1452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.050121032167226076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,816 INFO epoch # 1453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05204205896006897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,895 INFO epoch # 1454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.049913617462152615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:41,974 INFO epoch # 1455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04990124161122367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,055 INFO epoch # 1456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.053092764341272414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,134 INFO epoch # 1457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.054475442215334624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,218 INFO epoch # 1458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05112663586623967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,299 INFO epoch # 1459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04941644030623138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,380 INFO epoch # 1460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05152888101292774
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:42,381 INFO *** epoch 1460, rolling-avg-loss (window=10)= 0.051204952935222536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,459 INFO epoch # 1461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.057986010739114136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,540 INFO epoch # 1462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0542765602003783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,621 INFO epoch # 1463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.051561402215156704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,703 INFO epoch # 1464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05069222947349772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,798 INFO epoch # 1465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04977777478052303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,880 INFO epoch # 1466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.052117211278527975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:42,960 INFO epoch # 1467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05086796765681356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,040 INFO epoch # 1468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05229141761083156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,119 INFO epoch # 1469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.049687441787682474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,200 INFO epoch # 1470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.052826044731773436
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:43,200 INFO *** epoch 1470, rolling-avg-loss (window=10)= 0.05220840604742989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,281 INFO epoch # 1471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04938477207906544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,362 INFO epoch # 1472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0503270662156865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,445 INFO epoch # 1473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05200016108574346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,527 INFO epoch # 1474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0493854517408181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,607 INFO epoch # 1475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04814256523968652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,690 INFO epoch # 1476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.054506389424204826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,770 INFO epoch # 1477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04912205942673609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,852 INFO epoch # 1478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04935671389102936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:43,933 INFO epoch # 1479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05059175909264013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,013 INFO epoch # 1480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05160180095117539
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:44,014 INFO *** epoch 1480, rolling-avg-loss (window=10)= 0.05044187391467858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,095 INFO epoch # 1481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.050858037488069385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,175 INFO epoch # 1482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05067930609220639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,257 INFO epoch # 1483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04851343663176522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,339 INFO epoch # 1484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0485671311034821
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,419 INFO epoch # 1485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.052034868189366534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,498 INFO epoch # 1486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05248477426357567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,578 INFO epoch # 1487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05219471163582057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,658 INFO epoch # 1488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0491612262558192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,739 INFO epoch # 1489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05023143714061007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,820 INFO epoch # 1490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.049798484425991774
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:44,820 INFO *** epoch 1490, rolling-avg-loss (window=10)= 0.050452341322670693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,902 INFO epoch # 1491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04889923450537026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:44,982 INFO epoch # 1492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05241989245405421
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,064 INFO epoch # 1493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047687129757832736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,143 INFO epoch # 1494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05002629378577694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,224 INFO epoch # 1495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05410578002920374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,304 INFO epoch # 1496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04866701195714995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,386 INFO epoch # 1497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047200388624332845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,466 INFO epoch # 1498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04663018457358703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,548 INFO epoch # 1499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05307224893476814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,628 INFO epoch # 1500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05073769582668319
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:45,628 INFO *** epoch 1500, rolling-avg-loss (window=10)= 0.04994458604487591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,708 INFO epoch # 1501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05137561640003696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,788 INFO epoch # 1502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05112430185545236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,871 INFO epoch # 1503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04708475316874683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:45,950 INFO epoch # 1504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04744769204990007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,031 INFO epoch # 1505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0479121936368756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,110 INFO epoch # 1506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.046077869075816125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,190 INFO epoch # 1507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05128476273966953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,269 INFO epoch # 1508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.050231670145876706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,352 INFO epoch # 1509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04879362561041489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,431 INFO epoch # 1510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04760263627395034
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:46,431 INFO *** epoch 1510, rolling-avg-loss (window=10)= 0.04889351209567394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,510 INFO epoch # 1511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047514840553049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,593 INFO epoch # 1512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.046730891190236434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,676 INFO epoch # 1513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047287822468206286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,756 INFO epoch # 1514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.049135674693388864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,838 INFO epoch # 1515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.051902216218877584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,919 INFO epoch # 1516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04868803691351786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:46,998 INFO epoch # 1517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04957927856594324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,079 INFO epoch # 1518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05102748249191791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,159 INFO epoch # 1519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04907173401443288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,238 INFO epoch # 1520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04959904804127291
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:47,238 INFO *** epoch 1520, rolling-avg-loss (window=10)= 0.0490537025150843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,320 INFO epoch # 1521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.049693668785039335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,398 INFO epoch # 1522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04907130729407072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,477 INFO epoch # 1523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0505944881006144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,556 INFO epoch # 1524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05067491851514205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,636 INFO epoch # 1525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05237725470215082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,714 INFO epoch # 1526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04776258615311235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,794 INFO epoch # 1527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.050515398383140564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,874 INFO epoch # 1528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04918510472634807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:47,954 INFO epoch # 1529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04704583509010263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,035 INFO epoch # 1530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04544877473381348
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:48,035 INFO *** epoch 1530, rolling-avg-loss (window=10)= 0.049236933648353444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,116 INFO epoch # 1531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05339553038356826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,195 INFO epoch # 1532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04737517863395624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,275 INFO epoch # 1533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0461807053652592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,355 INFO epoch # 1534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05152218806324527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,434 INFO epoch # 1535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04876848263666034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,515 INFO epoch # 1536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.045841966464649886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,595 INFO epoch # 1537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047120647912379354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,675 INFO epoch # 1538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047546942718327045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,755 INFO epoch # 1539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04479397676186636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,838 INFO epoch # 1540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.048134972515981644
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:48,838 INFO *** epoch 1540, rolling-avg-loss (window=10)= 0.048068059145589356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,918 INFO epoch # 1541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0539951921091415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:48,997 INFO epoch # 1542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047608386434149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,077 INFO epoch # 1543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04703277055523358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,156 INFO epoch # 1544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0499545086058788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,236 INFO epoch # 1545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.051145536825060844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,315 INFO epoch # 1546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04673502442892641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,396 INFO epoch # 1547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04601226648082957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,475 INFO epoch # 1548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047175753861665726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,557 INFO epoch # 1549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05164947616867721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,640 INFO epoch # 1550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04479250335134566
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:49,640 INFO *** epoch 1550, rolling-avg-loss (window=10)= 0.04861014188209083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,721 INFO epoch # 1551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05006698876968585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,804 INFO epoch # 1552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.048919603403192014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,886 INFO epoch # 1553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.044940272986423224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:49,968 INFO epoch # 1554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0450636244204361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,050 INFO epoch # 1555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0444895577384159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,132 INFO epoch # 1556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04877921310253441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,214 INFO epoch # 1557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04896529746474698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,295 INFO epoch # 1558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04552481707651168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,380 INFO epoch # 1559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05570356355747208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,462 INFO epoch # 1560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04744583665160462
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:50,462 INFO *** epoch 1560, rolling-avg-loss (window=10)= 0.047989877517102285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,543 INFO epoch # 1561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.051335651136469096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,628 INFO epoch # 1562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047145942953648046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,711 INFO epoch # 1563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04529221949633211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,790 INFO epoch # 1564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04509686611709185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,871 INFO epoch # 1565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.049866201530676335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:50,950 INFO epoch # 1566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04747345787473023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,032 INFO epoch # 1567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047490487224422395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,116 INFO epoch # 1568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.045253306394442916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,198 INFO epoch # 1569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.046395778947044164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,279 INFO epoch # 1570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05222033086465672
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:51,279 INFO *** epoch 1570, rolling-avg-loss (window=10)= 0.04775702425395138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,365 INFO epoch # 1571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.048592639330308884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,446 INFO epoch # 1572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04907080007251352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,526 INFO epoch # 1573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04794937325641513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,606 INFO epoch # 1574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04629645892418921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,687 INFO epoch # 1575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0431148539937567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,768 INFO epoch # 1576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04234926670324057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,850 INFO epoch # 1577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.048703176667913795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:51,930 INFO epoch # 1578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04550124873640016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,011 INFO epoch # 1579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.043389782949816436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,091 INFO epoch # 1580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.045435088861268014
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:52,091 INFO *** epoch 1580, rolling-avg-loss (window=10)= 0.04604026894958224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,172 INFO epoch # 1581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04754374304320663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,251 INFO epoch # 1582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04463635745923966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,332 INFO epoch # 1583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.045854150215745904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,413 INFO epoch # 1584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04589507312630303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,491 INFO epoch # 1585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04728249547770247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,579 INFO epoch # 1586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04620484879706055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,660 INFO epoch # 1587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04443887167144567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,738 INFO epoch # 1588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0462616509757936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,820 INFO epoch # 1589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04425413231365383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,899 INFO epoch # 1590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.050654927821597084
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:52,899 INFO *** epoch 1590, rolling-avg-loss (window=10)= 0.04630262509017484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:52,978 INFO epoch # 1591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.045141393988160416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,056 INFO epoch # 1592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04433918063296005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,134 INFO epoch # 1593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04519362447899766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,213 INFO epoch # 1594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04666083777556196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,291 INFO epoch # 1595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.048661077016731724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,373 INFO epoch # 1596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.041871323832310736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,452 INFO epoch # 1597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05120303452713415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,529 INFO epoch # 1598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.044411950453650206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,609 INFO epoch # 1599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.044969167560338974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,686 INFO epoch # 1600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047081786091439426
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:53,686 INFO *** epoch 1600, rolling-avg-loss (window=10)= 0.04595333763572853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,765 INFO epoch # 1601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04616540530696511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,848 INFO epoch # 1602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04444345511728898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:53,926 INFO epoch # 1603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04510790720814839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,003 INFO epoch # 1604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04506120743462816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,083 INFO epoch # 1605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0430076424672734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,162 INFO epoch # 1606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.045360418909695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,240 INFO epoch # 1607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04294327745446935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,322 INFO epoch # 1608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.043534320197068155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,402 INFO epoch # 1609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042229712154949084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,480 INFO epoch # 1610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04433604920632206
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:54,480 INFO *** epoch 1610, rolling-avg-loss (window=10)= 0.04421893954568077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,557 INFO epoch # 1611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04572310217190534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,636 INFO epoch # 1612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04529681598069146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,715 INFO epoch # 1613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04498322430299595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,794 INFO epoch # 1614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.046245634264778346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,877 INFO epoch # 1615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05278306076070294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:54,954 INFO epoch # 1616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04445095598930493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,032 INFO epoch # 1617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047500237880740315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,114 INFO epoch # 1618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042943262844346464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,192 INFO epoch # 1619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.045424026320688426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,272 INFO epoch # 1620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0447663736413233
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:55,272 INFO *** epoch 1620, rolling-avg-loss (window=10)= 0.04601166941574775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,355 INFO epoch # 1621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04474784110789187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,436 INFO epoch # 1622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04320054082199931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,516 INFO epoch # 1623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04365973296808079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,595 INFO epoch # 1624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04752870215452276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,675 INFO epoch # 1625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0459822507109493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,756 INFO epoch # 1626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.047392783453688025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,836 INFO epoch # 1627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04524129597120918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:55,916 INFO epoch # 1628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04754591581877321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,000 INFO epoch # 1629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04734287993051112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,082 INFO epoch # 1630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04287236838717945
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:56,082 INFO *** epoch 1630, rolling-avg-loss (window=10)= 0.0455514311324805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,162 INFO epoch # 1631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05035003158263862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,245 INFO epoch # 1632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04202066914876923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,332 INFO epoch # 1633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05137540807481855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,414 INFO epoch # 1634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.046748174470849335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,495 INFO epoch # 1635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.045472642115782946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,574 INFO epoch # 1636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04480633226921782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,655 INFO epoch # 1637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04300829014391638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,737 INFO epoch # 1638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.044809644925408065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,817 INFO epoch # 1639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04680804640520364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,898 INFO epoch # 1640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042885532282525674
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:56,898 INFO *** epoch 1640, rolling-avg-loss (window=10)= 0.04582847714191303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:56,976 INFO epoch # 1641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04492365603800863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,056 INFO epoch # 1642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.045252738666022196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,136 INFO epoch # 1643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04293388320365921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,214 INFO epoch # 1644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04805918229976669
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,294 INFO epoch # 1645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04357322270516306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,376 INFO epoch # 1646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04452656421926804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,455 INFO epoch # 1647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04367798718158156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,536 INFO epoch # 1648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04815005854470655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,615 INFO epoch # 1649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.041884066769853234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,693 INFO epoch # 1650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04712052407558076
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:57,693 INFO *** epoch 1650, rolling-avg-loss (window=10)= 0.04501018837036099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,774 INFO epoch # 1651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.052981231710873544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,853 INFO epoch # 1652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04284260669373907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:57,933 INFO epoch # 1653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0504670066293329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,012 INFO epoch # 1654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.043380874587455764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,090 INFO epoch # 1655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04690932741505094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,170 INFO epoch # 1656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.046920451801270247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,248 INFO epoch # 1657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04362333603785373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,330 INFO epoch # 1658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04616192332468927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,410 INFO epoch # 1659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.043937199196079746
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,491 INFO epoch # 1660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.045715109939919785
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:58,491 INFO *** epoch 1660, rolling-avg-loss (window=10)= 0.0462939067336265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,571 INFO epoch # 1661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04630831739632413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,652 INFO epoch # 1662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.041812467970885336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,732 INFO epoch # 1663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04370506992563605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,813 INFO epoch # 1664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04586507269414142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,895 INFO epoch # 1665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04288916999939829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:58,975 INFO epoch # 1666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0430852695426438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,055 INFO epoch # 1667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.041846659034490585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,137 INFO epoch # 1668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042406986438436434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,217 INFO epoch # 1669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04529293579980731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,297 INFO epoch # 1670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04913797110202722
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:15:59,297 INFO *** epoch 1670, rolling-avg-loss (window=10)= 0.04423499199037906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,379 INFO epoch # 1671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04278241496649571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,460 INFO epoch # 1672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0443991141510196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,539 INFO epoch # 1673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04187112257932313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,622 INFO epoch # 1674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04752771108178422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,700 INFO epoch # 1675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042701334808953106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,780 INFO epoch # 1676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0453376435325481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,860 INFO epoch # 1677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.043855464551597834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:15:59,940 INFO epoch # 1678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040812226448906586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,018 INFO epoch # 1679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04127863381290808
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,099 INFO epoch # 1680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04237110516987741
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:00,099 INFO *** epoch 1680, rolling-avg-loss (window=10)= 0.04329367711034138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,177 INFO epoch # 1681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042827457014936954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,258 INFO epoch # 1682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.044357340841088444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,341 INFO epoch # 1683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04634829598944634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,425 INFO epoch # 1684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04115892021218315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,503 INFO epoch # 1685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.041556987445801497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,584 INFO epoch # 1686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04079945699777454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,665 INFO epoch # 1687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04367186175659299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,744 INFO epoch # 1688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04498260893160477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,824 INFO epoch # 1689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04278225381858647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,908 INFO epoch # 1690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04739908370538615
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:00,908 INFO *** epoch 1690, rolling-avg-loss (window=10)= 0.04358842667134013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:00,986 INFO epoch # 1691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0426505419309251
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,067 INFO epoch # 1692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.045565338601591066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,146 INFO epoch # 1693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.05223609966924414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,226 INFO epoch # 1694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04254236549604684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,307 INFO epoch # 1695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04365166794741526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,389 INFO epoch # 1696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040835797000909224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,467 INFO epoch # 1697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04033284151228145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,548 INFO epoch # 1698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042022295005153865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,629 INFO epoch # 1699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04412072489503771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,707 INFO epoch # 1700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042925632442347705
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:01,707 INFO *** epoch 1700, rolling-avg-loss (window=10)= 0.043688330450095236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,787 INFO epoch # 1701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042695125070167705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,870 INFO epoch # 1702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04624669865006581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:01,949 INFO epoch # 1703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04266265913611278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,031 INFO epoch # 1704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04021599885891192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,117 INFO epoch # 1705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04635295056505129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,198 INFO epoch # 1706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04057511140126735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,277 INFO epoch # 1707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040385459520621225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,360 INFO epoch # 1708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0414759911946021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,441 INFO epoch # 1709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04122966853901744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,519 INFO epoch # 1710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04118131131690461
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:02,519 INFO *** epoch 1710, rolling-avg-loss (window=10)= 0.042302097425272224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,600 INFO epoch # 1711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040663490508450195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,684 INFO epoch # 1712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04084426554618403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,766 INFO epoch # 1713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04039842702331953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,849 INFO epoch # 1714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04096565116196871
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:02,932 INFO epoch # 1715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040603295259643346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,013 INFO epoch # 1716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040117508091498166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,097 INFO epoch # 1717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04099517612485215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,176 INFO epoch # 1718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04267540230648592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,256 INFO epoch # 1719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04520642972784117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,335 INFO epoch # 1720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040983096609124914
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:03,335 INFO *** epoch 1720, rolling-avg-loss (window=10)= 0.04134527423593681
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,420 INFO epoch # 1721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042931038537062705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,504 INFO epoch # 1722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0395639780908823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,586 INFO epoch # 1723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042130513465963304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,664 INFO epoch # 1724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.043469541327795014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,749 INFO epoch # 1725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040582630288554356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,835 INFO epoch # 1726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0397837579366751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:03,921 INFO epoch # 1727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03985683436621912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,004 INFO epoch # 1728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.039677094551734626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,087 INFO epoch # 1729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042976621218258515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,170 INFO epoch # 1730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04276149527868256
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:04,170 INFO *** epoch 1730, rolling-avg-loss (window=10)= 0.04137335050618276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,252 INFO epoch # 1731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04092207766370848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,334 INFO epoch # 1732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03956572277820669
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,415 INFO epoch # 1733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03959960537031293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,496 INFO epoch # 1734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03949575498700142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,575 INFO epoch # 1735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04358196517569013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,655 INFO epoch # 1736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03945431369356811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,735 INFO epoch # 1737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04356906982138753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,816 INFO epoch # 1738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04067050066078082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,896 INFO epoch # 1739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.041231667855754495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:04,975 INFO epoch # 1740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04026030527893454
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:04,975 INFO *** epoch 1740, rolling-avg-loss (window=10)= 0.04083509832853451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,053 INFO epoch # 1741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03904862704803236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,135 INFO epoch # 1742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04069902663468383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,218 INFO epoch # 1743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04104919149540365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,299 INFO epoch # 1744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042212661559460685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,382 INFO epoch # 1745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.038332443829858676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,461 INFO epoch # 1746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03975058483774774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,541 INFO epoch # 1747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03909318058867939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,622 INFO epoch # 1748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042358529928606004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,700 INFO epoch # 1749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042544644500594586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,779 INFO epoch # 1750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040278105763718486
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:05,779 INFO *** epoch 1750, rolling-avg-loss (window=10)= 0.04053669961867854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,859 INFO epoch # 1751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03925540379714221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:05,937 INFO epoch # 1752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.044211949745658785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,016 INFO epoch # 1753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04853030189406127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,095 INFO epoch # 1754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04237329700845294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,173 INFO epoch # 1755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.039521690167021006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,253 INFO epoch # 1756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037408484815387055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,333 INFO epoch # 1757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040615746984258294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,415 INFO epoch # 1758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04142541601322591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,496 INFO epoch # 1759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04453888340503909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,574 INFO epoch # 1760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04471829638350755
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:06,574 INFO *** epoch 1760, rolling-avg-loss (window=10)= 0.04225994702137541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,652 INFO epoch # 1761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042008775140857324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,733 INFO epoch # 1762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03892265778267756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,811 INFO epoch # 1763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040617902093799785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,891 INFO epoch # 1764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03744992971769534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:06,969 INFO epoch # 1765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04211014756583609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,048 INFO epoch # 1766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03808592533459887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,129 INFO epoch # 1767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04167196052731015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,207 INFO epoch # 1768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0422146390483249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,286 INFO epoch # 1769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03928431490203366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,365 INFO epoch # 1770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03991784589015879
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:07,365 INFO *** epoch 1770, rolling-avg-loss (window=10)= 0.04022840980032925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,444 INFO epoch # 1771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03904734461684711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,524 INFO epoch # 1772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.039299279131228104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,603 INFO epoch # 1773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03915780992247164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,683 INFO epoch # 1774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03827289730543271
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,763 INFO epoch # 1775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04119851943687536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,844 INFO epoch # 1776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04185624129604548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:07,924 INFO epoch # 1777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04297619633143768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,002 INFO epoch # 1778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.044112194183981046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,082 INFO epoch # 1779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04158760575228371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,161 INFO epoch # 1780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03987799701280892
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:08,161 INFO *** epoch 1780, rolling-avg-loss (window=10)= 0.040738608498941176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,241 INFO epoch # 1781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040392494382103905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,322 INFO epoch # 1782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03792339962092228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,402 INFO epoch # 1783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03747315873624757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,482 INFO epoch # 1784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03843433302245103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,560 INFO epoch # 1785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04385780176380649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,641 INFO epoch # 1786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04091836797306314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,720 INFO epoch # 1787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03977211000164971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,800 INFO epoch # 1788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03887664483045228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,879 INFO epoch # 1789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.041136926505714655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:08,959 INFO epoch # 1790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04186406367807649
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:08,960 INFO *** epoch 1790, rolling-avg-loss (window=10)= 0.040064930051448754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,039 INFO epoch # 1791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.042022574780276045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,121 INFO epoch # 1792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03840714326361194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,200 INFO epoch # 1793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03764839831274003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,279 INFO epoch # 1794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03992637246847153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,362 INFO epoch # 1795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04116982105188072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,446 INFO epoch # 1796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0413959700090345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,527 INFO epoch # 1797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040813583764247596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,608 INFO epoch # 1798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04007394815562293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,685 INFO epoch # 1799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03996541100786999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,766 INFO epoch # 1800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03704497107537463
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:09,766 INFO *** epoch 1800, rolling-avg-loss (window=10)= 0.03984681938891299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,848 INFO epoch # 1801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04019420483382419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:09,928 INFO epoch # 1802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04112428519874811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,007 INFO epoch # 1803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.039447788265533745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,089 INFO epoch # 1804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040283040289068595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,168 INFO epoch # 1805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04037853784393519
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,247 INFO epoch # 1806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04226665693568066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,330 INFO epoch # 1807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03669856005581096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,411 INFO epoch # 1808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03944590705214068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,488 INFO epoch # 1809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.041070438106544316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,569 INFO epoch # 1810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04275637218961492
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:10,570 INFO *** epoch 1810, rolling-avg-loss (window=10)= 0.04036657907709014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,651 INFO epoch # 1811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03921348738367669
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,731 INFO epoch # 1812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03763483805232681
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,810 INFO epoch # 1813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03956166759599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,894 INFO epoch # 1814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040547421580413356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:10,974 INFO epoch # 1815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03898906378890388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,056 INFO epoch # 1816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03748512006131932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,136 INFO epoch # 1817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0368678500817623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,218 INFO epoch # 1818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03950505974353291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,298 INFO epoch # 1819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03884201101027429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,380 INFO epoch # 1820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0386455895495601
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:11,380 INFO *** epoch 1820, rolling-avg-loss (window=10)= 0.03872921088477597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,460 INFO epoch # 1821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03993160973186605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,540 INFO epoch # 1822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04030749696539715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,626 INFO epoch # 1823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03983856365084648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,708 INFO epoch # 1824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03687515822821297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,786 INFO epoch # 1825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03751090739388019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,869 INFO epoch # 1826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04042375652352348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:11,950 INFO epoch # 1827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03829927567858249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,039 INFO epoch # 1828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03838083625305444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,126 INFO epoch # 1829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03644774403073825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,207 INFO epoch # 1830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04267517162952572
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:12,207 INFO *** epoch 1830, rolling-avg-loss (window=10)= 0.03906905200856272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,286 INFO epoch # 1831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03945086357998662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,367 INFO epoch # 1832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03673271383740939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,447 INFO epoch # 1833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04016123845940456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,526 INFO epoch # 1834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03654618310974911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,607 INFO epoch # 1835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04022424930008128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,686 INFO epoch # 1836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03921921082655899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,765 INFO epoch # 1837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03804931207560003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,848 INFO epoch # 1838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0377545302035287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:12,926 INFO epoch # 1839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0376242911152076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,004 INFO epoch # 1840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03896218413137831
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:13,005 INFO *** epoch 1840, rolling-avg-loss (window=10)= 0.03847247766389046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,085 INFO epoch # 1841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04075431986711919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,163 INFO epoch # 1842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040934706514235586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,242 INFO epoch # 1843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03716487865312956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,322 INFO epoch # 1844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.039866468927357346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,403 INFO epoch # 1845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04284522702801041
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,481 INFO epoch # 1846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03659465690725483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,559 INFO epoch # 1847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037326638703234494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,640 INFO epoch # 1848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03687446171534248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,719 INFO epoch # 1849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03977833269163966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,797 INFO epoch # 1850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03783817167277448
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:13,797 INFO *** epoch 1850, rolling-avg-loss (window=10)= 0.0389977862680098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,879 INFO epoch # 1851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.041828718734905124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:13,957 INFO epoch # 1852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03800678288098425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,040 INFO epoch # 1853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03748307970818132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,122 INFO epoch # 1854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03772340604336932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,202 INFO epoch # 1855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037806303589604795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,280 INFO epoch # 1856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03877350181574002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,361 INFO epoch # 1857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03931151208234951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,439 INFO epoch # 1858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03746380071970634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,517 INFO epoch # 1859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.038068815832957625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,594 INFO epoch # 1860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036115224385866895
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:14,594 INFO *** epoch 1860, rolling-avg-loss (window=10)= 0.03825811457936652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,673 INFO epoch # 1861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03784583622473292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,750 INFO epoch # 1862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03740386798745021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,830 INFO epoch # 1863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.041782201995374635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,910 INFO epoch # 1864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037891515967203304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:14,991 INFO epoch # 1865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03665208542952314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,072 INFO epoch # 1866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0413965848274529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,153 INFO epoch # 1867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.038986138912150636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,233 INFO epoch # 1868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.039464747707825154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,314 INFO epoch # 1869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03923327385564335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,392 INFO epoch # 1870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03484581803786568
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:15,392 INFO *** epoch 1870, rolling-avg-loss (window=10)= 0.03855020709452219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,471 INFO epoch # 1871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036224444513209164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,549 INFO epoch # 1872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.038028202339774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,627 INFO epoch # 1873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03571229201043025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,706 INFO epoch # 1874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040886345494072884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,784 INFO epoch # 1875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03554702634573914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,866 INFO epoch # 1876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0366157787211705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:15,946 INFO epoch # 1877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03680081680067815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,023 INFO epoch # 1878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.039854906528489664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,102 INFO epoch # 1879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0357908095465973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,180 INFO epoch # 1880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03857739083468914
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:16,180 INFO *** epoch 1880, rolling-avg-loss (window=10)= 0.037403801313485016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,260 INFO epoch # 1881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.038981253339443356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,339 INFO epoch # 1882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03473204362671822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,418 INFO epoch # 1883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036018377577420324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,496 INFO epoch # 1884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035647137148771435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,573 INFO epoch # 1885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.038901266059838235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,675 INFO epoch # 1886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03628523898078129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,760 INFO epoch # 1887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03698125257506035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,841 INFO epoch # 1888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036445650417590514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:16,923 INFO epoch # 1889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03675050244783051
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,001 INFO epoch # 1890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03887835977366194
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:17,001 INFO *** epoch 1890, rolling-avg-loss (window=10)= 0.036962108194711615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,080 INFO epoch # 1891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.041154757200274616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,159 INFO epoch # 1892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03680184163386002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,237 INFO epoch # 1893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03385925298789516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,316 INFO epoch # 1894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04115952833672054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,397 INFO epoch # 1895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03954269367386587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,476 INFO epoch # 1896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035159684339305386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,554 INFO epoch # 1897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03685257799224928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,635 INFO epoch # 1898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04132271447451785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,714 INFO epoch # 1899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034253732417710125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,794 INFO epoch # 1900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036590325355064124
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:17,794 INFO *** epoch 1900, rolling-avg-loss (window=10)= 0.0376697108411463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,873 INFO epoch # 1901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03816283404012211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:17,953 INFO epoch # 1902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03465308831073344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,031 INFO epoch # 1903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03531185712199658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,113 INFO epoch # 1904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033643206203123555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,191 INFO epoch # 1905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035770437272731215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,273 INFO epoch # 1906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03888993029249832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,354 INFO epoch # 1907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03456879482837394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,432 INFO epoch # 1908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.039333743538009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,511 INFO epoch # 1909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0378330473031383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,589 INFO epoch # 1910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03647306113271043
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:18,589 INFO *** epoch 1910, rolling-avg-loss (window=10)= 0.03646400000434369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,668 INFO epoch # 1911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.040941897372249514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,746 INFO epoch # 1912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.038568263582419604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,827 INFO epoch # 1913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03796458325814456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,905 INFO epoch # 1914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034336574957706034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:18,984 INFO epoch # 1915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034052265604259446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,065 INFO epoch # 1916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034541225468274206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,150 INFO epoch # 1917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03512041046633385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,229 INFO epoch # 1918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.038150629261508584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,311 INFO epoch # 1919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036449275707127526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,392 INFO epoch # 1920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03757402568589896
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:19,392 INFO *** epoch 1920, rolling-avg-loss (window=10)= 0.03676991513639223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,471 INFO epoch # 1921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03890689753461629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,550 INFO epoch # 1922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03791271854424849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,635 INFO epoch # 1923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036341746570542455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,713 INFO epoch # 1924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03340032318374142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,793 INFO epoch # 1925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033460771694080904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,873 INFO epoch # 1926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03398627965361811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:19,954 INFO epoch # 1927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034345331630902365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,035 INFO epoch # 1928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03505317887174897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,114 INFO epoch # 1929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03514429720235057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,193 INFO epoch # 1930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036245217503164895
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:20,193 INFO *** epoch 1930, rolling-avg-loss (window=10)= 0.03547967623890145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,281 INFO epoch # 1931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03995509369997308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,363 INFO epoch # 1932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034497836604714394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,444 INFO epoch # 1933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0354762245551683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,525 INFO epoch # 1934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03557295026257634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,613 INFO epoch # 1935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03874294550041668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,694 INFO epoch # 1936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03544819224043749
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,773 INFO epoch # 1937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036626284912927076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,854 INFO epoch # 1938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03411381764453836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:20,936 INFO epoch # 1939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032288119837176055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,017 INFO epoch # 1940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0341056511097122
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:21,017 INFO *** epoch 1940, rolling-avg-loss (window=10)= 0.035682711636763995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,095 INFO epoch # 1941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03506954532349482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,175 INFO epoch # 1942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037406365969218314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,254 INFO epoch # 1943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035691921279067174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,334 INFO epoch # 1944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033957958803512156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,414 INFO epoch # 1945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03226093255216256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,492 INFO epoch # 1946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0340023560274858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,570 INFO epoch # 1947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036533519421936944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,649 INFO epoch # 1948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03581666172249243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,727 INFO epoch # 1949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03529417983372696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,806 INFO epoch # 1950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0327482299762778
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:21,806 INFO *** epoch 1950, rolling-avg-loss (window=10)= 0.034878167090937495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,887 INFO epoch # 1951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033577542344573885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:21,965 INFO epoch # 1952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03416817818651907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,044 INFO epoch # 1953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03497715128469281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,125 INFO epoch # 1954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03485390229616314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,203 INFO epoch # 1955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03578849497716874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,282 INFO epoch # 1956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03266729594906792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,363 INFO epoch # 1957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03396375908050686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,442 INFO epoch # 1958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03487876648432575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,519 INFO epoch # 1959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036508598655927926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,599 INFO epoch # 1960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036026814370416105
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:22,599 INFO *** epoch 1960, rolling-avg-loss (window=10)= 0.03474105036293622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,679 INFO epoch # 1961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03916168463183567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,758 INFO epoch # 1962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0354619306162931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,837 INFO epoch # 1963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03425122037879191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,917 INFO epoch # 1964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035914584324928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:22,998 INFO epoch # 1965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.04056274541653693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:23,079 INFO epoch # 1966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03632788028335199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:23,159 INFO epoch # 1967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03325740768923424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:23,240 INFO epoch # 1968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03207016852684319
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:23,322 INFO epoch # 1969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035200556070776656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:23,406 INFO epoch # 1970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0359327353653498
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:23,406 INFO *** epoch 1970, rolling-avg-loss (window=10)= 0.03581409133039415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:23,504 INFO epoch # 1971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03211794272647239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:23,589 INFO epoch # 1972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033157158351968974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:23,684 INFO epoch # 1973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034341283957473934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:23,772 INFO epoch # 1974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03429885889636353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:23,856 INFO epoch # 1975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03495427977759391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:23,936 INFO epoch # 1976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03224256637622602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,014 INFO epoch # 1977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032420369650935754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,095 INFO epoch # 1978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03389390843221918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,179 INFO epoch # 1979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03275878870044835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,259 INFO epoch # 1980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036121533426921815
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:24,259 INFO *** epoch 1980, rolling-avg-loss (window=10)= 0.033630669029662386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,343 INFO epoch # 1981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03437623525678646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,423 INFO epoch # 1982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03267900574428495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,501 INFO epoch # 1983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036499183683190495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,581 INFO epoch # 1984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03421823267126456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,660 INFO epoch # 1985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03388199498294853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,738 INFO epoch # 1986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03303906659130007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,819 INFO epoch # 1987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03406568961509038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,899 INFO epoch # 1988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032566772046266124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:24,980 INFO epoch # 1989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033077545260312036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,061 INFO epoch # 1990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03190150239970535
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:25,061 INFO *** epoch 1990, rolling-avg-loss (window=10)= 0.033630522825114896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,141 INFO epoch # 1991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03622101608198136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,220 INFO epoch # 1992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03365400974871591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,298 INFO epoch # 1993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0325588014384266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,377 INFO epoch # 1994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03195525531191379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,456 INFO epoch # 1995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033060389483580366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,534 INFO epoch # 1996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03890049541951157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,613 INFO epoch # 1997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03306570285349153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,691 INFO epoch # 1998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03661599283805117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,768 INFO epoch # 1999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031811812572414055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,848 INFO epoch # 2000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03375334816519171
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:25,848 INFO *** epoch 2000, rolling-avg-loss (window=10)= 0.03415968239132781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:25,927 INFO epoch # 2001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03338524917489849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,004 INFO epoch # 2002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03412733998266049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,083 INFO epoch # 2003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03174884105101228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,162 INFO epoch # 2004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031518460367806256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,242 INFO epoch # 2005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03226573715801351
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,321 INFO epoch # 2006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03493246831931174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,401 INFO epoch # 2007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03133791597792879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,479 INFO epoch # 2008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034546356357168406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,559 INFO epoch # 2009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0327905080630444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,638 INFO epoch # 2010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036445289850234985
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:26,639 INFO *** epoch 2010, rolling-avg-loss (window=10)= 0.033309816630207933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,717 INFO epoch # 2011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03670576194417663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,798 INFO epoch # 2012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037310694257030264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,878 INFO epoch # 2013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0333009394526016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:26,958 INFO epoch # 2014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030484376125968993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,037 INFO epoch # 2015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03291311042266898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,117 INFO epoch # 2016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03423295574611984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,197 INFO epoch # 2017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03717712507932447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,278 INFO epoch # 2018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03329552884679288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,359 INFO epoch # 2019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03425722266547382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,439 INFO epoch # 2020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03279866056982428
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:27,439 INFO *** epoch 2020, rolling-avg-loss (window=10)= 0.03424763751099817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,521 INFO epoch # 2021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032054102019174024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,601 INFO epoch # 2022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031820680655073375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,684 INFO epoch # 2023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03518042960786261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,763 INFO epoch # 2024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031539705058094114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,845 INFO epoch # 2025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03525543975410983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:27,926 INFO epoch # 2026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0320754052081611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,006 INFO epoch # 2027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031779778480995446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,089 INFO epoch # 2028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032045729603851214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,169 INFO epoch # 2029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035558340343413875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,250 INFO epoch # 2030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03406329496647231
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:28,250 INFO *** epoch 2030, rolling-avg-loss (window=10)= 0.03313729056972079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,334 INFO epoch # 2031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03902546703466214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,415 INFO epoch # 2032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033063982788007706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,493 INFO epoch # 2033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030809958931058645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,571 INFO epoch # 2034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03354108557687141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,652 INFO epoch # 2035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03543408008408733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,731 INFO epoch # 2036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03040614802739583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,812 INFO epoch # 2037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03457269162754528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,892 INFO epoch # 2038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03307831293204799
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:28,970 INFO epoch # 2039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03377600270323455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,050 INFO epoch # 2040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031240619500749744
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:29,050 INFO *** epoch 2040, rolling-avg-loss (window=10)= 0.03349483492056606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,134 INFO epoch # 2041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03494239249266684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,213 INFO epoch # 2042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03305489418562502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,292 INFO epoch # 2043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03307765905628912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,373 INFO epoch # 2044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03111005120445043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,451 INFO epoch # 2045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034648865897906944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,531 INFO epoch # 2046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030269404407590628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,612 INFO epoch # 2047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030329037428600714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,693 INFO epoch # 2048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032232039695372805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,772 INFO epoch # 2049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034172502637375146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,853 INFO epoch # 2050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033521064469823614
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:29,853 INFO *** epoch 2050, rolling-avg-loss (window=10)= 0.03273579114757012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:29,933 INFO epoch # 2051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030809705785941333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,011 INFO epoch # 2052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03410897409776226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,091 INFO epoch # 2053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0335846668749582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,169 INFO epoch # 2054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031665158341638744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,248 INFO epoch # 2055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03553502543945797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,329 INFO epoch # 2056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03834112110780552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,410 INFO epoch # 2057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03313380805775523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,490 INFO epoch # 2058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03149497520644218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,569 INFO epoch # 2059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03143052206723951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,654 INFO epoch # 2060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03296667421818711
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:30,654 INFO *** epoch 2060, rolling-avg-loss (window=10)= 0.0333070631197188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,734 INFO epoch # 2061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030838271894026548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,816 INFO epoch # 2062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031584343494614586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,896 INFO epoch # 2063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03463614149950445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:30,974 INFO epoch # 2064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030243297951528803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,053 INFO epoch # 2065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03227016411256045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,133 INFO epoch # 2066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03162101443740539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,212 INFO epoch # 2067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029663364868611097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,291 INFO epoch # 2068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035349677404155955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,372 INFO epoch # 2069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03206107870209962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,450 INFO epoch # 2070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0294452540401835
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:31,450 INFO *** epoch 2070, rolling-avg-loss (window=10)= 0.03177126084046904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,529 INFO epoch # 2071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0324606713547837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,610 INFO epoch # 2072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034050591581035405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,689 INFO epoch # 2073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031596138200256974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,766 INFO epoch # 2074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03155524734756909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,846 INFO epoch # 2075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029805891012074426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:31,924 INFO epoch # 2076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034740826522465795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,003 INFO epoch # 2077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031793228088645265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,082 INFO epoch # 2078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033862025855341926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,164 INFO epoch # 2079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03363561461446807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,246 INFO epoch # 2080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03733260300941765
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:32,246 INFO *** epoch 2080, rolling-avg-loss (window=10)= 0.03308328375860583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,328 INFO epoch # 2081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031094380421563983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,411 INFO epoch # 2082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030992310610599816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,493 INFO epoch # 2083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03434551405371167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,573 INFO epoch # 2084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03717950894497335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,654 INFO epoch # 2085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03660774577292614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,733 INFO epoch # 2086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030846442707115784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,814 INFO epoch # 2087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034760189038934186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,896 INFO epoch # 2088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031030986341647804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:32,975 INFO epoch # 2089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03446186921792105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,055 INFO epoch # 2090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033894765976583585
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:33,056 INFO *** epoch 2090, rolling-avg-loss (window=10)= 0.03352137130859774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,135 INFO epoch # 2091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03184611882898025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,217 INFO epoch # 2092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02931266071391292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,298 INFO epoch # 2093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031955200305674225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,382 INFO epoch # 2094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03074048724374734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,462 INFO epoch # 2095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030021486163605005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,542 INFO epoch # 2096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0327914577210322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,627 INFO epoch # 2097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030642119410913438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,707 INFO epoch # 2098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031054248131113127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,790 INFO epoch # 2099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03259135878761299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,875 INFO epoch # 2100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030050607514567673
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:33,875 INFO *** epoch 2100, rolling-avg-loss (window=10)= 0.031100574482115916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:33,963 INFO epoch # 2101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030675038520712405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,044 INFO epoch # 2102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03228391602169722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,124 INFO epoch # 2103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03352887695655227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,204 INFO epoch # 2104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03666695632273331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,286 INFO epoch # 2105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03106045300955884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,368 INFO epoch # 2106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0332179460383486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,448 INFO epoch # 2107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029592789360322058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,527 INFO epoch # 2108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03252456249902025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,606 INFO epoch # 2109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02986433359910734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,689 INFO epoch # 2110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03193346879561432
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:34,689 INFO *** epoch 2110, rolling-avg-loss (window=10)= 0.03213483411236666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,769 INFO epoch # 2111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03711746615590528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,851 INFO epoch # 2112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0309166451334022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:34,933 INFO epoch # 2113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03312589632696472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,016 INFO epoch # 2114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033941979811061174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,098 INFO epoch # 2115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030515132180880755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,177 INFO epoch # 2116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033243695565033704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,255 INFO epoch # 2117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03211271174950525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,336 INFO epoch # 2118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02927187961176969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,415 INFO epoch # 2119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02973917927010916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,493 INFO epoch # 2120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03461394892656244
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:35,493 INFO *** epoch 2120, rolling-avg-loss (window=10)= 0.032459853473119436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,572 INFO epoch # 2121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03270374363637529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,652 INFO epoch # 2122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031277887552278116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,729 INFO epoch # 2123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03032653039554134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,810 INFO epoch # 2124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029765021638013422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,890 INFO epoch # 2125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032521596003789455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:35,968 INFO epoch # 2126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03014200658071786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,047 INFO epoch # 2127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03053262660978362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,127 INFO epoch # 2128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032471464219270274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,205 INFO epoch # 2129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029739657242316753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,282 INFO epoch # 2130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029235682886792347
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:36,283 INFO *** epoch 2130, rolling-avg-loss (window=10)= 0.030871621676487847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,364 INFO epoch # 2131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029538164468249306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,443 INFO epoch # 2132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03454289384535514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,521 INFO epoch # 2133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032249008771032095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,598 INFO epoch # 2134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030111124244285747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,678 INFO epoch # 2135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030677795264637098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,756 INFO epoch # 2136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032125991769135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,836 INFO epoch # 2137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030299058591481298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,915 INFO epoch # 2138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030878317600581795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:36,993 INFO epoch # 2139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0346951560350135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,071 INFO epoch # 2140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02900674447300844
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:37,071 INFO *** epoch 2140, rolling-avg-loss (window=10)= 0.031412425506277944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,150 INFO epoch # 2141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030284036562079564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,228 INFO epoch # 2142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03433339510229416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,308 INFO epoch # 2143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03136221584281884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,391 INFO epoch # 2144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032271688018226996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,472 INFO epoch # 2145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03306702052941546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,553 INFO epoch # 2146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033835003356216475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,632 INFO epoch # 2147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03060929868661333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,712 INFO epoch # 2148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03148735893773846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,791 INFO epoch # 2149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033051164384232834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,870 INFO epoch # 2150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02991993958130479
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:37,870 INFO *** epoch 2150, rolling-avg-loss (window=10)= 0.03202211210009409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:37,951 INFO epoch # 2151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029075122758513317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,029 INFO epoch # 2152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0292603338311892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,108 INFO epoch # 2153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03557739203097299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,186 INFO epoch # 2154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03148200694704428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,266 INFO epoch # 2155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031234474008670077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,344 INFO epoch # 2156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030308660367154516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,425 INFO epoch # 2157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03008281788788736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,503 INFO epoch # 2158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033814031310612336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,581 INFO epoch # 2159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028606232081074268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,660 INFO epoch # 2160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030386872676899657
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:38,660 INFO *** epoch 2160, rolling-avg-loss (window=10)= 0.0309827943900018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,740 INFO epoch # 2161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03147381964663509
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,820 INFO epoch # 2162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02873029821785167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,900 INFO epoch # 2163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03149260635836981
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:38,978 INFO epoch # 2164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03341216457192786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:39,057 INFO epoch # 2165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031127887981710956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:39,138 INFO epoch # 2166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031600443384377286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:39,217 INFO epoch # 2167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029688920767512172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:39,295 INFO epoch # 2168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03043169219745323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:39,404 INFO epoch # 2169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029921420849859715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:39,520 INFO epoch # 2170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028395874804118648
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:39,520 INFO *** epoch 2170, rolling-avg-loss (window=10)= 0.030627512877981645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:39,642 INFO epoch # 2171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029949640622362494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:39,746 INFO epoch # 2172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03119302030245308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:39,828 INFO epoch # 2173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02894631284289062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:39,909 INFO epoch # 2174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03132295818068087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:39,987 INFO epoch # 2175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03226736007491127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,065 INFO epoch # 2176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0305188819183968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,144 INFO epoch # 2177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03226889154757373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,222 INFO epoch # 2178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035031999490456656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,302 INFO epoch # 2179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030312290095025674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,386 INFO epoch # 2180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03338781278580427
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:40,386 INFO *** epoch 2180, rolling-avg-loss (window=10)= 0.03151991678605555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,469 INFO epoch # 2181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030865799490129575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,549 INFO epoch # 2182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03566253025201149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,631 INFO epoch # 2183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030404468561755493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,709 INFO epoch # 2184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027638501051114872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,788 INFO epoch # 2185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029580682778032497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,868 INFO epoch # 2186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03153730896883644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:40,947 INFO epoch # 2187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030824354907963425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,025 INFO epoch # 2188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030906048516044393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,103 INFO epoch # 2189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028497515741037205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,182 INFO epoch # 2190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03196916394517757
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:41,182 INFO *** epoch 2190, rolling-avg-loss (window=10)= 0.030788637421210296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,262 INFO epoch # 2191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030013639712706208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,341 INFO epoch # 2192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03544537478592247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,425 INFO epoch # 2193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030850946408463642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,503 INFO epoch # 2194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03157819074112922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,582 INFO epoch # 2195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028721942944684997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,660 INFO epoch # 2196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03237740197801031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,740 INFO epoch # 2197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028522221255116165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,817 INFO epoch # 2198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027980118757113814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,895 INFO epoch # 2199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028016586700687185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:41,974 INFO epoch # 2200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028552527801366523
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:41,974 INFO *** epoch 2200, rolling-avg-loss (window=10)= 0.030205895108520054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,052 INFO epoch # 2201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031983890396077186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,136 INFO epoch # 2202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02923188215936534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,218 INFO epoch # 2203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03052266781742219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,299 INFO epoch # 2204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02883014816325158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,380 INFO epoch # 2205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02963445067871362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,460 INFO epoch # 2206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028463776427088305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,541 INFO epoch # 2207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027617059356998652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,630 INFO epoch # 2208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028817990008974448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,710 INFO epoch # 2209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03136728884419426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,791 INFO epoch # 2210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03135800187010318
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:42,791 INFO *** epoch 2210, rolling-avg-loss (window=10)= 0.029782715572218878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,872 INFO epoch # 2211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02883401676081121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:42,954 INFO epoch # 2212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027251558552961797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,035 INFO epoch # 2213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0314642601297237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,114 INFO epoch # 2214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028141764167230576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,194 INFO epoch # 2215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030820741987554356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,274 INFO epoch # 2216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03012258038506843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,356 INFO epoch # 2217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02920008808723651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,436 INFO epoch # 2218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029509289393899962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,516 INFO epoch # 2219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028788530034944415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,596 INFO epoch # 2220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030288938782177866
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:43,596 INFO *** epoch 2220, rolling-avg-loss (window=10)= 0.02944217682816088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,677 INFO epoch # 2221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027449275163235143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,759 INFO epoch # 2222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02975192028679885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,845 INFO epoch # 2223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03232953973929398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:43,924 INFO epoch # 2224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029439146775985137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,004 INFO epoch # 2225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028341680532321334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,088 INFO epoch # 2226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03268822678364813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,172 INFO epoch # 2227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02893378896987997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,253 INFO epoch # 2228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02849426883039996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,336 INFO epoch # 2229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02843116089934483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,420 INFO epoch # 2230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028044435486663133
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:44,420 INFO *** epoch 2230, rolling-avg-loss (window=10)= 0.029390344346757045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,500 INFO epoch # 2231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027707957284292206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,581 INFO epoch # 2232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03001454650075175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,662 INFO epoch # 2233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031249009509338066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,740 INFO epoch # 2234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028504054236691445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,819 INFO epoch # 2235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031124059489229694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,902 INFO epoch # 2236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030924158287234604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:44,982 INFO epoch # 2237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02727137549663894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,062 INFO epoch # 2238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029411670664558187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,144 INFO epoch # 2239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027467197796795517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,225 INFO epoch # 2240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0293508520553587
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:45,225 INFO *** epoch 2240, rolling-avg-loss (window=10)= 0.02930248813208891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,306 INFO epoch # 2241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029501723969588056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,388 INFO epoch # 2242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029723668150836602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,469 INFO epoch # 2243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02945778050343506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,551 INFO epoch # 2244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029087783390423283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,631 INFO epoch # 2245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02682011088472791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,710 INFO epoch # 2246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030330648965900764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,790 INFO epoch # 2247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026685563498176634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,869 INFO epoch # 2248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030440895148785785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:45,950 INFO epoch # 2249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02961584768490866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,029 INFO epoch # 2250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03185713151469827
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:46,029 INFO *** epoch 2250, rolling-avg-loss (window=10)= 0.029352115371148103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,110 INFO epoch # 2251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03465161542408168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,189 INFO epoch # 2252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02740379501483403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,268 INFO epoch # 2253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029199934855569154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,352 INFO epoch # 2254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029624819551827386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,433 INFO epoch # 2255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02664774635923095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,512 INFO epoch # 2256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028972279134904966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,591 INFO epoch # 2257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03014856990193948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,671 INFO epoch # 2258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031193864822853357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,752 INFO epoch # 2259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03173944272566587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,830 INFO epoch # 2260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02753257771837525
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:46,830 INFO *** epoch 2260, rolling-avg-loss (window=10)= 0.029711464550928213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,913 INFO epoch # 2261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028820935054682195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:46,993 INFO epoch # 2262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02982670141500421
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,072 INFO epoch # 2263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02834909323428292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,152 INFO epoch # 2264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02775025347364135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,231 INFO epoch # 2265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030266833084169775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,311 INFO epoch # 2266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028839251986937597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,394 INFO epoch # 2267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029707893525483087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,475 INFO epoch # 2268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029713498195633292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,556 INFO epoch # 2269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03148360628983937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,636 INFO epoch # 2270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028264375228900462
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:47,637 INFO *** epoch 2270, rolling-avg-loss (window=10)= 0.029302244148857425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,717 INFO epoch # 2271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02829842857317999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,803 INFO epoch # 2272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02723168526426889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,889 INFO epoch # 2273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026593342248816043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:47,969 INFO epoch # 2274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029547570447903126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,049 INFO epoch # 2275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0282865495828446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,128 INFO epoch # 2276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02778185100760311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,209 INFO epoch # 2277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02998155597015284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,287 INFO epoch # 2278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027627255956758745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,368 INFO epoch # 2279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028255556419026107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,448 INFO epoch # 2280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02829825434309896
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:48,448 INFO *** epoch 2280, rolling-avg-loss (window=10)= 0.028190204981365242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,527 INFO epoch # 2281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026666605815989897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,606 INFO epoch # 2282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026326311839511618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,686 INFO epoch # 2283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029700346058234572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,766 INFO epoch # 2284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02658948933822103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,847 INFO epoch # 2285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027332793571986258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:48,925 INFO epoch # 2286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027621020941296592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,004 INFO epoch # 2287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025323497626231983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,088 INFO epoch # 2288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027026968862628564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,168 INFO epoch # 2289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026846951848710887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,247 INFO epoch # 2290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026157027576118708
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:49,247 INFO *** epoch 2290, rolling-avg-loss (window=10)= 0.026959101347893012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,327 INFO epoch # 2291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026653802720829844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,407 INFO epoch # 2292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027475180046167225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,485 INFO epoch # 2293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031439329351997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,564 INFO epoch # 2294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027324318303726614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,643 INFO epoch # 2295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03032275816076435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,721 INFO epoch # 2296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02750264760106802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,801 INFO epoch # 2297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02555919482256286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,883 INFO epoch # 2298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027292140672216192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:49,961 INFO epoch # 2299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025539649184793234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,038 INFO epoch # 2300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030551975476555526
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:50,038 INFO *** epoch 2300, rolling-avg-loss (window=10)= 0.027966099634068087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,118 INFO epoch # 2301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030301397520815954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,195 INFO epoch # 2302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031009288708446547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,274 INFO epoch # 2303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02839479924296029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,354 INFO epoch # 2304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027411169445258565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,433 INFO epoch # 2305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03252453412278555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,511 INFO epoch # 2306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025926595102646388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,591 INFO epoch # 2307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025591462370357476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,669 INFO epoch # 2308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030365396931301802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,748 INFO epoch # 2309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028165459778392687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,828 INFO epoch # 2310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02922174776904285
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:50,828 INFO *** epoch 2310, rolling-avg-loss (window=10)= 0.02889118509920081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,907 INFO epoch # 2311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027848502271808684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:50,985 INFO epoch # 2312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029177262651501223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,062 INFO epoch # 2313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0269059841230046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,139 INFO epoch # 2314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027822613483294845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,219 INFO epoch # 2315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029367759940214455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,297 INFO epoch # 2316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02602812205441296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,376 INFO epoch # 2317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0264768288761843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,455 INFO epoch # 2318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028301289188675582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,533 INFO epoch # 2319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029119833634467795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,611 INFO epoch # 2320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030577900761272758
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:51,611 INFO *** epoch 2320, rolling-avg-loss (window=10)= 0.02816260969848372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,690 INFO epoch # 2321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027040332410251722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,768 INFO epoch # 2322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02508751448476687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,845 INFO epoch # 2323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02849564974894747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:51,924 INFO epoch # 2324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0262203331803903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,002 INFO epoch # 2325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029176504889619537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,079 INFO epoch # 2326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025456416100496426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,156 INFO epoch # 2327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030373098998097703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,233 INFO epoch # 2328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02784178248839453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,313 INFO epoch # 2329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02445631512091495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,392 INFO epoch # 2330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02687811476062052
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:52,393 INFO *** epoch 2330, rolling-avg-loss (window=10)= 0.027102606218250003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,475 INFO epoch # 2331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02764341188594699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,553 INFO epoch # 2332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028638284682529047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,633 INFO epoch # 2333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0273738519026665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,710 INFO epoch # 2334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029444044484989718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,788 INFO epoch # 2335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030051816866034642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,867 INFO epoch # 2336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024988079559989274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:52,946 INFO epoch # 2337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029134204873116687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,023 INFO epoch # 2338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025892028002999723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,100 INFO epoch # 2339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02667745904182084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,179 INFO epoch # 2340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027688275557011366
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:53,179 INFO *** epoch 2340, rolling-avg-loss (window=10)= 0.027753145685710478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,256 INFO epoch # 2341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026664334698580205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,334 INFO epoch # 2342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03205647299182601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,413 INFO epoch # 2343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0274960957467556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,491 INFO epoch # 2344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027428308472735807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,570 INFO epoch # 2345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029037043743301183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,651 INFO epoch # 2346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026445632480317727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,729 INFO epoch # 2347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025798110727919266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,806 INFO epoch # 2348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026306114625185728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,886 INFO epoch # 2349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02958593165385537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:53,963 INFO epoch # 2350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026224721354083158
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:53,963 INFO *** epoch 2350, rolling-avg-loss (window=10)= 0.027704276649456006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,041 INFO epoch # 2351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029254250752273947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,121 INFO epoch # 2352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02703491714783013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,200 INFO epoch # 2353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027448740147519857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,278 INFO epoch # 2354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025545410811901093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,359 INFO epoch # 2355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025970830960432068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,438 INFO epoch # 2356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026099936687387526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,516 INFO epoch # 2357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02756320414482616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,596 INFO epoch # 2358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02724778634728864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,674 INFO epoch # 2359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025457382755121216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,752 INFO epoch # 2360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025299134984379634
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:54,752 INFO *** epoch 2360, rolling-avg-loss (window=10)= 0.026692159473896027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,833 INFO epoch # 2361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027818457936518826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,911 INFO epoch # 2362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029530288215028122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:54,989 INFO epoch # 2363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026622871053405106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,067 INFO epoch # 2364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02703780887532048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,145 INFO epoch # 2365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026836131932213902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,225 INFO epoch # 2366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02663798167486675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,303 INFO epoch # 2367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027021133835660294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,382 INFO epoch # 2368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026039652264444157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,463 INFO epoch # 2369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03187923048972152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,541 INFO epoch # 2370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028478245250880718
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:55,541 INFO *** epoch 2370, rolling-avg-loss (window=10)= 0.027790180152805988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,619 INFO epoch # 2371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02819975558668375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,698 INFO epoch # 2372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025650990806752816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,777 INFO epoch # 2373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027191709290491417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,856 INFO epoch # 2374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02605776886048261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:55,935 INFO epoch # 2375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025169571134028956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,013 INFO epoch # 2376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02763660269556567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,091 INFO epoch # 2377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026923174620606005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,170 INFO epoch # 2378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024287167005240917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,249 INFO epoch # 2379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027344484842615202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,328 INFO epoch # 2380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02757625246886164
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:56,329 INFO *** epoch 2380, rolling-avg-loss (window=10)= 0.026603747731132898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,408 INFO epoch # 2381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02757543805637397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,486 INFO epoch # 2382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02497369131015148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,565 INFO epoch # 2383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025586021511116996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,643 INFO epoch # 2384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030776176688959822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,723 INFO epoch # 2385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031419370556250215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,802 INFO epoch # 2386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028764432267053053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,886 INFO epoch # 2387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02636349099338986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:56,964 INFO epoch # 2388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0266339901718311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,044 INFO epoch # 2389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030463774513918906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,124 INFO epoch # 2390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025017625870532356
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:57,124 INFO *** epoch 2390, rolling-avg-loss (window=10)= 0.027757401193957774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,203 INFO epoch # 2391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025913320248946548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,283 INFO epoch # 2392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027290397963952273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,364 INFO epoch # 2393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026731933205155656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,445 INFO epoch # 2394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026326361345127225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,524 INFO epoch # 2395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026207959483144805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,605 INFO epoch # 2396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024983845854876563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,692 INFO epoch # 2397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026592657435685396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,772 INFO epoch # 2398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030663727724459022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,852 INFO epoch # 2399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027232872729655355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:57,932 INFO epoch # 2400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02694030936982017
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:57,932 INFO *** epoch 2400, rolling-avg-loss (window=10)= 0.0268883385360823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,014 INFO epoch # 2401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02777545593562536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,097 INFO epoch # 2402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028182702808408067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,178 INFO epoch # 2403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034537454281235114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,273 INFO epoch # 2404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027017867716494948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,368 INFO epoch # 2405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027290173980873078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,463 INFO epoch # 2406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02514514941140078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,555 INFO epoch # 2407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02595754523645155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,640 INFO epoch # 2408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02614970610011369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,721 INFO epoch # 2409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02442078446620144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,801 INFO epoch # 2410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02566716994624585
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:58,801 INFO *** epoch 2410, rolling-avg-loss (window=10)= 0.027214400988304986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,884 INFO epoch # 2411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02393443271284923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:58,967 INFO epoch # 2412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02468378667254001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:59,049 INFO epoch # 2413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025191835666191764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:59,132 INFO epoch # 2414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02642401895718649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:59,220 INFO epoch # 2415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02660344267496839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:59,311 INFO epoch # 2416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02542742746300064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:59,401 INFO epoch # 2417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025988107052398846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:59,483 INFO epoch # 2418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02562621919787489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:59,564 INFO epoch # 2419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02652107024914585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:59,644 INFO epoch # 2420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024231961026089266
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:16:59,644 INFO *** epoch 2420, rolling-avg-loss (window=10)= 0.025463230167224536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:59,726 INFO epoch # 2421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026410196063807234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:59,805 INFO epoch # 2422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027458019962068647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:16:59,918 INFO epoch # 2423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028001175640383735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,013 INFO epoch # 2424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026272942399373278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,097 INFO epoch # 2425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025623329434893094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,179 INFO epoch # 2426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027377068283385597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,258 INFO epoch # 2427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02501665469026193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,344 INFO epoch # 2428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024081256095087156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,429 INFO epoch # 2429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025673651238321327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,510 INFO epoch # 2430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029197915137046948
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:00,510 INFO *** epoch 2430, rolling-avg-loss (window=10)= 0.026511220894462895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,591 INFO epoch # 2431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02813801848969888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,679 INFO epoch # 2432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025769794010557234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,761 INFO epoch # 2433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025060026586288586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,841 INFO epoch # 2434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026520323794102296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:00,923 INFO epoch # 2435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02704732894198969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,002 INFO epoch # 2436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026465302711585537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,090 INFO epoch # 2437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027132757793879136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,172 INFO epoch # 2438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034106966078979895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,251 INFO epoch # 2439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026733985083410516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,333 INFO epoch # 2440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025629566836869344
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:01,334 INFO *** epoch 2440, rolling-avg-loss (window=10)= 0.027260407032736113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,417 INFO epoch # 2441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02490537299308926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,495 INFO epoch # 2442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02603605535114184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,573 INFO epoch # 2443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02465682875481434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,654 INFO epoch # 2444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025832175655523315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,734 INFO epoch # 2445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024449840042507276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,814 INFO epoch # 2446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025299182249000296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,896 INFO epoch # 2447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02460199745837599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:01,974 INFO epoch # 2448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025646186782978475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,053 INFO epoch # 2449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02566710120299831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,131 INFO epoch # 2450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024936859059380367
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:02,131 INFO *** epoch 2450, rolling-avg-loss (window=10)= 0.025203159954980948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,211 INFO epoch # 2451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024583122663898394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,289 INFO epoch # 2452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030601010919781402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,372 INFO epoch # 2453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026033830654341727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,451 INFO epoch # 2454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02593758690636605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,530 INFO epoch # 2455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026704170217271894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,610 INFO epoch # 2456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02494217807543464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,690 INFO epoch # 2457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024444813425361644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,769 INFO epoch # 2458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026060228003188968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,849 INFO epoch # 2459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028090837557101622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:02,930 INFO epoch # 2460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0250199084111955
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:02,930 INFO *** epoch 2460, rolling-avg-loss (window=10)= 0.026241768683394184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,009 INFO epoch # 2461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023400396312354133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,089 INFO epoch # 2462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0252774769323878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,169 INFO epoch # 2463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023846803203923628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,248 INFO epoch # 2464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024963372765341774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,327 INFO epoch # 2465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023749564425088465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,407 INFO epoch # 2466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023233463885844685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,487 INFO epoch # 2467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026240352599415928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,566 INFO epoch # 2468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024390634411247447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,645 INFO epoch # 2469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023682276107138023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,724 INFO epoch # 2470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030218575528124347
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:03,724 INFO *** epoch 2470, rolling-avg-loss (window=10)= 0.024900291617086623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,805 INFO epoch # 2471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026546780252829194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,889 INFO epoch # 2472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023859299602918327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:03,970 INFO epoch # 2473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023825075419154018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:04,048 INFO epoch # 2474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027601248817518353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:04,129 INFO epoch # 2475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024750228214543313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:04,217 INFO epoch # 2476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025908836541930214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:04,297 INFO epoch # 2477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026103908865479752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:04,378 INFO epoch # 2478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02568660900578834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:04,510 INFO epoch # 2479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02454169708653353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:04,640 INFO epoch # 2480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024632192915305495
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:04,640 INFO *** epoch 2480, rolling-avg-loss (window=10)= 0.025345587672200055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:04,726 INFO epoch # 2481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02418606787978206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:04,813 INFO epoch # 2482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023339052218943834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:04,902 INFO epoch # 2483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024808326561469585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:04,982 INFO epoch # 2484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024861109675839543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,062 INFO epoch # 2485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026401919225463644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,142 INFO epoch # 2486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02774763567140326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,224 INFO epoch # 2487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023744550300762057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,305 INFO epoch # 2488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02398777034250088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,387 INFO epoch # 2489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02377144062484149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,468 INFO epoch # 2490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02665563250775449
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:05,468 INFO *** epoch 2490, rolling-avg-loss (window=10)= 0.024950350500876083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,549 INFO epoch # 2491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029331005236599594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,630 INFO epoch # 2492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0252389699453488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,715 INFO epoch # 2493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02600008947774768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,796 INFO epoch # 2494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022195462457602844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,877 INFO epoch # 2495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02259747017524205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:05,958 INFO epoch # 2496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02491157574695535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,044 INFO epoch # 2497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025331678334623575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,126 INFO epoch # 2498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02526003122329712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,206 INFO epoch # 2499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025010619836393744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,286 INFO epoch # 2500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027857538050739095
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:06,286 INFO *** epoch 2500, rolling-avg-loss (window=10)= 0.025373444048454985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,372 INFO epoch # 2501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031429638649569824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,456 INFO epoch # 2502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027656557940645143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,540 INFO epoch # 2503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026715362328104675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,633 INFO epoch # 2504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02507330328808166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,726 INFO epoch # 2505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025527451623929664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,804 INFO epoch # 2506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027478295858600177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,887 INFO epoch # 2507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025354733283165842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:06,965 INFO epoch # 2508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024503218533936888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,044 INFO epoch # 2509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024507292167982087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,123 INFO epoch # 2510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024694474443094805
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:07,124 INFO *** epoch 2510, rolling-avg-loss (window=10)= 0.026294032811711075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,207 INFO epoch # 2511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022902624506969005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,289 INFO epoch # 2512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026831068680621684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,370 INFO epoch # 2513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023737470619380474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,450 INFO epoch # 2514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022910522849997506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,529 INFO epoch # 2515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024061225063633174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,610 INFO epoch # 2516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023221285693580285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,689 INFO epoch # 2517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022561580291949213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,767 INFO epoch # 2518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023232597421156242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,848 INFO epoch # 2519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022895154223078862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:07,927 INFO epoch # 2520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023763740085996687
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:07,927 INFO *** epoch 2520, rolling-avg-loss (window=10)= 0.023611726943636314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,005 INFO epoch # 2521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02404685175861232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,084 INFO epoch # 2522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025422758975764737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,163 INFO epoch # 2523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024142347829183564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,241 INFO epoch # 2524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025112552946666256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,321 INFO epoch # 2525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025189074309309945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,400 INFO epoch # 2526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023970627429662272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,481 INFO epoch # 2527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02894165694306139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,561 INFO epoch # 2528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029075519792968407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,643 INFO epoch # 2529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02630657618283294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,725 INFO epoch # 2530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02406191048794426
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:08,725 INFO *** epoch 2530, rolling-avg-loss (window=10)= 0.02562698766560061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,806 INFO epoch # 2531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02496862989210058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,890 INFO epoch # 2532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024063362187007442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:08,971 INFO epoch # 2533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024595456750830635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,051 INFO epoch # 2534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02587143928394653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,134 INFO epoch # 2535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024744051101151854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,219 INFO epoch # 2536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025183190737152472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,304 INFO epoch # 2537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022360015122103505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,387 INFO epoch # 2538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023367094807326794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,472 INFO epoch # 2539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024662182229803875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,558 INFO epoch # 2540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024868094886187464
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:09,558 INFO *** epoch 2540, rolling-avg-loss (window=10)= 0.024468351699761116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,641 INFO epoch # 2541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023803059550118633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,724 INFO epoch # 2542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02342152493656613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,804 INFO epoch # 2543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021656423283275217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,887 INFO epoch # 2544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022755646059522405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:09,965 INFO epoch # 2545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02324746738304384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,043 INFO epoch # 2546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023045320180244744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,120 INFO epoch # 2547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022684263094561175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,200 INFO epoch # 2548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0242243159445934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,278 INFO epoch # 2549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02111425594193861
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,356 INFO epoch # 2550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02383497150731273
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:10,356 INFO *** epoch 2550, rolling-avg-loss (window=10)= 0.022978724788117688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,436 INFO epoch # 2551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025361554027767852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,514 INFO epoch # 2552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0230422902823193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,594 INFO epoch # 2553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023685049440246075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,675 INFO epoch # 2554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02451890942757018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,758 INFO epoch # 2555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023929242743179202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,839 INFO epoch # 2556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022667931698379107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:10,922 INFO epoch # 2557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023226575707667507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,001 INFO epoch # 2558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02287992340279743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,079 INFO epoch # 2559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0248675026523415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,157 INFO epoch # 2560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026888341875746846
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:11,157 INFO *** epoch 2560, rolling-avg-loss (window=10)= 0.0241067321258015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,235 INFO epoch # 2561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0221965447999537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,312 INFO epoch # 2562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024646033823955804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,391 INFO epoch # 2563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024342179211089388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,469 INFO epoch # 2564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025377703161211684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,546 INFO epoch # 2565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02245679235784337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,624 INFO epoch # 2566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02418884728103876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,702 INFO epoch # 2567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023835946602048352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,782 INFO epoch # 2568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022697081381920725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,864 INFO epoch # 2569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022799473619670607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:11,946 INFO epoch # 2570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02221471126540564
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:11,946 INFO *** epoch 2570, rolling-avg-loss (window=10)= 0.023475531350413803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,032 INFO epoch # 2571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021170408581383526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,110 INFO epoch # 2572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02431199184502475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,190 INFO epoch # 2573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025992234441218898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,271 INFO epoch # 2574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024060777883278206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,356 INFO epoch # 2575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02231553827004973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,440 INFO epoch # 2576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022984260402154177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,519 INFO epoch # 2577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02701269745011814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,599 INFO epoch # 2578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02456756797619164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,695 INFO epoch # 2579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0260272502200678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,782 INFO epoch # 2580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02258659340441227
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:12,782 INFO *** epoch 2580, rolling-avg-loss (window=10)= 0.024102932047389912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,872 INFO epoch # 2581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02256308586220257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:12,957 INFO epoch # 2582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022253255388932303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,045 INFO epoch # 2583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02309929160401225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,126 INFO epoch # 2584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024754840065725148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,207 INFO epoch # 2585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023580674736876972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,304 INFO epoch # 2586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022456022910773754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,392 INFO epoch # 2587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02299274763208814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,477 INFO epoch # 2588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02821496655815281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,562 INFO epoch # 2589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025266005381126888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,644 INFO epoch # 2590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02697885993984528
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:13,644 INFO *** epoch 2590, rolling-avg-loss (window=10)= 0.024215975007973612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,727 INFO epoch # 2591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02399087094818242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,809 INFO epoch # 2592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023637747537577525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,889 INFO epoch # 2593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024691407365025952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:13,970 INFO epoch # 2594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02461930019489955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,051 INFO epoch # 2595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025608553405618295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,131 INFO epoch # 2596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026117924629943445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,212 INFO epoch # 2597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021413364054751582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,289 INFO epoch # 2598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020827355969231576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,369 INFO epoch # 2599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025884340313496068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,450 INFO epoch # 2600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022964159361436032
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:14,450 INFO *** epoch 2600, rolling-avg-loss (window=10)= 0.023975502378016246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,529 INFO epoch # 2601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021964655490592122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,611 INFO epoch # 2602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022447214025305584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,690 INFO epoch # 2603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025873266509734094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,771 INFO epoch # 2604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024025859660468996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,850 INFO epoch # 2605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021766800055047497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:14,929 INFO epoch # 2606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027382480213418603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,010 INFO epoch # 2607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023660423088585958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,090 INFO epoch # 2608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028654856578214094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,170 INFO epoch # 2609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021088074485305697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,249 INFO epoch # 2610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02212783941649832
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:15,249 INFO *** epoch 2610, rolling-avg-loss (window=10)= 0.023899146952317096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,328 INFO epoch # 2611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025128794804913923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,409 INFO epoch # 2612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022429766177083366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,489 INFO epoch # 2613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020755054196342826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,568 INFO epoch # 2614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022912604617886245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,649 INFO epoch # 2615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02443834563018754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,728 INFO epoch # 2616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022669701371341944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,807 INFO epoch # 2617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023258561122929677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,888 INFO epoch # 2618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022050209750887007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:15,967 INFO epoch # 2619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023909062088932842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,049 INFO epoch # 2620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025780064665013924
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:16,049 INFO *** epoch 2620, rolling-avg-loss (window=10)= 0.02333321644255193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,135 INFO epoch # 2621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023183847166365013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,217 INFO epoch # 2622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022549021217855625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,298 INFO epoch # 2623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020394283143104985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,378 INFO epoch # 2624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026831079478142783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,462 INFO epoch # 2625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02101938751002308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,545 INFO epoch # 2626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022823013438028283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,637 INFO epoch # 2627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022290520777460188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,729 INFO epoch # 2628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025393521907972172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,811 INFO epoch # 2629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026051122753415257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,889 INFO epoch # 2630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024655925459228456
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:16,889 INFO *** epoch 2630, rolling-avg-loss (window=10)= 0.023519172285159585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:16,967 INFO epoch # 2631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02548341333749704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,045 INFO epoch # 2632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024136500258464366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,121 INFO epoch # 2633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023274325707461685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,198 INFO epoch # 2634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024007071886444464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,277 INFO epoch # 2635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022769189628888853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,355 INFO epoch # 2636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021066577173769474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,434 INFO epoch # 2637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02003058533591684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,516 INFO epoch # 2638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0239559528126847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,595 INFO epoch # 2639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026376894049462862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,676 INFO epoch # 2640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02361675669089891
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:17,676 INFO *** epoch 2640, rolling-avg-loss (window=10)= 0.023471726688148918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,757 INFO epoch # 2641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023998325996217318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,856 INFO epoch # 2642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02345160616096109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:17,938 INFO epoch # 2643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024800192477414384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,017 INFO epoch # 2644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023226790071930736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,097 INFO epoch # 2645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024536404467653483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,179 INFO epoch # 2646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021503194147953764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,262 INFO epoch # 2647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022417131549445912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,345 INFO epoch # 2648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02290140987315681
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,428 INFO epoch # 2649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022592809720663354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,512 INFO epoch # 2650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022146899107610807
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:18,512 INFO *** epoch 2650, rolling-avg-loss (window=10)= 0.023157476357300765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,594 INFO epoch # 2651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022609067178564146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,678 INFO epoch # 2652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021411605761386454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,763 INFO epoch # 2653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023002476096735336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,849 INFO epoch # 2654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024903735757106915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:18,933 INFO epoch # 2655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025362027052324265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,015 INFO epoch # 2656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021275363629683852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,099 INFO epoch # 2657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022000196593580768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,183 INFO epoch # 2658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024699900386622176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,267 INFO epoch # 2659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022848891094326973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,349 INFO epoch # 2660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024042728677159175
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:19,349 INFO *** epoch 2660, rolling-avg-loss (window=10)= 0.023215599222749007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,432 INFO epoch # 2661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02227714838227257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,513 INFO epoch # 2662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02752417730516754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,591 INFO epoch # 2663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02210993427434005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,670 INFO epoch # 2664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021710857487050816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,749 INFO epoch # 2665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022262932936428115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,827 INFO epoch # 2666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023346584785031155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,907 INFO epoch # 2667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026843274463317357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:19,985 INFO epoch # 2668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02102489469689317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,062 INFO epoch # 2669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023343135340837762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,141 INFO epoch # 2670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023226687873830087
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:20,141 INFO *** epoch 2670, rolling-avg-loss (window=10)= 0.023366962754516864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,221 INFO epoch # 2671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02090909008984454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,299 INFO epoch # 2672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020396740015712567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,382 INFO epoch # 2673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023796709720045328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,460 INFO epoch # 2674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02212604833766818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,539 INFO epoch # 2675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022674305335385725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,621 INFO epoch # 2676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02219243929721415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,703 INFO epoch # 2677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021250318852253258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,783 INFO epoch # 2678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02420217618055176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,865 INFO epoch # 2679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020928837184328586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:20,945 INFO epoch # 2680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023996984993573278
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:20,945 INFO *** epoch 2680, rolling-avg-loss (window=10)= 0.022247365000657736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,024 INFO epoch # 2681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024074137181742117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,107 INFO epoch # 2682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022768979746615514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,190 INFO epoch # 2683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022441954642999917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,271 INFO epoch # 2684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023319839412579313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,354 INFO epoch # 2685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022429280274081975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,436 INFO epoch # 2686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0219284174090717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,515 INFO epoch # 2687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020662072332925163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,594 INFO epoch # 2688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021907502610702068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,674 INFO epoch # 2689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02435592544497922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,754 INFO epoch # 2690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021450963336974382
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:21,754 INFO *** epoch 2690, rolling-avg-loss (window=10)= 0.022533907239267136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,834 INFO epoch # 2691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022605839098105207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:21,915 INFO epoch # 2692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022342925818520598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,000 INFO epoch # 2693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02254550931684207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,083 INFO epoch # 2694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02053170719591435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,167 INFO epoch # 2695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02044642122928053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,249 INFO epoch # 2696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02001405443297699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,331 INFO epoch # 2697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022872557281516492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,409 INFO epoch # 2698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02021155646070838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,488 INFO epoch # 2699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021329262672225013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,568 INFO epoch # 2700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02760127661167644
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:22,568 INFO *** epoch 2700, rolling-avg-loss (window=10)= 0.022050111011776608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,648 INFO epoch # 2701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02929600267088972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,727 INFO epoch # 2702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020517446100711823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,805 INFO epoch # 2703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023725488979835063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,887 INFO epoch # 2704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023261972150066867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:22,965 INFO epoch # 2705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020901171534205787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,044 INFO epoch # 2706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01968585247232113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,124 INFO epoch # 2707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02128971859929152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,203 INFO epoch # 2708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022098629269748926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,281 INFO epoch # 2709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023382806888548657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,361 INFO epoch # 2710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021854052058188245
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:23,361 INFO *** epoch 2710, rolling-avg-loss (window=10)= 0.022601314072380772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,439 INFO epoch # 2711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02244967533624731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,518 INFO epoch # 2712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020976907573640347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,596 INFO epoch # 2713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02046782584511675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,678 INFO epoch # 2714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019752761829295196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,759 INFO epoch # 2715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02119528755429201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,839 INFO epoch # 2716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021017064253101125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:23,922 INFO epoch # 2717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02401802726672031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,001 INFO epoch # 2718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0245531598047819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,079 INFO epoch # 2719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020606259640771896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,158 INFO epoch # 2720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021119728233315982
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:24,159 INFO *** epoch 2720, rolling-avg-loss (window=10)= 0.021615669733728284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,237 INFO epoch # 2721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019718640309292823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,317 INFO epoch # 2722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022437011313741095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,397 INFO epoch # 2723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021506118195247836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,477 INFO epoch # 2724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02014606594457291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,558 INFO epoch # 2725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020895060210023075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,637 INFO epoch # 2726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022531074224389158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,717 INFO epoch # 2727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022419489483581856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,797 INFO epoch # 2728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02064894264913164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,878 INFO epoch # 2729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02161343244370073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:24,961 INFO epoch # 2730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02200577754410915
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:24,961 INFO *** epoch 2730, rolling-avg-loss (window=10)= 0.021392161231779026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,041 INFO epoch # 2731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021261362708173692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,120 INFO epoch # 2732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020618773938622326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,199 INFO epoch # 2733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02095775568159297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,278 INFO epoch # 2734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0206979151989799
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,358 INFO epoch # 2735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025059126608539373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,438 INFO epoch # 2736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023141340061556548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,521 INFO epoch # 2737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021849091848707758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,602 INFO epoch # 2738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02283874637214467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,683 INFO epoch # 2739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02371793956262991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,764 INFO epoch # 2740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021466031321324408
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:25,764 INFO *** epoch 2740, rolling-avg-loss (window=10)= 0.022160808330227155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,843 INFO epoch # 2741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019909119058866054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:25,923 INFO epoch # 2742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019317239377414808
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,004 INFO epoch # 2743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024944517557742074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,084 INFO epoch # 2744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02118743756727781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,166 INFO epoch # 2745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020543965089018457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,245 INFO epoch # 2746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023223910538945347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,328 INFO epoch # 2747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021485318371560425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,409 INFO epoch # 2748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021854492282727733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,489 INFO epoch # 2749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02185016419389285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,571 INFO epoch # 2750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021820533991558477
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:26,571 INFO *** epoch 2750, rolling-avg-loss (window=10)= 0.021613669802900404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,652 INFO epoch # 2751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021745170321082696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,732 INFO epoch # 2752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022222749888896942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,813 INFO epoch # 2753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024296318559208885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,893 INFO epoch # 2754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02426338187069632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:26,980 INFO epoch # 2755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018947547185234725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,064 INFO epoch # 2756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019222414819523692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,144 INFO epoch # 2757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019780883681960404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,225 INFO epoch # 2758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02050993571174331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,305 INFO epoch # 2759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02554355634492822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,388 INFO epoch # 2760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021580829081358388
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:27,388 INFO *** epoch 2760, rolling-avg-loss (window=10)= 0.02181127874646336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,469 INFO epoch # 2761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021898411912843585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,549 INFO epoch # 2762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023920779553009197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,630 INFO epoch # 2763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019436015325482003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,712 INFO epoch # 2764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021533572871703655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,792 INFO epoch # 2765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024159391090506688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,873 INFO epoch # 2766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023612643912201747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:27,953 INFO epoch # 2767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02125767822144553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,033 INFO epoch # 2768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018333058498683386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,113 INFO epoch # 2769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0204277329321485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,193 INFO epoch # 2770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019611457217251882
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:28,194 INFO *** epoch 2770, rolling-avg-loss (window=10)= 0.021419074153527617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,275 INFO epoch # 2771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020897143956972286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,358 INFO epoch # 2772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01997835472866427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,442 INFO epoch # 2773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01976468350039795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,523 INFO epoch # 2774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02315782743971795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,604 INFO epoch # 2775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021776822890387848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,683 INFO epoch # 2776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02371081549790688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,761 INFO epoch # 2777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020004338439321145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,839 INFO epoch # 2778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01885568755096756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,917 INFO epoch # 2779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020477983940509148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:28,997 INFO epoch # 2780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019926975917769596
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:28,997 INFO *** epoch 2780, rolling-avg-loss (window=10)= 0.020855063386261464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:29,077 INFO epoch # 2781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02057936493656598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:29,155 INFO epoch # 2782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027164344748598523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:29,235 INFO epoch # 2783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021056851794128306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:29,313 INFO epoch # 2784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020596643444150686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:29,392 INFO epoch # 2785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020080629008589312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:29,470 INFO epoch # 2786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019896078389137983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:29,562 INFO epoch # 2787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019979702454293147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:29,657 INFO epoch # 2788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02338794548995793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:29,749 INFO epoch # 2789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02093323302688077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:29,842 INFO epoch # 2790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019729489969904535
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:29,842 INFO *** epoch 2790, rolling-avg-loss (window=10)= 0.02134042832622072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:29,923 INFO epoch # 2791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019843602931359783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,002 INFO epoch # 2792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0219209206989035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,086 INFO epoch # 2793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02070056230877526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,164 INFO epoch # 2794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02112370156100951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,243 INFO epoch # 2795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01967533864080906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,321 INFO epoch # 2796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021397099204477854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,402 INFO epoch # 2797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021203509415499866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,480 INFO epoch # 2798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019209292251616716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,570 INFO epoch # 2799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019529529032297432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,680 INFO epoch # 2800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020376557848067023
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:30,680 INFO *** epoch 2800, rolling-avg-loss (window=10)= 0.0204980113892816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,774 INFO epoch # 2801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022652134211966768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,857 INFO epoch # 2802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02147489866183605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:30,939 INFO epoch # 2803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020283885780372657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,020 INFO epoch # 2804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021068056201329455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,100 INFO epoch # 2805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0195571604417637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,179 INFO epoch # 2806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019033607793971896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,258 INFO epoch # 2807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020962707290891558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,338 INFO epoch # 2808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0228716007550247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,418 INFO epoch # 2809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020674123603384942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,498 INFO epoch # 2810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021132822148501873
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:31,498 INFO *** epoch 2810, rolling-avg-loss (window=10)= 0.02097109968890436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,581 INFO epoch # 2811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024400795096880756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,665 INFO epoch # 2812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019556582730729133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,746 INFO epoch # 2813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021174461027840152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,827 INFO epoch # 2814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02211298909969628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,908 INFO epoch # 2815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024269908244605176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:31,989 INFO epoch # 2816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020633258449379355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,073 INFO epoch # 2817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02533977736311499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,151 INFO epoch # 2818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01928930592839606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,230 INFO epoch # 2819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019190169114153832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,309 INFO epoch # 2820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019860917964251712
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:32,309 INFO *** epoch 2820, rolling-avg-loss (window=10)= 0.021582816501904745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,391 INFO epoch # 2821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019579218642320484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,473 INFO epoch # 2822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019979260381660424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,557 INFO epoch # 2823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019323777494719252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,638 INFO epoch # 2824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01959563378477469
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,717 INFO epoch # 2825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021886119968257844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,796 INFO epoch # 2826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022716823412338272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,877 INFO epoch # 2827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02000533383397851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:32,956 INFO epoch # 2828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020837878837483004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,041 INFO epoch # 2829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021694193783332594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,122 INFO epoch # 2830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01768401832669042
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:33,122 INFO *** epoch 2830, rolling-avg-loss (window=10)= 0.020330225846555548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,204 INFO epoch # 2831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02071151722338982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,282 INFO epoch # 2832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020529346453258768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,361 INFO epoch # 2833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018132149780285545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,442 INFO epoch # 2834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021543171460507438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,523 INFO epoch # 2835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019123394275084138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,602 INFO epoch # 2836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022395350213628262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,684 INFO epoch # 2837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02053682347468566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,764 INFO epoch # 2838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022010736152878962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,842 INFO epoch # 2839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02168116078246385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:33,921 INFO epoch # 2840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019652692397357896
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:33,921 INFO *** epoch 2840, rolling-avg-loss (window=10)= 0.020631634221354035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,001 INFO epoch # 2841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019880753286997788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,085 INFO epoch # 2842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020847759093157947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,166 INFO epoch # 2843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019142334480420686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,245 INFO epoch # 2844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02107738287304528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,324 INFO epoch # 2845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027957482452620752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,404 INFO epoch # 2846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01848888918175362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,486 INFO epoch # 2847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020546272193314508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,565 INFO epoch # 2848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019465279328869656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,644 INFO epoch # 2849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019899216196790803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,723 INFO epoch # 2850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022481626801891252
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:34,723 INFO *** epoch 2850, rolling-avg-loss (window=10)= 0.020978699588886228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,801 INFO epoch # 2851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02081194205675274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,881 INFO epoch # 2852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018994449375895783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:34,960 INFO epoch # 2853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024115064268698916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,042 INFO epoch # 2854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01994292285235133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,123 INFO epoch # 2855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019953848197474144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,203 INFO epoch # 2856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018963352689752355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,282 INFO epoch # 2857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02141705011308659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,361 INFO epoch # 2858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019189627171726897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,441 INFO epoch # 2859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017679650060017593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,521 INFO epoch # 2860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01899664630764164
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:35,521 INFO *** epoch 2860, rolling-avg-loss (window=10)= 0.0200064553093398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,603 INFO epoch # 2861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019168376020388678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,682 INFO epoch # 2862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019376333264517598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,760 INFO epoch # 2863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020239681762177497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,839 INFO epoch # 2864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02117896372510586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,918 INFO epoch # 2865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020178256643703207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:35,998 INFO epoch # 2866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023888682713732123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,081 INFO epoch # 2867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01957389090966899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,163 INFO epoch # 2868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022614350949879736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,245 INFO epoch # 2869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019547676623915322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,329 INFO epoch # 2870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018871166903409176
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:36,329 INFO *** epoch 2870, rolling-avg-loss (window=10)= 0.020463737951649817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,413 INFO epoch # 2871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01978404763212893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,494 INFO epoch # 2872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019148251361912116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,578 INFO epoch # 2873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020433532306924462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,658 INFO epoch # 2874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02053501871705521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,736 INFO epoch # 2875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019074661351623945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,814 INFO epoch # 2876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01831667531223502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,894 INFO epoch # 2877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020070018275873736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:36,976 INFO epoch # 2878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021262667258270085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,056 INFO epoch # 2879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02437237842241302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,136 INFO epoch # 2880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0197578328079544
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:37,136 INFO *** epoch 2880, rolling-avg-loss (window=10)= 0.020275508344639093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,214 INFO epoch # 2881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01881521544419229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,296 INFO epoch # 2882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022148715623188764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,377 INFO epoch # 2883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019274255813797936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,455 INFO epoch # 2884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021478716415003873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,535 INFO epoch # 2885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01807213801657781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,625 INFO epoch # 2886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017959203221835196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,718 INFO epoch # 2887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01733632021205267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,798 INFO epoch # 2888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018382879410637543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,885 INFO epoch # 2889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019546944997273386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:37,976 INFO epoch # 2890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018781717080855742
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:37,976 INFO *** epoch 2890, rolling-avg-loss (window=10)= 0.01917961062354152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:38,060 INFO epoch # 2891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020104088325751945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:38,149 INFO epoch # 2892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020522116858046502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:38,233 INFO epoch # 2893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01772790328686824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:38,316 INFO epoch # 2894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01851389853982255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:38,402 INFO epoch # 2895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0178113755537197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:38,484 INFO epoch # 2896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018430691212415695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:38,565 INFO epoch # 2897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018706009243032895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:38,660 INFO epoch # 2898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019113244634354487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:38,762 INFO epoch # 2899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021519787260331213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:38,846 INFO epoch # 2900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02175718202488497
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:38,846 INFO *** epoch 2900, rolling-avg-loss (window=10)= 0.01942062969392282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:38,932 INFO epoch # 2901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022123481772723608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:39,016 INFO epoch # 2902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01996430242434144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:39,103 INFO epoch # 2903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02118929140851833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:39,184 INFO epoch # 2904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018893158572609536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:39,299 INFO epoch # 2905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018372669306700118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:39,426 INFO epoch # 2906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01992255070945248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:39,529 INFO epoch # 2907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020815686977584846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:39,629 INFO epoch # 2908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020625818346161395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:39,728 INFO epoch # 2909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019055693002883345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:39,822 INFO epoch # 2910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020199170787236653
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:39,822 INFO *** epoch 2910, rolling-avg-loss (window=10)= 0.020116182330821176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:39,912 INFO epoch # 2911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017629125679377466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:39,991 INFO epoch # 2912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018947274249512702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,070 INFO epoch # 2913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02195101635879837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,148 INFO epoch # 2914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018796329357428476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,230 INFO epoch # 2915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019454176945146173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,308 INFO epoch # 2916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019201506685931236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,386 INFO epoch # 2917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021740682190284133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,464 INFO epoch # 2918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018903810327174142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,549 INFO epoch # 2919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020523711311398074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,638 INFO epoch # 2920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02032284773304127
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:40,639 INFO *** epoch 2920, rolling-avg-loss (window=10)= 0.019747048083809206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,729 INFO epoch # 2921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018403511436190456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,821 INFO epoch # 2922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02014879345369991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,909 INFO epoch # 2923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01925671097706072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:40,997 INFO epoch # 2924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01905476627871394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:41,083 INFO epoch # 2925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021028554008807987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:41,174 INFO epoch # 2926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018629275626153685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:41,258 INFO epoch # 2927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018479582548025064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:41,368 INFO epoch # 2928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01885999893420376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:41,497 INFO epoch # 2929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018238764561829157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:41,628 INFO epoch # 2930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02045760834880639
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:41,628 INFO *** epoch 2930, rolling-avg-loss (window=10)= 0.019255756617349108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:41,765 INFO epoch # 2931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018526331841712818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:41,897 INFO epoch # 2932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017436432084650733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:41,985 INFO epoch # 2933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017146779122413136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:42,066 INFO epoch # 2934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019089726862148382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:42,147 INFO epoch # 2935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01808661660470534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:42,225 INFO epoch # 2936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01800434329197742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:42,308 INFO epoch # 2937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02172338866512291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:42,395 INFO epoch # 2938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017369493187288754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:42,478 INFO epoch # 2939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018779752586851828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:42,558 INFO epoch # 2940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017653675211477093
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:42,559 INFO *** epoch 2940, rolling-avg-loss (window=10)= 0.018381653945834842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:42,638 INFO epoch # 2941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019601165942731313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:42,763 INFO epoch # 2942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021612818105495535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:42,882 INFO epoch # 2943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017758902307832614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:42,979 INFO epoch # 2944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018528992542997003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,113 INFO epoch # 2945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01845326230977662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,198 INFO epoch # 2946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01735570542223286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,278 INFO epoch # 2947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018490866874344647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,356 INFO epoch # 2948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018619946917169727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,434 INFO epoch # 2949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018097141641192138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,511 INFO epoch # 2950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01750606314453762
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:43,511 INFO *** epoch 2950, rolling-avg-loss (window=10)= 0.018602486520831008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,592 INFO epoch # 2951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020236856129486114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,671 INFO epoch # 2952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018684235619730316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,750 INFO epoch # 2953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021407406457001343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,828 INFO epoch # 2954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018561270117061213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,908 INFO epoch # 2955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016631886872346513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:43,987 INFO epoch # 2956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023235185173689388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:44,067 INFO epoch # 2957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020231558533851057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:44,151 INFO epoch # 2958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018028739345027134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:44,238 INFO epoch # 2959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01824290734657552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:44,323 INFO epoch # 2960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018618354573845863
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:44,323 INFO *** epoch 2960, rolling-avg-loss (window=10)= 0.019387840016861446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:44,410 INFO epoch # 2961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022289759770501405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:44,495 INFO epoch # 2962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02061656507430598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:44,574 INFO epoch # 2963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019553945618099533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:44,675 INFO epoch # 2964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020197372534312308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:44,792 INFO epoch # 2965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018932306600618176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:44,890 INFO epoch # 2966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018838763993699104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:44,978 INFO epoch # 2967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018405641181743704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:45,069 INFO epoch # 2968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017854350589914247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:45,157 INFO epoch # 2969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019051985189435072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:45,242 INFO epoch # 2970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016913829851546325
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:45,243 INFO *** epoch 2970, rolling-avg-loss (window=10)= 0.019265452040417585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:45,321 INFO epoch # 2971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020847374136792496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:45,406 INFO epoch # 2972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017248064148589037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:45,489 INFO epoch # 2973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02089624575455673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:45,584 INFO epoch # 2974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02011555968783796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:45,668 INFO epoch # 2975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019226152449846268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:45,755 INFO epoch # 2976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017632326707825996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:45,836 INFO epoch # 2977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017557156199472956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:45,922 INFO epoch # 2978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016825373837491497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,009 INFO epoch # 2979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018812077556503937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,090 INFO epoch # 2980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017902228733873926
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:46,090 INFO *** epoch 2980, rolling-avg-loss (window=10)= 0.01870625592127908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,172 INFO epoch # 2981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0198565561149735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,257 INFO epoch # 2982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016928960729273967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,342 INFO epoch # 2983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018173422649852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,421 INFO epoch # 2984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023428236541803926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,500 INFO epoch # 2985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01676686288556084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,584 INFO epoch # 2986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01892508548917249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,673 INFO epoch # 2987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01890555099816993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,761 INFO epoch # 2988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018410222692182288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,850 INFO epoch # 2989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019639278936665505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:46,935 INFO epoch # 2990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022117753193015233
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:46,935 INFO *** epoch 2990, rolling-avg-loss (window=10)= 0.019315193023066966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,014 INFO epoch # 2991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017919489022460766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,092 INFO epoch # 2992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018106031100614928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,170 INFO epoch # 2993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021581400505965576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,254 INFO epoch # 2994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021237329550785944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,345 INFO epoch # 2995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01916027898550965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,426 INFO epoch # 2996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019237361702835187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,507 INFO epoch # 2997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018085405419697054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,600 INFO epoch # 2998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020653413463151082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,693 INFO epoch # 2999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018322524454561062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,788 INFO epoch # 3000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018250078064738773
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:47,788 INFO *** epoch 3000, rolling-avg-loss (window=10)= 0.019255331227032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,881 INFO epoch # 3001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01763119380484568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:47,973 INFO epoch # 3002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017011148651363328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:48,079 INFO epoch # 3003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01905928656924516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:48,184 INFO epoch # 3004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01888378951116465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:48,281 INFO epoch # 3005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017434116525691934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:48,376 INFO epoch # 3006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017984402758884244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:48,476 INFO epoch # 3007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019182502161129378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:48,574 INFO epoch # 3008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018371198908425868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:48,659 INFO epoch # 3009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016753753850935027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:48,736 INFO epoch # 3010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01787701528519392
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:48,736 INFO *** epoch 3010, rolling-avg-loss (window=10)= 0.018018840802687917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:48,843 INFO epoch # 3011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0198611455707578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:48,927 INFO epoch # 3012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019259472785051912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,011 INFO epoch # 3013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02325035799003672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,095 INFO epoch # 3014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020562408113619313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,180 INFO epoch # 3015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01781044674862642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,263 INFO epoch # 3016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01839418604504317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,345 INFO epoch # 3017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02341892216645647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,426 INFO epoch # 3018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021988404827425256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,508 INFO epoch # 3019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01764086434559431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,588 INFO epoch # 3020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020551600275211968
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:49,588 INFO *** epoch 3020, rolling-avg-loss (window=10)= 0.020273780886782333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,666 INFO epoch # 3021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018707782597630285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,746 INFO epoch # 3022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01932035741629079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,827 INFO epoch # 3023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022740748681826517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,906 INFO epoch # 3024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01741223559656646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:49,985 INFO epoch # 3025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018534909089794382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,063 INFO epoch # 3026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019535270272172056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,141 INFO epoch # 3027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018725047484622337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,221 INFO epoch # 3028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018976762105012313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,299 INFO epoch # 3029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018250404100399464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,377 INFO epoch # 3030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017583308232133277
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:50,378 INFO *** epoch 3030, rolling-avg-loss (window=10)= 0.018978682557644788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,457 INFO epoch # 3031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01710932380228769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,538 INFO epoch # 3032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017915286880452186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,616 INFO epoch # 3033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016840094613144174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,695 INFO epoch # 3034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019745473633520305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,773 INFO epoch # 3035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018217013144749217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,851 INFO epoch # 3036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017985490427236073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:50,931 INFO epoch # 3037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017096724870498292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,009 INFO epoch # 3038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01664946296659764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,091 INFO epoch # 3039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017511298283352517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,169 INFO epoch # 3040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01883029229065869
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:51,169 INFO *** epoch 3040, rolling-avg-loss (window=10)= 0.01779004609124968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,246 INFO epoch # 3041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015772393555380404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,325 INFO epoch # 3042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01800381427165121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,405 INFO epoch # 3043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0162103013790329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,482 INFO epoch # 3044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017636119009694085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,560 INFO epoch # 3045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017450344894314185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,639 INFO epoch # 3046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018232551956316456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,717 INFO epoch # 3047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021021260108682327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,795 INFO epoch # 3048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019561291657737456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,873 INFO epoch # 3049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019438066039583646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:51,952 INFO epoch # 3050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018584656034363434
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:51,952 INFO *** epoch 3050, rolling-avg-loss (window=10)= 0.01819107989067561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,031 INFO epoch # 3051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017543404508614913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,112 INFO epoch # 3052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018628083853400312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,192 INFO epoch # 3053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017391635483363643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,270 INFO epoch # 3054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019372327034943737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,348 INFO epoch # 3055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01604717056034133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,426 INFO epoch # 3056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01686844071082305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,504 INFO epoch # 3057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01868351749726571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,586 INFO epoch # 3058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0207946502341656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,664 INFO epoch # 3059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015952332993038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,742 INFO epoch # 3060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02013834210811183
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:52,742 INFO *** epoch 3060, rolling-avg-loss (window=10)= 0.018141990498406813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,820 INFO epoch # 3061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0199246687698178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,899 INFO epoch # 3062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017857980492408387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:52,980 INFO epoch # 3063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017204916177433915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,060 INFO epoch # 3064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02167347411159426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,141 INFO epoch # 3065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020314158377004787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,219 INFO epoch # 3066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01760931008902844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,298 INFO epoch # 3067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016360237234039232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,377 INFO epoch # 3068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019365946907782927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,456 INFO epoch # 3069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017061690290574916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,534 INFO epoch # 3070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017925084481248632
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:53,535 INFO *** epoch 3070, rolling-avg-loss (window=10)= 0.01852974669309333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,614 INFO epoch # 3071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017694170455797575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,692 INFO epoch # 3072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018761111772619188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,771 INFO epoch # 3073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016836201728438027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,848 INFO epoch # 3074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016822151679662056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:53,926 INFO epoch # 3075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017370363682857715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,004 INFO epoch # 3076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01701360045990441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,087 INFO epoch # 3077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017370119618135504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,166 INFO epoch # 3078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01750029194226954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,243 INFO epoch # 3079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01792522611503955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,322 INFO epoch # 3080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018192579474998638
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:54,322 INFO *** epoch 3080, rolling-avg-loss (window=10)= 0.017548581692972222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,409 INFO epoch # 3081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01660240210185293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,490 INFO epoch # 3082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016068465462012682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,571 INFO epoch # 3083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016020434442907572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,649 INFO epoch # 3084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01704831572715193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,727 INFO epoch # 3085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01635066870949231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,810 INFO epoch # 3086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01888945839891676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,897 INFO epoch # 3087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02436232115724124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:54,979 INFO epoch # 3088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0175866236386355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,062 INFO epoch # 3089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018661768233869225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,142 INFO epoch # 3090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020121332461712882
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:55,143 INFO *** epoch 3090, rolling-avg-loss (window=10)= 0.018171179033379302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,222 INFO epoch # 3091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017902839710586704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,300 INFO epoch # 3092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019551480596419424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,380 INFO epoch # 3093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019511492893798277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,458 INFO epoch # 3094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0165769869054202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,540 INFO epoch # 3095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020309109619120136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,619 INFO epoch # 3096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017294955177931115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,698 INFO epoch # 3097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017287103197304532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,778 INFO epoch # 3098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01626620747265406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,859 INFO epoch # 3099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016257912342553027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:55,939 INFO epoch # 3100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017020318162394688
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:55,939 INFO *** epoch 3100, rolling-avg-loss (window=10)= 0.017797840607818215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,021 INFO epoch # 3101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016749277245253325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,100 INFO epoch # 3102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019617072684923187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,181 INFO epoch # 3103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01648828835459426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,261 INFO epoch # 3104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016151431249454618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,340 INFO epoch # 3105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01692741569422651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,420 INFO epoch # 3106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01589952090580482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,500 INFO epoch # 3107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016494752373546362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,581 INFO epoch # 3108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017717456066748127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,659 INFO epoch # 3109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01771204362739809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,737 INFO epoch # 3110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017419633382814936
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:56,737 INFO *** epoch 3110, rolling-avg-loss (window=10)= 0.017117689158476425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,815 INFO epoch # 3111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017959800257813185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,893 INFO epoch # 3112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015750307284179144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:56,970 INFO epoch # 3113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017344146413961425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,055 INFO epoch # 3114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020117293184739538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,136 INFO epoch # 3115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015366092324256897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,216 INFO epoch # 3116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016411568867624737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,296 INFO epoch # 3117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01829406459000893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,377 INFO epoch # 3118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01771964185172692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,457 INFO epoch # 3119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018910771337687038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,538 INFO epoch # 3120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018238665812532417
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:57,538 INFO *** epoch 3120, rolling-avg-loss (window=10)= 0.01761123519245302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,619 INFO epoch # 3121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01819280258496292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,697 INFO epoch # 3122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0178113482252229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,776 INFO epoch # 3123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020276469280361198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,854 INFO epoch # 3124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018214937474112958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:57,934 INFO epoch # 3125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01758159299788531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,012 INFO epoch # 3126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01987391896545887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,092 INFO epoch # 3127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01710608282883186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,169 INFO epoch # 3128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01695442209893372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,249 INFO epoch # 3129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022368014528183267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,326 INFO epoch # 3130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0166966851829784
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:58,326 INFO *** epoch 3130, rolling-avg-loss (window=10)= 0.01850762741669314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,405 INFO epoch # 3131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01616246065532323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,482 INFO epoch # 3132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016294682005536743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,563 INFO epoch # 3133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015924645194900222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,641 INFO epoch # 3134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019299287727335468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,720 INFO epoch # 3135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015418945098645054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,798 INFO epoch # 3136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015516838786425069
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,876 INFO epoch # 3137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016352462975191884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:58,954 INFO epoch # 3138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01840830154833384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,036 INFO epoch # 3139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016028790661948733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,115 INFO epoch # 3140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016187212415388785
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:59,116 INFO *** epoch 3140, rolling-avg-loss (window=10)= 0.016559362706902902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,193 INFO epoch # 3141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017032760166330263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,271 INFO epoch # 3142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01682566686940845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,351 INFO epoch # 3143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016758269819547422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,429 INFO epoch # 3144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015931201240164228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,510 INFO epoch # 3145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016596729125012644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,591 INFO epoch # 3146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016316888053552248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,669 INFO epoch # 3147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017273467150516808
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,748 INFO epoch # 3148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01791647785285022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,827 INFO epoch # 3149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019956096861278638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,905 INFO epoch # 3150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019627548899734393
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:17:59,905 INFO *** epoch 3150, rolling-avg-loss (window=10)= 0.01742351060383953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:17:59,982 INFO epoch # 3151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018897692149039358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,061 INFO epoch # 3152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020459862367715687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,139 INFO epoch # 3153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016536524330149405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,218 INFO epoch # 3154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017445762176066637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,296 INFO epoch # 3155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017712306696921587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,374 INFO epoch # 3156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016756337106926367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,452 INFO epoch # 3157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016402659777668305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,530 INFO epoch # 3158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018563140838523395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,608 INFO epoch # 3159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016138563136337325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,685 INFO epoch # 3160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01726108367438428
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:00,685 INFO *** epoch 3160, rolling-avg-loss (window=10)= 0.017617393225373236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,759 INFO epoch # 3161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016974331461824477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,834 INFO epoch # 3162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01670342028955929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,911 INFO epoch # 3163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01546690976829268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:00,987 INFO epoch # 3164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017789975681807846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,068 INFO epoch # 3165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016413249613833614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,145 INFO epoch # 3166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01653416694898624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,225 INFO epoch # 3167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018747467343928292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,302 INFO epoch # 3168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017679503769613802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,381 INFO epoch # 3169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01552900264505297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,459 INFO epoch # 3170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01969091731007211
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:01,459 INFO *** epoch 3170, rolling-avg-loss (window=10)= 0.017152894483297133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,540 INFO epoch # 3171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016879562797839753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,618 INFO epoch # 3172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01919399556936696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,698 INFO epoch # 3173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01784614336793311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,775 INFO epoch # 3174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01555119657132309
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,854 INFO epoch # 3175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016712485114112496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:01,931 INFO epoch # 3176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01873056653130334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,010 INFO epoch # 3177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016875025496119633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,090 INFO epoch # 3178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015204992931103334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,171 INFO epoch # 3179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01978273114946205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,248 INFO epoch # 3180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017491244463599287
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:02,248 INFO *** epoch 3180, rolling-avg-loss (window=10)= 0.017426794399216305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,327 INFO epoch # 3181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01818761801405344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,407 INFO epoch # 3182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017995351561694406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,502 INFO epoch # 3183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01657964133482892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,594 INFO epoch # 3184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01590560720069334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,679 INFO epoch # 3185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019940184472943656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,759 INFO epoch # 3186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019203177696908824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,839 INFO epoch # 3187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018454570235917345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,918 INFO epoch # 3188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017828428899520077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:02,999 INFO epoch # 3189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023558198066893965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:03,083 INFO epoch # 3190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015880584716796875
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:03,083 INFO *** epoch 3190, rolling-avg-loss (window=10)= 0.018353336220025086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:03,165 INFO epoch # 3191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01629982329905033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:03,242 INFO epoch # 3192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016138093880726956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:03,325 INFO epoch # 3193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018588688399177045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:03,405 INFO epoch # 3194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014763725819648243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:03,484 INFO epoch # 3195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01731386083702091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:03,565 INFO epoch # 3196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01728398335399106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:03,649 INFO epoch # 3197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01956829591654241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:03,726 INFO epoch # 3198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017387540356139652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:03,804 INFO epoch # 3199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017000937536067795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:03,909 INFO epoch # 3200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01725808453920763
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:03,909 INFO *** epoch 3200, rolling-avg-loss (window=10)= 0.017160303393757202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,007 INFO epoch # 3201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017910548660438508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,090 INFO epoch # 3202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018489403766579926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,170 INFO epoch # 3203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01694853458320722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,248 INFO epoch # 3204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017867739850771613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,325 INFO epoch # 3205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022081034956499934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,403 INFO epoch # 3206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01891055449959822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,481 INFO epoch # 3207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016175473763723858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,559 INFO epoch # 3208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020116959261940792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,637 INFO epoch # 3209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01719900206080638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,715 INFO epoch # 3210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017818510910728946
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:04,715 INFO *** epoch 3210, rolling-avg-loss (window=10)= 0.01835177623142954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,792 INFO epoch # 3211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01633415464311838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,872 INFO epoch # 3212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017905561457155272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:04,951 INFO epoch # 3213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016198425175389275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,034 INFO epoch # 3214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01904098447994329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,111 INFO epoch # 3215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015444696313352324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,191 INFO epoch # 3216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014657646024716087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,269 INFO epoch # 3217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015628219916834496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,348 INFO epoch # 3218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018853433575714007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,427 INFO epoch # 3219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015530413831584156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,508 INFO epoch # 3220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018378654291154817
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:05,508 INFO *** epoch 3220, rolling-avg-loss (window=10)= 0.01679721897089621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,588 INFO epoch # 3221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016528665728401393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,668 INFO epoch # 3222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01991879842535127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,748 INFO epoch # 3223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02111785147280898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,828 INFO epoch # 3224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014977403305238113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,909 INFO epoch # 3225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017086195526644588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:05,989 INFO epoch # 3226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018021540017798543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,070 INFO epoch # 3227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018047805060632527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,147 INFO epoch # 3228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015912233502604067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,225 INFO epoch # 3229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020096573964110576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,305 INFO epoch # 3230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016138463834067807
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:06,306 INFO *** epoch 3230, rolling-avg-loss (window=10)= 0.017784553083765788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,384 INFO epoch # 3231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018228419314255007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,463 INFO epoch # 3232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016297544338158332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,542 INFO epoch # 3233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018096019732183777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,622 INFO epoch # 3234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015556033657048829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,700 INFO epoch # 3235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01483826023468282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,778 INFO epoch # 3236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01664624732802622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,857 INFO epoch # 3237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01583888565073721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:06,935 INFO epoch # 3238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01786193871521391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,013 INFO epoch # 3239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014841862939647399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,093 INFO epoch # 3240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016177744488231838
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:07,093 INFO *** epoch 3240, rolling-avg-loss (window=10)= 0.016438295639818534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,171 INFO epoch # 3241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016244839353021234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,262 INFO epoch # 3242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016878633032320067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,351 INFO epoch # 3243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015769975623697974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,435 INFO epoch # 3244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018133530727936886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,531 INFO epoch # 3245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016091385405161418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,613 INFO epoch # 3246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017265550486627035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,696 INFO epoch # 3247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016140094041475095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,779 INFO epoch # 3248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017112879824708216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,863 INFO epoch # 3249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015546032242127694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:07,949 INFO epoch # 3250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016122333021485247
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:07,949 INFO *** epoch 3250, rolling-avg-loss (window=10)= 0.016530525375856087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:08,030 INFO epoch # 3251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01602841846033698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:08,109 INFO epoch # 3252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018105711802490987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:08,201 INFO epoch # 3253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018324787335586734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:08,334 INFO epoch # 3254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016163932465133257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:08,472 INFO epoch # 3255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017848253424745053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:08,603 INFO epoch # 3256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016014090055250563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:08,733 INFO epoch # 3257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015665114973671734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:08,844 INFO epoch # 3258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017158869275590405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:08,930 INFO epoch # 3259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021395814619609155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,014 INFO epoch # 3260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018853716901503503
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:09,014 INFO *** epoch 3260, rolling-avg-loss (window=10)= 0.017555870931391836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,094 INFO epoch # 3261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017719260067678988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,171 INFO epoch # 3262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015751777944387868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,284 INFO epoch # 3263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021314979807357304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,377 INFO epoch # 3264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01747881851042621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,463 INFO epoch # 3265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0158820531796664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,545 INFO epoch # 3266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01698149772710167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,625 INFO epoch # 3267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019038412181544118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,710 INFO epoch # 3268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017961097182706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,793 INFO epoch # 3269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01737907795177307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,875 INFO epoch # 3270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017429530154913664
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:09,876 INFO *** epoch 3270, rolling-avg-loss (window=10)= 0.01769365047075553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:09,961 INFO epoch # 3271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015442535674083047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:10,042 INFO epoch # 3272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016311551233229693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:10,126 INFO epoch # 3273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016522105754120275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:10,211 INFO epoch # 3274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02149674471002072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:10,291 INFO epoch # 3275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016755838732933626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:10,371 INFO epoch # 3276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017259116313653067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:10,452 INFO epoch # 3277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018194860458606854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:10,570 INFO epoch # 3278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018273356021381915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:10,677 INFO epoch # 3279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015169232385233045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:10,765 INFO epoch # 3280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016875870351213962
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:10,765 INFO *** epoch 3280, rolling-avg-loss (window=10)= 0.01723012116344762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:10,846 INFO epoch # 3281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01589318881451618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:10,925 INFO epoch # 3282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015130992847844027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,001 INFO epoch # 3283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018510490976041183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,081 INFO epoch # 3284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01855121908010915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,158 INFO epoch # 3285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018475665667210706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,235 INFO epoch # 3286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01571628189412877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,312 INFO epoch # 3287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018773449366563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,388 INFO epoch # 3288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01922096720954869
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,465 INFO epoch # 3289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01624400082801003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,545 INFO epoch # 3290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016866230507730506
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:11,545 INFO *** epoch 3290, rolling-avg-loss (window=10)= 0.017338248719170224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,627 INFO epoch # 3291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014330791382235475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,708 INFO epoch # 3292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0156815647642361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,789 INFO epoch # 3293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015559710343950428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,868 INFO epoch # 3294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016275010275421664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:11,948 INFO epoch # 3295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014886310687870719
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,048 INFO epoch # 3296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0163383697217796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,129 INFO epoch # 3297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015334938012529165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,208 INFO epoch # 3298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01437373278895393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,284 INFO epoch # 3299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015576059799059294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,361 INFO epoch # 3300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017594049058970995
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:12,361 INFO *** epoch 3300, rolling-avg-loss (window=10)= 0.015595053683500736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,437 INFO epoch # 3301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01824742811731994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,516 INFO epoch # 3302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01735290883516427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,595 INFO epoch # 3303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01614639263425488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,691 INFO epoch # 3304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01676649680302944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,774 INFO epoch # 3305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0164593367226189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,853 INFO epoch # 3306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01652766001643613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:12,931 INFO epoch # 3307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01945415578666143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,008 INFO epoch # 3308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01725038689619396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,088 INFO epoch # 3309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01669998507713899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,165 INFO epoch # 3310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015278446633601561
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:13,165 INFO *** epoch 3310, rolling-avg-loss (window=10)= 0.01701831975224195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,242 INFO epoch # 3311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016379026259528473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,319 INFO epoch # 3312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015473225677851588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,397 INFO epoch # 3313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0170271142560523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,474 INFO epoch # 3314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015608000190695748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,555 INFO epoch # 3315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014919185479811858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,632 INFO epoch # 3316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01773964345920831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,709 INFO epoch # 3317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01997726761328522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,787 INFO epoch # 3318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015654289309168234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,863 INFO epoch # 3319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014450684859184548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:13,940 INFO epoch # 3320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015451366954948753
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:13,940 INFO *** epoch 3320, rolling-avg-loss (window=10)= 0.016267980405973503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,018 INFO epoch # 3321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02158590345061384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,096 INFO epoch # 3322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017466235396568663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,174 INFO epoch # 3323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015955055408994667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,251 INFO epoch # 3324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015436786750797182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,328 INFO epoch # 3325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015983533929102123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,405 INFO epoch # 3326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019709012849489227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,483 INFO epoch # 3327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015948090513120405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,562 INFO epoch # 3328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01623276794271078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,640 INFO epoch # 3329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015535293248831294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,716 INFO epoch # 3330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019182954274583608
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:14,716 INFO *** epoch 3330, rolling-avg-loss (window=10)= 0.01730356337648118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,793 INFO epoch # 3331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017821312532760203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,870 INFO epoch # 3332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016646686344756745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:14,947 INFO epoch # 3333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017461949726566672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,026 INFO epoch # 3334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01635227019141894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,102 INFO epoch # 3335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015543031768174842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,178 INFO epoch # 3336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0174691787105985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,255 INFO epoch # 3337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015049314795760438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,332 INFO epoch # 3338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01768013194669038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,410 INFO epoch # 3339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020390441939525772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,491 INFO epoch # 3340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014977904807892628
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:15,491 INFO *** epoch 3340, rolling-avg-loss (window=10)= 0.016939222276414513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,575 INFO epoch # 3341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016420702784671448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,671 INFO epoch # 3342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016044741569203325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,796 INFO epoch # 3343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01722123913350515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,914 INFO epoch # 3344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016356980049749836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:15,993 INFO epoch # 3345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01448879925737856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:16,073 INFO epoch # 3346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015196060310699977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:16,171 INFO epoch # 3347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01567507720028516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:16,267 INFO epoch # 3348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016884913893591147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:16,352 INFO epoch # 3349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014808359745074995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:16,434 INFO epoch # 3350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01695091104193125
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:16,434 INFO *** epoch 3350, rolling-avg-loss (window=10)= 0.016004778498609085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:16,519 INFO epoch # 3351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01739417386124842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:16,604 INFO epoch # 3352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023761680262396112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:16,707 INFO epoch # 3353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015982481694663875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:16,792 INFO epoch # 3354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019569005962694064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:16,879 INFO epoch # 3355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01690745263476856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:16,969 INFO epoch # 3356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014199102777638473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,058 INFO epoch # 3357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01603496643656399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,139 INFO epoch # 3358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015969704734743573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,224 INFO epoch # 3359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015082951533258893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,310 INFO epoch # 3360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015071586356498301
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:17,310 INFO *** epoch 3360, rolling-avg-loss (window=10)= 0.016997310625447425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,391 INFO epoch # 3361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015023795443994459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,475 INFO epoch # 3362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01596657425398007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,566 INFO epoch # 3363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014700092084240168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,656 INFO epoch # 3364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01724900609406177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,738 INFO epoch # 3365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016132447693962604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,817 INFO epoch # 3366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018725326299318112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,895 INFO epoch # 3367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01880742030334659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:17,972 INFO epoch # 3368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016823050973471254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,050 INFO epoch # 3369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017014245539030526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,128 INFO epoch # 3370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01705633572419174
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:18,129 INFO *** epoch 3370, rolling-avg-loss (window=10)= 0.016749829440959728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,206 INFO epoch # 3371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014683103101560846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,284 INFO epoch # 3372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015584638065774925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,362 INFO epoch # 3373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015767648103064857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,442 INFO epoch # 3374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017369151551974937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,521 INFO epoch # 3375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016132807766553015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,601 INFO epoch # 3376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01456275962118525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,680 INFO epoch # 3377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015968433770467527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,758 INFO epoch # 3378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018085361065459438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,836 INFO epoch # 3379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016553608598769642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,914 INFO epoch # 3380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01575092683196999
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:18,915 INFO *** epoch 3380, rolling-avg-loss (window=10)= 0.016045843847678043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:18,993 INFO epoch # 3381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017024622233293485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,074 INFO epoch # 3382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017838051833678037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,152 INFO epoch # 3383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016864992343471386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,229 INFO epoch # 3384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01641269629180897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,307 INFO epoch # 3385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016786874388344586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,386 INFO epoch # 3386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015222680231090635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,465 INFO epoch # 3387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015027036351966672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,547 INFO epoch # 3388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017635964883083943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,626 INFO epoch # 3389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016736607038183138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,710 INFO epoch # 3390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015937255113385618
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:19,710 INFO *** epoch 3390, rolling-avg-loss (window=10)= 0.016548678070830648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,793 INFO epoch # 3391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0165138452721294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,874 INFO epoch # 3392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01595900100073777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:19,952 INFO epoch # 3393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015825251291971654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:20,058 INFO epoch # 3394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016538306823349558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:20,171 INFO epoch # 3395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017405042133759707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:20,259 INFO epoch # 3396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01601712536648847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:20,343 INFO epoch # 3397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014424364431761205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:20,428 INFO epoch # 3398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016340938964276575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:20,509 INFO epoch # 3399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016834644673508592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:20,591 INFO epoch # 3400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01592516523669474
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:20,591 INFO *** epoch 3400, rolling-avg-loss (window=10)= 0.016178368519467766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:20,671 INFO epoch # 3401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015236039951560088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:20,756 INFO epoch # 3402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01468878670129925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:20,866 INFO epoch # 3403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01865646969235968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:21,005 INFO epoch # 3404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015553468256257474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:21,122 INFO epoch # 3405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015608208894263953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:21,249 INFO epoch # 3406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01652674069191562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:21,344 INFO epoch # 3407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014003552496433258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:21,430 INFO epoch # 3408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016448474838398397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:21,539 INFO epoch # 3409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0150581352354493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:21,657 INFO epoch # 3410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015862863321672194
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:21,657 INFO *** epoch 3410, rolling-avg-loss (window=10)= 0.015764274007960922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:21,744 INFO epoch # 3411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015456868233741261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:21,828 INFO epoch # 3412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016520124190719798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:21,913 INFO epoch # 3413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016892671803361736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:21,997 INFO epoch # 3414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015311860523070209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:22,081 INFO epoch # 3415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02104602404870093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:22,171 INFO epoch # 3416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01518084121926222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:22,254 INFO epoch # 3417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017478506109910086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:22,343 INFO epoch # 3418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01645945086784195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:22,424 INFO epoch # 3419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01748258866427932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:22,505 INFO epoch # 3420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01750905426160898
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:22,505 INFO *** epoch 3420, rolling-avg-loss (window=10)= 0.01693379899224965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:22,603 INFO epoch # 3421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014972027653129771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:22,748 INFO epoch # 3422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015092168876435608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:22,857 INFO epoch # 3423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016302845149766654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:22,938 INFO epoch # 3424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013238517072750255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,019 INFO epoch # 3425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015355847383034416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,097 INFO epoch # 3426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014357457854202949
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,176 INFO epoch # 3427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014858228532830253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,254 INFO epoch # 3428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015404534715344198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,332 INFO epoch # 3429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016334579762769863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,411 INFO epoch # 3430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015592176365316845
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:23,412 INFO *** epoch 3430, rolling-avg-loss (window=10)= 0.015150838336558082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,492 INFO epoch # 3431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014714356933836825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,570 INFO epoch # 3432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016502499900525436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,649 INFO epoch # 3433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017205082607688382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,759 INFO epoch # 3434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013894727322622202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,863 INFO epoch # 3435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013944524602266029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:23,945 INFO epoch # 3436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01730406291608233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:24,061 INFO epoch # 3437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017267438364797272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:24,158 INFO epoch # 3438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015437469352036715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:24,241 INFO epoch # 3439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015455440341611393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:24,321 INFO epoch # 3440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01606131901644403
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:24,322 INFO *** epoch 3440, rolling-avg-loss (window=10)= 0.015778692135791063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:24,399 INFO epoch # 3441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01944771625858266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:24,477 INFO epoch # 3442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01562541411840357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:24,574 INFO epoch # 3443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01747004954086151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:24,656 INFO epoch # 3444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016626275624730624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:24,745 INFO epoch # 3445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015892647686996497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:24,845 INFO epoch # 3446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01662890493753366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:24,929 INFO epoch # 3447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015465036107343622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,011 INFO epoch # 3448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015145253390073776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,094 INFO epoch # 3449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013740670547122136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,175 INFO epoch # 3450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014233790192520246
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:25,175 INFO *** epoch 3450, rolling-avg-loss (window=10)= 0.01602757584041683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,255 INFO epoch # 3451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015552116237813607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,336 INFO epoch # 3452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019364995852811262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,416 INFO epoch # 3453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01651328594016377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,498 INFO epoch # 3454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01509880309458822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,583 INFO epoch # 3455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015533020836301148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,665 INFO epoch # 3456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01875219409703277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,753 INFO epoch # 3457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01684907822345849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,839 INFO epoch # 3458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015024549036752433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:25,921 INFO epoch # 3459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01875999683397822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,001 INFO epoch # 3460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014951049466617405
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:26,001 INFO *** epoch 3460, rolling-avg-loss (window=10)= 0.016639908961951732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,092 INFO epoch # 3461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014843065204331651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,186 INFO epoch # 3462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016796662166598253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,272 INFO epoch # 3463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014505630038911477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,355 INFO epoch # 3464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014989904520916753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,438 INFO epoch # 3465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015791867874213494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,518 INFO epoch # 3466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019403555168537423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,595 INFO epoch # 3467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015770879457704723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,672 INFO epoch # 3468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018382434005616233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,752 INFO epoch # 3469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014202812104485929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,829 INFO epoch # 3470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013487681382684968
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:26,830 INFO *** epoch 3470, rolling-avg-loss (window=10)= 0.01581744919240009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,907 INFO epoch # 3471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01406441250583157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:26,987 INFO epoch # 3472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015842698921915144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,068 INFO epoch # 3473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017970373053685762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,147 INFO epoch # 3474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017207046228577383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,229 INFO epoch # 3475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016712003212887794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,311 INFO epoch # 3476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015424478027853183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,393 INFO epoch # 3477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0147258616561885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,474 INFO epoch # 3478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01728771600755863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,555 INFO epoch # 3479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015360043704276904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,635 INFO epoch # 3480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014265867750509642
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:27,636 INFO *** epoch 3480, rolling-avg-loss (window=10)= 0.01588605010692845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,719 INFO epoch # 3481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01448613009415567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,797 INFO epoch # 3482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015875431112363003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,878 INFO epoch # 3483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01533556698996108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:27,958 INFO epoch # 3484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014438287151278928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:28,046 INFO epoch # 3485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01603508269181475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:28,129 INFO epoch # 3486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018850603053579107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:28,210 INFO epoch # 3487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017155866691609845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:28,299 INFO epoch # 3488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015604879794409499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:28,451 INFO epoch # 3489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015046300861286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:28,545 INFO epoch # 3490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014978275634348392
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:28,545 INFO *** epoch 3490, rolling-avg-loss (window=10)= 0.015780642407480627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:28,625 INFO epoch # 3491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016000892370357178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:28,722 INFO epoch # 3492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01591511635342613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:28,867 INFO epoch # 3493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014884464195347391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:28,974 INFO epoch # 3494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014331146943732165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:29,083 INFO epoch # 3495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015402815726702102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:29,192 INFO epoch # 3496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017333925265120342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:29,300 INFO epoch # 3497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015930421301163733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:29,389 INFO epoch # 3498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016326214943546802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:29,471 INFO epoch # 3499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014628690885729156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:29,558 INFO epoch # 3500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017046022694557905
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:29,558 INFO *** epoch 3500, rolling-avg-loss (window=10)= 0.01577997106796829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:29,686 INFO epoch # 3501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014986068781581707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:29,774 INFO epoch # 3502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016101492510642856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:29,859 INFO epoch # 3503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01960974906978663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:29,948 INFO epoch # 3504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016916811538976617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,034 INFO epoch # 3505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015373101879958995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,121 INFO epoch # 3506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014568085927749053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,204 INFO epoch # 3507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01569687984738266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,285 INFO epoch # 3508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014965898430091329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,364 INFO epoch # 3509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015216880245134234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,443 INFO epoch # 3510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014756703487364575
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:30,443 INFO *** epoch 3510, rolling-avg-loss (window=10)= 0.015819167171866867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,522 INFO epoch # 3511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013158880188711919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,602 INFO epoch # 3512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016231769375735894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,682 INFO epoch # 3513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015111579166841693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,767 INFO epoch # 3514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01368685312627349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,850 INFO epoch # 3515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014697787642944604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:30,929 INFO epoch # 3516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017137504037236795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,010 INFO epoch # 3517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015423860226292163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,093 INFO epoch # 3518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018436168815242127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,176 INFO epoch # 3519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014730330207385123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,256 INFO epoch # 3520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013908619970607106
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:31,256 INFO *** epoch 3520, rolling-avg-loss (window=10)= 0.015252335275727091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,334 INFO epoch # 3521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014772908834856935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,413 INFO epoch # 3522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01483783267030958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,492 INFO epoch # 3523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014228480868041515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,574 INFO epoch # 3524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016333137959009036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,655 INFO epoch # 3525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016083244248875417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,735 INFO epoch # 3526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015324476960813627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,818 INFO epoch # 3527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01606335765973199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,897 INFO epoch # 3528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01687491056509316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:31,977 INFO epoch # 3529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015195594300166704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,059 INFO epoch # 3530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014096786137088202
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:32,060 INFO *** epoch 3530, rolling-avg-loss (window=10)= 0.015381073020398617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,145 INFO epoch # 3531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015672550071030855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,233 INFO epoch # 3532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015047264751046896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,315 INFO epoch # 3533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016786220730864443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,396 INFO epoch # 3534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02360174321802333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,483 INFO epoch # 3535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01574617215374019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,573 INFO epoch # 3536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016416349928476848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,658 INFO epoch # 3537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018428381474222988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,743 INFO epoch # 3538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015476712040253915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,825 INFO epoch # 3539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018263707519508898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,907 INFO epoch # 3540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015177178720477968
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:32,907 INFO *** epoch 3540, rolling-avg-loss (window=10)= 0.01706162806076463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:32,989 INFO epoch # 3541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014346946481964551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:33,104 INFO epoch # 3542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016451475879875943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:33,218 INFO epoch # 3543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0164783416694263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:33,304 INFO epoch # 3544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01954309968277812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:33,388 INFO epoch # 3545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01658171380404383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:33,474 INFO epoch # 3546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014513173489831388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:33,556 INFO epoch # 3547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01400364696746692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:33,639 INFO epoch # 3548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014702805026900023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:33,719 INFO epoch # 3549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015511258083279245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:33,798 INFO epoch # 3550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019972491907537915
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:33,799 INFO *** epoch 3550, rolling-avg-loss (window=10)= 0.016210495299310422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:33,877 INFO epoch # 3551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016125069654663093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:33,956 INFO epoch # 3552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01452977632288821
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,036 INFO epoch # 3553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014199990415363573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,121 INFO epoch # 3554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014417618149309419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,200 INFO epoch # 3555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014006567856995389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,277 INFO epoch # 3556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01525824713462498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,355 INFO epoch # 3557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014157970566884615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,433 INFO epoch # 3558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017890618852106854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,511 INFO epoch # 3559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01725332070782315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,591 INFO epoch # 3560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013412357118795626
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:34,591 INFO *** epoch 3560, rolling-avg-loss (window=10)= 0.01512515367794549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,669 INFO epoch # 3561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01504036872938741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,747 INFO epoch # 3562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01509646899648942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,827 INFO epoch # 3563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013365509861614555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,906 INFO epoch # 3564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016563122015213594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:34,985 INFO epoch # 3565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0171291596343508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:35,065 INFO epoch # 3566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019849014759529382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:35,144 INFO epoch # 3567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01647815853357315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:35,222 INFO epoch # 3568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014247857528971508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:35,316 INFO epoch # 3569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015866151923546568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:35,397 INFO epoch # 3570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01583115480025299
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:35,398 INFO *** epoch 3570, rolling-avg-loss (window=10)= 0.01594669667829294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:35,489 INFO epoch # 3571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015616415825206786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:35,572 INFO epoch # 3572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016091427532956004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:35,654 INFO epoch # 3573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01583850002498366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:35,742 INFO epoch # 3574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014416439225897193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:35,830 INFO epoch # 3575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012988998663786333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:35,922 INFO epoch # 3576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013449964229948819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,007 INFO epoch # 3577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01504469380597584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,132 INFO epoch # 3578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014817872026469558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,219 INFO epoch # 3579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013543201770517044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,303 INFO epoch # 3580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015051018126541749
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:36,304 INFO *** epoch 3580, rolling-avg-loss (window=10)= 0.014685853123228298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,386 INFO epoch # 3581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015816196639207192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,470 INFO epoch # 3582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015499492423259653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,554 INFO epoch # 3583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014021919094375335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,634 INFO epoch # 3584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015014517994131893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,713 INFO epoch # 3585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014522969810059294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,793 INFO epoch # 3586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015047364242491312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,875 INFO epoch # 3587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017286753063672222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:36,955 INFO epoch # 3588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01647150330245495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:37,042 INFO epoch # 3589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015402835953864269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:37,125 INFO epoch # 3590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01476372881734278
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:37,126 INFO *** epoch 3590, rolling-avg-loss (window=10)= 0.01538472813408589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:37,227 INFO epoch # 3591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015421571835759096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:37,315 INFO epoch # 3592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014764486593776383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:37,395 INFO epoch # 3593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01586955647508148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:37,486 INFO epoch # 3594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015848047376493923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:37,569 INFO epoch # 3595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018191104973084293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:37,665 INFO epoch # 3596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014510386157780886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:37,760 INFO epoch # 3597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01477112341672182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:37,842 INFO epoch # 3598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01832267371355556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:37,928 INFO epoch # 3599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015331456161220558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,009 INFO epoch # 3600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01838769720052369
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:38,009 INFO *** epoch 3600, rolling-avg-loss (window=10)= 0.016141810390399768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,091 INFO epoch # 3601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014471932139713317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,174 INFO epoch # 3602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014292911713710055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,258 INFO epoch # 3603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01663327112328261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,346 INFO epoch # 3604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014913673694536556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,432 INFO epoch # 3605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01583192078396678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,516 INFO epoch # 3606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016143778804689646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,603 INFO epoch # 3607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0158471856120741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,730 INFO epoch # 3608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015796740248333663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,818 INFO epoch # 3609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014577121037291363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,903 INFO epoch # 3610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013676726390258409
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:38,903 INFO *** epoch 3610, rolling-avg-loss (window=10)= 0.01521852615478565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:38,985 INFO epoch # 3611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02075211427290924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:39,072 INFO epoch # 3612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014245732381823473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:39,154 INFO epoch # 3613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013760448331595398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:39,240 INFO epoch # 3614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014603938805521466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:39,320 INFO epoch # 3615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014186675514793023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:39,401 INFO epoch # 3616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017027632420649752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:39,528 INFO epoch # 3617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014229981185053475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:39,631 INFO epoch # 3618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01606750079372432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:39,719 INFO epoch # 3619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013754500410868786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:39,827 INFO epoch # 3620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015894948795903474
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:39,827 INFO *** epoch 3620, rolling-avg-loss (window=10)= 0.015452347291284241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:39,913 INFO epoch # 3621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014479380959528498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:40,031 INFO epoch # 3622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01557865359063726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:40,139 INFO epoch # 3623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016092162957647815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:40,224 INFO epoch # 3624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013883670086215716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:40,305 INFO epoch # 3625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015143228563829325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:40,429 INFO epoch # 3626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018400797533104196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:40,603 INFO epoch # 3627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013522380540962331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:40,695 INFO epoch # 3628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01323399052489549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:40,784 INFO epoch # 3629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013701074334676377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:40,870 INFO epoch # 3630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014015423817909323
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:40,870 INFO *** epoch 3630, rolling-avg-loss (window=10)= 0.014805076290940633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:40,950 INFO epoch # 3631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017486732482211664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:41,030 INFO epoch # 3632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01585969601001125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:41,110 INFO epoch # 3633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015873133888817392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:41,194 INFO epoch # 3634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015500854744459502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:41,276 INFO epoch # 3635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013527038070606068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:41,358 INFO epoch # 3636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014388085561222397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:41,458 INFO epoch # 3637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016374510116293095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:41,611 INFO epoch # 3638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014716416146256961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:41,735 INFO epoch # 3639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015387118270155042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:41,816 INFO epoch # 3640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014668505507870577
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:41,816 INFO *** epoch 3640, rolling-avg-loss (window=10)= 0.015378209079790395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:41,897 INFO epoch # 3641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015919939149171114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:41,976 INFO epoch # 3642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01489649637369439
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,055 INFO epoch # 3643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013649199245264754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,133 INFO epoch # 3644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014293232001364231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,211 INFO epoch # 3645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014032388833584264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,289 INFO epoch # 3646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021138953568879515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,370 INFO epoch # 3647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01371728796948446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,449 INFO epoch # 3648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015176034314208664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,533 INFO epoch # 3649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018913195875938982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,617 INFO epoch # 3650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01409390322805848
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:42,617 INFO *** epoch 3650, rolling-avg-loss (window=10)= 0.015583063055964885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,706 INFO epoch # 3651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013304648484336212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,789 INFO epoch # 3652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015437919777468778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,870 INFO epoch # 3653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016249347565462813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:42,955 INFO epoch # 3654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01588093522877898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,034 INFO epoch # 3655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013863284562830813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,113 INFO epoch # 3656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016646461677737534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,192 INFO epoch # 3657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015017906756838784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,271 INFO epoch # 3658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013441743882140145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,350 INFO epoch # 3659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013947147264843807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,437 INFO epoch # 3660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01928589410817949
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:43,437 INFO *** epoch 3660, rolling-avg-loss (window=10)= 0.015307528930861736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,517 INFO epoch # 3661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014742459054104984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,596 INFO epoch # 3662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01601605405448936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,674 INFO epoch # 3663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01439746210235171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,790 INFO epoch # 3664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014488903543679044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,883 INFO epoch # 3665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013312836425029673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:43,970 INFO epoch # 3666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014071749188588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:44,051 INFO epoch # 3667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01375630144320894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:44,157 INFO epoch # 3668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014542396878823638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:44,258 INFO epoch # 3669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016294597400701605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:44,354 INFO epoch # 3670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017033999371051323
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:44,354 INFO *** epoch 3670, rolling-avg-loss (window=10)= 0.014865675946202828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:44,465 INFO epoch # 3671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014166525899781846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:44,559 INFO epoch # 3672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016828985273605213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:44,643 INFO epoch # 3673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013764750445261598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:44,726 INFO epoch # 3674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01480107607494574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:44,839 INFO epoch # 3675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015258002895279787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:44,932 INFO epoch # 3676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013950447566458024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,020 INFO epoch # 3677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015153393789660186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,108 INFO epoch # 3678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013909997040173039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,190 INFO epoch # 3679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014390979136805981
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,268 INFO epoch # 3680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015242704321281053
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:45,268 INFO *** epoch 3680, rolling-avg-loss (window=10)= 0.014746686244325246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,348 INFO epoch # 3681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016079897905001417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,438 INFO epoch # 3682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016711514646885917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,520 INFO epoch # 3683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015390560860396363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,602 INFO epoch # 3684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016492283190018497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,692 INFO epoch # 3685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013468978519085795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,779 INFO epoch # 3686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014492190486635081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,871 INFO epoch # 3687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01669686437526252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:45,957 INFO epoch # 3688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014306981916888617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,037 INFO epoch # 3689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014634063525591046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,116 INFO epoch # 3690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014057065374800004
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:46,116 INFO *** epoch 3690, rolling-avg-loss (window=10)= 0.015233040080056525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,236 INFO epoch # 3691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014792311121709645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,330 INFO epoch # 3692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014428102775127627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,418 INFO epoch # 3693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017438523544115014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,504 INFO epoch # 3694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01637723029125482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,589 INFO epoch # 3695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014961825901991688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,672 INFO epoch # 3696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014104941903497092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,754 INFO epoch # 3697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013444247058941983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,833 INFO epoch # 3698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01531135402910877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,913 INFO epoch # 3699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01603553154563997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:46,996 INFO epoch # 3700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015072106005391106
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:46,996 INFO *** epoch 3700, rolling-avg-loss (window=10)= 0.015196617417677772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:47,077 INFO epoch # 3701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023020178748993203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:47,158 INFO epoch # 3702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015261142907547764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:47,241 INFO epoch # 3703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014740299244294874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:47,323 INFO epoch # 3704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014471603426500224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:47,404 INFO epoch # 3705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016397268715081736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:47,517 INFO epoch # 3706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01411534141516313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:47,633 INFO epoch # 3707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015267783266608603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:47,725 INFO epoch # 3708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01678656105650589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:47,812 INFO epoch # 3709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014604115349357016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:47,894 INFO epoch # 3710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01494300908234436
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:47,894 INFO *** epoch 3710, rolling-avg-loss (window=10)= 0.01596073032123968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:47,975 INFO epoch # 3711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014888838762999512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,056 INFO epoch # 3712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015057578828418627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,145 INFO epoch # 3713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014530777552863583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,251 INFO epoch # 3714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014609777586883865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,332 INFO epoch # 3715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012990002811420709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,412 INFO epoch # 3716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015710794454207644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,493 INFO epoch # 3717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019165853867889382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,575 INFO epoch # 3718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015444254138856195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,654 INFO epoch # 3719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015255819060257636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,734 INFO epoch # 3720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018800778256263584
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:48,734 INFO *** epoch 3720, rolling-avg-loss (window=10)= 0.015645447532006074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,812 INFO epoch # 3721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014740201164386235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,891 INFO epoch # 3722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01686177824740298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:48,970 INFO epoch # 3723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016164296030183323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,101 INFO epoch # 3724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015720819545094855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,179 INFO epoch # 3725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015540257911197841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,258 INFO epoch # 3726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015954992835759185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,337 INFO epoch # 3727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015112939858227037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,415 INFO epoch # 3728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014673359117296059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,493 INFO epoch # 3729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013106722239172086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,571 INFO epoch # 3730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013743499119300395
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:49,571 INFO *** epoch 3730, rolling-avg-loss (window=10)= 0.015161886606802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,650 INFO epoch # 3731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016231219691690058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,729 INFO epoch # 3732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015924576407996938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,807 INFO epoch # 3733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013751591235632077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,887 INFO epoch # 3734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015190718899248168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:49,965 INFO epoch # 3735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014540897216647863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,046 INFO epoch # 3736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015156469627982005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,124 INFO epoch # 3737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013429557293420658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,203 INFO epoch # 3738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014097868115641177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,281 INFO epoch # 3739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014414946766919456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,359 INFO epoch # 3740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014888756035361439
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:50,360 INFO *** epoch 3740, rolling-avg-loss (window=10)= 0.014762660129053983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,439 INFO epoch # 3741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014147517445962876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,518 INFO epoch # 3742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014349675810080953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,597 INFO epoch # 3743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014665020004031248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,676 INFO epoch # 3744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014775707750231959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,755 INFO epoch # 3745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015003333712229505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,833 INFO epoch # 3746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013943706566351466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,911 INFO epoch # 3747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014004803975694813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:50,989 INFO epoch # 3748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014920898494892754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,071 INFO epoch # 3749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015283787812222727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,150 INFO epoch # 3750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015660140372347087
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:51,150 INFO *** epoch 3750, rolling-avg-loss (window=10)= 0.01467545919440454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,229 INFO epoch # 3751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015242901572491974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,308 INFO epoch # 3752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013975287452922203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,386 INFO epoch # 3753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015524764952715486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,466 INFO epoch # 3754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014514050242723897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,549 INFO epoch # 3755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016195670948945917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,628 INFO epoch # 3756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014270330197177827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,707 INFO epoch # 3757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018039162387140095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,791 INFO epoch # 3758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01579330778622534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,874 INFO epoch # 3759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01603406495996751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:51,952 INFO epoch # 3760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013603744941065088
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:51,953 INFO *** epoch 3760, rolling-avg-loss (window=10)= 0.015319328544137534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:52,032 INFO epoch # 3761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016248271174845286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:52,187 INFO epoch # 3762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0148407940287143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:52,308 INFO epoch # 3763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018109406737494282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:52,439 INFO epoch # 3764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013335593190276995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:52,543 INFO epoch # 3765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013339292723685503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:52,628 INFO epoch # 3766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0137570186634548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:52,710 INFO epoch # 3767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01480916835134849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:52,791 INFO epoch # 3768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01308040859294124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:52,875 INFO epoch # 3769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016007026584702544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:52,956 INFO epoch # 3770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014649159842520021
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:52,956 INFO *** epoch 3770, rolling-avg-loss (window=10)= 0.014817613988998345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,038 INFO epoch # 3771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015525075024925172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,117 INFO epoch # 3772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016558715258724988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,198 INFO epoch # 3773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01373613448231481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,280 INFO epoch # 3774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012729475507512689
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,362 INFO epoch # 3775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013442683790344745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,443 INFO epoch # 3776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01679692907782737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,524 INFO epoch # 3777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01589574955869466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,606 INFO epoch # 3778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016725748137105256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,687 INFO epoch # 3779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01634500401269179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,769 INFO epoch # 3780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014702280546771362
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:53,769 INFO *** epoch 3780, rolling-avg-loss (window=10)= 0.015245779539691284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,849 INFO epoch # 3781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016394365462474525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:53,928 INFO epoch # 3782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016913271800149232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:54,007 INFO epoch # 3783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014037767527042888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:54,093 INFO epoch # 3784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015686332568293437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:54,224 INFO epoch # 3785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016923995739489328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:54,354 INFO epoch # 3786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014289732716861181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:54,438 INFO epoch # 3787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013830077645252459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:54,520 INFO epoch # 3788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0152939217077801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:54,601 INFO epoch # 3789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01704615655762609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:54,683 INFO epoch # 3790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014840860923868604
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:54,683 INFO *** epoch 3790, rolling-avg-loss (window=10)= 0.015525648264883785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:54,763 INFO epoch # 3791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014981340063968673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:54,842 INFO epoch # 3792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01393373729661107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:54,921 INFO epoch # 3793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015161498013185337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,000 INFO epoch # 3794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014469335408648476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,081 INFO epoch # 3795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014096589220571332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,160 INFO epoch # 3796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01744502445217222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,239 INFO epoch # 3797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01483379045384936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,317 INFO epoch # 3798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018358779867412522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,397 INFO epoch # 3799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015294391821953468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,479 INFO epoch # 3800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01694289555598516
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:55,479 INFO *** epoch 3800, rolling-avg-loss (window=10)= 0.015551738215435762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,561 INFO epoch # 3801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013727242359891534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,639 INFO epoch # 3802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01363037210830953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,717 INFO epoch # 3803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013905438638175838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,795 INFO epoch # 3804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013172809369280003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,874 INFO epoch # 3805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012627213727682829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:55,952 INFO epoch # 3806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017028235859470442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,034 INFO epoch # 3807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01443824268062599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,118 INFO epoch # 3808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013746970813372172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,198 INFO epoch # 3809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013028193337959237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,276 INFO epoch # 3810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013476413078024052
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:56,276 INFO *** epoch 3810, rolling-avg-loss (window=10)= 0.013878113197279163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,355 INFO epoch # 3811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014149708207696676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,433 INFO epoch # 3812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01379720726981759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,512 INFO epoch # 3813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014387516246642917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,594 INFO epoch # 3814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013529995630960912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,672 INFO epoch # 3815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014178618177538738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,751 INFO epoch # 3816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01506076639634557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,831 INFO epoch # 3817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013780653156572953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,910 INFO epoch # 3818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013782187379547395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:56,989 INFO epoch # 3819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015515173130552284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,072 INFO epoch # 3820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01433359867951367
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:57,073 INFO *** epoch 3820, rolling-avg-loss (window=10)= 0.01425154242751887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,155 INFO epoch # 3821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017049146685167216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,235 INFO epoch # 3822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01480123630608432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,315 INFO epoch # 3823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016526143706869334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,394 INFO epoch # 3824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012724086467642337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,472 INFO epoch # 3825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014685763817396946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,554 INFO epoch # 3826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016148978596902452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,633 INFO epoch # 3827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014508895503240637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,713 INFO epoch # 3828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016566354490350932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,793 INFO epoch # 3829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014136046862404328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,871 INFO epoch # 3830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012793015368515626
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:57,872 INFO *** epoch 3830, rolling-avg-loss (window=10)= 0.014993966780457413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:57,954 INFO epoch # 3831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012979688515770249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,046 INFO epoch # 3832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015488269636989571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,136 INFO epoch # 3833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013152536252164282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,219 INFO epoch # 3834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01411799444758799
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,301 INFO epoch # 3835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014388030715053901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,384 INFO epoch # 3836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01497391186421737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,464 INFO epoch # 3837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014924117393093184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,544 INFO epoch # 3838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012857815221650526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,624 INFO epoch # 3839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01429773709969595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,706 INFO epoch # 3840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014589896352845244
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:58,706 INFO *** epoch 3840, rolling-avg-loss (window=10)= 0.014176999749906827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,787 INFO epoch # 3841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015162626514211297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,866 INFO epoch # 3842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016747935122111812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:58,944 INFO epoch # 3843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015872873569605872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,024 INFO epoch # 3844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014077250976697542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,103 INFO epoch # 3845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016888410013052635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,183 INFO epoch # 3846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017375495343003422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,266 INFO epoch # 3847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015623132174368948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,350 INFO epoch # 3848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01708082245022524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,435 INFO epoch # 3849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01502019895997364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,521 INFO epoch # 3850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013916070514824241
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:18:59,521 INFO *** epoch 3850, rolling-avg-loss (window=10)= 0.015776481563807464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,601 INFO epoch # 3851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012491161411162466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,682 INFO epoch # 3852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01309616798243951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,764 INFO epoch # 3853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01439230123651214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,844 INFO epoch # 3854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015005611276137643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:18:59,922 INFO epoch # 3855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013782553098280914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,024 INFO epoch # 3856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015395850408822298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,121 INFO epoch # 3857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014076959254452959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,209 INFO epoch # 3858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014368646981893107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,290 INFO epoch # 3859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014276873436756432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,373 INFO epoch # 3860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01390073198126629
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:00,373 INFO *** epoch 3860, rolling-avg-loss (window=10)= 0.014078685706772376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,454 INFO epoch # 3861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014083791815210134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,541 INFO epoch # 3862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0204958894464653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,621 INFO epoch # 3863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016384910137276165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,699 INFO epoch # 3864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015393082212540321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,777 INFO epoch # 3865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014357995183672756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,902 INFO epoch # 3866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012881882314104587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:00,988 INFO epoch # 3867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014181628488586284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,074 INFO epoch # 3868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013412543528829701
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,156 INFO epoch # 3869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014343370334245265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,239 INFO epoch # 3870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015209444900392555
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:01,239 INFO *** epoch 3870, rolling-avg-loss (window=10)= 0.015074453836132307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,319 INFO epoch # 3871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013415284280199558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,397 INFO epoch # 3872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012588058962137438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,477 INFO epoch # 3873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015007774054538459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,556 INFO epoch # 3874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012359363710856996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,636 INFO epoch # 3875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015360658595454879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,716 INFO epoch # 3876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016209628694923595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,800 INFO epoch # 3877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015254538113367744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,882 INFO epoch # 3878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016305137745803222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:01,963 INFO epoch # 3879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015359125987743028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,042 INFO epoch # 3880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013643787759065162
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:02,042 INFO *** epoch 3880, rolling-avg-loss (window=10)= 0.014550335790409008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,159 INFO epoch # 3881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013320659229066223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,257 INFO epoch # 3882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013012815194088034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,346 INFO epoch # 3883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014328334786114283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,428 INFO epoch # 3884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015004037995822728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,511 INFO epoch # 3885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014386473107151687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,591 INFO epoch # 3886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015076828931341879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,673 INFO epoch # 3887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017512131467810832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,753 INFO epoch # 3888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01483119603653904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,835 INFO epoch # 3889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013019659425481223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,916 INFO epoch # 3890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015231575613142923
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:02,916 INFO *** epoch 3890, rolling-avg-loss (window=10)= 0.014572371178655886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:02,998 INFO epoch # 3891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013064944520010613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,081 INFO epoch # 3892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014173145027598366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,163 INFO epoch # 3893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012902028669486754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,243 INFO epoch # 3894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013039689729339443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,325 INFO epoch # 3895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013705775651033036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,406 INFO epoch # 3896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013959505697130226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,487 INFO epoch # 3897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014454733987804502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,568 INFO epoch # 3898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016958296168013476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,648 INFO epoch # 3899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013839417064446025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,726 INFO epoch # 3900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015026894674520008
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:03,726 INFO *** epoch 3900, rolling-avg-loss (window=10)= 0.014112443118938245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,804 INFO epoch # 3901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01998413330875337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,883 INFO epoch # 3902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0167362983484054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:03,961 INFO epoch # 3903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012992316827876493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,043 INFO epoch # 3904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014810229244176298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,128 INFO epoch # 3905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015537552113528363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,210 INFO epoch # 3906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014578530375729315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,288 INFO epoch # 3907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012989455455681309
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,368 INFO epoch # 3908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015510215089307167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,448 INFO epoch # 3909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011879574143677019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,530 INFO epoch # 3910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013128654274623841
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:04,530 INFO *** epoch 3910, rolling-avg-loss (window=10)= 0.014814695918175858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,609 INFO epoch # 3911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01424115781992441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,692 INFO epoch # 3912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013597043420304544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,773 INFO epoch # 3913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014700802392326295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,852 INFO epoch # 3914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011995011343969963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:04,931 INFO epoch # 3915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012918634442030452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,009 INFO epoch # 3916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015329499510698952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,092 INFO epoch # 3917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01479904465668369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,175 INFO epoch # 3918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015173250198131427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,255 INFO epoch # 3919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016091997138573788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,336 INFO epoch # 3920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012986001864192076
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:05,336 INFO *** epoch 3920, rolling-avg-loss (window=10)= 0.01418324427868356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,415 INFO epoch # 3921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013717720023123547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,503 INFO epoch # 3922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01486092024424579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,588 INFO epoch # 3923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013888830711948685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,669 INFO epoch # 3924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013816403326927684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,748 INFO epoch # 3925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015693942783400416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,828 INFO epoch # 3926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015085038758115843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,910 INFO epoch # 3927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013852022661012597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:05,988 INFO epoch # 3928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01387214130954817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,070 INFO epoch # 3929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014945870905648917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,150 INFO epoch # 3930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01405305070511531
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:06,150 INFO *** epoch 3930, rolling-avg-loss (window=10)= 0.014378594142908697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,229 INFO epoch # 3931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015820106738829054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,308 INFO epoch # 3932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01289138245920185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,389 INFO epoch # 3933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01255692339327652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,468 INFO epoch # 3934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01283386089198757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,547 INFO epoch # 3935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01279148725734558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,628 INFO epoch # 3936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014544781231961679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,707 INFO epoch # 3937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013098967785481364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,787 INFO epoch # 3938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014052568571059965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,867 INFO epoch # 3939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01720003479567822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:06,946 INFO epoch # 3940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012832021559006535
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:06,946 INFO *** epoch 3940, rolling-avg-loss (window=10)= 0.013862213468382834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,030 INFO epoch # 3941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015576233476167545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,111 INFO epoch # 3942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0166132859303616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,190 INFO epoch # 3943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015458716385182925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,270 INFO epoch # 3944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014476474752882496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,350 INFO epoch # 3945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016990433548926376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,429 INFO epoch # 3946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014231188222765923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,508 INFO epoch # 3947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013754990810411982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,588 INFO epoch # 3948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01347944482404273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,667 INFO epoch # 3949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013715881534153596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,750 INFO epoch # 3950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012969974355655722
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:07,750 INFO *** epoch 3950, rolling-avg-loss (window=10)= 0.01472666238405509
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,830 INFO epoch # 3951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013606567124952562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,909 INFO epoch # 3952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01588937363703735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:07,988 INFO epoch # 3953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016519120370503515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,068 INFO epoch # 3954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013837440026691183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,148 INFO epoch # 3955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01369615294970572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,231 INFO epoch # 3956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01355923964001704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,311 INFO epoch # 3957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01389616701635532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,390 INFO epoch # 3958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01430988809443079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,469 INFO epoch # 3959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01386336429277435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,552 INFO epoch # 3960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013664868340129033
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:08,552 INFO *** epoch 3960, rolling-avg-loss (window=10)= 0.014284218149259686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,631 INFO epoch # 3961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01655029992980417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,715 INFO epoch # 3962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01550462878367398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,794 INFO epoch # 3963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014747550871106796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,873 INFO epoch # 3964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01193591584888054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:08,953 INFO epoch # 3965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013864753032976296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,037 INFO epoch # 3966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018370901001617312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,117 INFO epoch # 3967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01322970875480678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,197 INFO epoch # 3968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014256817041314207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,278 INFO epoch # 3969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012721060891635716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,357 INFO epoch # 3970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013492363985278644
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:09,357 INFO *** epoch 3970, rolling-avg-loss (window=10)= 0.014467400014109445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,436 INFO epoch # 3971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015290142735466361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,516 INFO epoch # 3972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01354454254033044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,597 INFO epoch # 3973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013281524792546406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,676 INFO epoch # 3974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011890327135915868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,754 INFO epoch # 3975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014016867324244231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,833 INFO epoch # 3976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016143208238645457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,912 INFO epoch # 3977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016729187904275022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:09,991 INFO epoch # 3978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015993620887456927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,072 INFO epoch # 3979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013242131521110423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,152 INFO epoch # 3980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012695805955445394
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:10,152 INFO *** epoch 3980, rolling-avg-loss (window=10)= 0.014282735903543653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,232 INFO epoch # 3981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012753974209772423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,311 INFO epoch # 3982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01422926303348504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,389 INFO epoch # 3983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013374259287957102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,469 INFO epoch # 3984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013005877975956537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,548 INFO epoch # 3985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014477119053481147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,628 INFO epoch # 3986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015871505209361203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,710 INFO epoch # 3987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014736801487742923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,788 INFO epoch # 3988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017053339994163252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,867 INFO epoch # 3989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014104813148151152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:10,948 INFO epoch # 3990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015479672147193924
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:10,948 INFO *** epoch 3990, rolling-avg-loss (window=10)= 0.01450866255472647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,029 INFO epoch # 3991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013150930040865205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,108 INFO epoch # 3992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015265666297636926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,190 INFO epoch # 3993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014143337175482884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,272 INFO epoch # 3994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015438206217368133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,351 INFO epoch # 3995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014049008139409125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,432 INFO epoch # 3996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014875158696668223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,511 INFO epoch # 3997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014103627996519208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,591 INFO epoch # 3998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01517705907463096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,671 INFO epoch # 3999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014153705051285215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,750 INFO epoch # 4000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01498859615821857
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:11,750 INFO *** epoch 4000, rolling-avg-loss (window=10)= 0.014534529484808444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,828 INFO epoch # 4001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01528910524211824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,907 INFO epoch # 4002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015257697406923398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:11,986 INFO epoch # 4003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014244102014345117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,066 INFO epoch # 4004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012281317889573984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,146 INFO epoch # 4005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012884534153272398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,226 INFO epoch # 4006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013095546804834157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,308 INFO epoch # 4007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012708401554846205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,389 INFO epoch # 4008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012407150861690752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,469 INFO epoch # 4009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013836748359608464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,552 INFO epoch # 4010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012925611968967132
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:12,552 INFO *** epoch 4010, rolling-avg-loss (window=10)= 0.013493021625617985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,638 INFO epoch # 4011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01248871193092782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,718 INFO epoch # 4012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011899287026608363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,797 INFO epoch # 4013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01371055624622386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,878 INFO epoch # 4014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01401759500731714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:12,958 INFO epoch # 4015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013718083398998715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,038 INFO epoch # 4016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01237537499400787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,117 INFO epoch # 4017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013205075563746504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,196 INFO epoch # 4018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014754405317944475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,276 INFO epoch # 4019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012620028835954145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,358 INFO epoch # 4020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011675002897391096
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:13,358 INFO *** epoch 4020, rolling-avg-loss (window=10)= 0.013046412121911998
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,437 INFO epoch # 4021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01406759578094352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,516 INFO epoch # 4022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01621748688921798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,596 INFO epoch # 4023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016311181912897155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,678 INFO epoch # 4024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014991448580985889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,762 INFO epoch # 4025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014280515162681695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,840 INFO epoch # 4026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014746345623279922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,919 INFO epoch # 4027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013816167993354611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:13,997 INFO epoch # 4028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017526133960927837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,079 INFO epoch # 4029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014570210099918768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,159 INFO epoch # 4030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012598105386132374
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:14,159 INFO *** epoch 4030, rolling-avg-loss (window=10)= 0.014912519139033974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,237 INFO epoch # 4031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014002179028466344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,318 INFO epoch # 4032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018986642142408527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,397 INFO epoch # 4033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014444977408857085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,479 INFO epoch # 4034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012527115424745716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,559 INFO epoch # 4035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013374435191508383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,637 INFO epoch # 4036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015838375664316118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,729 INFO epoch # 4037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016340635396772996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,818 INFO epoch # 4038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01364038692554459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,901 INFO epoch # 4039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012606129035702907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:14,983 INFO epoch # 4040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013479771892889403
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:14,983 INFO *** epoch 4040, rolling-avg-loss (window=10)= 0.014524064811121206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:15,095 INFO epoch # 4041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018694112717639655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:15,216 INFO epoch # 4042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015006078698206693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:15,313 INFO epoch # 4043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013173792933230288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:15,436 INFO epoch # 4044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013031519265496172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:15,533 INFO epoch # 4045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01376760761195328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:15,631 INFO epoch # 4046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012432545358024072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:15,716 INFO epoch # 4047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015548899347777478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:15,799 INFO epoch # 4048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013096086258883588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:15,879 INFO epoch # 4049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015137879076064564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:15,961 INFO epoch # 4050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014654339887783863
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:15,961 INFO *** epoch 4050, rolling-avg-loss (window=10)= 0.014454286115505966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,042 INFO epoch # 4051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01218035195779521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,125 INFO epoch # 4052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013375724716752302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,207 INFO epoch # 4053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014091709337662905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,288 INFO epoch # 4054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014062546339118853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,369 INFO epoch # 4055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014531652996083722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,447 INFO epoch # 4056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012658261708565988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,527 INFO epoch # 4057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01248257867700886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,611 INFO epoch # 4058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013419834329397418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,711 INFO epoch # 4059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01591556309722364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,796 INFO epoch # 4060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016219232027651742
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:16,797 INFO *** epoch 4060, rolling-avg-loss (window=10)= 0.013893745518726065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,878 INFO epoch # 4061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01294230148778297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:16,957 INFO epoch # 4062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013202391171944328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,038 INFO epoch # 4063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013967822553240694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,118 INFO epoch # 4064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013730989710893482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,198 INFO epoch # 4065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015568708855425939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,277 INFO epoch # 4066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019441365366219543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,356 INFO epoch # 4067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020338234171504155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,435 INFO epoch # 4068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013327513006515801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,517 INFO epoch # 4069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013751368198427372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,599 INFO epoch # 4070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014063453243579715
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:17,599 INFO *** epoch 4070, rolling-avg-loss (window=10)= 0.0150334147765534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,682 INFO epoch # 4071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017582503249286674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,761 INFO epoch # 4072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018094387240125798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,839 INFO epoch # 4073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014819925447227433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:17,918 INFO epoch # 4074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01451335805177223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:18,004 INFO epoch # 4075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01427980003063567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:18,084 INFO epoch # 4076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014733004834852181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:18,164 INFO epoch # 4077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014940646913601086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:18,244 INFO epoch # 4078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011701393945259042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:18,365 INFO epoch # 4079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012528979204944335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:18,469 INFO epoch # 4080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013844844128470868
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:18,469 INFO *** epoch 4080, rolling-avg-loss (window=10)= 0.014703884304617531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:18,568 INFO epoch # 4081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016059207409853116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:18,666 INFO epoch # 4082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014498691190965474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:18,762 INFO epoch # 4083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014921507186954841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:18,854 INFO epoch # 4084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014188001820002683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:18,947 INFO epoch # 4085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013532811834011227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,038 INFO epoch # 4086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015050026049721055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,113 INFO epoch # 4087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011954416491789743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,191 INFO epoch # 4088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01683254595263861
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,269 INFO epoch # 4089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01368259682203643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,347 INFO epoch # 4090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013573251926572993
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:19,348 INFO *** epoch 4090, rolling-avg-loss (window=10)= 0.014429305668454618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,428 INFO epoch # 4091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014290995488408953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,507 INFO epoch # 4092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01162893524451647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,587 INFO epoch # 4093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012955103709828109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,670 INFO epoch # 4094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012381186315906234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,750 INFO epoch # 4095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013099372750730254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,831 INFO epoch # 4096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01592440811509732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,909 INFO epoch # 4097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01389246160397306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:19,986 INFO epoch # 4098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013350402543437667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,068 INFO epoch # 4099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015988219529390335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,148 INFO epoch # 4100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013350902823731303
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:20,148 INFO *** epoch 4100, rolling-avg-loss (window=10)= 0.01368619881250197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,228 INFO epoch # 4101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01213855391688412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,312 INFO epoch # 4102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01340010913554579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,396 INFO epoch # 4103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012875839427579194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,475 INFO epoch # 4104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013771459489362314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,551 INFO epoch # 4105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013428447520709597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,633 INFO epoch # 4106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01219207962276414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,715 INFO epoch # 4107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012535814443253912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,792 INFO epoch # 4108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014218027019524015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,869 INFO epoch # 4109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015555245030554943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:20,947 INFO epoch # 4110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01418154923885595
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:20,947 INFO *** epoch 4110, rolling-avg-loss (window=10)= 0.013429712484503397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,026 INFO epoch # 4111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013204214003053494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,104 INFO epoch # 4112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014004873111844063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,182 INFO epoch # 4113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014831222157226875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,261 INFO epoch # 4114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011770550852816086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,342 INFO epoch # 4115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011284538661129773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,424 INFO epoch # 4116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013458444591378793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,504 INFO epoch # 4117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013077108073048294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,585 INFO epoch # 4118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014047368022147566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,665 INFO epoch # 4119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012176968390122056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,745 INFO epoch # 4120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013115364432451315
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:21,745 INFO *** epoch 4120, rolling-avg-loss (window=10)= 0.013097065229521832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,824 INFO epoch # 4121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013820026884786785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,903 INFO epoch # 4122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016640322384773754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:21,981 INFO epoch # 4123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014081825982430018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,060 INFO epoch # 4124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017154240049421787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,140 INFO epoch # 4125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019615552373579703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,219 INFO epoch # 4126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013393018452916294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,299 INFO epoch # 4127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012263237673323601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,382 INFO epoch # 4128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013411109553999268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,463 INFO epoch # 4129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012401455227518454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,543 INFO epoch # 4130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018270083965035155
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:22,543 INFO *** epoch 4130, rolling-avg-loss (window=10)= 0.015105087254778483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,621 INFO epoch # 4131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015206539406790398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,698 INFO epoch # 4132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012864111486123875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,776 INFO epoch # 4133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01183080457849428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,860 INFO epoch # 4134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013280318045872264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:22,942 INFO epoch # 4135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014498241260298528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,019 INFO epoch # 4136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015545045418548398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,098 INFO epoch # 4137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016616789871477522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,174 INFO epoch # 4138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012215041555464268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,251 INFO epoch # 4139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012855160661274567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,331 INFO epoch # 4140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013947928026027512
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:23,331 INFO *** epoch 4140, rolling-avg-loss (window=10)= 0.01388599803103716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,411 INFO epoch # 4141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01152945616922807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,494 INFO epoch # 4142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014280960996984504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,575 INFO epoch # 4143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014148589325486682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,656 INFO epoch # 4144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012695552431978285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,736 INFO epoch # 4145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012789620086550713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,816 INFO epoch # 4146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011826969959656708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,900 INFO epoch # 4147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013932346322690137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:23,981 INFO epoch # 4148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012483402140787803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,062 INFO epoch # 4149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015729386810562573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,140 INFO epoch # 4150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01361942857329268
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:24,140 INFO *** epoch 4150, rolling-avg-loss (window=10)= 0.013303571281721816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,216 INFO epoch # 4151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01360931567614898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,294 INFO epoch # 4152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013743772462476045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,371 INFO epoch # 4153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013175220621633343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,451 INFO epoch # 4154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014740385886398144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,534 INFO epoch # 4155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0141451131348731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,612 INFO epoch # 4156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012856172499596141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,691 INFO epoch # 4157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013331741385627538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,771 INFO epoch # 4158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013402681477600709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,848 INFO epoch # 4159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013518675958039239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:24,933 INFO epoch # 4160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012693869095528498
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:24,933 INFO *** epoch 4160, rolling-avg-loss (window=10)= 0.013521694819792174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,012 INFO epoch # 4161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013818668143358082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,093 INFO epoch # 4162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014521309291012585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,171 INFO epoch # 4163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015201386413536966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,278 INFO epoch # 4164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014158457284793258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,362 INFO epoch # 4165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012297157249122392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,442 INFO epoch # 4166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012288276848266833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,520 INFO epoch # 4167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0135761650162749
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,601 INFO epoch # 4168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014336969870782923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,679 INFO epoch # 4169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012374298443319276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,757 INFO epoch # 4170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01220984710380435
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:25,758 INFO *** epoch 4170, rolling-avg-loss (window=10)= 0.013478253566427156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,834 INFO epoch # 4171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013257053797133267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,911 INFO epoch # 4172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015180214628344402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:25,989 INFO epoch # 4173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015522196656093001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,067 INFO epoch # 4174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013572057825513184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,144 INFO epoch # 4175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013137810601620004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,221 INFO epoch # 4176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014438956772210076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,297 INFO epoch # 4177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01616611599456519
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,374 INFO epoch # 4178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01177509494300466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,452 INFO epoch # 4179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01327313696674537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,532 INFO epoch # 4180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0147244319668971
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:26,533 INFO *** epoch 4180, rolling-avg-loss (window=10)= 0.014104707015212626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,613 INFO epoch # 4181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01462645994615741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,693 INFO epoch # 4182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013376017072005197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,770 INFO epoch # 4183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014099712410825305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,848 INFO epoch # 4184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01259927025239449
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:26,924 INFO epoch # 4185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014422193584323395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,001 INFO epoch # 4186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0134591303212801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,079 INFO epoch # 4187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015987097678589635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,157 INFO epoch # 4188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0179041081428295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,237 INFO epoch # 4189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013777197600575164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,316 INFO epoch # 4190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013422195203020237
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:27,316 INFO *** epoch 4190, rolling-avg-loss (window=10)= 0.014367338221200043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,393 INFO epoch # 4191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013477188505930826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,472 INFO epoch # 4192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011445293392171152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,555 INFO epoch # 4193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011720168382453267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,642 INFO epoch # 4194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015054953706567176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,723 INFO epoch # 4195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012675748046603985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,802 INFO epoch # 4196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013738276684307493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,908 INFO epoch # 4197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012564928430947475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:27,999 INFO epoch # 4198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01869811002688948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,084 INFO epoch # 4199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013781071203993633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,166 INFO epoch # 4200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01376989840355236
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:28,166 INFO *** epoch 4200, rolling-avg-loss (window=10)= 0.013692563678341685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,248 INFO epoch # 4201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015405452897539362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,328 INFO epoch # 4202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013522803710657172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,406 INFO epoch # 4203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013457566717988811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,485 INFO epoch # 4204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013764283445198089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,562 INFO epoch # 4205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013392519642366096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,640 INFO epoch # 4206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015111503365915269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,719 INFO epoch # 4207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013224950089352205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,796 INFO epoch # 4208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01688684563850984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,875 INFO epoch # 4209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014258125374908559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:28,952 INFO epoch # 4210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012874210486188531
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:28,952 INFO *** epoch 4210, rolling-avg-loss (window=10)= 0.014189826136862394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,030 INFO epoch # 4211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011425851072999649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,114 INFO epoch # 4212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013978397590108216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,191 INFO epoch # 4213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01337199067347683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,267 INFO epoch # 4214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013475117972120643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,345 INFO epoch # 4215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013580389466369525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,423 INFO epoch # 4216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014370215460075997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,499 INFO epoch # 4217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013257060199975967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,579 INFO epoch # 4218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01387459022225812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,656 INFO epoch # 4219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016421560576418415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,735 INFO epoch # 4220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016469274152768776
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:29,735 INFO *** epoch 4220, rolling-avg-loss (window=10)= 0.014022444738657214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,813 INFO epoch # 4221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012693869459326379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,890 INFO epoch # 4222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013695491652470082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:29,967 INFO epoch # 4223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012281021947273985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,048 INFO epoch # 4224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01470063635497354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,126 INFO epoch # 4225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014033070488949306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,207 INFO epoch # 4226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01088042338960804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,287 INFO epoch # 4227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011748282937332988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,364 INFO epoch # 4228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015237524479744025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,441 INFO epoch # 4229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012970468582352623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,519 INFO epoch # 4230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012890198719105683
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:30,520 INFO *** epoch 4230, rolling-avg-loss (window=10)= 0.013113098801113665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,603 INFO epoch # 4231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011506969858601224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,681 INFO epoch # 4232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012316906751948409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,758 INFO epoch # 4233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012599378111190163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,835 INFO epoch # 4234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011630525143118575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,912 INFO epoch # 4235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013703608070500195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:30,990 INFO epoch # 4236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015479058609344065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,070 INFO epoch # 4237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012281576491659507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,150 INFO epoch # 4238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013289346912642941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,230 INFO epoch # 4239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01189575096941553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,311 INFO epoch # 4240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012127148918807507
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:31,311 INFO *** epoch 4240, rolling-avg-loss (window=10)= 0.012683026983722812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,391 INFO epoch # 4241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01289607495709788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,470 INFO epoch # 4242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012199312128359452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,554 INFO epoch # 4243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015706179023254663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,636 INFO epoch # 4244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013769664976280183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,715 INFO epoch # 4245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01187326159561053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,792 INFO epoch # 4246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012121983876568265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,869 INFO epoch # 4247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012539757910417393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:31,954 INFO epoch # 4248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014055317762540653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,032 INFO epoch # 4249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013690609426703304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,109 INFO epoch # 4250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013531939053791575
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:32,109 INFO *** epoch 4250, rolling-avg-loss (window=10)= 0.01323841007106239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,187 INFO epoch # 4251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012644597329199314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,301 INFO epoch # 4252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01555116222880315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,384 INFO epoch # 4253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01393394528713543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,463 INFO epoch # 4254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01440899791487027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,543 INFO epoch # 4255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01432125549763441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,624 INFO epoch # 4256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01386005649692379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,704 INFO epoch # 4257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012602671748027205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,781 INFO epoch # 4258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013082570454571396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,859 INFO epoch # 4259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011302795901428908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:32,955 INFO epoch # 4260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014542896693455987
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:32,955 INFO *** epoch 4260, rolling-avg-loss (window=10)= 0.013625094955204985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,048 INFO epoch # 4261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012302317598368973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,131 INFO epoch # 4262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019260034256149083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,226 INFO epoch # 4263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014988945978984702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,312 INFO epoch # 4264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012753245464409702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,395 INFO epoch # 4265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01361011338303797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,477 INFO epoch # 4266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014478037468506955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,560 INFO epoch # 4267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014746218017535284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,648 INFO epoch # 4268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0161174363020109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,732 INFO epoch # 4269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020747782051330432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,813 INFO epoch # 4270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013939420328824781
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:33,813 INFO *** epoch 4270, rolling-avg-loss (window=10)= 0.015294355084915879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,901 INFO epoch # 4271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012391909040161408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:33,981 INFO epoch # 4272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013209566110162996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,060 INFO epoch # 4273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014880350005114451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,138 INFO epoch # 4274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016490809357492253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,214 INFO epoch # 4275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012047764481394552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,291 INFO epoch # 4276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01301671790133696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,368 INFO epoch # 4277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017041403058101423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,448 INFO epoch # 4278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014168445268296637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,528 INFO epoch # 4279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012399253770126961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,611 INFO epoch # 4280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013767145544989035
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:34,611 INFO *** epoch 4280, rolling-avg-loss (window=10)= 0.013941336453717668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,692 INFO epoch # 4281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012726148728688713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,771 INFO epoch # 4282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013343608123250306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,850 INFO epoch # 4283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012518438990809955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:34,930 INFO epoch # 4284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013540644431486726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,010 INFO epoch # 4285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012679067440330982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,088 INFO epoch # 4286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013432147621642798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,165 INFO epoch # 4287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012925992181408219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,244 INFO epoch # 4288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014572247135220096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,323 INFO epoch # 4289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016390984324971214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,403 INFO epoch # 4290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01430468395119533
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:35,404 INFO *** epoch 4290, rolling-avg-loss (window=10)= 0.013643396292900433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,483 INFO epoch # 4291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012316665714024566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,564 INFO epoch # 4292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012880483824119437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,661 INFO epoch # 4293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01241265024873428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,750 INFO epoch # 4294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011819773106253706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,831 INFO epoch # 4295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012249176201294176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,909 INFO epoch # 4296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013472600621753372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:35,987 INFO epoch # 4297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014342159978696145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:36,125 INFO epoch # 4298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015142154996283352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:36,248 INFO epoch # 4299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01276380132912891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:36,363 INFO epoch # 4300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014085803486523218
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:36,363 INFO *** epoch 4300, rolling-avg-loss (window=10)= 0.013148526950681116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:36,486 INFO epoch # 4301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012839312446885742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:36,593 INFO epoch # 4302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011278538477199618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:36,673 INFO epoch # 4303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012952035220223479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:36,753 INFO epoch # 4304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013535768011934124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:36,831 INFO epoch # 4305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012042759932228364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:36,908 INFO epoch # 4306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013125949073582888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:36,985 INFO epoch # 4307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015851659671170637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,064 INFO epoch # 4308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015846960435737856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,141 INFO epoch # 4309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013088984691421501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,218 INFO epoch # 4310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012101047977921553
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:37,218 INFO *** epoch 4310, rolling-avg-loss (window=10)= 0.013266301593830576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,296 INFO epoch # 4311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013400611060205847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,375 INFO epoch # 4312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014722854000865482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,452 INFO epoch # 4313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011779717657191213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,530 INFO epoch # 4314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014110542324488051
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,610 INFO epoch # 4315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012691164381976705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,689 INFO epoch # 4316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01330232531472575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,771 INFO epoch # 4317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014497860160190612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,851 INFO epoch # 4318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012894340165075846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:37,929 INFO epoch # 4319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012463470084185246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,011 INFO epoch # 4320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012107945280149579
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:38,011 INFO *** epoch 4320, rolling-avg-loss (window=10)= 0.013197083042905433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,091 INFO epoch # 4321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014367672585649416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,168 INFO epoch # 4322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011608252083533444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,245 INFO epoch # 4323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012119534498197027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,322 INFO epoch # 4324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013771498983260244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,400 INFO epoch # 4325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011133572756079957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,477 INFO epoch # 4326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011783638765336946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,554 INFO epoch # 4327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013364711921894923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,632 INFO epoch # 4328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012351526092970744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,710 INFO epoch # 4329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014225965351215564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,786 INFO epoch # 4330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012166052481916267
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:38,786 INFO *** epoch 4330, rolling-avg-loss (window=10)= 0.012689242552005453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,863 INFO epoch # 4331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01394679791701492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:38,939 INFO epoch # 4332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014781600693822838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,015 INFO epoch # 4333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01373255804355722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,093 INFO epoch # 4334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012274322943994775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,170 INFO epoch # 4335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013077464114758186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,256 INFO epoch # 4336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013048513617832214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,343 INFO epoch # 4337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01452970944228582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,430 INFO epoch # 4338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015384632511995733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,510 INFO epoch # 4339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01282711340172682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,622 INFO epoch # 4340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011659482668619603
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:39,622 INFO *** epoch 4340, rolling-avg-loss (window=10)= 0.013526219535560813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,717 INFO epoch # 4341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012678342085564509
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,800 INFO epoch # 4342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013267133341287263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,879 INFO epoch # 4343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01233856228645891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:39,964 INFO epoch # 4344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01239174444344826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,045 INFO epoch # 4345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012791462548193522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,130 INFO epoch # 4346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011960594172705896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,212 INFO epoch # 4347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011804431560449302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,296 INFO epoch # 4348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014441520179389045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,377 INFO epoch # 4349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012836389170843177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,459 INFO epoch # 4350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013138221329427324
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:40,459 INFO *** epoch 4350, rolling-avg-loss (window=10)= 0.012764840111776721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,538 INFO epoch # 4351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013020250102272257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,617 INFO epoch # 4352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016397036146372557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,696 INFO epoch # 4353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012029341924062464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,777 INFO epoch # 4354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013565379413194023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,861 INFO epoch # 4355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014475435455096886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:40,945 INFO epoch # 4356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0140461454866454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,022 INFO epoch # 4357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013024604428210296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,102 INFO epoch # 4358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012869689089711756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,182 INFO epoch # 4359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012170357760624029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,265 INFO epoch # 4360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014076565275900066
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:41,265 INFO *** epoch 4360, rolling-avg-loss (window=10)= 0.013567480508208973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,359 INFO epoch # 4361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015744018674013205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,446 INFO epoch # 4362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0127553325437475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,522 INFO epoch # 4363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01471240256796591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,601 INFO epoch # 4364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014891691898810677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,681 INFO epoch # 4365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014510800087009557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,759 INFO epoch # 4366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014130822572042234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,837 INFO epoch # 4367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012644307484151796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,915 INFO epoch # 4368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011516956030391157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:41,992 INFO epoch # 4369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012894655432319269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,072 INFO epoch # 4370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014217130810720846
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:42,072 INFO *** epoch 4370, rolling-avg-loss (window=10)= 0.013801811810117215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,152 INFO epoch # 4371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01296877843560651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,230 INFO epoch # 4372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012603647730429657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,306 INFO epoch # 4373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01287968745600665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,383 INFO epoch # 4374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012144920037826523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,459 INFO epoch # 4375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013982804011902772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,536 INFO epoch # 4376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014132671043626033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,616 INFO epoch # 4377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012612177553819492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,694 INFO epoch # 4378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013027363427681848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,771 INFO epoch # 4379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0116687489207834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,850 INFO epoch # 4380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011714693158864975
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:42,850 INFO *** epoch 4380, rolling-avg-loss (window=10)= 0.012773549177654785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:42,929 INFO epoch # 4381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014475982112344354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,037 INFO epoch # 4382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012553130713058636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,128 INFO epoch # 4383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015291980365873314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,214 INFO epoch # 4384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016088860429590568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,295 INFO epoch # 4385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01234541306621395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,374 INFO epoch # 4386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01213894055399578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,452 INFO epoch # 4387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011363863392034546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,529 INFO epoch # 4388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012745815547532402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,612 INFO epoch # 4389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013712292755371891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,695 INFO epoch # 4390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01201200595824048
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:43,695 INFO *** epoch 4390, rolling-avg-loss (window=10)= 0.013272828489425592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,773 INFO epoch # 4391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015812465906492434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,851 INFO epoch # 4392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014777587508433498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:43,929 INFO epoch # 4393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011287877721770201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,006 INFO epoch # 4394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011680367555527482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,085 INFO epoch # 4395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011931008208193816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,162 INFO epoch # 4396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012617518135812134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,241 INFO epoch # 4397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0127833526203176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,319 INFO epoch # 4398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01527707782224752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,397 INFO epoch # 4399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01766108415904455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,475 INFO epoch # 4400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011949671330512501
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:44,475 INFO *** epoch 4400, rolling-avg-loss (window=10)= 0.013577801096835174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,557 INFO epoch # 4401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01431400874571409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,647 INFO epoch # 4402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01263346457562875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,765 INFO epoch # 4403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011200491193449125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,850 INFO epoch # 4404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011425912205595523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:44,932 INFO epoch # 4405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013682478544069454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,018 INFO epoch # 4406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01265037689881865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,099 INFO epoch # 4407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013401043062913232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,177 INFO epoch # 4408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014504217033390887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,255 INFO epoch # 4409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014314684245618992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,334 INFO epoch # 4410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013700106428586878
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:45,334 INFO *** epoch 4410, rolling-avg-loss (window=10)= 0.013182678293378557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,412 INFO epoch # 4411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012165274150902405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,493 INFO epoch # 4412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014549381186952814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,577 INFO epoch # 4413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012899519730126485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,657 INFO epoch # 4414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015183611714746803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,737 INFO epoch # 4415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013015740099945106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,817 INFO epoch # 4416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013319996214704588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,898 INFO epoch # 4417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012791673885658383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:45,979 INFO epoch # 4418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013636077266710345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,056 INFO epoch # 4419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012146060995291919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,135 INFO epoch # 4420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012947345487191342
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:46,135 INFO *** epoch 4420, rolling-avg-loss (window=10)= 0.013265468073223019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,215 INFO epoch # 4421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012157339006080292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,293 INFO epoch # 4422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012493574642576277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,370 INFO epoch # 4423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01284220146771986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,448 INFO epoch # 4424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01300353957049083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,528 INFO epoch # 4425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010493392808712088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,614 INFO epoch # 4426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012463204329833388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,696 INFO epoch # 4427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013993418659083545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,776 INFO epoch # 4428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014288114980445243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,856 INFO epoch # 4429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013973102482850663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:46,933 INFO epoch # 4430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011155382875585929
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:46,933 INFO *** epoch 4430, rolling-avg-loss (window=10)= 0.012686327082337811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,010 INFO epoch # 4431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013373276000493206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,092 INFO epoch # 4432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01388402642623987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,177 INFO epoch # 4433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012858153146225959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,260 INFO epoch # 4434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012689773619058542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,340 INFO epoch # 4435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011557426798390225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,418 INFO epoch # 4436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01216388068132801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,521 INFO epoch # 4437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014428871043492109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,608 INFO epoch # 4438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01238067020312883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,691 INFO epoch # 4439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011556706558621954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,771 INFO epoch # 4440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01204168210097123
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:47,771 INFO *** epoch 4440, rolling-avg-loss (window=10)= 0.012693446657794993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,852 INFO epoch # 4441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012556581365060993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:47,932 INFO epoch # 4442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012753235292620957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,013 INFO epoch # 4443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011924955222639255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,094 INFO epoch # 4444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012484750419389457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,175 INFO epoch # 4445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014089288728428073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,254 INFO epoch # 4446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013429358979919925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,332 INFO epoch # 4447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011572983101359569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,415 INFO epoch # 4448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01315031795820687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,495 INFO epoch # 4449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013355721966945566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,574 INFO epoch # 4450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012681698557571508
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:48,574 INFO *** epoch 4450, rolling-avg-loss (window=10)= 0.012799889159214217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,654 INFO epoch # 4451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012523233541287482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,731 INFO epoch # 4452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012440004727977794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,810 INFO epoch # 4453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013322742612217553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,888 INFO epoch # 4454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011531659387401305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:48,965 INFO epoch # 4455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01213411832577549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,042 INFO epoch # 4456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01184403289516922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,125 INFO epoch # 4457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013486013587680645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,202 INFO epoch # 4458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011457092252385337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,279 INFO epoch # 4459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011483531299745664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,357 INFO epoch # 4460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012446626162272878
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:49,357 INFO *** epoch 4460, rolling-avg-loss (window=10)= 0.012266905479191337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,435 INFO epoch # 4461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012648094183532521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,516 INFO epoch # 4462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012125771638238803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,596 INFO epoch # 4463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013391246233368292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,676 INFO epoch # 4464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014473186194663867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,753 INFO epoch # 4465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012980659841559827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,830 INFO epoch # 4466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014731255723745562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,908 INFO epoch # 4467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014203211743733846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:49,988 INFO epoch # 4468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012812432134523988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:50,069 INFO epoch # 4469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011805941874627024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:50,152 INFO epoch # 4470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013531010074075311
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:50,152 INFO *** epoch 4470, rolling-avg-loss (window=10)= 0.013270280964206904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:50,241 INFO epoch # 4471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011600313184317201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:50,329 INFO epoch # 4472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01357682462548837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:50,417 INFO epoch # 4473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013066029045148753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:50,499 INFO epoch # 4474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015739430891699158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:50,673 INFO epoch # 4475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014999899314716458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:50,781 INFO epoch # 4476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01255610842781607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:50,886 INFO epoch # 4477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02243696089135483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:50,988 INFO epoch # 4478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018374797306023538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:51,091 INFO epoch # 4479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016074201819719747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:51,199 INFO epoch # 4480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014619043955462985
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:51,199 INFO *** epoch 4480, rolling-avg-loss (window=10)= 0.015304360946174711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:51,296 INFO epoch # 4481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016580886527663097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:51,390 INFO epoch # 4482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014130284951534122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:51,481 INFO epoch # 4483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018742578075034544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:51,574 INFO epoch # 4484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012286562996450812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:51,668 INFO epoch # 4485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012414146593073383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:51,763 INFO epoch # 4486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011877079727128148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:51,859 INFO epoch # 4487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012353686150163412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:51,953 INFO epoch # 4488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012345072100288235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,040 INFO epoch # 4489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012436146469553933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,119 INFO epoch # 4490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012550487124826759
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:52,119 INFO *** epoch 4490, rolling-avg-loss (window=10)= 0.013571693071571644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,197 INFO epoch # 4491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012747079657856375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,274 INFO epoch # 4492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012118147584260441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,352 INFO epoch # 4493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0138536738086259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,432 INFO epoch # 4494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012380329877487384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,513 INFO epoch # 4495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011972229272942059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,594 INFO epoch # 4496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01158075116109103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,670 INFO epoch # 4497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013294357064296491
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,747 INFO epoch # 4498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01365274221461732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,827 INFO epoch # 4499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013812919409247115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,905 INFO epoch # 4500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012466944011976011
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:52,906 INFO *** epoch 4500, rolling-avg-loss (window=10)= 0.012787917406240013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:52,983 INFO epoch # 4501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012189209679490887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,065 INFO epoch # 4502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012449787245714106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,144 INFO epoch # 4503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012604741583345458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,223 INFO epoch # 4504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012456653217668645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,299 INFO epoch # 4505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0156027468474349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,377 INFO epoch # 4506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013728463760344312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,454 INFO epoch # 4507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012402905398630537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,536 INFO epoch # 4508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013801667621009983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,618 INFO epoch # 4509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012323573915637098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,706 INFO epoch # 4510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013186840456910431
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:53,707 INFO *** epoch 4510, rolling-avg-loss (window=10)= 0.013074658972618636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,791 INFO epoch # 4511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012115853700379375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,878 INFO epoch # 4512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011585417028982192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:53,958 INFO epoch # 4513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012322118855081499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,041 INFO epoch # 4514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010988262030878104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,124 INFO epoch # 4515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011944113997742534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,207 INFO epoch # 4516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01569129824929405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,289 INFO epoch # 4517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015983180041075684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,372 INFO epoch # 4518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012456829863367602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,455 INFO epoch # 4519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013533701174310409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,541 INFO epoch # 4520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012332656726357527
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:54,541 INFO *** epoch 4520, rolling-avg-loss (window=10)= 0.012895343166746898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,626 INFO epoch # 4521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012077025137841702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,707 INFO epoch # 4522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01245482949889265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,787 INFO epoch # 4523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011825055866211187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,872 INFO epoch # 4524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013207081603468396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:54,955 INFO epoch # 4525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013056690222583711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,036 INFO epoch # 4526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011521150634507649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,119 INFO epoch # 4527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013660413096658885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,201 INFO epoch # 4528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012841535979532637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,287 INFO epoch # 4529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01395027371472679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,377 INFO epoch # 4530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01337061329104472
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:55,377 INFO *** epoch 4530, rolling-avg-loss (window=10)= 0.012796466904546833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,470 INFO epoch # 4531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014765063373488374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,573 INFO epoch # 4532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013444750948110595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,665 INFO epoch # 4533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012747759399644565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,750 INFO epoch # 4534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016446532303234562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,834 INFO epoch # 4535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010926972245215438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,915 INFO epoch # 4536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014230849003070034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:55,998 INFO epoch # 4537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013661961085745133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:56,088 INFO epoch # 4538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012657759427384008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:56,172 INFO epoch # 4539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01249699718755437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:56,256 INFO epoch # 4540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01120439826627262
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:56,256 INFO *** epoch 4540, rolling-avg-loss (window=10)= 0.01325830432397197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:56,358 INFO epoch # 4541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011964694043854252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:56,452 INFO epoch # 4542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01188569114310667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:56,539 INFO epoch # 4543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013188151206122711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:56,630 INFO epoch # 4544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011724083116860129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:56,715 INFO epoch # 4545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011273863747192081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:56,799 INFO epoch # 4546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011990572704235092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:56,881 INFO epoch # 4547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012908871794934385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:56,963 INFO epoch # 4548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013196368818171322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,046 INFO epoch # 4549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012597624081536196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,130 INFO epoch # 4550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014393507211934775
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:57,130 INFO *** epoch 4550, rolling-avg-loss (window=10)= 0.01251234278679476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,216 INFO epoch # 4551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014642959184129722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,295 INFO epoch # 4552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014284670731285587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,380 INFO epoch # 4553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012253456909093075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,461 INFO epoch # 4554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011295189819065854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,545 INFO epoch # 4555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013568701047915965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,631 INFO epoch # 4556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012941643508384004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,713 INFO epoch # 4557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011749813391361386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,793 INFO epoch # 4558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013502624206012115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,881 INFO epoch # 4559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014382472712895833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:57,962 INFO epoch # 4560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014020163434906863
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:57,963 INFO *** epoch 4560, rolling-avg-loss (window=10)= 0.01326416949450504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:58,080 INFO epoch # 4561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013657410207088105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:58,209 INFO epoch # 4562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012096289923647419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:58,339 INFO epoch # 4563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015729354578070343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:58,468 INFO epoch # 4564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016092735444544815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:58,596 INFO epoch # 4565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013344135048100725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:58,725 INFO epoch # 4566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010778689946164377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:58,855 INFO epoch # 4567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01066344948776532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:58,984 INFO epoch # 4568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01158641168876784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:59,113 INFO epoch # 4569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012587193006766029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:59,242 INFO epoch # 4570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013648089152411558
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:19:59,242 INFO *** epoch 4570, rolling-avg-loss (window=10)= 0.013018375848332653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:59,370 INFO epoch # 4571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013629865614348091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:59,500 INFO epoch # 4572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012344784263405018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:59,628 INFO epoch # 4573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012710292925476097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:59,755 INFO epoch # 4574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011945195015869103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:19:59,884 INFO epoch # 4575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01603225851431489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:00,016 INFO epoch # 4576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015586355395498686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:00,146 INFO epoch # 4577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014163385232677683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:00,275 INFO epoch # 4578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012050824319885578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:00,404 INFO epoch # 4579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011612998874625191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:00,532 INFO epoch # 4580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012260610863449983
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:00,533 INFO *** epoch 4580, rolling-avg-loss (window=10)= 0.013233657101955032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:00,661 INFO epoch # 4581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013882561950595118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:00,791 INFO epoch # 4582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01210131513653323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:00,919 INFO epoch # 4583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01206532699870877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:01,047 INFO epoch # 4584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013623374063172378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:01,176 INFO epoch # 4585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014407968046725728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:01,305 INFO epoch # 4586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015478482615435496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:01,433 INFO epoch # 4587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011944584577577189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:01,559 INFO epoch # 4588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013749582838499919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:01,688 INFO epoch # 4589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012675966165261343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:01,814 INFO epoch # 4590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01449836403480731
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:01,815 INFO *** epoch 4590, rolling-avg-loss (window=10)= 0.013442752642731648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:01,944 INFO epoch # 4591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012346626434009522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:02,071 INFO epoch # 4592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011834647550131194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:02,200 INFO epoch # 4593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015662304489524104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:02,328 INFO epoch # 4594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015133462846279144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:02,455 INFO epoch # 4595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014907141725416295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:02,583 INFO epoch # 4596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01107777890865691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:02,711 INFO epoch # 4597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01250527701631654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:02,838 INFO epoch # 4598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012576069857459515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:02,966 INFO epoch # 4599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011168721044668928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:03,093 INFO epoch # 4600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013322965256520547
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:03,093 INFO *** epoch 4600, rolling-avg-loss (window=10)= 0.01305349951289827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:03,222 INFO epoch # 4601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012471234222175553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:03,349 INFO epoch # 4602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013488006545230746
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:03,477 INFO epoch # 4603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01210352596535813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:03,604 INFO epoch # 4604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013367076971917413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:03,732 INFO epoch # 4605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011556280092918314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:03,859 INFO epoch # 4606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01223234496137593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:03,988 INFO epoch # 4607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014235089445719495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:04,117 INFO epoch # 4608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01751110608165618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:04,246 INFO epoch # 4609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015266493093804456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:04,374 INFO epoch # 4610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017840154701843858
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:04,374 INFO *** epoch 4610, rolling-avg-loss (window=10)= 0.014007131208200008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:04,501 INFO epoch # 4611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011442799208452925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:04,628 INFO epoch # 4612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013116414367686957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:04,755 INFO epoch # 4613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012017190128972288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:04,882 INFO epoch # 4614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013896025557187386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:05,009 INFO epoch # 4615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011918228265130892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:05,137 INFO epoch # 4616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01142755507316906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:05,266 INFO epoch # 4617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011283723899396136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:05,392 INFO epoch # 4618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012207807703816798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:05,519 INFO epoch # 4619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011414846405386925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:05,648 INFO epoch # 4620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013782401380012743
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:05,648 INFO *** epoch 4620, rolling-avg-loss (window=10)= 0.012250699198921211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:05,776 INFO epoch # 4621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012545918172691017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:05,907 INFO epoch # 4622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012558258124045096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:06,035 INFO epoch # 4623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012163729377789423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:06,168 INFO epoch # 4624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012898084954940714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:06,298 INFO epoch # 4625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013536801692680456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:06,426 INFO epoch # 4626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012106479014619254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:06,554 INFO epoch # 4627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011305666921543889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:06,692 INFO epoch # 4628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013311285700183362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:06,822 INFO epoch # 4629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014405459340196103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:06,951 INFO epoch # 4630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011275059965555556
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:06,951 INFO *** epoch 4630, rolling-avg-loss (window=10)= 0.012610674326424487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:07,079 INFO epoch # 4631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014770194145967253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:07,208 INFO epoch # 4632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012005319935269654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:07,334 INFO epoch # 4633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019419333344558254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:07,462 INFO epoch # 4634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013927409236202948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:07,589 INFO epoch # 4635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012143769825343043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:07,718 INFO epoch # 4636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01577609940432012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:07,845 INFO epoch # 4637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014680360101920087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:07,972 INFO epoch # 4638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010449485707795247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:08,100 INFO epoch # 4639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011055207032768521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:08,229 INFO epoch # 4640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015240211694617756
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:08,230 INFO *** epoch 4640, rolling-avg-loss (window=10)= 0.013946739042876289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:08,356 INFO epoch # 4641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011851508417748846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:08,485 INFO epoch # 4642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011601922138652299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:08,621 INFO epoch # 4643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011578568824916147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:08,754 INFO epoch # 4644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011734769483155105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:08,882 INFO epoch # 4645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0117937414906919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:09,011 INFO epoch # 4646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011309767942293547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:09,141 INFO epoch # 4647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012044817121932283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:09,270 INFO epoch # 4648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012284408381674439
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:09,397 INFO epoch # 4649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012953110126545653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:09,525 INFO epoch # 4650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01329625780635979
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:09,525 INFO *** epoch 4650, rolling-avg-loss (window=10)= 0.012044887173397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:09,653 INFO epoch # 4651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012706092922599055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:09,782 INFO epoch # 4652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013022080151131377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:09,909 INFO epoch # 4653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012972074357094243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:10,039 INFO epoch # 4654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011127926627523266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:10,169 INFO epoch # 4655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011395935973268934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:10,297 INFO epoch # 4656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012099555970053189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:10,425 INFO epoch # 4657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012417973019182682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:10,552 INFO epoch # 4658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012274508830159903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:10,681 INFO epoch # 4659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013006782857701182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:10,810 INFO epoch # 4660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013299235157319345
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:10,810 INFO *** epoch 4660, rolling-avg-loss (window=10)= 0.012432216586603317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:10,937 INFO epoch # 4661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01202987862052396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:11,064 INFO epoch # 4662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01324779241986107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:11,193 INFO epoch # 4663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012631574092665687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:11,319 INFO epoch # 4664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01236088210134767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:11,446 INFO epoch # 4665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01121434711967595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:11,574 INFO epoch # 4666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013064335769740865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:11,703 INFO epoch # 4667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013086553575703874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:11,832 INFO epoch # 4668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01190233658417128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:11,959 INFO epoch # 4669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011540674211573787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:12,088 INFO epoch # 4670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011474043523776345
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:12,088 INFO *** epoch 4670, rolling-avg-loss (window=10)= 0.012255241801904048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:12,215 INFO epoch # 4671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012422708881786093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:12,343 INFO epoch # 4672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015674420064897276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:12,470 INFO epoch # 4673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013099714240524918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:12,598 INFO epoch # 4674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012949189484061208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:12,727 INFO epoch # 4675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015681151984608732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:12,855 INFO epoch # 4676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013215843791840598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:12,983 INFO epoch # 4677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01123582519358024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:13,110 INFO epoch # 4678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013202118541812524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:13,239 INFO epoch # 4679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01480573893059045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:13,366 INFO epoch # 4680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011509205301990733
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:13,366 INFO *** epoch 4680, rolling-avg-loss (window=10)= 0.013379591641569277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:13,494 INFO epoch # 4681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014833414927124977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:13,620 INFO epoch # 4682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013089724045130424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:13,749 INFO epoch # 4683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011290229289443232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:13,877 INFO epoch # 4684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012015316329780035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:14,005 INFO epoch # 4685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013739036192419007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:14,135 INFO epoch # 4686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012816371323424391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:14,264 INFO epoch # 4687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012183536156953778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:14,392 INFO epoch # 4688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012770542060025036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:14,520 INFO epoch # 4689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014272930740844458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:14,649 INFO epoch # 4690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011343517719069496
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:14,649 INFO *** epoch 4690, rolling-avg-loss (window=10)= 0.012835461878421484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:14,778 INFO epoch # 4691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01336473504488822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:14,905 INFO epoch # 4692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013220772045315243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:15,032 INFO epoch # 4693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012219165786518715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:15,160 INFO epoch # 4694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012713049276499078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:15,287 INFO epoch # 4695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011879271507496014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:15,415 INFO epoch # 4696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012300357499043457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:15,543 INFO epoch # 4697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012403749984514434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:15,672 INFO epoch # 4698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012762473626935389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:15,801 INFO epoch # 4699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01447047850524541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:15,929 INFO epoch # 4700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011745788215193897
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:15,930 INFO *** epoch 4700, rolling-avg-loss (window=10)= 0.012707984149164986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:16,057 INFO epoch # 4701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01254667890316341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:16,187 INFO epoch # 4702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012474868228309788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:16,316 INFO epoch # 4703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013538605024223216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:16,446 INFO epoch # 4704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011368949271854945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:16,574 INFO epoch # 4705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011847079716972075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:16,692 INFO epoch # 4706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012525027996161953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:16,778 INFO epoch # 4707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01723237919213716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:16,861 INFO epoch # 4708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01638221823668573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:16,943 INFO epoch # 4709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014822153592831455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,025 INFO epoch # 4710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012361510933260433
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:17,025 INFO *** epoch 4710, rolling-avg-loss (window=10)= 0.013509947109560017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,111 INFO epoch # 4711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012979342645849101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,193 INFO epoch # 4712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011425779193814378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,274 INFO epoch # 4713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012897005042759702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,353 INFO epoch # 4714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012472415284719318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,433 INFO epoch # 4715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012146988345193677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,514 INFO epoch # 4716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014054744693567045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,598 INFO epoch # 4717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01364179617667105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,681 INFO epoch # 4718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014693516059196554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,764 INFO epoch # 4719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013274697805172764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,846 INFO epoch # 4720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012504311336670071
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:17,846 INFO *** epoch 4720, rolling-avg-loss (window=10)= 0.013009059658361365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:17,928 INFO epoch # 4721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01242670133797219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,011 INFO epoch # 4722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012567728059366345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,094 INFO epoch # 4723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011626512059592642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,178 INFO epoch # 4724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011854634998599067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,260 INFO epoch # 4725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013820492604281753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,342 INFO epoch # 4726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012731044698739424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,422 INFO epoch # 4727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014321050679427572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,506 INFO epoch # 4728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010675012061255984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,591 INFO epoch # 4729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01110688467451837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,674 INFO epoch # 4730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013023937557591125
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:18,674 INFO *** epoch 4730, rolling-avg-loss (window=10)= 0.012415399873134447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,756 INFO epoch # 4731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011885360872838646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,837 INFO epoch # 4732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011693699649185874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:18,919 INFO epoch # 4733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011869990063132718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,000 INFO epoch # 4734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012829638784751296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,087 INFO epoch # 4735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012717058401904069
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,172 INFO epoch # 4736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014187067004968412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,253 INFO epoch # 4737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013499941152986139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,336 INFO epoch # 4738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011309820445603691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,424 INFO epoch # 4739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011634063528617844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,503 INFO epoch # 4740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01162954984465614
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:19,503 INFO *** epoch 4740, rolling-avg-loss (window=10)= 0.012325618974864483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,586 INFO epoch # 4741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013365440507186577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,670 INFO epoch # 4742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011903367747436278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,750 INFO epoch # 4743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01315118455386255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,829 INFO epoch # 4744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011541298619704321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,910 INFO epoch # 4745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011962059979850892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:19,992 INFO epoch # 4746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011054794085794128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,083 INFO epoch # 4747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011321040205075406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,167 INFO epoch # 4748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012415089920978062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,248 INFO epoch # 4749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013520954991690814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,331 INFO epoch # 4750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013407514386926778
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:20,331 INFO *** epoch 4750, rolling-avg-loss (window=10)= 0.01236427449985058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,413 INFO epoch # 4751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012398492719512433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,492 INFO epoch # 4752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010931810196780134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,574 INFO epoch # 4753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011140112139401026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,657 INFO epoch # 4754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012007696015643887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,736 INFO epoch # 4755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012667131697526202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,816 INFO epoch # 4756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013427495796349831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,898 INFO epoch # 4757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011345547798555344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:20,978 INFO epoch # 4758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012072882382199168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,061 INFO epoch # 4759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011456860243924893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,145 INFO epoch # 4760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011251097777858377
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:21,145 INFO *** epoch 4760, rolling-avg-loss (window=10)= 0.011869912676775129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,226 INFO epoch # 4761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01378443012072239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,305 INFO epoch # 4762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012499763077357784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,387 INFO epoch # 4763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014608677636715584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,467 INFO epoch # 4764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014549527273629792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,551 INFO epoch # 4765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012794676382327452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,639 INFO epoch # 4766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012880886846687645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,721 INFO epoch # 4767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010299076544470154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,801 INFO epoch # 4768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011265769571764395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,883 INFO epoch # 4769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011404396042053122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:21,963 INFO epoch # 4770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015025871951365843
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:21,963 INFO *** epoch 4770, rolling-avg-loss (window=10)= 0.012911307544709417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,044 INFO epoch # 4771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011826223526441026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,129 INFO epoch # 4772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012364137204713188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,221 INFO epoch # 4773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01254323786997702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,304 INFO epoch # 4774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011574356052733492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,384 INFO epoch # 4775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011964199657086283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,466 INFO epoch # 4776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011524768589879386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,551 INFO epoch # 4777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015587059635436162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,640 INFO epoch # 4778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012407366928528063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,721 INFO epoch # 4779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011604678758885711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,803 INFO epoch # 4780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015015558601589873
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:22,803 INFO *** epoch 4780, rolling-avg-loss (window=10)= 0.01264115868252702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,890 INFO epoch # 4781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010927830502623692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:22,972 INFO epoch # 4782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012405316840158775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,053 INFO epoch # 4783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011563934145669919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,137 INFO epoch # 4784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012465593841625378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,218 INFO epoch # 4785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011040264420444146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,299 INFO epoch # 4786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01307248289231211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,382 INFO epoch # 4787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014560976080247201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,462 INFO epoch # 4788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013548152914154343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,545 INFO epoch # 4789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011217911291169003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,633 INFO epoch # 4790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011370241627446376
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:23,634 INFO *** epoch 4790, rolling-avg-loss (window=10)= 0.012217270455585095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,714 INFO epoch # 4791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013899749203119427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,794 INFO epoch # 4792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010753144706541207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,875 INFO epoch # 4793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011385475678252988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:23,956 INFO epoch # 4794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01600475428858772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,037 INFO epoch # 4795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014984412569901906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,119 INFO epoch # 4796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013187549950089306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,202 INFO epoch # 4797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013183921575546265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,284 INFO epoch # 4798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01113716910185758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,375 INFO epoch # 4799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011741557013010606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,456 INFO epoch # 4800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011923971440410241
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:24,456 INFO *** epoch 4800, rolling-avg-loss (window=10)= 0.012820170552731724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,538 INFO epoch # 4801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010972696560202166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,620 INFO epoch # 4802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012106935435440391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,701 INFO epoch # 4803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012712915253359824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,781 INFO epoch # 4804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014959303851355799
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,865 INFO epoch # 4805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012926681534736417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:24,949 INFO epoch # 4806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011254106182605028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,038 INFO epoch # 4807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012328877026448026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,123 INFO epoch # 4808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011975954621448182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,203 INFO epoch # 4809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012214731134008616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,283 INFO epoch # 4810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01296754612121731
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:25,283 INFO *** epoch 4810, rolling-avg-loss (window=10)= 0.012441974772082176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,366 INFO epoch # 4811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013623287304653786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,447 INFO epoch # 4812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01126793966977857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,527 INFO epoch # 4813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010744504717877135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,613 INFO epoch # 4814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010531529020227026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,705 INFO epoch # 4815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011819654639111832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,785 INFO epoch # 4816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012511620501754805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,865 INFO epoch # 4817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011275571858277544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:25,946 INFO epoch # 4818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01159428663959261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,028 INFO epoch # 4819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012190619177999906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,111 INFO epoch # 4820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01063620435888879
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:26,111 INFO *** epoch 4820, rolling-avg-loss (window=10)= 0.0116195217888162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,192 INFO epoch # 4821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011526681279065087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,272 INFO epoch # 4822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012989390837901738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,354 INFO epoch # 4823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01288788253441453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,443 INFO epoch # 4824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012663453977438621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,524 INFO epoch # 4825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014259800140280277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,605 INFO epoch # 4826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01453683860017918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,686 INFO epoch # 4827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011540785984834656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,765 INFO epoch # 4828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01115417190885637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,847 INFO epoch # 4829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0133186471866793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:26,930 INFO epoch # 4830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010884243951295502
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:26,931 INFO *** epoch 4830, rolling-avg-loss (window=10)= 0.012576189640094526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,020 INFO epoch # 4831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010624260641634464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,104 INFO epoch # 4832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013859285303624347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,186 INFO epoch # 4833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014423844826524146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,265 INFO epoch # 4834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012656456136028282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,347 INFO epoch # 4835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011946248581807595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,427 INFO epoch # 4836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011920527264010161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,508 INFO epoch # 4837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013435268265311606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,593 INFO epoch # 4838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012945090667926706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,674 INFO epoch # 4839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011021789396181703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,755 INFO epoch # 4840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011004738888004795
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:27,755 INFO *** epoch 4840, rolling-avg-loss (window=10)= 0.01238375099710538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,838 INFO epoch # 4841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011004706248058937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,919 INFO epoch # 4842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012029457386233844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:27,999 INFO epoch # 4843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01315501298813615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:28,084 INFO epoch # 4844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011419776259572245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:28,166 INFO epoch # 4845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012943038789671846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:28,257 INFO epoch # 4846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011057029190851608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:28,340 INFO epoch # 4847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012385394453303888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:28,420 INFO epoch # 4848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013064281432889402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:28,501 INFO epoch # 4849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012925610062666237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:28,583 INFO epoch # 4850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014128369133686647
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:28,583 INFO *** epoch 4850, rolling-avg-loss (window=10)= 0.01241126759450708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:28,667 INFO epoch # 4851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014785589242819697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:28,749 INFO epoch # 4852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014405220324988477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:28,831 INFO epoch # 4853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013503207388566807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:28,910 INFO epoch # 4854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011439410882303491
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,001 INFO epoch # 4855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012706160297966562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,083 INFO epoch # 4856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011557428515516222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,164 INFO epoch # 4857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010925170106929727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,244 INFO epoch # 4858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010401005856692791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,324 INFO epoch # 4859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013175503248930909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,408 INFO epoch # 4860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01288291763921734
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:29,409 INFO *** epoch 4860, rolling-avg-loss (window=10)= 0.012578161350393201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,493 INFO epoch # 4861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011700786344590597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,584 INFO epoch # 4862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010704978441935964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,667 INFO epoch # 4863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010494748879864346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,749 INFO epoch # 4864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013283337000757456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,833 INFO epoch # 4865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010831119368958753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,914 INFO epoch # 4866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010131959126738366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:29,994 INFO epoch # 4867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010810010717250407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:30,082 INFO epoch # 4868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011467194824945182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:30,171 INFO epoch # 4869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013122260446834844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:30,255 INFO epoch # 4870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012151852366514504
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:30,256 INFO *** epoch 4870, rolling-avg-loss (window=10)= 0.011469824751839042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:30,343 INFO epoch # 4871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012114648307033349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:30,426 INFO epoch # 4872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012295269472815562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:30,512 INFO epoch # 4873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011286640554317273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:30,596 INFO epoch # 4874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012381649532471783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:30,679 INFO epoch # 4875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012622783164260909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:30,759 INFO epoch # 4876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011974079447099939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:30,844 INFO epoch # 4877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01252993538219016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:30,928 INFO epoch # 4878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01125739152485039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,014 INFO epoch # 4879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011237713333684951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,118 INFO epoch # 4880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010718116158386692
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:31,118 INFO *** epoch 4880, rolling-avg-loss (window=10)= 0.011841822687711102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,200 INFO epoch # 4881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010885488241910934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,286 INFO epoch # 4882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011454460429376923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,374 INFO epoch # 4883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011567554538487457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,456 INFO epoch # 4884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012401825442793779
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,536 INFO epoch # 4885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011018476259778254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,622 INFO epoch # 4886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012562066593091004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,705 INFO epoch # 4887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011502964080136735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,790 INFO epoch # 4888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012605498544871807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,870 INFO epoch # 4889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011399977243854664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:31,951 INFO epoch # 4890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011502551817102358
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:31,952 INFO *** epoch 4890, rolling-avg-loss (window=10)= 0.011690086319140392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,037 INFO epoch # 4891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011014039497240447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,126 INFO epoch # 4892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013151134146028198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,209 INFO epoch # 4893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011638858864898793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,296 INFO epoch # 4894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01114145376777742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,380 INFO epoch # 4895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012964714638656005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,467 INFO epoch # 4896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011889507004525512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,551 INFO epoch # 4897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011853043877636082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,644 INFO epoch # 4898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012356179060589056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,732 INFO epoch # 4899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013230840238975361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,817 INFO epoch # 4900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014371931029018015
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:32,818 INFO *** epoch 4900, rolling-avg-loss (window=10)= 0.01236117021253449
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,904 INFO epoch # 4901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011277221419732086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:32,986 INFO epoch # 4902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012765113904606551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,070 INFO epoch # 4903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010594030594802462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,154 INFO epoch # 4904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011728641533409245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,244 INFO epoch # 4905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012999895814573392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,338 INFO epoch # 4906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011013394308974966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,420 INFO epoch # 4907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012580613532918505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,504 INFO epoch # 4908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011553136893780902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,585 INFO epoch # 4909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012161618360551074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,670 INFO epoch # 4910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013202002141042612
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:33,671 INFO *** epoch 4910, rolling-avg-loss (window=10)= 0.011987566850439179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,751 INFO epoch # 4911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012754783027048688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,834 INFO epoch # 4912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011283489868219476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,914 INFO epoch # 4913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014567385558621027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:33,996 INFO epoch # 4914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012899313194793649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,083 INFO epoch # 4915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012111326679587364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,166 INFO epoch # 4916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010244768920529168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,246 INFO epoch # 4917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01336534677830059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,331 INFO epoch # 4918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011379008239600807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,411 INFO epoch # 4919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01154161383601604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,493 INFO epoch # 4920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013225310496636666
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:34,493 INFO *** epoch 4920, rolling-avg-loss (window=10)= 0.012337234659935348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,576 INFO epoch # 4921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012221229582792148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,660 INFO epoch # 4922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012804333411622792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,740 INFO epoch # 4923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013744020587182604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,822 INFO epoch # 4924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01130761273088865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,903 INFO epoch # 4925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014215194431017153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:34,985 INFO epoch # 4926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012503578051109798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,066 INFO epoch # 4927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01260294925305061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,151 INFO epoch # 4928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017229103803401813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,231 INFO epoch # 4929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01172471366589889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,314 INFO epoch # 4930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011750854770070873
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:35,314 INFO *** epoch 4930, rolling-avg-loss (window=10)= 0.013010359028703533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,395 INFO epoch # 4931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011543648826773278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,476 INFO epoch # 4932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010734139461419545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,559 INFO epoch # 4933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01286472869105637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,642 INFO epoch # 4934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011152131264680065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,723 INFO epoch # 4935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013717770605580881
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,803 INFO epoch # 4936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011668841005302966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,885 INFO epoch # 4937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013187029704567976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:35,965 INFO epoch # 4938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013391854212386534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,045 INFO epoch # 4939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013083643018035218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,128 INFO epoch # 4940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013675260386662558
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:36,128 INFO *** epoch 4940, rolling-avg-loss (window=10)= 0.012501904717646539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,209 INFO epoch # 4941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0115771174459951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,288 INFO epoch # 4942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010913172824075446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,371 INFO epoch # 4943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011859720216307323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,451 INFO epoch # 4944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011379021321772598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,535 INFO epoch # 4945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011412876352551393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,619 INFO epoch # 4946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011728976212907583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,699 INFO epoch # 4947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013448265424813144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,779 INFO epoch # 4948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015233367783366702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,862 INFO epoch # 4949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013146696423063986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:36,942 INFO epoch # 4950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010730840542237274
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:36,942 INFO *** epoch 4950, rolling-avg-loss (window=10)= 0.012143005454709055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,023 INFO epoch # 4951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01140338672848884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,107 INFO epoch # 4952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011698518952471204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,188 INFO epoch # 4953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014613929903134704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,268 INFO epoch # 4954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012312773120356724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,350 INFO epoch # 4955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010705279200919904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,429 INFO epoch # 4956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013031734124524519
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,512 INFO epoch # 4957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01127502012241166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,596 INFO epoch # 4958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01264192015514709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,677 INFO epoch # 4959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01145698233449366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,757 INFO epoch # 4960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014017824098118581
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:37,757 INFO *** epoch 4960, rolling-avg-loss (window=10)= 0.012315736874006689
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,840 INFO epoch # 4961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012441890605259687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:37,920 INFO epoch # 4962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012769695807946846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,001 INFO epoch # 4963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010379826708231121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,086 INFO epoch # 4964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012413030293828342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,168 INFO epoch # 4965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013349517656024545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,247 INFO epoch # 4966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011591006215894595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,328 INFO epoch # 4967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013013775809668005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,408 INFO epoch # 4968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012266869554878213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,489 INFO epoch # 4969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010080570966238156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,571 INFO epoch # 4970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010252180058159865
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:38,571 INFO *** epoch 4970, rolling-avg-loss (window=10)= 0.011855836367612938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,656 INFO epoch # 4971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011054045404307544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,736 INFO epoch # 4972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011632000605459325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,819 INFO epoch # 4973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010709689027862623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,901 INFO epoch # 4974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012581406059325673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:38,985 INFO epoch # 4975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010776137438369915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,070 INFO epoch # 4976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015128306273254566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,154 INFO epoch # 4977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013347239000722766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,236 INFO epoch # 4978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014157539771986194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,318 INFO epoch # 4979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013438586618576664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,402 INFO epoch # 4980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01244295114884153
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:39,402 INFO *** epoch 4980, rolling-avg-loss (window=10)= 0.01252679013487068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,483 INFO epoch # 4981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013403199292952195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,565 INFO epoch # 4982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014903614821378142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,651 INFO epoch # 4983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01151654263958335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,734 INFO epoch # 4984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013265299363411032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,818 INFO epoch # 4985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01276534040516708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,901 INFO epoch # 4986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012249407518538646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:39,984 INFO epoch # 4987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01138924247061368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,069 INFO epoch # 4988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012860633243690245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,150 INFO epoch # 4989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011831143412564415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,230 INFO epoch # 4990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011402853473555297
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:40,230 INFO *** epoch 4990, rolling-avg-loss (window=10)= 0.012558727664145408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,312 INFO epoch # 4991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013371478242333978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,391 INFO epoch # 4992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011752826954761986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,474 INFO epoch # 4993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01143047062214464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,557 INFO epoch # 4994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012312395570916124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,643 INFO epoch # 4995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012275849774596281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,722 INFO epoch # 4996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012072734723915346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,804 INFO epoch # 4997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011020742982509546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,885 INFO epoch # 4998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013880300830351189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:40,983 INFO epoch # 4999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015154725144384429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:41,074 INFO epoch # 5000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013799479813314974
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:41,075 INFO *** epoch 5000, rolling-avg-loss (window=10)= 0.01270710046592285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:41,166 INFO epoch # 5001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011406181292841211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:41,251 INFO epoch # 5002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010238565897452645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:41,331 INFO epoch # 5003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010397110221674666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:41,436 INFO epoch # 5004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010748600114311557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:41,520 INFO epoch # 5005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01152255549095571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:41,609 INFO epoch # 5006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01027397894358728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:41,693 INFO epoch # 5007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011316562187857926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:41,779 INFO epoch # 5008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012509765379945748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:41,861 INFO epoch # 5009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012875225802417845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:41,942 INFO epoch # 5010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010912130499491468
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:41,942 INFO *** epoch 5010, rolling-avg-loss (window=10)= 0.011220067583053606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,028 INFO epoch # 5011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012432024988811463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,112 INFO epoch # 5012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010609191813273355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,193 INFO epoch # 5013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012277709160116501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,272 INFO epoch # 5014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012316455045947805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,357 INFO epoch # 5015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011476016152300872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,440 INFO epoch # 5016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011385683916159905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,519 INFO epoch # 5017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01193983519624453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,604 INFO epoch # 5018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011452079328591935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,711 INFO epoch # 5019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01575687772128731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,798 INFO epoch # 5020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011191333993338048
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:42,798 INFO *** epoch 5020, rolling-avg-loss (window=10)= 0.012083720731607173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,881 INFO epoch # 5021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010778147945529781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:42,961 INFO epoch # 5022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013586200278950855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,043 INFO epoch # 5023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01338095500250347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,120 INFO epoch # 5024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011707114230375737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,200 INFO epoch # 5025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011716699882526882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,277 INFO epoch # 5026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011125870885734912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,355 INFO epoch # 5027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012359968211967498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,431 INFO epoch # 5028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012148961803177372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,511 INFO epoch # 5029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012839370552683249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,589 INFO epoch # 5030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013404904908384196
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:43,589 INFO *** epoch 5030, rolling-avg-loss (window=10)= 0.012304819370183396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,666 INFO epoch # 5031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011748538570827805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,742 INFO epoch # 5032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012857990092015825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,819 INFO epoch # 5033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011443275026977062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,896 INFO epoch # 5034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011275493729044683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:43,972 INFO epoch # 5035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011719652524334379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,049 INFO epoch # 5036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013997043599374592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,131 INFO epoch # 5037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012762984755681828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,207 INFO epoch # 5038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010689514165278524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,284 INFO epoch # 5039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009633198351366445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,361 INFO epoch # 5040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01175824004894821
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:44,361 INFO *** epoch 5040, rolling-avg-loss (window=10)= 0.011788593086384936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,437 INFO epoch # 5041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017051975126378238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,515 INFO epoch # 5042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012988360293093137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,593 INFO epoch # 5043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014311611332232133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,673 INFO epoch # 5044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015721084797405638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,754 INFO epoch # 5045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01213315493077971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,836 INFO epoch # 5046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011399605122278444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,915 INFO epoch # 5047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01246592459938256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:44,992 INFO epoch # 5048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01176886043685954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,069 INFO epoch # 5049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011287467365036719
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,149 INFO epoch # 5050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010767122359538916
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:45,149 INFO *** epoch 5050, rolling-avg-loss (window=10)= 0.012989516636298503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,226 INFO epoch # 5051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010693444739445113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,303 INFO epoch # 5052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011988117810687982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,379 INFO epoch # 5053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010722764360252768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,464 INFO epoch # 5054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01167599408654496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,545 INFO epoch # 5055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011328452441375703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,628 INFO epoch # 5056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01297731144586578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,710 INFO epoch # 5057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012725237116683275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,790 INFO epoch # 5058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01203521316347178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,869 INFO epoch # 5059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011089381543570198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:45,949 INFO epoch # 5060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011553340271348134
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:45,949 INFO *** epoch 5060, rolling-avg-loss (window=10)= 0.011678925697924569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,029 INFO epoch # 5061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010993915857397951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,110 INFO epoch # 5062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013159185473341495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,191 INFO epoch # 5063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013215381404734217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,273 INFO epoch # 5064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01136374456109479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,352 INFO epoch # 5065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01020620614872314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,431 INFO epoch # 5066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011508576761116274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,512 INFO epoch # 5067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014102326153079048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,593 INFO epoch # 5068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01337951686582528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,673 INFO epoch # 5069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01141671382356435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,754 INFO epoch # 5070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015208211378194392
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:46,755 INFO *** epoch 5070, rolling-avg-loss (window=10)= 0.012455377842707093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,834 INFO epoch # 5071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010997798235621303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,913 INFO epoch # 5072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010615042061544955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:46,990 INFO epoch # 5073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012617969958228059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,069 INFO epoch # 5074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013020280050113797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,148 INFO epoch # 5075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010836003450094722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,224 INFO epoch # 5076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01012541723321192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,300 INFO epoch # 5077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01281940340413712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,376 INFO epoch # 5078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012372230339678936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,454 INFO epoch # 5079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012071615448803641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,533 INFO epoch # 5080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010967191752570216
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:47,533 INFO *** epoch 5080, rolling-avg-loss (window=10)= 0.011644295193400466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,612 INFO epoch # 5081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012459710116672795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,690 INFO epoch # 5082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013577435063780285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,767 INFO epoch # 5083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013309959409525618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,844 INFO epoch # 5084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011255850520683452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,921 INFO epoch # 5085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010628575764712878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:47,998 INFO epoch # 5086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013339710640138946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:48,079 INFO epoch # 5087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010784726444398984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:48,156 INFO epoch # 5088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010795755209983326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:48,233 INFO epoch # 5089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010503995639737695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:48,311 INFO epoch # 5090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011229801690205932
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:48,311 INFO *** epoch 5090, rolling-avg-loss (window=10)= 0.011788552049983992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:48,390 INFO epoch # 5091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010417501267511398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:48,483 INFO epoch # 5092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010931575729046017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:48,586 INFO epoch # 5093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015595295350067317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:48,671 INFO epoch # 5094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012931332719745114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:48,760 INFO epoch # 5095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012238515351782553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:48,856 INFO epoch # 5096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010584282732452266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:48,936 INFO epoch # 5097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01103593317384366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,020 INFO epoch # 5098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01194719710474601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,119 INFO epoch # 5099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010733765753684565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,213 INFO epoch # 5100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010452136033563875
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:49,214 INFO *** epoch 5100, rolling-avg-loss (window=10)= 0.011686753521644277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,291 INFO epoch # 5101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011607386186369695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,368 INFO epoch # 5102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01112553387065418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,445 INFO epoch # 5103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012931329423736315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,522 INFO epoch # 5104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011878302117111161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,601 INFO epoch # 5105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010696610988816246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,678 INFO epoch # 5106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011966070618655067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,755 INFO epoch # 5107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014220921977539547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,831 INFO epoch # 5108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012518793097115122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,909 INFO epoch # 5109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01012122674001148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:49,986 INFO epoch # 5110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010408139249193482
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:49,986 INFO *** epoch 5110, rolling-avg-loss (window=10)= 0.01174743142692023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,063 INFO epoch # 5111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010852163963136263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,142 INFO epoch # 5112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011860915677971207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,219 INFO epoch # 5113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01201223574753385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,296 INFO epoch # 5114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011071006520069204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,374 INFO epoch # 5115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011512381795910187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,451 INFO epoch # 5116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014478326222160831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,527 INFO epoch # 5117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011021510690625291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,610 INFO epoch # 5118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010516924026887864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,690 INFO epoch # 5119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011609009627136402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,768 INFO epoch # 5120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01127773043117486
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:50,768 INFO *** epoch 5120, rolling-avg-loss (window=10)= 0.011621220470260596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,846 INFO epoch # 5121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010880292116780765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,922 INFO epoch # 5122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012468911576434039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:50,999 INFO epoch # 5123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010706621054850984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,080 INFO epoch # 5124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010859989080927335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,158 INFO epoch # 5125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010265287171932869
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,236 INFO epoch # 5126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010699231977923773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,313 INFO epoch # 5127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012613595186849125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,391 INFO epoch # 5128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011350450527970679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,469 INFO epoch # 5129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012245268415426835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,547 INFO epoch # 5130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012555911249364726
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:51,547 INFO *** epoch 5130, rolling-avg-loss (window=10)= 0.011464555835846112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,627 INFO epoch # 5131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011963558761635795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,704 INFO epoch # 5132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011116764857433736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,784 INFO epoch # 5133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011374640424037352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,863 INFO epoch # 5134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011643913152511232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:51,941 INFO epoch # 5135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011926730381674133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,019 INFO epoch # 5136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01264525591977872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,099 INFO epoch # 5137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01103841233998537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,177 INFO epoch # 5138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011258862185059115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,255 INFO epoch # 5139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012157983568613417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,332 INFO epoch # 5140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013322165556019172
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:52,332 INFO *** epoch 5140, rolling-avg-loss (window=10)= 0.011844828714674804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,410 INFO epoch # 5141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011528148330398835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,487 INFO epoch # 5142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011316884134430438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,566 INFO epoch # 5143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01270208957430441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,644 INFO epoch # 5144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013298413367010653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,720 INFO epoch # 5145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013366827421123162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,797 INFO epoch # 5146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011792144854553044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,873 INFO epoch # 5147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012143212850787677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:52,950 INFO epoch # 5148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010940624561044388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,026 INFO epoch # 5149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013104751909850165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,106 INFO epoch # 5150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010525421530473977
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:53,107 INFO *** epoch 5150, rolling-avg-loss (window=10)= 0.012071851853397675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,184 INFO epoch # 5151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009857820434262976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,260 INFO epoch # 5152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013327599808690138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,338 INFO epoch # 5153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01069487541099079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,415 INFO epoch # 5154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010360757543821819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,491 INFO epoch # 5155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01201320206746459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,570 INFO epoch # 5156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011084421159466729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,646 INFO epoch # 5157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011155386091559194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,722 INFO epoch # 5158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010325592491426505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,799 INFO epoch # 5159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01254608116869349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,875 INFO epoch # 5160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0108331807423383
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:53,875 INFO *** epoch 5160, rolling-avg-loss (window=10)= 0.011219891691871453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:53,952 INFO epoch # 5161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012066240262356587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,028 INFO epoch # 5162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013249496565549634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,109 INFO epoch # 5163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01328671956434846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,188 INFO epoch # 5164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012213364519993775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,265 INFO epoch # 5165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010420245816931129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,341 INFO epoch # 5166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011473716149339452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,418 INFO epoch # 5167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012514848058344796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,495 INFO epoch # 5168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01100283023697557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,574 INFO epoch # 5169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009395352521096356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,664 INFO epoch # 5170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011299746533040889
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:54,664 INFO *** epoch 5170, rolling-avg-loss (window=10)= 0.011692256022797665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,743 INFO epoch # 5171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009990454345825128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,820 INFO epoch # 5172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010644259717082605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,900 INFO epoch # 5173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011222077235288452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:54,978 INFO epoch # 5174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010336316438042559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,055 INFO epoch # 5175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011658440533210523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,134 INFO epoch # 5176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011906022351467982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,211 INFO epoch # 5177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0113153539277846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,288 INFO epoch # 5178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010638363703037612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,364 INFO epoch # 5179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01321539185300935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,441 INFO epoch # 5180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013211414639954455
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:55,441 INFO *** epoch 5180, rolling-avg-loss (window=10)= 0.011413809474470326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,518 INFO epoch # 5181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011779733598814346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,597 INFO epoch # 5182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011713283747667447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,675 INFO epoch # 5183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011221640248550102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,753 INFO epoch # 5184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01142922516737599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,829 INFO epoch # 5185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015092976958840154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,907 INFO epoch # 5186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015290070412447676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:55,983 INFO epoch # 5187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012006018339889124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,059 INFO epoch # 5188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011177401785971597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,140 INFO epoch # 5189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011440677662903909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,216 INFO epoch # 5190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010337861865991727
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:56,216 INFO *** epoch 5190, rolling-avg-loss (window=10)= 0.012148888978845206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,293 INFO epoch # 5191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01236705484916456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,371 INFO epoch # 5192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011477734733489342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,448 INFO epoch # 5193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012301865150220692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,525 INFO epoch # 5194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01089342701015994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,605 INFO epoch # 5195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011721811446477659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,682 INFO epoch # 5196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010817013768246397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,759 INFO epoch # 5197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01161192990548443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,836 INFO epoch # 5198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01185401096881833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,913 INFO epoch # 5199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010618574335239828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:56,989 INFO epoch # 5200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010863738250918686
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:56,989 INFO *** epoch 5200, rolling-avg-loss (window=10)= 0.011452716041821987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,069 INFO epoch # 5201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012059634827892296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,146 INFO epoch # 5202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0126385986659443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,223 INFO epoch # 5203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013367289051529951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,299 INFO epoch # 5204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013520849766791798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,377 INFO epoch # 5205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011332857407978736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,455 INFO epoch # 5206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010058247236884199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,535 INFO epoch # 5207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017103924677940086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,617 INFO epoch # 5208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012803563935449347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,694 INFO epoch # 5209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014276280417107046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,771 INFO epoch # 5210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015370259701739997
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:57,772 INFO *** epoch 5210, rolling-avg-loss (window=10)= 0.013253150568925776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,851 INFO epoch # 5211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010739209174062125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:57,929 INFO epoch # 5212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011522381100803614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,006 INFO epoch # 5213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01175327472446952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,085 INFO epoch # 5214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015101365104783326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,162 INFO epoch # 5215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01057065145141678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,241 INFO epoch # 5216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010229964675090741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,319 INFO epoch # 5217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01084047558833845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,398 INFO epoch # 5218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012619001252460293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,475 INFO epoch # 5219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01151367850980023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,551 INFO epoch # 5220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012775850176694803
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:58,551 INFO *** epoch 5220, rolling-avg-loss (window=10)= 0.011766585175791988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,630 INFO epoch # 5221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010734620962466579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,706 INFO epoch # 5222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011039275414077565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,783 INFO epoch # 5223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01121013805095572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,860 INFO epoch # 5224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0115068362938473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:58,937 INFO epoch # 5225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012242191602126695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,015 INFO epoch # 5226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012382801651256159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,095 INFO epoch # 5227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012450717724277638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,173 INFO epoch # 5228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012821470067137852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,250 INFO epoch # 5229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014177217177348211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,326 INFO epoch # 5230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01371384308731649
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:20:59,326 INFO *** epoch 5230, rolling-avg-loss (window=10)= 0.012227911203081022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,402 INFO epoch # 5231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013340697536477819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,479 INFO epoch # 5232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011949680614634417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,557 INFO epoch # 5233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010385104251326993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,636 INFO epoch # 5234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00939894633484073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,717 INFO epoch # 5235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010999772595823742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,793 INFO epoch # 5236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01018402555200737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,870 INFO epoch # 5237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010686304129194468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:20:59,947 INFO epoch # 5238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01226149927242659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,023 INFO epoch # 5239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011574866482988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,103 INFO epoch # 5240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01144114050839562
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:00,103 INFO *** epoch 5240, rolling-avg-loss (window=10)= 0.011222203727811575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,180 INFO epoch # 5241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012820718831790145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,258 INFO epoch # 5242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010887104479479603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,334 INFO epoch # 5243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011053449001337867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,411 INFO epoch # 5244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012141068786149845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,487 INFO epoch # 5245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011408607722842135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,567 INFO epoch # 5246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011925557293579914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,649 INFO epoch # 5247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009781003529496957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,728 INFO epoch # 5248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01075858561671339
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,806 INFO epoch # 5249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013591753158834763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,885 INFO epoch # 5250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012348976248176768
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:00,885 INFO *** epoch 5250, rolling-avg-loss (window=10)= 0.011671682466840139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:00,966 INFO epoch # 5251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011525613677804358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:01,044 INFO epoch # 5252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010052339828689583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:01,147 INFO epoch # 5253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011043824735679664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:01,262 INFO epoch # 5254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011736319887859281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:01,363 INFO epoch # 5255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01148245282820426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:01,450 INFO epoch # 5256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011548674105142709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:01,537 INFO epoch # 5257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010516567708691582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:01,619 INFO epoch # 5258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015191385304206051
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:01,696 INFO epoch # 5259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012055546016199514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:01,776 INFO epoch # 5260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010694078635424376
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:01,776 INFO *** epoch 5260, rolling-avg-loss (window=10)= 0.011584680272790138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:01,853 INFO epoch # 5261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013315346935996786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:01,929 INFO epoch # 5262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01190256945847068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,006 INFO epoch # 5263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011283963482128456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,085 INFO epoch # 5264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010260010574711487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,164 INFO epoch # 5265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011435471154982224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,240 INFO epoch # 5266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010793579946039245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,317 INFO epoch # 5267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011138636007672176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,393 INFO epoch # 5268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010669292816601228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,470 INFO epoch # 5269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012057235158863477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,547 INFO epoch # 5270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011707868077792227
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:02,547 INFO *** epoch 5270, rolling-avg-loss (window=10)= 0.0114563973613258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,629 INFO epoch # 5271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011695068824337795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,706 INFO epoch # 5272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01099413534393534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,783 INFO epoch # 5273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011789195297751576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,859 INFO epoch # 5274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012047755371895619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:02,936 INFO epoch # 5275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010412655261461623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,012 INFO epoch # 5276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01085730922932271
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,093 INFO epoch # 5277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010293033468769863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,171 INFO epoch # 5278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011496435166918673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,248 INFO epoch # 5279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010163647530134767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,324 INFO epoch # 5280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012287747449590825
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:03,324 INFO *** epoch 5280, rolling-avg-loss (window=10)= 0.011203698294411878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,401 INFO epoch # 5281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012285115299164318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,478 INFO epoch # 5282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010886983625823632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,555 INFO epoch # 5283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012046102012391202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,638 INFO epoch # 5284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011707348690833896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,715 INFO epoch # 5285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010193312235060148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,791 INFO epoch # 5286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011619827069807798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,868 INFO epoch # 5287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012554295353766065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:03,944 INFO epoch # 5288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014272778993472457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,021 INFO epoch # 5289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01058013184228912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,100 INFO epoch # 5290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017008664581226185
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:04,100 INFO *** epoch 5290, rolling-avg-loss (window=10)= 0.012315455970383481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,179 INFO epoch # 5291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012487319167121314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,255 INFO epoch # 5292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013925264574936591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,332 INFO epoch # 5293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011132282168546226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,411 INFO epoch # 5294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010434047428134363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,489 INFO epoch # 5295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010482173922355287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,567 INFO epoch # 5296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011391269450541586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,649 INFO epoch # 5297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010376968508353457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,729 INFO epoch # 5298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011619142125709914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,806 INFO epoch # 5299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010156437201658264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,883 INFO epoch # 5300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010407354217022657
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:04,884 INFO *** epoch 5300, rolling-avg-loss (window=10)= 0.011241225876437966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:04,961 INFO epoch # 5301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010443309700349346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,038 INFO epoch # 5302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011887653163284995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,117 INFO epoch # 5303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010836030523933005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,195 INFO epoch # 5304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009727159354952164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,272 INFO epoch # 5305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011522550281370059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,351 INFO epoch # 5306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010616161569487303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,433 INFO epoch # 5307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010285169089911506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,511 INFO epoch # 5308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010362146029365249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,588 INFO epoch # 5309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01188258569163736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,666 INFO epoch # 5310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015535267681116238
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:05,666 INFO *** epoch 5310, rolling-avg-loss (window=10)= 0.011309803308540723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,747 INFO epoch # 5311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013691384461708367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,828 INFO epoch # 5312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010734647083154414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,906 INFO epoch # 5313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010042493988294154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:05,983 INFO epoch # 5314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011495471910166088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,060 INFO epoch # 5315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009650173349655233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,137 INFO epoch # 5316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01254427149251569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,214 INFO epoch # 5317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01254880968190264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,294 INFO epoch # 5318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012289403297472745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,371 INFO epoch # 5319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012067713891156018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,448 INFO epoch # 5320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010671342228306457
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:06,448 INFO *** epoch 5320, rolling-avg-loss (window=10)= 0.011573571138433181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,524 INFO epoch # 5321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0112872205208987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,603 INFO epoch # 5322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010262104464345612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,680 INFO epoch # 5323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009850554692093283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,757 INFO epoch # 5324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011478217638796195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,833 INFO epoch # 5325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010953560078633018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,912 INFO epoch # 5326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010752170375781134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:06,991 INFO epoch # 5327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011029940826119855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,071 INFO epoch # 5328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010440870522870682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,150 INFO epoch # 5329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009703332856588531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,227 INFO epoch # 5330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010811202140757814
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:07,227 INFO *** epoch 5330, rolling-avg-loss (window=10)= 0.010656917411688482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,313 INFO epoch # 5331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011403579657780938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,396 INFO epoch # 5332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011839749437058344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,476 INFO epoch # 5333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010633974183292594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,561 INFO epoch # 5334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010026262840256095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,644 INFO epoch # 5335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011320091143716127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,728 INFO epoch # 5336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010365295500378124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,810 INFO epoch # 5337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010850209611817263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,891 INFO epoch # 5338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012394179109833203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:07,975 INFO epoch # 5339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013166621385607868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,054 INFO epoch # 5340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012378798011923209
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:08,054 INFO *** epoch 5340, rolling-avg-loss (window=10)= 0.011437876088166377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,133 INFO epoch # 5341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011901177887921222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,209 INFO epoch # 5342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010521251111640595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,292 INFO epoch # 5343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012473788548959419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,374 INFO epoch # 5344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014990672258136328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,455 INFO epoch # 5345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01149197890481446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,535 INFO epoch # 5346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012039131644996814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,618 INFO epoch # 5347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010815862828167155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,695 INFO epoch # 5348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011162445283844136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,773 INFO epoch # 5349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010800060801557265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,853 INFO epoch # 5350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011115931811218616
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:08,853 INFO *** epoch 5350, rolling-avg-loss (window=10)= 0.011731230108125601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:08,984 INFO epoch # 5351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012999476923141629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,075 INFO epoch # 5352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012871851839008741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,159 INFO epoch # 5353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013036375486990437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,241 INFO epoch # 5354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013332111033378169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,324 INFO epoch # 5355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01170336251379922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,405 INFO epoch # 5356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012051998332026415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,484 INFO epoch # 5357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011584773412323557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,560 INFO epoch # 5358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011424373704358004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,644 INFO epoch # 5359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011318351302179508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,723 INFO epoch # 5360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010870196980249602
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:09,723 INFO *** epoch 5360, rolling-avg-loss (window=10)= 0.012119287152745527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,802 INFO epoch # 5361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010405850320239551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,881 INFO epoch # 5362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010101784646394663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:09,960 INFO epoch # 5363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012962916458491236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,036 INFO epoch # 5364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010929694137303159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,115 INFO epoch # 5365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010247429658193141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,194 INFO epoch # 5366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01056844241975341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,274 INFO epoch # 5367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01062039690441452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,354 INFO epoch # 5368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010285584008670412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,430 INFO epoch # 5369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011659887721179985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,510 INFO epoch # 5370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010815827496116981
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:10,511 INFO *** epoch 5370, rolling-avg-loss (window=10)= 0.010859781377075706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,591 INFO epoch # 5371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009849006935837679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,669 INFO epoch # 5372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01098304137121886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,749 INFO epoch # 5373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010423290499602444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,828 INFO epoch # 5374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009597061900421977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,905 INFO epoch # 5375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014152906922390684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:10,982 INFO epoch # 5376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011003847845131531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,059 INFO epoch # 5377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010083328845212236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,139 INFO epoch # 5378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009656605223426595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,216 INFO epoch # 5379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009873197865090333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,293 INFO epoch # 5380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010841873612662312
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:11,293 INFO *** epoch 5380, rolling-avg-loss (window=10)= 0.010646416102099465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,371 INFO epoch # 5381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012150334296165965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,448 INFO epoch # 5382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010483792182640173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,524 INFO epoch # 5383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011661166165140457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,603 INFO epoch # 5384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011823847336927429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,680 INFO epoch # 5385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013955643255030736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,758 INFO epoch # 5386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011481105830171145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,837 INFO epoch # 5387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01061637304519536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:11,917 INFO epoch # 5388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012540829251520336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,004 INFO epoch # 5389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010521096410229802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,094 INFO epoch # 5390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011185995943378657
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:12,094 INFO *** epoch 5390, rolling-avg-loss (window=10)= 0.011642018371640006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,178 INFO epoch # 5391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011393381864763796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,264 INFO epoch # 5392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011282226783805527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,353 INFO epoch # 5393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01151125819887966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,441 INFO epoch # 5394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013531041258829646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,529 INFO epoch # 5395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011925976126804017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,615 INFO epoch # 5396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012706457637250423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,695 INFO epoch # 5397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015297732054023072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,774 INFO epoch # 5398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01099355611950159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,857 INFO epoch # 5399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010351182259910274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:12,936 INFO epoch # 5400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01609695424849633
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:12,936 INFO *** epoch 5400, rolling-avg-loss (window=10)= 0.012508976655226434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,016 INFO epoch # 5401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0111546154221287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,097 INFO epoch # 5402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016533895599422976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,177 INFO epoch # 5403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010832310697878711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,253 INFO epoch # 5404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010445234904182144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,338 INFO epoch # 5405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01158382416178938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,419 INFO epoch # 5406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009861233513220213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,496 INFO epoch # 5407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011540875289938413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,573 INFO epoch # 5408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01173770111927297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,656 INFO epoch # 5409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014259275936638005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,739 INFO epoch # 5410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01207890096702613
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:13,739 INFO *** epoch 5410, rolling-avg-loss (window=10)= 0.012002786761149764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,815 INFO epoch # 5411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010118470258021262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:13,899 INFO epoch # 5412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010205260681686923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,012 INFO epoch # 5413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010521105257794261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,098 INFO epoch # 5414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011420713461120613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,181 INFO epoch # 5415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011423488984291907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,258 INFO epoch # 5416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012562496136524715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,336 INFO epoch # 5417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011189508208190091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,414 INFO epoch # 5418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01018918355111964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,499 INFO epoch # 5419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012520073520136066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,580 INFO epoch # 5420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014926407136954367
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:14,580 INFO *** epoch 5420, rolling-avg-loss (window=10)= 0.011507670719583984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,667 INFO epoch # 5421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010074128018459305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,746 INFO epoch # 5422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010571595164947212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,865 INFO epoch # 5423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010889547993429005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:14,968 INFO epoch # 5424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0104130686231656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:15,064 INFO epoch # 5425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010765455444925465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:15,156 INFO epoch # 5426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011424110605730675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:15,257 INFO epoch # 5427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00962484379124362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:15,402 INFO epoch # 5428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01105602434836328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:15,497 INFO epoch # 5429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011953076100326143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:15,586 INFO epoch # 5430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01613026118138805
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:15,586 INFO *** epoch 5430, rolling-avg-loss (window=10)= 0.011290211127197835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:15,672 INFO epoch # 5431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014123607805231586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:15,755 INFO epoch # 5432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01118767871230375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:15,836 INFO epoch # 5433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011405366734834388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:15,918 INFO epoch # 5434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009708154917461798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,000 INFO epoch # 5435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011624272490735166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,083 INFO epoch # 5436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01242392254061997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,162 INFO epoch # 5437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011620185527135618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,244 INFO epoch # 5438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011888056353200227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,328 INFO epoch # 5439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011297093049506657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,409 INFO epoch # 5440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009755990038684104
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:16,409 INFO *** epoch 5440, rolling-avg-loss (window=10)= 0.011503432816971327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,490 INFO epoch # 5441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009342595578345936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,576 INFO epoch # 5442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010258564492687583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,677 INFO epoch # 5443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010611805540975183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,763 INFO epoch # 5444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012188409746158868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,843 INFO epoch # 5445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010852356943360064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:16,921 INFO epoch # 5446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012779047174262814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:17,007 INFO epoch # 5447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01018922258663224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:17,095 INFO epoch # 5448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011176403670106083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:17,173 INFO epoch # 5449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009565430322254542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:17,252 INFO epoch # 5450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011848360212752596
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:17,252 INFO *** epoch 5450, rolling-avg-loss (window=10)= 0.010881219626753591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:17,372 INFO epoch # 5451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010189519802224822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:17,506 INFO epoch # 5452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010005505755543709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:17,633 INFO epoch # 5453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010810602179844864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:17,729 INFO epoch # 5454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010365444089984521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:17,817 INFO epoch # 5455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009612284680770244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:17,900 INFO epoch # 5456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009720028887386434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:17,980 INFO epoch # 5457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010185162696870975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,058 INFO epoch # 5458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01061015540471999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,137 INFO epoch # 5459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012231821936438791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,223 INFO epoch # 5460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0099052050645696
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:18,224 INFO *** epoch 5460, rolling-avg-loss (window=10)= 0.010363573049835395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,301 INFO epoch # 5461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010343195172026753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,380 INFO epoch # 5462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012732955030514859
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,462 INFO epoch # 5463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010727148168371059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,541 INFO epoch # 5464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009958192007616162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,622 INFO epoch # 5465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009983397932955995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,706 INFO epoch # 5466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011548273148946464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,790 INFO epoch # 5467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010818112379638478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,875 INFO epoch # 5468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011697163252392784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:18,960 INFO epoch # 5469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013393742992775515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,046 INFO epoch # 5470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012461927595722955
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:19,046 INFO *** epoch 5470, rolling-avg-loss (window=10)= 0.011366410768096103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,139 INFO epoch # 5471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01056618129950948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,220 INFO epoch # 5472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011088984567322768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,304 INFO epoch # 5473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012462942715501413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,390 INFO epoch # 5474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011387064005248249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,473 INFO epoch # 5475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010983648026012816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,559 INFO epoch # 5476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010936865844996646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,650 INFO epoch # 5477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013521975619369186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,733 INFO epoch # 5478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015236894992995076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,815 INFO epoch # 5479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009685657292720862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,895 INFO epoch # 5480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011370946594979614
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:19,895 INFO *** epoch 5480, rolling-avg-loss (window=10)= 0.01172411609586561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:19,973 INFO epoch # 5481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011853721924126148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,054 INFO epoch # 5482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01139340859663207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,137 INFO epoch # 5483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014230714834411629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,217 INFO epoch # 5484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010289659243426286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,300 INFO epoch # 5485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011087991544627585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,383 INFO epoch # 5486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013025199397816323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,465 INFO epoch # 5487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01062505213485565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,544 INFO epoch # 5488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011791567390901037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,628 INFO epoch # 5489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01031629896897357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,712 INFO epoch # 5490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013317647419171408
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:20,712 INFO *** epoch 5490, rolling-avg-loss (window=10)= 0.01179312614549417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,792 INFO epoch # 5491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011196727995411493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,871 INFO epoch # 5492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011718585883500054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:20,951 INFO epoch # 5493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01479130916413851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,029 INFO epoch # 5494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012948455798323266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,107 INFO epoch # 5495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0111915570741985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,185 INFO epoch # 5496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010631261975504458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,263 INFO epoch # 5497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010709231450164225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,342 INFO epoch # 5498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011471292382339016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,422 INFO epoch # 5499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011536160091054626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,503 INFO epoch # 5500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012311003476497717
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:21,503 INFO *** epoch 5500, rolling-avg-loss (window=10)= 0.011850558529113186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,581 INFO epoch # 5501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011747703727451153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,662 INFO epoch # 5502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011692966087139212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,748 INFO epoch # 5503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017792042868677527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,841 INFO epoch # 5504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012105643836548552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:21,922 INFO epoch # 5505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01042544205847662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,005 INFO epoch # 5506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012934048543684185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,084 INFO epoch # 5507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012666995768086053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,163 INFO epoch # 5508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009923691490257625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,241 INFO epoch # 5509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01266268415201921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,320 INFO epoch # 5510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014112349395873025
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:22,320 INFO *** epoch 5510, rolling-avg-loss (window=10)= 0.012606356792821316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,399 INFO epoch # 5511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013485719697200693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,478 INFO epoch # 5512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010655687015969306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,571 INFO epoch # 5513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010687852147384547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,654 INFO epoch # 5514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011216956132557243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,733 INFO epoch # 5515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010024030460044742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,824 INFO epoch # 5516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010256667388603091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:22,948 INFO epoch # 5517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010666904316167347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,035 INFO epoch # 5518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01155151633429341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,122 INFO epoch # 5519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011908994907571469
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,208 INFO epoch # 5520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010869270554394461
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:23,209 INFO *** epoch 5520, rolling-avg-loss (window=10)= 0.011132359895418631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,291 INFO epoch # 5521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012739938436425291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,375 INFO epoch # 5522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01247252244502306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,476 INFO epoch # 5523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010570326470769942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,558 INFO epoch # 5524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011320844845613465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,643 INFO epoch # 5525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011356387854903005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,729 INFO epoch # 5526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01295622052566614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,815 INFO epoch # 5527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01096844814310316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,902 INFO epoch # 5528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01078663079533726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:23,987 INFO epoch # 5529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011308627581456676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:24,079 INFO epoch # 5530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011115665605757385
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:24,079 INFO *** epoch 5530, rolling-avg-loss (window=10)= 0.011559561270405538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:24,199 INFO epoch # 5531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010603824208374135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:24,313 INFO epoch # 5532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009961285591998603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:24,415 INFO epoch # 5533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009647550454246812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:24,493 INFO epoch # 5534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011767963049351238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:24,573 INFO epoch # 5535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013758755769231357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:24,653 INFO epoch # 5536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017377215350279585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:24,733 INFO epoch # 5537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01367881559417583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:24,812 INFO epoch # 5538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011071697641455103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:24,891 INFO epoch # 5539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010469929504324682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:24,970 INFO epoch # 5540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010134766482224222
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:24,970 INFO *** epoch 5540, rolling-avg-loss (window=10)= 0.011847180364566157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:25,065 INFO epoch # 5541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010519297196879052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:25,146 INFO epoch # 5542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011635166214546189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:25,224 INFO epoch # 5543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010403246284113266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:25,303 INFO epoch # 5544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009672569271060638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:25,382 INFO epoch # 5545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013650366847286932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:25,461 INFO epoch # 5546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01081290033471305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:25,543 INFO epoch # 5547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012168720713816583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:25,625 INFO epoch # 5548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010818683032994159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:25,705 INFO epoch # 5549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010456316224008333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:25,784 INFO epoch # 5550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012495404502260499
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:25,784 INFO *** epoch 5550, rolling-avg-loss (window=10)= 0.01126326706216787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:25,878 INFO epoch # 5551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010957651305943727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:26,001 INFO epoch # 5552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013286744331708178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:26,104 INFO epoch # 5553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010120123224623967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:26,203 INFO epoch # 5554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010112056057550944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:26,375 INFO epoch # 5555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011453272360085975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:26,479 INFO epoch # 5556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01142274470475968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:26,582 INFO epoch # 5557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009297343000071123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:26,678 INFO epoch # 5558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010787546547362581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:26,779 INFO epoch # 5559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011890608337125741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:26,885 INFO epoch # 5560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010733639974205289
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:26,885 INFO *** epoch 5560, rolling-avg-loss (window=10)= 0.011006172984343721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:26,973 INFO epoch # 5561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010479772245162167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:27,058 INFO epoch # 5562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011593258364882786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:27,144 INFO epoch # 5563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010131831135367975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:27,225 INFO epoch # 5564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011072110864915885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:27,316 INFO epoch # 5565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012034373183269054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:27,428 INFO epoch # 5566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009933546505635604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:27,525 INFO epoch # 5567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010617877145705279
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:27,619 INFO epoch # 5568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011821475454780739
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:27,708 INFO epoch # 5569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010485583334229887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:27,792 INFO epoch # 5570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010878285334911197
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:27,792 INFO *** epoch 5570, rolling-avg-loss (window=10)= 0.010904811356886058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:27,874 INFO epoch # 5571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00997195384115912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:27,970 INFO epoch # 5572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011109827079053503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:28,061 INFO epoch # 5573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010736475349403918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:28,151 INFO epoch # 5574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011473617792944424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:28,235 INFO epoch # 5575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01289759275096003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:28,317 INFO epoch # 5576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010240976247587241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:28,398 INFO epoch # 5577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010905468981945887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:28,482 INFO epoch # 5578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010165749423322268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:28,580 INFO epoch # 5579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011021912308933679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:28,718 INFO epoch # 5580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009505240697762929
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:28,719 INFO *** epoch 5580, rolling-avg-loss (window=10)= 0.010802881447307299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:28,855 INFO epoch # 5581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010709687288908754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:28,990 INFO epoch # 5582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011222940738662146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:29,088 INFO epoch # 5583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01219815469085006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:29,176 INFO epoch # 5584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011457427419372834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:29,260 INFO epoch # 5585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012743783692712896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:29,344 INFO epoch # 5586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013111180625855923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:29,428 INFO epoch # 5587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009722347633214667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:29,513 INFO epoch # 5588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010086485388455912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:29,598 INFO epoch # 5589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010149675887078047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:29,685 INFO epoch # 5590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014315441498183645
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:29,685 INFO *** epoch 5590, rolling-avg-loss (window=10)= 0.011571712486329488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:29,773 INFO epoch # 5591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010420315331430174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:29,860 INFO epoch # 5592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012631638819584623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:29,947 INFO epoch # 5593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010185000137425959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,035 INFO epoch # 5594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011164945011842065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,124 INFO epoch # 5595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01665121027326677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,211 INFO epoch # 5596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010554734806646593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,298 INFO epoch # 5597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012474174989620224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,385 INFO epoch # 5598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010121964733116329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,471 INFO epoch # 5599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011505361675517634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,558 INFO epoch # 5600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010057129446067847
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:30,558 INFO *** epoch 5600, rolling-avg-loss (window=10)= 0.011576647522451822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,645 INFO epoch # 5601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010310754980309866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,729 INFO epoch # 5602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012070434808265418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,819 INFO epoch # 5603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009611223686079029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,903 INFO epoch # 5604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011428550773416646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:30,985 INFO epoch # 5605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01093932609364856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:31,070 INFO epoch # 5606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01204952249827329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:31,169 INFO epoch # 5607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010342852678149939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:31,255 INFO epoch # 5608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014759703364688903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:31,347 INFO epoch # 5609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011206381808733568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:31,435 INFO epoch # 5610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01046071013843175
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:31,436 INFO *** epoch 5610, rolling-avg-loss (window=10)= 0.011317946082999696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:31,518 INFO epoch # 5611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012946707967785187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:31,606 INFO epoch # 5612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010789563311845995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:31,696 INFO epoch # 5613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01063319570675958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:31,789 INFO epoch # 5614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012343627677182667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:31,872 INFO epoch # 5615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009393445179739501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:31,952 INFO epoch # 5616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009635477923438884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:32,030 INFO epoch # 5617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010679011844331399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:32,120 INFO epoch # 5618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010194416616286617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:32,207 INFO epoch # 5619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009765939161297865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:32,290 INFO epoch # 5620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011390402891265694
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:32,290 INFO *** epoch 5620, rolling-avg-loss (window=10)= 0.010777178827993339
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:32,374 INFO epoch # 5621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009788203984498978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:32,456 INFO epoch # 5622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009824174441746436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:32,539 INFO epoch # 5623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011661586133413948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:32,622 INFO epoch # 5624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012628341966774315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:32,701 INFO epoch # 5625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013480712630553171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:32,784 INFO epoch # 5626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011711202954757027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:32,952 INFO epoch # 5627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009938943359884433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,042 INFO epoch # 5628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00965511410322506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,132 INFO epoch # 5629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010718559875385836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,220 INFO epoch # 5630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01139309823338408
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:33,220 INFO *** epoch 5630, rolling-avg-loss (window=10)= 0.011079993768362328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,304 INFO epoch # 5631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010922135988948867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,389 INFO epoch # 5632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011247298127273098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,473 INFO epoch # 5633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0125504868774442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,551 INFO epoch # 5634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011816380996606313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,631 INFO epoch # 5635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010872245897189714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,714 INFO epoch # 5636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009760781162185594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,796 INFO epoch # 5637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00998567629721947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,880 INFO epoch # 5638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011949372885283083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:33,963 INFO epoch # 5639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012134286153013818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,045 INFO epoch # 5640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010631287725118455
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:34,045 INFO *** epoch 5640, rolling-avg-loss (window=10)= 0.01118699521102826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,126 INFO epoch # 5641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011689970262523275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,205 INFO epoch # 5642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013170971753424965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,290 INFO epoch # 5643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009889425768051296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,374 INFO epoch # 5644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010286007483955473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,454 INFO epoch # 5645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011866798624396324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,548 INFO epoch # 5646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01084676626487635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,632 INFO epoch # 5647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010643496862030588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,711 INFO epoch # 5648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011014722113031894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,789 INFO epoch # 5649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01202207655296661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,865 INFO epoch # 5650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01463120526750572
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:34,865 INFO *** epoch 5650, rolling-avg-loss (window=10)= 0.01160614409527625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:34,994 INFO epoch # 5651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01123064239800442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:35,141 INFO epoch # 5652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009891986584989354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:35,247 INFO epoch # 5653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010872982747969218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:35,346 INFO epoch # 5654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010751459456514567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:35,441 INFO epoch # 5655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009217549952154513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:35,540 INFO epoch # 5656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009425288815691601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:35,635 INFO epoch # 5657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009907330353598809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:35,716 INFO epoch # 5658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010130473150638863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:35,802 INFO epoch # 5659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008810533028736245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:35,881 INFO epoch # 5660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010544344258960336
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:35,881 INFO *** epoch 5660, rolling-avg-loss (window=10)= 0.010078259074725792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:35,961 INFO epoch # 5661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011116131878225133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:36,043 INFO epoch # 5662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011422327748732641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:36,125 INFO epoch # 5663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010187586449319497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:36,204 INFO epoch # 5664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011297704942990094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:36,282 INFO epoch # 5665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010826783298398368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:36,399 INFO epoch # 5666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01178885564149823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:36,521 INFO epoch # 5667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012215679831570014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:36,642 INFO epoch # 5668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011635032380581833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:36,750 INFO epoch # 5669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011874086769239511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:36,830 INFO epoch # 5670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01393461883708369
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:36,830 INFO *** epoch 5670, rolling-avg-loss (window=10)= 0.0116298807777639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:36,911 INFO epoch # 5671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011517941056808922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:36,994 INFO epoch # 5672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011093956112745218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:37,075 INFO epoch # 5673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012601273279869929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:37,155 INFO epoch # 5674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009801815933315083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:37,273 INFO epoch # 5675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009990525722969323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:37,410 INFO epoch # 5676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010682239560992457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:37,509 INFO epoch # 5677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01038605721987551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:37,615 INFO epoch # 5678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009431327940546907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:37,704 INFO epoch # 5679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010598739041597582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:37,793 INFO epoch # 5680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011222206041566096
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:37,793 INFO *** epoch 5680, rolling-avg-loss (window=10)= 0.010732608191028703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:37,879 INFO epoch # 5681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010809658386278898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:37,961 INFO epoch # 5682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010249811588437296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,059 INFO epoch # 5683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013993535409099422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,161 INFO epoch # 5684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01157887680165004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,238 INFO epoch # 5685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011013945564627647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,316 INFO epoch # 5686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010642580200510565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,398 INFO epoch # 5687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010480922865099274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,478 INFO epoch # 5688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011270106784650125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,559 INFO epoch # 5689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009621553545002826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,640 INFO epoch # 5690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010804495912452694
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:38,641 INFO *** epoch 5690, rolling-avg-loss (window=10)= 0.011046548705780878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,725 INFO epoch # 5691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012821931421058252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,807 INFO epoch # 5692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010320216380932834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,890 INFO epoch # 5693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010597248299745843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:38,977 INFO epoch # 5694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01290686168067623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,056 INFO epoch # 5695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012592876519192941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,139 INFO epoch # 5696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011531857002410106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,227 INFO epoch # 5697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009998100220400374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,309 INFO epoch # 5698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010861713279155083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,391 INFO epoch # 5699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01114475321082864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,469 INFO epoch # 5700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010598828564980067
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:39,469 INFO *** epoch 5700, rolling-avg-loss (window=10)= 0.011337438657938037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,546 INFO epoch # 5701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010189954824454617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,627 INFO epoch # 5702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010147234715986997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,706 INFO epoch # 5703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010595208805170842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,787 INFO epoch # 5704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011097166061517783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,867 INFO epoch # 5705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009868761655525304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:39,947 INFO epoch # 5706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010392627213150263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,033 INFO epoch # 5707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009322392448666506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,115 INFO epoch # 5708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010421445760584902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,198 INFO epoch # 5709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011194367776624858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,283 INFO epoch # 5710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009327517647761852
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:40,283 INFO *** epoch 5710, rolling-avg-loss (window=10)= 0.010255667690944392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,394 INFO epoch # 5711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008925154194002971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,475 INFO epoch # 5712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009497231047134846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,568 INFO epoch # 5713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010849675978533924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,651 INFO epoch # 5714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009316015100921504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,737 INFO epoch # 5715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009787882139789872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,817 INFO epoch # 5716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011354608330293559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,897 INFO epoch # 5717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013127186161000282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:40,979 INFO epoch # 5718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01218189115752466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:41,057 INFO epoch # 5719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011986569516011514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:41,135 INFO epoch # 5720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012792123481631279
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:41,135 INFO *** epoch 5720, rolling-avg-loss (window=10)= 0.010981833710684442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:41,234 INFO epoch # 5721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011128132609883323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:41,340 INFO epoch # 5722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01206977540277876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:41,432 INFO epoch # 5723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0095529837635695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:41,515 INFO epoch # 5724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013123601471306756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:41,599 INFO epoch # 5725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010365150941652246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:41,684 INFO epoch # 5726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008741919591557235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:41,766 INFO epoch # 5727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010068095725728199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:41,842 INFO epoch # 5728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010384953013272025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:41,918 INFO epoch # 5729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010809134248120245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,003 INFO epoch # 5730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010316161424270831
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:42,003 INFO *** epoch 5730, rolling-avg-loss (window=10)= 0.010655990819213913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,092 INFO epoch # 5731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010049264470580965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,173 INFO epoch # 5732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010776838884339668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,252 INFO epoch # 5733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010281686452799477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,331 INFO epoch # 5734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00937822838750435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,411 INFO epoch # 5735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009986399963963777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,489 INFO epoch # 5736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00943099454161711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,575 INFO epoch # 5737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010655758633220103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,664 INFO epoch # 5738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013904773026297335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,743 INFO epoch # 5739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010236677335342392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,822 INFO epoch # 5740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00951050598087022
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:42,822 INFO *** epoch 5740, rolling-avg-loss (window=10)= 0.01042111276765354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:42,899 INFO epoch # 5741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013654520924319513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,022 INFO epoch # 5742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010784880636492744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,106 INFO epoch # 5743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010568515892373398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,186 INFO epoch # 5744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010279164736857638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,268 INFO epoch # 5745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013717798203288112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,347 INFO epoch # 5746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010264554890454747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,426 INFO epoch # 5747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010036869454779662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,502 INFO epoch # 5748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01119395619025454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,579 INFO epoch # 5749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010947692062472925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,676 INFO epoch # 5750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010897940504946746
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:43,676 INFO *** epoch 5750, rolling-avg-loss (window=10)= 0.011234589349624003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,757 INFO epoch # 5751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013904659048421308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,841 INFO epoch # 5752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010789952444611117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,919 INFO epoch # 5753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00981209388555726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:43,996 INFO epoch # 5754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00998069868364837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:44,079 INFO epoch # 5755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011413912536227144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:44,163 INFO epoch # 5756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009458627333515324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:44,245 INFO epoch # 5757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010797772411024198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:44,326 INFO epoch # 5758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0106459817616269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:44,407 INFO epoch # 5759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012764311250066385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:44,523 INFO epoch # 5760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009962875628843904
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:44,523 INFO *** epoch 5760, rolling-avg-loss (window=10)= 0.010953088498354191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:44,610 INFO epoch # 5761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009103579737711698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:44,692 INFO epoch # 5762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010215975213213824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:44,774 INFO epoch # 5763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01142334591713734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:44,857 INFO epoch # 5764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01159467117395252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:44,942 INFO epoch # 5765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010200869844993576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,021 INFO epoch # 5766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011093056557001546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,098 INFO epoch # 5767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012537214759504423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,180 INFO epoch # 5768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012392507924232632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,262 INFO epoch # 5769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01075735357881058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,342 INFO epoch # 5770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009715186228277162
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:45,342 INFO *** epoch 5770, rolling-avg-loss (window=10)= 0.01090337609348353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,424 INFO epoch # 5771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01103788310138043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,503 INFO epoch # 5772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015677443443564698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,600 INFO epoch # 5773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011031942529371008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,680 INFO epoch # 5774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010730697758845054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,764 INFO epoch # 5775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010225152618659195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,841 INFO epoch # 5776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011181873807800002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:45,922 INFO epoch # 5777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011193745354830753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,006 INFO epoch # 5778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01004549908975605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,085 INFO epoch # 5779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00948876600159565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,166 INFO epoch # 5780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008340051645063795
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:46,166 INFO *** epoch 5780, rolling-avg-loss (window=10)= 0.010895305535086663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,255 INFO epoch # 5781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010224700556136668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,360 INFO epoch # 5782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013169324796763249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,450 INFO epoch # 5783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01041133729449939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,531 INFO epoch # 5784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010411675677460153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,610 INFO epoch # 5785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010641714092344046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,691 INFO epoch # 5786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010561602997768205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,774 INFO epoch # 5787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009511318785371259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,858 INFO epoch # 5788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01019567343610106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:46,935 INFO epoch # 5789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010274770174873993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,011 INFO epoch # 5790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01080046172864968
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:47,011 INFO *** epoch 5790, rolling-avg-loss (window=10)= 0.01062025795399677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,100 INFO epoch # 5791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010288528617820702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,225 INFO epoch # 5792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009923520454321988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,312 INFO epoch # 5793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012167351887910627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,406 INFO epoch # 5794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009441040994715877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,494 INFO epoch # 5795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00975282522267662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,584 INFO epoch # 5796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010400798120826948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,670 INFO epoch # 5797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01100059458258329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,747 INFO epoch # 5798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009909012689604424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,824 INFO epoch # 5799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012998229314689524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,901 INFO epoch # 5800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01071719772880897
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:47,902 INFO *** epoch 5800, rolling-avg-loss (window=10)= 0.010659909961395897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:47,980 INFO epoch # 5801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009736014719237573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:48,058 INFO epoch # 5802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01067413312557619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:48,136 INFO epoch # 5803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011246162815950811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:48,214 INFO epoch # 5804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010733468836406246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:48,322 INFO epoch # 5805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009661461670475546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:48,426 INFO epoch # 5806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013675325200892985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:48,513 INFO epoch # 5807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01626782891980838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:48,600 INFO epoch # 5808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010720085352659225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:48,779 INFO epoch # 5809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011366698650817852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:48,886 INFO epoch # 5810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011545160723471781
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:48,886 INFO *** epoch 5810, rolling-avg-loss (window=10)= 0.011562634001529659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:48,989 INFO epoch # 5811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009934107671142556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:49,091 INFO epoch # 5812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011292319861240685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:49,194 INFO epoch # 5813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011827050824649632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:49,294 INFO epoch # 5814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011435457665356807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:49,393 INFO epoch # 5815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009709071222459897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:49,491 INFO epoch # 5816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01084580247697886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:49,589 INFO epoch # 5817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01021142346507986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:49,687 INFO epoch # 5818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013875071730581112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:49,786 INFO epoch # 5819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01038303520908812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:49,883 INFO epoch # 5820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01171646686270833
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:49,883 INFO *** epoch 5820, rolling-avg-loss (window=10)= 0.011122980698928586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:49,980 INFO epoch # 5821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009802494285395369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:50,077 INFO epoch # 5822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011824572262412403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:50,174 INFO epoch # 5823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010719092388171703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:50,272 INFO epoch # 5824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00900401319086086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:50,369 INFO epoch # 5825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009971311577828601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:50,467 INFO epoch # 5826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013791626668535173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:50,592 INFO epoch # 5827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010214221649221145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:50,698 INFO epoch # 5828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009039257602125872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:50,804 INFO epoch # 5829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009911515895510092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:50,912 INFO epoch # 5830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01040566539450083
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:50,912 INFO *** epoch 5830, rolling-avg-loss (window=10)= 0.010468377091456205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:51,018 INFO epoch # 5831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015938361510052346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:51,121 INFO epoch # 5832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01584693224867806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:51,222 INFO epoch # 5833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009537113583064638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:51,319 INFO epoch # 5834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011457455417257734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:51,416 INFO epoch # 5835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011662773235002533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:51,513 INFO epoch # 5836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013340552446607035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:51,610 INFO epoch # 5837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00931854621740058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:51,708 INFO epoch # 5838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010928942676400766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:51,805 INFO epoch # 5839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01300888094556285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:51,902 INFO epoch # 5840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010790250467834994
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:51,902 INFO *** epoch 5840, rolling-avg-loss (window=10)= 0.012182980874786153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:51,999 INFO epoch # 5841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010459445446031168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:52,096 INFO epoch # 5842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010494483765796758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:52,195 INFO epoch # 5843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009999844325648155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:52,292 INFO epoch # 5844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010155283714993857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:52,389 INFO epoch # 5845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00933808534318814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:52,488 INFO epoch # 5846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01001878063834738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:52,585 INFO epoch # 5847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011080555166699924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:52,685 INFO epoch # 5848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01152320989058353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:52,781 INFO epoch # 5849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013502153087756597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:52,879 INFO epoch # 5850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01207604823866859
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:52,879 INFO *** epoch 5850, rolling-avg-loss (window=10)= 0.01086478896177141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:52,977 INFO epoch # 5851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0131395869248081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:53,074 INFO epoch # 5852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013900354664656334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:53,172 INFO epoch # 5853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010610627956339158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:53,269 INFO epoch # 5854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011454939580289647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:53,366 INFO epoch # 5855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01127891123178415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:53,465 INFO epoch # 5856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012167827881057747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:53,562 INFO epoch # 5857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012395764351822436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:53,661 INFO epoch # 5858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011471971229184419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:53,758 INFO epoch # 5859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011475769184471574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:53,855 INFO epoch # 5860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00946863469289383
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:53,855 INFO *** epoch 5860, rolling-avg-loss (window=10)= 0.01173643876973074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:53,954 INFO epoch # 5861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011263247288297862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:54,051 INFO epoch # 5862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011236809499678202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:54,150 INFO epoch # 5863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010344785521738231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:54,248 INFO epoch # 5864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010316787302144803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:54,345 INFO epoch # 5865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013071091976598836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:54,442 INFO epoch # 5866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010343458721763454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:54,540 INFO epoch # 5867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011309170215099584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:54,637 INFO epoch # 5868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009232650241756346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:54,735 INFO epoch # 5869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01036095942981774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:54,832 INFO epoch # 5870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009490460310189519
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:54,832 INFO *** epoch 5870, rolling-avg-loss (window=10)= 0.010696942050708457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:54,931 INFO epoch # 5871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010839544323971495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:55,028 INFO epoch # 5872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01126134250080213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:55,125 INFO epoch # 5873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010624360496876761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:55,224 INFO epoch # 5874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011039261786208954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:55,321 INFO epoch # 5875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013123303739121184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:55,418 INFO epoch # 5876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011459820918389596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:55,516 INFO epoch # 5877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01120845960394945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:55,616 INFO epoch # 5878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009973277970857453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:55,715 INFO epoch # 5879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009058316383743659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:55,812 INFO epoch # 5880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010231708198261913
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:55,812 INFO *** epoch 5880, rolling-avg-loss (window=10)= 0.010881939592218259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:55,907 INFO epoch # 5881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010888484219321981
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:56,004 INFO epoch # 5882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010176393523579463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:56,101 INFO epoch # 5883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0098635868052952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:56,198 INFO epoch # 5884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009629243657400366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:56,297 INFO epoch # 5885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010953938712191302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:56,394 INFO epoch # 5886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01093239952751901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:56,494 INFO epoch # 5887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009588095577782951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:56,595 INFO epoch # 5888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010272037208778784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:56,696 INFO epoch # 5889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00972602744877804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:56,797 INFO epoch # 5890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009869392539258115
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:56,797 INFO *** epoch 5890, rolling-avg-loss (window=10)= 0.01018995992199052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:56,894 INFO epoch # 5891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009177517204079777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:56,991 INFO epoch # 5892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009367408529215027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,086 INFO epoch # 5893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010659216553904116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,165 INFO epoch # 5894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00899509195733117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,244 INFO epoch # 5895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009050961765751708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,323 INFO epoch # 5896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009413669518835377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,406 INFO epoch # 5897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0097807863203343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,486 INFO epoch # 5898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011954648318351246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,566 INFO epoch # 5899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012582336392370053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,647 INFO epoch # 5900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009802161359402817
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:57,647 INFO *** epoch 5900, rolling-avg-loss (window=10)= 0.010078379791957559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,729 INFO epoch # 5901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011209509342734236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,808 INFO epoch # 5902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010777144183521159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,888 INFO epoch # 5903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008898996660718694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:57,969 INFO epoch # 5904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009286287211580202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,050 INFO epoch # 5905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011064214253565297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,130 INFO epoch # 5906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011948381135880481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,212 INFO epoch # 5907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010603786795400083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,292 INFO epoch # 5908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011782664572820067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,372 INFO epoch # 5909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010881579873966984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,454 INFO epoch # 5910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011547005095053464
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:58,454 INFO *** epoch 5910, rolling-avg-loss (window=10)= 0.010799956912524066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,534 INFO epoch # 5911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010736534837633371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,615 INFO epoch # 5912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009174318678560667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,698 INFO epoch # 5913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01049686006444972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,778 INFO epoch # 5914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009834044292801991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,862 INFO epoch # 5915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011532839911524206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:58,945 INFO epoch # 5916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010679629849619232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,024 INFO epoch # 5917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009628662570321467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,104 INFO epoch # 5918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01147538438817719
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,187 INFO epoch # 5919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010573355175438337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,266 INFO epoch # 5920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011529910887475125
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:21:59,266 INFO *** epoch 5920, rolling-avg-loss (window=10)= 0.01056615406560013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,345 INFO epoch # 5921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011698633461492136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,424 INFO epoch # 5922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012735268930555321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,505 INFO epoch # 5923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00959323141432833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,585 INFO epoch # 5924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011110967774584424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,670 INFO epoch # 5925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009036137234943453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,750 INFO epoch # 5926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008943433567765169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,829 INFO epoch # 5927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009624182130210102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,908 INFO epoch # 5928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009549693342705723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:21:59,991 INFO epoch # 5929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010801783675560728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,072 INFO epoch # 5930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009965726239897776
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:00,072 INFO *** epoch 5930, rolling-avg-loss (window=10)= 0.010305905777204315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,153 INFO epoch # 5931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009826580208027735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,233 INFO epoch # 5932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011876633638166822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,312 INFO epoch # 5933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009713966355775483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,392 INFO epoch # 5934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009563366016664077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,478 INFO epoch # 5935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01079257655510446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,558 INFO epoch # 5936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009851310918747913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,645 INFO epoch # 5937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012885820178780705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,726 INFO epoch # 5938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01050575150293298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,806 INFO epoch # 5939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009753195874509402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,885 INFO epoch # 5940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0106625637417892
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:00,886 INFO *** epoch 5940, rolling-avg-loss (window=10)= 0.010543176499049877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:00,967 INFO epoch # 5941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010648025127011351
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,051 INFO epoch # 5942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010327844043786172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,132 INFO epoch # 5943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010107657340995502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,215 INFO epoch # 5944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011852130759507418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,298 INFO epoch # 5945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009631767316022888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,379 INFO epoch # 5946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011788983989390545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,459 INFO epoch # 5947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011484793445561081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,538 INFO epoch # 5948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011022949707694352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,623 INFO epoch # 5949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00899403314178926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,703 INFO epoch # 5950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011020989855751395
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:01,703 INFO *** epoch 5950, rolling-avg-loss (window=10)= 0.010687917472750996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,786 INFO epoch # 5951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01265154300199356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,865 INFO epoch # 5952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010893054364714772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:01,946 INFO epoch # 5953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010394165328762028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,026 INFO epoch # 5954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009351587985293008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,107 INFO epoch # 5955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009838707235758193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,188 INFO epoch # 5956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011381488759070635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,271 INFO epoch # 5957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011499185158754699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,351 INFO epoch # 5958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010721239596023224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,431 INFO epoch # 5959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010778211057186127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,510 INFO epoch # 5960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010989931499352679
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:02,510 INFO *** epoch 5960, rolling-avg-loss (window=10)= 0.010849911398690893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,592 INFO epoch # 5961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01328502272372134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,673 INFO epoch # 5962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011015526164555922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,754 INFO epoch # 5963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010917321953456849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,834 INFO epoch # 5964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011458019245765172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,913 INFO epoch # 5965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00969370391976554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:02,993 INFO epoch # 5966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010419608952361159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,073 INFO epoch # 5967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010364882997237146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,166 INFO epoch # 5968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012607476659468375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,250 INFO epoch # 5969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009920231823343784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,334 INFO epoch # 5970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011160146037582308
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:03,334 INFO *** epoch 5970, rolling-avg-loss (window=10)= 0.01108419404772576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,416 INFO epoch # 5971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013885508829844184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,499 INFO epoch # 5972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011505594462505542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,584 INFO epoch # 5973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00925686044502072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,667 INFO epoch # 5974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011732138256775215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,749 INFO epoch # 5975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01024109088757541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,830 INFO epoch # 5976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01005729198368499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,911 INFO epoch # 5977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009894622533465736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:03,991 INFO epoch # 5978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009101531701162457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:04,070 INFO epoch # 5979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010452486443682574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:04,187 INFO epoch # 5980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01092653012892697
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:04,187 INFO *** epoch 5980, rolling-avg-loss (window=10)= 0.01070536556726438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:04,268 INFO epoch # 5981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01227670036314521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:04,348 INFO epoch # 5982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011892076174262911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:04,427 INFO epoch # 5983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010765634549898095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:04,507 INFO epoch # 5984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010852130399143789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:04,587 INFO epoch # 5985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010254025175527204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:04,667 INFO epoch # 5986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010352110024541616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:04,749 INFO epoch # 5987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009719165012938902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:04,845 INFO epoch # 5988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009557209035847336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:04,930 INFO epoch # 5989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011125363322207704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,011 INFO epoch # 5990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009799379564356059
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:05,012 INFO *** epoch 5990, rolling-avg-loss (window=10)= 0.010659379362186883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,092 INFO epoch # 5991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0103197421412915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,174 INFO epoch # 5992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010167102431296371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,255 INFO epoch # 5993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010314581930288114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,336 INFO epoch # 5994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010081866836117115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,416 INFO epoch # 5995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010275914763042238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,497 INFO epoch # 5996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013407677470240742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,577 INFO epoch # 5997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01419022842310369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,661 INFO epoch # 5998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012326343785389327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,743 INFO epoch # 5999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010060387947305571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,824 INFO epoch # 6000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009887273226922844
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:05,825 INFO *** epoch 6000, rolling-avg-loss (window=10)= 0.011103111895499751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,905 INFO epoch # 6001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00899960668903077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:05,985 INFO epoch # 6002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009715637759654783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,065 INFO epoch # 6003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009691603270766791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,148 INFO epoch # 6004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009585264757333789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,229 INFO epoch # 6005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00928265207039658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,310 INFO epoch # 6006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013713180931517854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,391 INFO epoch # 6007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009868175504379906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,471 INFO epoch # 6008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010944957655738108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,554 INFO epoch # 6009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011255719466134906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,641 INFO epoch # 6010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009149876976152882
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:06,641 INFO *** epoch 6010, rolling-avg-loss (window=10)= 0.010220667508110636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,724 INFO epoch # 6011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009093101616599597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,805 INFO epoch # 6012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01140259543171851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,890 INFO epoch # 6013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011926356863114052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:06,972 INFO epoch # 6014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01047872110211756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,053 INFO epoch # 6015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01123190502403304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,135 INFO epoch # 6016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011673712273477577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,216 INFO epoch # 6017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012442675986676477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,296 INFO epoch # 6018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01177994254976511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,376 INFO epoch # 6019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010758639866253361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,456 INFO epoch # 6020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01005927134247031
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:07,456 INFO *** epoch 6020, rolling-avg-loss (window=10)= 0.011084692205622559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,537 INFO epoch # 6021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009761827299371362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,621 INFO epoch # 6022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011195020284503698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,701 INFO epoch # 6023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01056077222165186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,783 INFO epoch # 6024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01131127392000053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,863 INFO epoch # 6025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009334018715890124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:07,943 INFO epoch # 6026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009659195406129584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,023 INFO epoch # 6027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010227106613456272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,102 INFO epoch # 6028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010185091523453593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,186 INFO epoch # 6029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01040718647709582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,266 INFO epoch # 6030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010108982649398968
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:08,266 INFO *** epoch 6030, rolling-avg-loss (window=10)= 0.01027504751109518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,345 INFO epoch # 6031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011511631062603556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,425 INFO epoch # 6032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009806857371586375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,505 INFO epoch # 6033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009498844534391537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,586 INFO epoch # 6034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009639375493861735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,668 INFO epoch # 6035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011364831610990223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,750 INFO epoch # 6036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009491326345596462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,829 INFO epoch # 6037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011018813791451976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,908 INFO epoch # 6038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011746132397092879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:08,987 INFO epoch # 6039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009626638879126403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,068 INFO epoch # 6040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01011954612476984
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:09,068 INFO *** epoch 6040, rolling-avg-loss (window=10)= 0.010382399761147099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,150 INFO epoch # 6041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01060002006124705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,230 INFO epoch # 6042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011941519594984129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,313 INFO epoch # 6043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01042575119936373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,393 INFO epoch # 6044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012975611592992209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,473 INFO epoch # 6045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010930371194262989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,552 INFO epoch # 6046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012265353245311417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,636 INFO epoch # 6047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01099872513441369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,716 INFO epoch # 6048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012491913774283603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,797 INFO epoch # 6049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010438642784720287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,877 INFO epoch # 6050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00974370246694889
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:09,877 INFO *** epoch 6050, rolling-avg-loss (window=10)= 0.0112811611048528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:09,957 INFO epoch # 6051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009456451021833345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,036 INFO epoch # 6052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009405086631886661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,117 INFO epoch # 6053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010493067282368429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,200 INFO epoch # 6054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012417815902153961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,280 INFO epoch # 6055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012892160390038043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,360 INFO epoch # 6056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011430184007622302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,442 INFO epoch # 6057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013124765100656077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,522 INFO epoch # 6058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012564617354655638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,603 INFO epoch # 6059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010361499807913788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,688 INFO epoch # 6060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009925554586516228
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:10,688 INFO *** epoch 6060, rolling-avg-loss (window=10)= 0.011207120208564448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,768 INFO epoch # 6061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008888913493137807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,848 INFO epoch # 6062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009316601455793716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:10,927 INFO epoch # 6063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01143890293315053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,008 INFO epoch # 6064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009392039821250364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,090 INFO epoch # 6065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01084724058455322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,175 INFO epoch # 6066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00925303554686252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,258 INFO epoch # 6067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01070303273445461
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,338 INFO epoch # 6068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011468924902146682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,419 INFO epoch # 6069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00897806951252278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,499 INFO epoch # 6070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009602165111573413
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:11,499 INFO *** epoch 6070, rolling-avg-loss (window=10)= 0.009988892609544563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,579 INFO epoch # 6071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010690341085137334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,663 INFO epoch # 6072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009260035381885245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,743 INFO epoch # 6073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010104792294441722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,823 INFO epoch # 6074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008771180255280342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,904 INFO epoch # 6075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011701317722327076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:11,987 INFO epoch # 6076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010946296592010185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,067 INFO epoch # 6077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010936969600152224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,151 INFO epoch # 6078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01044393856136594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,241 INFO epoch # 6079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010175358889682684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,325 INFO epoch # 6080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010613283411657903
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:12,325 INFO *** epoch 6080, rolling-avg-loss (window=10)= 0.010364351379394066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,406 INFO epoch # 6081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00994680274015991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,490 INFO epoch # 6082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010541647003265098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,572 INFO epoch # 6083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011461577378213406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,657 INFO epoch # 6084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011712421284755692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,739 INFO epoch # 6085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010603137343423441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,821 INFO epoch # 6086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010508883773582056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,901 INFO epoch # 6087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010709011519793421
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:12,980 INFO epoch # 6088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011338780663209036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,059 INFO epoch # 6089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008699233396328054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,141 INFO epoch # 6090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010411335519165732
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:13,141 INFO *** epoch 6090, rolling-avg-loss (window=10)= 0.010593283062189585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,222 INFO epoch # 6091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012127759284339845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,302 INFO epoch # 6092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010248808612232096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,385 INFO epoch # 6093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010572238534223288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,467 INFO epoch # 6094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009854150106548332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,546 INFO epoch # 6095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00976390129653737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,632 INFO epoch # 6096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011384669385734014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,713 INFO epoch # 6097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010532913642236963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,794 INFO epoch # 6098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010564120369963348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,875 INFO epoch # 6099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010199189520790242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:13,955 INFO epoch # 6100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011823124921647832
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:13,955 INFO *** epoch 6100, rolling-avg-loss (window=10)= 0.010707087567425333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,035 INFO epoch # 6101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010562964096607175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,122 INFO epoch # 6102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011991749634034932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,204 INFO epoch # 6103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01077225201879628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,288 INFO epoch # 6104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010815739195095375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,371 INFO epoch # 6105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010057378312922083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,454 INFO epoch # 6106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009840086109761614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,537 INFO epoch # 6107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010938571824226528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,625 INFO epoch # 6108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010096690137288533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,709 INFO epoch # 6109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009676602203398943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,791 INFO epoch # 6110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012298071174882352
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:14,792 INFO *** epoch 6110, rolling-avg-loss (window=10)= 0.010705010470701382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,871 INFO epoch # 6111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00964051544724498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:14,951 INFO epoch # 6112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011691851817886345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,032 INFO epoch # 6113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011673542401695158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,113 INFO epoch # 6114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013472394028212875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,193 INFO epoch # 6115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00940472204820253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,273 INFO epoch # 6116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010332194273360074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,353 INFO epoch # 6117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00977569928363664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,433 INFO epoch # 6118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00910634444153402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,514 INFO epoch # 6119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011297640769043937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,595 INFO epoch # 6120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010687778529245406
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:15,595 INFO *** epoch 6120, rolling-avg-loss (window=10)= 0.010708268304006197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,679 INFO epoch # 6121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011113950036815368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,758 INFO epoch # 6122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013701555057195947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,838 INFO epoch # 6123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01056563373276731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:15,920 INFO epoch # 6124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010604075039736927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,000 INFO epoch # 6125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008588189026340842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,081 INFO epoch # 6126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009372662178066093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,165 INFO epoch # 6127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01077632079977775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,250 INFO epoch # 6128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011466471070889384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,333 INFO epoch # 6129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008369163166207727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,412 INFO epoch # 6130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009807153677684255
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:16,413 INFO *** epoch 6130, rolling-avg-loss (window=10)= 0.010436517378548161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,493 INFO epoch # 6131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0094192819451564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,575 INFO epoch # 6132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012017823697533458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,674 INFO epoch # 6133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010706329296226613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,772 INFO epoch # 6134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010241040101391263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,856 INFO epoch # 6135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01289512132643722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:16,939 INFO epoch # 6136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00994789366814075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,021 INFO epoch # 6137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009824977911193855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,103 INFO epoch # 6138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011069559681345709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,186 INFO epoch # 6139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008995373667858075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,266 INFO epoch # 6140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009912071313010529
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:17,266 INFO *** epoch 6140, rolling-avg-loss (window=10)= 0.010502947260829388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,346 INFO epoch # 6141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009568591267452575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,427 INFO epoch # 6142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01047565923363436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,508 INFO epoch # 6143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010569825026323088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,589 INFO epoch # 6144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010192547939368524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,673 INFO epoch # 6145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012385466776322573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,754 INFO epoch # 6146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008656384583446197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,834 INFO epoch # 6147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009027997992234305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:17,914 INFO epoch # 6148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008101963547233026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,000 INFO epoch # 6149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011965208337642252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,080 INFO epoch # 6150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010769458225695416
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:18,080 INFO *** epoch 6150, rolling-avg-loss (window=10)= 0.010171310292935231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,162 INFO epoch # 6151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009101036841457244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,243 INFO epoch # 6152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008943106935475953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,324 INFO epoch # 6153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012072763522155583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,406 INFO epoch # 6154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010008185563492589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,486 INFO epoch # 6155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009814516277401708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,566 INFO epoch # 6156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01048516600712901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,651 INFO epoch # 6157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011624925282376353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,736 INFO epoch # 6158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00838767991081113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,816 INFO epoch # 6159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009048653504578397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,897 INFO epoch # 6160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011566350833163597
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:18,897 INFO *** epoch 6160, rolling-avg-loss (window=10)= 0.010105238467804156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:18,976 INFO epoch # 6161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00936930703028338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,057 INFO epoch # 6162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011052386194933206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,142 INFO epoch # 6163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01098534616176039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,223 INFO epoch # 6164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009242422558600083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,303 INFO epoch # 6165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012167020278866403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,386 INFO epoch # 6166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011548044058145024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,467 INFO epoch # 6167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012510955610196106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,550 INFO epoch # 6168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01150496584159555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,633 INFO epoch # 6169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009338181749626528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,720 INFO epoch # 6170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009649129322497174
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:19,720 INFO *** epoch 6170, rolling-avg-loss (window=10)= 0.010736775880650385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,800 INFO epoch # 6171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008803527249256149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,881 INFO epoch # 6172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008669179376738612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:19,963 INFO epoch # 6173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012575009764987044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,046 INFO epoch # 6174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010286405944498256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,126 INFO epoch # 6175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009480821383476723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,211 INFO epoch # 6176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012083631620043889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,291 INFO epoch # 6177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009765335904376116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,374 INFO epoch # 6178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010274029613356106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,455 INFO epoch # 6179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009610886212612968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,539 INFO epoch # 6180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011832636722829193
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:20,539 INFO *** epoch 6180, rolling-avg-loss (window=10)= 0.010338146379217506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,619 INFO epoch # 6181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012269713872228749
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,704 INFO epoch # 6182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010095443962200079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,784 INFO epoch # 6183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010841156225069426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,864 INFO epoch # 6184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008615231985459104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:20,944 INFO epoch # 6185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009165295276034158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,026 INFO epoch # 6186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013351631787372753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,107 INFO epoch # 6187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010205630955169909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,188 INFO epoch # 6188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009378519273013808
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,270 INFO epoch # 6189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008732881615287624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,351 INFO epoch # 6190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010652179364115
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:21,351 INFO *** epoch 6190, rolling-avg-loss (window=10)= 0.01033076843159506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,431 INFO epoch # 6191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00935378341091564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,511 INFO epoch # 6192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009001082842587493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,593 INFO epoch # 6193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009170467841613572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,676 INFO epoch # 6194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011780529515817761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,759 INFO epoch # 6195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010283115603670012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,839 INFO epoch # 6196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009344435013190378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:21,919 INFO epoch # 6197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009921557022607885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,001 INFO epoch # 6198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009583327657310292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,081 INFO epoch # 6199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010732951668614987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,166 INFO epoch # 6200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010178906166402157
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:22,166 INFO *** epoch 6200, rolling-avg-loss (window=10)= 0.009935015674273018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,253 INFO epoch # 6201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01005376064858865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,339 INFO epoch # 6202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009855768686975352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,422 INFO epoch # 6203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009670012645074166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,504 INFO epoch # 6204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009950861734978389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,583 INFO epoch # 6205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01020454749232158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,667 INFO epoch # 6206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010304186391294934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,749 INFO epoch # 6207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010929052761639468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,829 INFO epoch # 6208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010044190043117851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,910 INFO epoch # 6209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010079615080030635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:22,991 INFO epoch # 6210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009362676326418296
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:22,991 INFO *** epoch 6210, rolling-avg-loss (window=10)= 0.010045467181043932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,071 INFO epoch # 6211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009487159717536997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,156 INFO epoch # 6212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010243800112220924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,236 INFO epoch # 6213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009798763872822747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,317 INFO epoch # 6214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014508364445646293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,401 INFO epoch # 6215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010453098700963892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,483 INFO epoch # 6216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009843648236710578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,563 INFO epoch # 6217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010920185028226115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,646 INFO epoch # 6218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011842433115816675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,728 INFO epoch # 6219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009667891696153674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,808 INFO epoch # 6220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010543354714172892
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:23,808 INFO *** epoch 6220, rolling-avg-loss (window=10)= 0.01073086996402708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,890 INFO epoch # 6221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01284676935756579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:23,969 INFO epoch # 6222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011351211956935003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,049 INFO epoch # 6223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009575866104569286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,133 INFO epoch # 6224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01108442438999191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,213 INFO epoch # 6225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010405569031718187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,293 INFO epoch # 6226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011119779825094156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,375 INFO epoch # 6227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008783829296589829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,456 INFO epoch # 6228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009912468944094144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,536 INFO epoch # 6229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010175160801736638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,619 INFO epoch # 6230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013692090884433128
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:24,620 INFO *** epoch 6230, rolling-avg-loss (window=10)= 0.010894717059272807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,703 INFO epoch # 6231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00974182716163341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,783 INFO epoch # 6232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010512571832805406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,863 INFO epoch # 6233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009901745106617454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:24,943 INFO epoch # 6234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009201066764944699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,025 INFO epoch # 6235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010330493729270529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,105 INFO epoch # 6236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009618076321203262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,191 INFO epoch # 6237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010569166086497717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,270 INFO epoch # 6238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009684587726951577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,349 INFO epoch # 6239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009744800903717987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,429 INFO epoch # 6240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009199464846460614
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:25,430 INFO *** epoch 6240, rolling-avg-loss (window=10)= 0.009850380048010265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,514 INFO epoch # 6241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010528154205530882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,596 INFO epoch # 6242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010176100928219967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,680 INFO epoch # 6243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009846693646977656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,760 INFO epoch # 6244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008646339927508961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,840 INFO epoch # 6245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0118043113470776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:25,920 INFO epoch # 6246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009649491599702742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,002 INFO epoch # 6247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010885890973440837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,082 INFO epoch # 6248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0119144672062248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,165 INFO epoch # 6249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009175574698019773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,246 INFO epoch # 6250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009569427711539902
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:26,246 INFO *** epoch 6250, rolling-avg-loss (window=10)= 0.010219645224424312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,329 INFO epoch # 6251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012232943161507137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,409 INFO epoch # 6252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010131951101357117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,489 INFO epoch # 6253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010924983507720754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,568 INFO epoch # 6254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012440130318282172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,652 INFO epoch # 6255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010416587785584852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,733 INFO epoch # 6256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009843102743616328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,815 INFO epoch # 6257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0103118343395181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:26,949 INFO epoch # 6258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010008146215113811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,033 INFO epoch # 6259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009265664892154746
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,113 INFO epoch # 6260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011143390009237919
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:27,113 INFO *** epoch 6260, rolling-avg-loss (window=10)= 0.010671873407409294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,193 INFO epoch # 6261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008960088744061068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,274 INFO epoch # 6262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010061574306746479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,354 INFO epoch # 6263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009795636033231858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,436 INFO epoch # 6264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011330161767546088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,517 INFO epoch # 6265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010814373774337582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,596 INFO epoch # 6266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010782170051243156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,681 INFO epoch # 6267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011733020044630393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,761 INFO epoch # 6268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009953015178325586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,841 INFO epoch # 6269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009727296164783183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:27,921 INFO epoch # 6270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010860958165721968
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:27,922 INFO *** epoch 6270, rolling-avg-loss (window=10)= 0.010401829423062735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:28,003 INFO epoch # 6271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008357585116755217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:28,083 INFO epoch # 6272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008893159138096962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:28,167 INFO epoch # 6273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00909599376609549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:28,248 INFO epoch # 6274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010265015327604488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:28,328 INFO epoch # 6275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010371639022196177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:28,410 INFO epoch # 6276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008948202186729759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:28,496 INFO epoch # 6277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010555545472016092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:28,599 INFO epoch # 6278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010884699353482574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:28,688 INFO epoch # 6279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01003849789412925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:28,768 INFO epoch # 6280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00992021393176401
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:28,769 INFO *** epoch 6280, rolling-avg-loss (window=10)= 0.009733055120887003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:28,879 INFO epoch # 6281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008708608213055413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,015 INFO epoch # 6282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010808727063704282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,123 INFO epoch # 6283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010871473437873647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,207 INFO epoch # 6284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012120118510210887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,288 INFO epoch # 6285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009852292256255168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,367 INFO epoch # 6286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009225849324138835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,443 INFO epoch # 6287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008592237099946942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,518 INFO epoch # 6288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010184899780142587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,592 INFO epoch # 6289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010970133036607876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,669 INFO epoch # 6290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00908638854889432
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:29,669 INFO *** epoch 6290, rolling-avg-loss (window=10)= 0.010042072727082995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,745 INFO epoch # 6291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009895384340779856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,824 INFO epoch # 6292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010938322913716547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,900 INFO epoch # 6293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012436020384484436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:29,976 INFO epoch # 6294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010944642534013838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,054 INFO epoch # 6295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012131819254136644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,133 INFO epoch # 6296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009840891318162903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,210 INFO epoch # 6297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009382886790263001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,287 INFO epoch # 6298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009809215356654022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,364 INFO epoch # 6299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009586307729477994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,439 INFO epoch # 6300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009386737845488824
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:30,440 INFO *** epoch 6300, rolling-avg-loss (window=10)= 0.010435222846717806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,516 INFO epoch # 6301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013059720658930019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,594 INFO epoch # 6302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009454312465095427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,673 INFO epoch # 6303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014002523879753426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,751 INFO epoch # 6304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011162683353177272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,829 INFO epoch # 6305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009664608478487935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,906 INFO epoch # 6306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009238183803972788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:30,983 INFO epoch # 6307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008815373901597923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,060 INFO epoch # 6308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00948419859923888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,138 INFO epoch # 6309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010195962197030894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,215 INFO epoch # 6310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009535828692605719
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:31,215 INFO *** epoch 6310, rolling-avg-loss (window=10)= 0.010461339602989029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,292 INFO epoch # 6311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008611705452494789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,369 INFO epoch # 6312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009841073340794537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,446 INFO epoch # 6313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010109466049470939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,522 INFO epoch # 6314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009451414662180468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,598 INFO epoch # 6315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010206675695371814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,676 INFO epoch # 6316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010801899683428928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,753 INFO epoch # 6317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008972980023827404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,830 INFO epoch # 6318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00978244183352217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,907 INFO epoch # 6319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009807362454012036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:31,987 INFO epoch # 6320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010744453567895107
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:31,987 INFO *** epoch 6320, rolling-avg-loss (window=10)= 0.00983294727629982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,064 INFO epoch # 6321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009609084663679823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,141 INFO epoch # 6322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009082800286705606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,218 INFO epoch # 6323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009082917516934685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,296 INFO epoch # 6324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013956749739008956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,373 INFO epoch # 6325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009584938830812462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,449 INFO epoch # 6326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01076226461736951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,527 INFO epoch # 6327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009487069874012377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,604 INFO epoch # 6328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010979061255056877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,683 INFO epoch # 6329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00950174180616159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,760 INFO epoch # 6330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009479816042585298
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:32,761 INFO *** epoch 6330, rolling-avg-loss (window=10)= 0.010152644463232718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,837 INFO epoch # 6331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010212711989879608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,915 INFO epoch # 6332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010236291782348417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:32,992 INFO epoch # 6333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009503908149781637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,069 INFO epoch # 6334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00852024136838736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,151 INFO epoch # 6335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008859369852871168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,235 INFO epoch # 6336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009487242612522095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,311 INFO epoch # 6337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009957022903108737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,388 INFO epoch # 6338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011604621700826101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,465 INFO epoch # 6339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009718194633023813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,542 INFO epoch # 6340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009623058009310625
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:33,542 INFO *** epoch 6340, rolling-avg-loss (window=10)= 0.009772266300205956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,621 INFO epoch # 6341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00965458144492004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,699 INFO epoch # 6342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01639168155088555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,775 INFO epoch # 6343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011522667089593597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,852 INFO epoch # 6344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01041893551882822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:33,929 INFO epoch # 6345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009827327477978542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,006 INFO epoch # 6346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010647679548128508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,087 INFO epoch # 6347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008884861592378002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,177 INFO epoch # 6348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009113479987718165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,259 INFO epoch # 6349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009606485968106426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,338 INFO epoch # 6350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011024555613403209
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:34,339 INFO *** epoch 6350, rolling-avg-loss (window=10)= 0.010709225579194025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,420 INFO epoch # 6351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01175639704160858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,496 INFO epoch # 6352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011175002422532998
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,574 INFO epoch # 6353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01013133283413481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,654 INFO epoch # 6354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009958012247807346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,732 INFO epoch # 6355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008873338425473776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,808 INFO epoch # 6356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010624265458318405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,888 INFO epoch # 6357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01004888401075732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:34,964 INFO epoch # 6358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013939679192844778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,043 INFO epoch # 6359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009263163388823159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,121 INFO epoch # 6360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010386599496996496
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:35,121 INFO *** epoch 6360, rolling-avg-loss (window=10)= 0.010615667451929767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,199 INFO epoch # 6361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008587830074247904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,276 INFO epoch # 6362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008982286126411054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,353 INFO epoch # 6363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00903622150508454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,433 INFO epoch # 6364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009067213293747045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,513 INFO epoch # 6365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01031039586814586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,591 INFO epoch # 6366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009606696767150424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,670 INFO epoch # 6367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009938020390109159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,746 INFO epoch # 6368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011387806523998734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,823 INFO epoch # 6369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011300973448669538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,902 INFO epoch # 6370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011374621957656927
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:35,903 INFO *** epoch 6370, rolling-avg-loss (window=10)= 0.00995920659552212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:35,980 INFO epoch # 6371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010074018558952957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,057 INFO epoch # 6372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011682040902087465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,136 INFO epoch # 6373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008671603696711827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,212 INFO epoch # 6374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009142545910435729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,289 INFO epoch # 6375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009923497935233172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,366 INFO epoch # 6376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008200594245863613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,445 INFO epoch # 6377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008577186628826894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,521 INFO epoch # 6378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008706491265911609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,598 INFO epoch # 6379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00862385759683093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,676 INFO epoch # 6380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010332544523407705
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:36,676 INFO *** epoch 6380, rolling-avg-loss (window=10)= 0.00939343812642619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,754 INFO epoch # 6381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011138269474031404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,830 INFO epoch # 6382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010325008246582001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,908 INFO epoch # 6383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010261907482345123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:36,985 INFO epoch # 6384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010027738782810047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,065 INFO epoch # 6385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012330164972809143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,142 INFO epoch # 6386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012086162518244237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,219 INFO epoch # 6387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012609621859155595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,296 INFO epoch # 6388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011178284461493604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,373 INFO epoch # 6389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010793904737511184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,450 INFO epoch # 6390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009697832545498386
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:37,451 INFO *** epoch 6390, rolling-avg-loss (window=10)= 0.011044889508048073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,528 INFO epoch # 6391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01114497790695168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,605 INFO epoch # 6392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01157014117052313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,684 INFO epoch # 6393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010762436053482816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,761 INFO epoch # 6394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009339658819953911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,838 INFO epoch # 6395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010051061799458694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,916 INFO epoch # 6396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01003703112655785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:37,994 INFO epoch # 6397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009502847504336387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,071 INFO epoch # 6398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009656956608523615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,149 INFO epoch # 6399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010452777358295862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,226 INFO epoch # 6400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0101134908181848
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:38,226 INFO *** epoch 6400, rolling-avg-loss (window=10)= 0.010263137916626874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,303 INFO epoch # 6401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009488934883847833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,380 INFO epoch # 6402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012291593957343139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,458 INFO epoch # 6403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009857451019342989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,535 INFO epoch # 6404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010277808469254524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,612 INFO epoch # 6405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008133548624755349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,690 INFO epoch # 6406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009004764317069203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,767 INFO epoch # 6407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00995807280560257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,844 INFO epoch # 6408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010507671453524381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:38,922 INFO epoch # 6409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009303459868533537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,000 INFO epoch # 6410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009830066490394529
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:39,000 INFO *** epoch 6410, rolling-avg-loss (window=10)= 0.009865337188966806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,078 INFO epoch # 6411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011784070222347509
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,155 INFO epoch # 6412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010072217672131956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,233 INFO epoch # 6413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010466259904205799
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,310 INFO epoch # 6414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008963152220530901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,387 INFO epoch # 6415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010036793421022594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,464 INFO epoch # 6416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011230230535147712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,542 INFO epoch # 6417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013332783739315346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,620 INFO epoch # 6418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010031413679826073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,698 INFO epoch # 6419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014072062986087985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,775 INFO epoch # 6420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009477246370806824
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:39,775 INFO *** epoch 6420, rolling-avg-loss (window=10)= 0.01094662307514227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,851 INFO epoch # 6421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010370752184826415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:39,928 INFO epoch # 6422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008846179334796034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,006 INFO epoch # 6423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009056794944626745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,083 INFO epoch # 6424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01014239552023355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,162 INFO epoch # 6425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009633141162339598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,239 INFO epoch # 6426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01204955451976275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,315 INFO epoch # 6427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009055193251697347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,392 INFO epoch # 6428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011540689578396268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,469 INFO epoch # 6429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009771391720278189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,547 INFO epoch # 6430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008691092560184188
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:40,547 INFO *** epoch 6430, rolling-avg-loss (window=10)= 0.009915718477714108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,628 INFO epoch # 6431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009613297152100131
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,711 INFO epoch # 6432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010753041598945856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,792 INFO epoch # 6433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010094864468555897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,872 INFO epoch # 6434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010404989930975717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:40,954 INFO epoch # 6435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012475040181016084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,035 INFO epoch # 6436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015379434189526364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,112 INFO epoch # 6437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009595234085281845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,190 INFO epoch # 6438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009502822453214321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,269 INFO epoch # 6439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010721531245508231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,346 INFO epoch # 6440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009909606102155522
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:41,346 INFO *** epoch 6440, rolling-avg-loss (window=10)= 0.010844986140727996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,423 INFO epoch # 6441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01134643023397075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,503 INFO epoch # 6442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009894853217701893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,582 INFO epoch # 6443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008617545980087016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,660 INFO epoch # 6444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008718094701180235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,737 INFO epoch # 6445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010810564082930796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,820 INFO epoch # 6446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009838337784458417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,904 INFO epoch # 6447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00964347405533772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:41,983 INFO epoch # 6448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00990568072302267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,060 INFO epoch # 6449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011174468352692202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,138 INFO epoch # 6450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009090673811442684
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:42,139 INFO *** epoch 6450, rolling-avg-loss (window=10)= 0.009904012294282439
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,216 INFO epoch # 6451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009379815732245333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,292 INFO epoch # 6452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009869669796898961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,369 INFO epoch # 6453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009844331128988415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,446 INFO epoch # 6454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009793740537134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,523 INFO epoch # 6455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009886692238069372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,602 INFO epoch # 6456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008906714749173261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,679 INFO epoch # 6457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00904098189494107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,756 INFO epoch # 6458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010796863825817127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,832 INFO epoch # 6459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010860521440918092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,909 INFO epoch # 6460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010441491918754764
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:42,909 INFO *** epoch 6460, rolling-avg-loss (window=10)= 0.009882082326294039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:42,990 INFO epoch # 6461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009708434299682267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,077 INFO epoch # 6462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01041543815517798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,161 INFO epoch # 6463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00870331222540699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,240 INFO epoch # 6464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009227854614437092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,319 INFO epoch # 6465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009473789374169428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,397 INFO epoch # 6466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008566163560317364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,474 INFO epoch # 6467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008549368736566976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,552 INFO epoch # 6468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008336237973708194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,637 INFO epoch # 6469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010177901240240317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,732 INFO epoch # 6470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013344966515433043
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:43,732 INFO *** epoch 6470, rolling-avg-loss (window=10)= 0.009650346669513966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,811 INFO epoch # 6471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010703908359573688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,889 INFO epoch # 6472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009020961311762221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:43,968 INFO epoch # 6473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010458585413289256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,051 INFO epoch # 6474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012068769996403717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,137 INFO epoch # 6475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01192486594663933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,220 INFO epoch # 6476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010052208657725714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,302 INFO epoch # 6477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011487264309835155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,381 INFO epoch # 6478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009655293863033876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,458 INFO epoch # 6479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010092760006955359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,537 INFO epoch # 6480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008341433749592397
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:44,538 INFO *** epoch 6480, rolling-avg-loss (window=10)= 0.010380605161481071
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,619 INFO epoch # 6481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009120003902353346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,703 INFO epoch # 6482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009405331671587192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,784 INFO epoch # 6483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009778447230928577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,861 INFO epoch # 6484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008809473329165485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:44,938 INFO epoch # 6485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012559993105242029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,017 INFO epoch # 6486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009390710918523837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,094 INFO epoch # 6487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010451052854477894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,172 INFO epoch # 6488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010045536619145423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,248 INFO epoch # 6489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01016344728122931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,325 INFO epoch # 6490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009298949807998724
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:45,326 INFO *** epoch 6490, rolling-avg-loss (window=10)= 0.009902294672065182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,405 INFO epoch # 6491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008974822783784475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,483 INFO epoch # 6492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011166478143422864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,560 INFO epoch # 6493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01192859017464798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,640 INFO epoch # 6494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00886427396471845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,719 INFO epoch # 6495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008144376988639124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,796 INFO epoch # 6496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008335151404025964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,873 INFO epoch # 6497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010997417513863184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:45,951 INFO epoch # 6498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009149441131739877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,029 INFO epoch # 6499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009111232342547737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,106 INFO epoch # 6500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00996790735371178
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:46,106 INFO *** epoch 6500, rolling-avg-loss (window=10)= 0.009663969180110143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,183 INFO epoch # 6501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010355742197134532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,259 INFO epoch # 6502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012857160065323114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,336 INFO epoch # 6503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009057305585884023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,414 INFO epoch # 6504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008975502671091817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,490 INFO epoch # 6505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009324183076387271
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,567 INFO epoch # 6506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009749685843416955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,646 INFO epoch # 6507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009146254596998915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,724 INFO epoch # 6508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010650380099832546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,800 INFO epoch # 6509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007718493623542599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,876 INFO epoch # 6510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008765990743995644
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:46,877 INFO *** epoch 6510, rolling-avg-loss (window=10)= 0.009660069850360742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:46,953 INFO epoch # 6511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009527769892883953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,033 INFO epoch # 6512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010616909348755144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,109 INFO epoch # 6513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010294876818079501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,186 INFO epoch # 6514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010507191342185251
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,263 INFO epoch # 6515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009968214348191395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,342 INFO epoch # 6516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010597158994642086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,420 INFO epoch # 6517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012764244085701648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,500 INFO epoch # 6518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009003943065181375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,578 INFO epoch # 6519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01017900682927575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,658 INFO epoch # 6520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009299690733314492
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:47,658 INFO *** epoch 6520, rolling-avg-loss (window=10)= 0.01027590054582106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,737 INFO epoch # 6521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012885499148978852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,816 INFO epoch # 6522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009099160713958554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,896 INFO epoch # 6523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008759417396504432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:47,976 INFO epoch # 6524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009706380631541833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,056 INFO epoch # 6525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010706820612540469
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,134 INFO epoch # 6526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00962187493860256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,211 INFO epoch # 6527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008535094209946692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,288 INFO epoch # 6528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0084686916670762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,366 INFO epoch # 6529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01030923907092074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,445 INFO epoch # 6530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008897332947526593
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:48,445 INFO *** epoch 6530, rolling-avg-loss (window=10)= 0.009698951133759692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,522 INFO epoch # 6531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009735076113429386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,600 INFO epoch # 6532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011100136958702933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,679 INFO epoch # 6533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01035210574627854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,770 INFO epoch # 6534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008611534467490856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,855 INFO epoch # 6535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009812798562052194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:48,934 INFO epoch # 6536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009535693330690265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,012 INFO epoch # 6537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009800253508728929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,093 INFO epoch # 6538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010392776675871573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,170 INFO epoch # 6539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010676660051103681
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,246 INFO epoch # 6540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009577098877343815
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:49,246 INFO *** epoch 6540, rolling-avg-loss (window=10)= 0.009959413429169218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,323 INFO epoch # 6541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010944468289380893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,399 INFO epoch # 6542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009380614021210931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,476 INFO epoch # 6543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010045106551842764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,553 INFO epoch # 6544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010879081062739715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,631 INFO epoch # 6545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010683618740586098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,708 INFO epoch # 6546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010674602628569119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,785 INFO epoch # 6547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009779906366020441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,861 INFO epoch # 6548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010465916981047485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:49,938 INFO epoch # 6549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008376357895031106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,017 INFO epoch # 6550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011563779364223592
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:50,017 INFO *** epoch 6550, rolling-avg-loss (window=10)= 0.010279345190065214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,094 INFO epoch # 6551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012220224722113926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,172 INFO epoch # 6552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011954925153986551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,249 INFO epoch # 6553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008867050819389988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,326 INFO epoch # 6554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009899914213747252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,408 INFO epoch # 6555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010588924327748828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,485 INFO epoch # 6556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008823618496535346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,562 INFO epoch # 6557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009901109471684322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,640 INFO epoch # 6558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010458700511662755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,719 INFO epoch # 6559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009268968104152009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,796 INFO epoch # 6560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009340184355096426
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:50,796 INFO *** epoch 6560, rolling-avg-loss (window=10)= 0.01013236201761174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,873 INFO epoch # 6561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009538036654703319
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:50,950 INFO epoch # 6562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011046349827665836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,028 INFO epoch # 6563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009940246760379523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,105 INFO epoch # 6564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01347630909003783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,182 INFO epoch # 6565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010941723223368172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,259 INFO epoch # 6566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011343871206918266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,335 INFO epoch # 6567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009659377705247607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,412 INFO epoch # 6568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009630836197175086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,489 INFO epoch # 6569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010529295046580955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,566 INFO epoch # 6570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009871111040411051
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:51,566 INFO *** epoch 6570, rolling-avg-loss (window=10)= 0.010597715675248764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,644 INFO epoch # 6571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012487458203395363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,721 INFO epoch # 6572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009601586949429475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,801 INFO epoch # 6573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008942539563577157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,878 INFO epoch # 6574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009304745341069065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:51,955 INFO epoch # 6575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009323718630184885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,033 INFO epoch # 6576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010244838485959917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,109 INFO epoch # 6577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009196180675644428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,188 INFO epoch # 6578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009548644127789885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,264 INFO epoch # 6579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011043315185816027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,341 INFO epoch # 6580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009150810503342655
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:52,341 INFO *** epoch 6580, rolling-avg-loss (window=10)= 0.009884383766620885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,417 INFO epoch # 6581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008955398167017847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,493 INFO epoch # 6582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0084628766780952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,570 INFO epoch # 6583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010826089659531135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,647 INFO epoch # 6584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01120181406440679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,723 INFO epoch # 6585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010295005871739704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,800 INFO epoch # 6586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011089824882219546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,876 INFO epoch # 6587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011490769735246431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:52,953 INFO epoch # 6588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008904288275516592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,030 INFO epoch # 6589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00932794356776867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,108 INFO epoch # 6590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010100778090418316
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:53,108 INFO *** epoch 6590, rolling-avg-loss (window=10)= 0.010065478899196022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,185 INFO epoch # 6591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011221514563658275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,262 INFO epoch # 6592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008179704957001377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,338 INFO epoch # 6593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009805246685573366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,415 INFO epoch # 6594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011379741961718537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,492 INFO epoch # 6595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009832305382587947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,568 INFO epoch # 6596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008925313770305365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,652 INFO epoch # 6597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008595442959631328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,732 INFO epoch # 6598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008481520664645359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,810 INFO epoch # 6599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010251786712615285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,886 INFO epoch # 6600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008389625829295255
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:53,887 INFO *** epoch 6600, rolling-avg-loss (window=10)= 0.00950622034870321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:53,963 INFO epoch # 6601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00913727354054572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,040 INFO epoch # 6602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010491709457710385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,122 INFO epoch # 6603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012672691998886876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,203 INFO epoch # 6604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009439402114367113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,282 INFO epoch # 6605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008162928599631414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,361 INFO epoch # 6606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010380116822489072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,438 INFO epoch # 6607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010596691427053884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,518 INFO epoch # 6608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008866002375725657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,595 INFO epoch # 6609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00906931264034938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,672 INFO epoch # 6610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010603275462926831
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:54,672 INFO *** epoch 6610, rolling-avg-loss (window=10)= 0.009941940443968632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,749 INFO epoch # 6611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011018005116056884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,828 INFO epoch # 6612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010785556238261051
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,905 INFO epoch # 6613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010957478385535069
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:54,981 INFO epoch # 6614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010401638777693734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,060 INFO epoch # 6615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00981670938199386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,137 INFO epoch # 6616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010481041041202843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,214 INFO epoch # 6617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008776875809417106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,291 INFO epoch # 6618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010190665343543515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,367 INFO epoch # 6619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008783954101090785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,443 INFO epoch # 6620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010506458667805418
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:55,443 INFO *** epoch 6620, rolling-avg-loss (window=10)= 0.010171838286260027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,520 INFO epoch # 6621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01034020802762825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,597 INFO epoch # 6622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009420231268450152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,677 INFO epoch # 6623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010914049795246683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,754 INFO epoch # 6624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01032006141031161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,832 INFO epoch # 6625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009873470233287662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,909 INFO epoch # 6626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009254062948457431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:55,987 INFO epoch # 6627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008589210483478382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,065 INFO epoch # 6628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008918452935176902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,145 INFO epoch # 6629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010033513157395646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,223 INFO epoch # 6630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009905303100822493
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:56,223 INFO *** epoch 6630, rolling-avg-loss (window=10)= 0.009756856336025521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,302 INFO epoch # 6631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010565390060946811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,379 INFO epoch # 6632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011434826868935488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,456 INFO epoch # 6633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011447640201367904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,533 INFO epoch # 6634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009930647203873377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,612 INFO epoch # 6635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008715882395335939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,691 INFO epoch # 6636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008715784235391766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,770 INFO epoch # 6637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010092240059748292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,846 INFO epoch # 6638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011175158091646153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:56,926 INFO epoch # 6639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011148089251946658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,005 INFO epoch # 6640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008583575698139612
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:57,005 INFO *** epoch 6640, rolling-avg-loss (window=10)= 0.0101809234067332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,084 INFO epoch # 6641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008866562158800662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,165 INFO epoch # 6642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010131744958925992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,244 INFO epoch # 6643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00983599697065074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,320 INFO epoch # 6644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008661062645842321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,400 INFO epoch # 6645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010153062365134247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,477 INFO epoch # 6646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009436898319108877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,554 INFO epoch # 6647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009659450894105248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,634 INFO epoch # 6648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00812731673795497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,711 INFO epoch # 6649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008979685022495687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,787 INFO epoch # 6650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011886286825756542
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:57,788 INFO *** epoch 6650, rolling-avg-loss (window=10)= 0.009573806689877529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,865 INFO epoch # 6651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010029728538938798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:57,942 INFO epoch # 6652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010815696947247488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,021 INFO epoch # 6653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00842883657605853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,099 INFO epoch # 6654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00967413728358224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,178 INFO epoch # 6655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008286238306027371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,255 INFO epoch # 6656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009529038958135061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,332 INFO epoch # 6657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009614508562663104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,410 INFO epoch # 6658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008616018229076872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,487 INFO epoch # 6659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009986633798689581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,564 INFO epoch # 6660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01063222145603504
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:58,564 INFO *** epoch 6660, rolling-avg-loss (window=10)= 0.009561305865645409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,644 INFO epoch # 6661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009856648102868348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,720 INFO epoch # 6662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01007291508722119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,799 INFO epoch # 6663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009314582552178763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,876 INFO epoch # 6664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010410528440843336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:58,953 INFO epoch # 6665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009939157695043832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,031 INFO epoch # 6666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010746157582616434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,111 INFO epoch # 6667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010059335676487535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,188 INFO epoch # 6668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008632974982901942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,265 INFO epoch # 6669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010732390655903146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,342 INFO epoch # 6670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010660732645192184
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:22:59,342 INFO *** epoch 6670, rolling-avg-loss (window=10)= 0.010042542342125671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,420 INFO epoch # 6671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009656088164774701
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,496 INFO epoch # 6672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009254521886759903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,574 INFO epoch # 6673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009567407854774501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,654 INFO epoch # 6674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00970436487114057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,731 INFO epoch # 6675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009044994614669122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,808 INFO epoch # 6676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008544078620616347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,885 INFO epoch # 6677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008903176560124848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:22:59,962 INFO epoch # 6678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01102663649362512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,039 INFO epoch # 6679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010301064001396298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,115 INFO epoch # 6680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009466485185839701
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:00,115 INFO *** epoch 6680, rolling-avg-loss (window=10)= 0.009546881825372111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,213 INFO epoch # 6681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010044595983345062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,312 INFO epoch # 6682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008305583163746633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,393 INFO epoch # 6683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007924322751932777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,471 INFO epoch # 6684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00850705480115721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,550 INFO epoch # 6685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00899239787395345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,630 INFO epoch # 6686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011804832116467878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,714 INFO epoch # 6687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007997848566446919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,793 INFO epoch # 6688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009047913838003296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,871 INFO epoch # 6689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010148394605494104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:00,947 INFO epoch # 6690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012013947707600892
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:00,948 INFO *** epoch 6690, rolling-avg-loss (window=10)= 0.009478689140814822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,026 INFO epoch # 6691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009060777207196224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,102 INFO epoch # 6692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009878720120468643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,181 INFO epoch # 6693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009367590719193686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,259 INFO epoch # 6694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012086498623830266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,337 INFO epoch # 6695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012715094213490374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,414 INFO epoch # 6696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008389009410166182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,491 INFO epoch # 6697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008288485929369926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,569 INFO epoch # 6698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008201457407267299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,655 INFO epoch # 6699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010201711993431672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,734 INFO epoch # 6700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010130813148862217
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:01,734 INFO *** epoch 6700, rolling-avg-loss (window=10)= 0.00983201587732765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,814 INFO epoch # 6701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01210539681778755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,892 INFO epoch # 6702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009703878284199163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:01,973 INFO epoch # 6703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009486907554673962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,053 INFO epoch # 6704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010706670771469362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,133 INFO epoch # 6705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008455792973109055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,212 INFO epoch # 6706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008919429033994675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,289 INFO epoch # 6707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008234123772126622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,367 INFO epoch # 6708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010283813244313933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,446 INFO epoch # 6709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009312645153841004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,522 INFO epoch # 6710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011276420424110256
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:02,523 INFO *** epoch 6710, rolling-avg-loss (window=10)= 0.009848507802962559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,599 INFO epoch # 6711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008959734652307816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,676 INFO epoch # 6712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00887686206260696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,752 INFO epoch # 6713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009039664233569056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,829 INFO epoch # 6714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008504912053467706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,905 INFO epoch # 6715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011087813196354546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:02,982 INFO epoch # 6716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009908006337354891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,061 INFO epoch # 6717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010800160773214884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,138 INFO epoch # 6718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01070106167026097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,214 INFO epoch # 6719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009081691358005628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,294 INFO epoch # 6720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009416007276740856
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:03,294 INFO *** epoch 6720, rolling-avg-loss (window=10)= 0.009637591361388332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,371 INFO epoch # 6721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009878083437797613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,448 INFO epoch # 6722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010069347379612736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,526 INFO epoch # 6723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008581656213209499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,603 INFO epoch # 6724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008589271245000418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,683 INFO epoch # 6725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010326259536668658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,760 INFO epoch # 6726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008773822744842619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,837 INFO epoch # 6727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008870036203006748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,914 INFO epoch # 6728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009153224687906913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:03,992 INFO epoch # 6729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011186610114236828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,070 INFO epoch # 6730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01145692060526926
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:04,070 INFO *** epoch 6730, rolling-avg-loss (window=10)= 0.009688523216755129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,150 INFO epoch # 6731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009377770766150206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,228 INFO epoch # 6732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009904151818773244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,305 INFO epoch # 6733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008508631835866254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,382 INFO epoch # 6734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00951878813066287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,460 INFO epoch # 6735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010926832183031365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,537 INFO epoch # 6736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012339175395027269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,614 INFO epoch # 6737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009857905220997054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,695 INFO epoch # 6738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00882590747642098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,772 INFO epoch # 6739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009515439080132637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,850 INFO epoch # 6740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009425004871445708
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:04,850 INFO *** epoch 6740, rolling-avg-loss (window=10)= 0.009819960677850759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:04,928 INFO epoch # 6741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00824718561489135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,006 INFO epoch # 6742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009765979018993676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,082 INFO epoch # 6743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013026698739849962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,161 INFO epoch # 6744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009827097223023884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,237 INFO epoch # 6745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010068265386507846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,315 INFO epoch # 6746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009637555747758597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,392 INFO epoch # 6747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009004969499073923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,468 INFO epoch # 6748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009616890325560234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,545 INFO epoch # 6749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00904748729954008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,623 INFO epoch # 6750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009162559821561445
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:05,623 INFO *** epoch 6750, rolling-avg-loss (window=10)= 0.0097404688676761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,700 INFO epoch # 6751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009032702815602534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,778 INFO epoch # 6752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008662207234010566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,856 INFO epoch # 6753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009744058348587714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:05,933 INFO epoch # 6754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008975725242635235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,010 INFO epoch # 6755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009010815883812029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,088 INFO epoch # 6756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00928497601125855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,166 INFO epoch # 6757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00939294723502826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,243 INFO epoch # 6758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00933570327470079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,322 INFO epoch # 6759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00923421120387502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,400 INFO epoch # 6760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00871784768969519
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:06,400 INFO *** epoch 6760, rolling-avg-loss (window=10)= 0.00913911949392059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,477 INFO epoch # 6761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009242317340977024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,554 INFO epoch # 6762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00981612729083281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,633 INFO epoch # 6763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008510488652973436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,710 INFO epoch # 6764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010147147841053084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,790 INFO epoch # 6765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008032545920286793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,866 INFO epoch # 6766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00997913761966629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:06,943 INFO epoch # 6767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00884469378797803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,023 INFO epoch # 6768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008944077126216143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,102 INFO epoch # 6769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009809794428292662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,180 INFO epoch # 6770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008506424150255043
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:07,180 INFO *** epoch 6770, rolling-avg-loss (window=10)= 0.009183275415853131
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,259 INFO epoch # 6771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008850472630001605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,337 INFO epoch # 6772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00899811399722239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,415 INFO epoch # 6773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010269319318467751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,491 INFO epoch # 6774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009182747278828174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,569 INFO epoch # 6775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009016022384457756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,647 INFO epoch # 6776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008923202185542323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,728 INFO epoch # 6777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01206192173413001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,805 INFO epoch # 6778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009143739691353403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,882 INFO epoch # 6779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009208451490849257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:07,961 INFO epoch # 6780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011677534057525918
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:07,962 INFO *** epoch 6780, rolling-avg-loss (window=10)= 0.00973315247683786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,045 INFO epoch # 6781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009200874192174524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,123 INFO epoch # 6782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007699952802795451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,204 INFO epoch # 6783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011324610073643271
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,283 INFO epoch # 6784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009650783453253098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,364 INFO epoch # 6785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010071132965094876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,443 INFO epoch # 6786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008442791244306136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,523 INFO epoch # 6787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011621605677646585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,599 INFO epoch # 6788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009430021316802595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,679 INFO epoch # 6789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009325314436864574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,759 INFO epoch # 6790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008822352850984316
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:08,759 INFO *** epoch 6790, rolling-avg-loss (window=10)= 0.009558943901356542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,837 INFO epoch # 6791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008763399855524767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,915 INFO epoch # 6792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009738085936987773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:08,992 INFO epoch # 6793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010134968113561627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,070 INFO epoch # 6794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01265187947137747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,149 INFO epoch # 6795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009809725313971285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,226 INFO epoch # 6796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010123462154297158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,303 INFO epoch # 6797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008773972193012014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,381 INFO epoch # 6798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009016721029183827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,459 INFO epoch # 6799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010204360936768353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,536 INFO epoch # 6800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010037657506472897
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:09,536 INFO *** epoch 6800, rolling-avg-loss (window=10)= 0.009925423251115718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,615 INFO epoch # 6801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009817703896260355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,694 INFO epoch # 6802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009530197057756595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,776 INFO epoch # 6803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010680282466637436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,856 INFO epoch # 6804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010449531328049488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:09,935 INFO epoch # 6805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009035020651936065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,016 INFO epoch # 6806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009489763921010308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,100 INFO epoch # 6807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010780631368106697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,177 INFO epoch # 6808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00849868579825852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,255 INFO epoch # 6809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010871488208067603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,332 INFO epoch # 6810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007754245489195455
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:10,332 INFO *** epoch 6810, rolling-avg-loss (window=10)= 0.009690755018527852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,410 INFO epoch # 6811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008335681646713056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,487 INFO epoch # 6812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076278654305497184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,564 INFO epoch # 6813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010398062033345923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,642 INFO epoch # 6814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010078215069370344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,722 INFO epoch # 6815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009772231744136661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,798 INFO epoch # 6816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008948701135523152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,876 INFO epoch # 6817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010577647750324104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:10,953 INFO epoch # 6818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011301434897177387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,032 INFO epoch # 6819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009862919585430063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,109 INFO epoch # 6820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00917728944477858
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:11,109 INFO *** epoch 6820, rolling-avg-loss (window=10)= 0.009608004873734899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,187 INFO epoch # 6821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009248726135410834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,264 INFO epoch # 6822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008514721092069522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,344 INFO epoch # 6823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011072604385844897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,422 INFO epoch # 6824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009236652127583511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,501 INFO epoch # 6825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00943897544493666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,581 INFO epoch # 6826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009803137174458243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,665 INFO epoch # 6827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00865527553105494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,746 INFO epoch # 6828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010599119879771024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,828 INFO epoch # 6829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008640675718197599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,908 INFO epoch # 6830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008281135007564444
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:11,908 INFO *** epoch 6830, rolling-avg-loss (window=10)= 0.009349102249689167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:11,985 INFO epoch # 6831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008719665085664019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,062 INFO epoch # 6832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009283403676818125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,140 INFO epoch # 6833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010876143256609794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,217 INFO epoch # 6834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009523627973976545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,294 INFO epoch # 6835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009058611394721083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,372 INFO epoch # 6836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007692381695960648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,451 INFO epoch # 6837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007964076125063002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,528 INFO epoch # 6838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009471316043345723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,604 INFO epoch # 6839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008795077803370077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,685 INFO epoch # 6840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009935288166161627
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:12,685 INFO *** epoch 6840, rolling-avg-loss (window=10)= 0.009131959122169065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,764 INFO epoch # 6841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009515088400803506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,842 INFO epoch # 6842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008714632807823364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,918 INFO epoch # 6843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01031781654455699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:12,995 INFO epoch # 6844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0110776323126629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,072 INFO epoch # 6845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009743226939463057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,153 INFO epoch # 6846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008847684017382562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,229 INFO epoch # 6847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010544947435846552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,305 INFO epoch # 6848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008604420276242308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,382 INFO epoch # 6849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009587612061295658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,459 INFO epoch # 6850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009206967828504276
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:13,459 INFO *** epoch 6850, rolling-avg-loss (window=10)= 0.009616002862458117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,535 INFO epoch # 6851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010880628411541693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,612 INFO epoch # 6852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009445634117582813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,690 INFO epoch # 6853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009581847734807525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,767 INFO epoch # 6854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009082480275537819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,844 INFO epoch # 6855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010329383963835426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,920 INFO epoch # 6856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010340706459828652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:13,996 INFO epoch # 6857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00890387233812362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,073 INFO epoch # 6858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008569693236495368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,152 INFO epoch # 6859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009680444840341806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,228 INFO epoch # 6860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009402011914062314
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:14,228 INFO *** epoch 6860, rolling-avg-loss (window=10)= 0.009621670329215704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,304 INFO epoch # 6861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011670593899907544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,383 INFO epoch # 6862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010889024342759512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,459 INFO epoch # 6863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010277896020852495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,536 INFO epoch # 6864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008981776176369749
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,613 INFO epoch # 6865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008623187350167427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,693 INFO epoch # 6866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010823268050444312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,770 INFO epoch # 6867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014318582761916332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,846 INFO epoch # 6868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020740089938044548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,922 INFO epoch # 6869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013094865425955504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:14,999 INFO epoch # 6870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008767004459514283
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:14,999 INFO *** epoch 6870, rolling-avg-loss (window=10)= 0.01181862884259317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,076 INFO epoch # 6871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009371943277074024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,154 INFO epoch # 6872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009639544761739671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,231 INFO epoch # 6873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013768834571237676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,308 INFO epoch # 6874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013994161105074454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,386 INFO epoch # 6875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009465700837608892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,463 INFO epoch # 6876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00928246915282216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,541 INFO epoch # 6877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009466562187299132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,618 INFO epoch # 6878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008935076533816755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,696 INFO epoch # 6879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00808770683943294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,773 INFO epoch # 6880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009511594427749515
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:15,773 INFO *** epoch 6880, rolling-avg-loss (window=10)= 0.010152359369385522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,849 INFO epoch # 6881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008513350920111407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:15,926 INFO epoch # 6882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007974867468874436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,003 INFO epoch # 6883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007645080127986148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,081 INFO epoch # 6884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008755101625865791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,159 INFO epoch # 6885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009421427035704255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,236 INFO epoch # 6886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00794298698019702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,314 INFO epoch # 6887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009360875621496234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,393 INFO epoch # 6888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01108291040873155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,471 INFO epoch # 6889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008818361820885912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,548 INFO epoch # 6890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008385711844312027
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:16,548 INFO *** epoch 6890, rolling-avg-loss (window=10)= 0.008790067385416479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,638 INFO epoch # 6891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008638844330562279
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,726 INFO epoch # 6892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011756128238630481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,806 INFO epoch # 6893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010686509507650044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,884 INFO epoch # 6894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008678517762746196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:16,960 INFO epoch # 6895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009341572746052407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,037 INFO epoch # 6896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008194705544156022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,113 INFO epoch # 6897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008785783415078185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,192 INFO epoch # 6898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008058280443947297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,269 INFO epoch # 6899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008486259306664579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,347 INFO epoch # 6900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008262474751973059
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:17,347 INFO *** epoch 6900, rolling-avg-loss (window=10)= 0.009088907604746056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,424 INFO epoch # 6901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010682586143957451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,501 INFO epoch # 6902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011057160125346854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,578 INFO epoch # 6903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007851554910303093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,657 INFO epoch # 6904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008055704478465486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,733 INFO epoch # 6905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009965291246771812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,809 INFO epoch # 6906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009152194572379813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,885 INFO epoch # 6907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008362266424228437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:17,961 INFO epoch # 6908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01159576176723931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,038 INFO epoch # 6909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012996557692531496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,116 INFO epoch # 6910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009763341302459594
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:18,116 INFO *** epoch 6910, rolling-avg-loss (window=10)= 0.009948241866368335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,195 INFO epoch # 6911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010666765418136492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,271 INFO epoch # 6912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011931108325370587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,348 INFO epoch # 6913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008790393418166786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,426 INFO epoch # 6914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010807788275997154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,505 INFO epoch # 6915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010209294297965243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,582 INFO epoch # 6916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00883333547244547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,661 INFO epoch # 6917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009886720712529495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,738 INFO epoch # 6918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01034748058009427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,814 INFO epoch # 6919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0115986620367039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,890 INFO epoch # 6920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009441104455618188
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:18,890 INFO *** epoch 6920, rolling-avg-loss (window=10)= 0.010251265299302759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:18,966 INFO epoch # 6921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008212439373892266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,042 INFO epoch # 6922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00845450099586742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,123 INFO epoch # 6923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010595299252599943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,199 INFO epoch # 6924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014797392388572916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,275 INFO epoch # 6925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012604488176293671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,352 INFO epoch # 6926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008198833013011608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,429 INFO epoch # 6927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0093799574606237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,505 INFO epoch # 6928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009364470301079564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,581 INFO epoch # 6929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010225315032585058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,662 INFO epoch # 6930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008481011078401934
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:19,662 INFO *** epoch 6930, rolling-avg-loss (window=10)= 0.010031370707292809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,738 INFO epoch # 6931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00869193966354942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,815 INFO epoch # 6932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008697545796167105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,892 INFO epoch # 6933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00799056265532272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:19,968 INFO epoch # 6934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008309616045153234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,044 INFO epoch # 6935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010329974713386036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,122 INFO epoch # 6936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007878429998527281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,199 INFO epoch # 6937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008030944583879318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,275 INFO epoch # 6938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008459089840471279
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,352 INFO epoch # 6939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009911179484333843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,436 INFO epoch # 6940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007958548732858617
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:20,436 INFO *** epoch 6940, rolling-avg-loss (window=10)= 0.008625783151364886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,513 INFO epoch # 6941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009597981523256749
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,593 INFO epoch # 6942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009640117365051992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,673 INFO epoch # 6943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00964183558971854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,750 INFO epoch # 6944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0096657264730311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,827 INFO epoch # 6945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009192940968205221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,907 INFO epoch # 6946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007707855045737233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:20,983 INFO epoch # 6947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008519227762008086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,060 INFO epoch # 6948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010896012361627072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,138 INFO epoch # 6949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008278632500150707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,216 INFO epoch # 6950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009755201885127462
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:21,216 INFO *** epoch 6950, rolling-avg-loss (window=10)= 0.009289553147391416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,296 INFO epoch # 6951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00854600066668354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,375 INFO epoch # 6952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008925504094804637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,452 INFO epoch # 6953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008348187649971806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,529 INFO epoch # 6954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009303502563852817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,615 INFO epoch # 6955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007749403310299385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,703 INFO epoch # 6956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009760483226273209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,786 INFO epoch # 6957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00906554852554109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,866 INFO epoch # 6958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009185951203107834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:21,959 INFO epoch # 6959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009220249332429375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,044 INFO epoch # 6960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011617148367804475
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:22,044 INFO *** epoch 6960, rolling-avg-loss (window=10)= 0.009172197894076816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,124 INFO epoch # 6961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008035409744479693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,204 INFO epoch # 6962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00806802422448527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,285 INFO epoch # 6963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008545192751626018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,363 INFO epoch # 6964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011688601764035411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,441 INFO epoch # 6965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010618401873216499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,519 INFO epoch # 6966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009562313614878803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,598 INFO epoch # 6967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011582788793020882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,677 INFO epoch # 6968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008610914388555102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,755 INFO epoch # 6969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0150426435720874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,831 INFO epoch # 6970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010474491151398979
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:22,832 INFO *** epoch 6970, rolling-avg-loss (window=10)= 0.010222878187778405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,908 INFO epoch # 6971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008315933737321757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:22,986 INFO epoch # 6972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008947219350375235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,063 INFO epoch # 6973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008890993405657355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,141 INFO epoch # 6974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008278844601591118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,220 INFO epoch # 6975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009025512488733511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,300 INFO epoch # 6976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010649805699358694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,381 INFO epoch # 6977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010104859509738162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,460 INFO epoch # 6978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016567177342949435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,540 INFO epoch # 6979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008617429724836256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,618 INFO epoch # 6980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009116243192693219
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:23,618 INFO *** epoch 6980, rolling-avg-loss (window=10)= 0.009851401905325474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,695 INFO epoch # 6981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010832759755430743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,771 INFO epoch # 6982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008611351877334528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,849 INFO epoch # 6983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009154817336820997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:23,929 INFO epoch # 6984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011741376452846453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,008 INFO epoch # 6985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008768342260736972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,090 INFO epoch # 6986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008443674916634336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,170 INFO epoch # 6987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009186258539557457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,247 INFO epoch # 6988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008732263871934265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,324 INFO epoch # 6989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010829441642272286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,403 INFO epoch # 6990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010433610397740267
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:24,403 INFO *** epoch 6990, rolling-avg-loss (window=10)= 0.00967338970513083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,481 INFO epoch # 6991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010381414045696147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,566 INFO epoch # 6992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009763404203113168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,649 INFO epoch # 6993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008189335276256315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,732 INFO epoch # 6994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008733482471143361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,818 INFO epoch # 6995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008413718052906916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,900 INFO epoch # 6996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009451821926631965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:24,984 INFO epoch # 6997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009738495282363147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,085 INFO epoch # 6998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010156148353416938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,179 INFO epoch # 6999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009842651925282553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,262 INFO epoch # 7000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009213944300427102
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:25,262 INFO *** epoch 7000, rolling-avg-loss (window=10)= 0.009388441583723761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,342 INFO epoch # 7001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008978683647001162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,424 INFO epoch # 7002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00889138261845801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,504 INFO epoch # 7003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009082366072107106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,587 INFO epoch # 7004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008116162942314986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,668 INFO epoch # 7005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008672521857079118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,749 INFO epoch # 7006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009247733614756726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,829 INFO epoch # 7007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009830108996538911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,910 INFO epoch # 7008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009298362187109888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:25,988 INFO epoch # 7009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010345367423724383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,069 INFO epoch # 7010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0092794674783363
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:26,069 INFO *** epoch 7010, rolling-avg-loss (window=10)= 0.00917421568374266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,150 INFO epoch # 7011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009739152592374012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,232 INFO epoch # 7012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007862496109737549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,312 INFO epoch # 7013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00899706609925488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,391 INFO epoch # 7014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009146252246864606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,473 INFO epoch # 7015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008579655492212623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,552 INFO epoch # 7016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013364037804421969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,634 INFO epoch # 7017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009561528095218819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,731 INFO epoch # 7018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010071523807710037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,817 INFO epoch # 7019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009533433098113164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,901 INFO epoch # 7020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011332687281537801
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:26,901 INFO *** epoch 7020, rolling-avg-loss (window=10)= 0.009818783262744546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:26,982 INFO epoch # 7021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008039852320507634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,066 INFO epoch # 7022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009418456073035486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,148 INFO epoch # 7023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012394181219860911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,233 INFO epoch # 7024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008755217975704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,316 INFO epoch # 7025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009543920496071223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,400 INFO epoch # 7026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009547871923132334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,483 INFO epoch # 7027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009226489550201222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,566 INFO epoch # 7028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010907702679105569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,650 INFO epoch # 7029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012708982249023393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,732 INFO epoch # 7030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010937988248770125
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:27,732 INFO *** epoch 7030, rolling-avg-loss (window=10)= 0.01014806627354119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,815 INFO epoch # 7031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009742157184518874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,895 INFO epoch # 7032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009482194611337036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:27,977 INFO epoch # 7033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009298208824475296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,056 INFO epoch # 7034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009015786890813615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,138 INFO epoch # 7035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008712562266737223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,218 INFO epoch # 7036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00921352869772818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,300 INFO epoch # 7037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007997206019354053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,382 INFO epoch # 7038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009078621413209476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,464 INFO epoch # 7039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01060439481807407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,542 INFO epoch # 7040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010885759358643554
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:28,542 INFO *** epoch 7040, rolling-avg-loss (window=10)= 0.009403042008489137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,619 INFO epoch # 7041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00927963136928156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,696 INFO epoch # 7042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00943782263493631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,773 INFO epoch # 7043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00914769052178599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,849 INFO epoch # 7044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009023968945257366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:28,926 INFO epoch # 7045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010112142328580376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,003 INFO epoch # 7046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009372569213155657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,081 INFO epoch # 7047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009197766899887938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,158 INFO epoch # 7048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009039194897923153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,235 INFO epoch # 7049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00770769632072188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,312 INFO epoch # 7050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010815197296324186
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:29,312 INFO *** epoch 7050, rolling-avg-loss (window=10)= 0.00931336804278544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,388 INFO epoch # 7051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009509087132755667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,464 INFO epoch # 7052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007852716800698545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,541 INFO epoch # 7053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007834588846890256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,618 INFO epoch # 7054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008327577757881954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,695 INFO epoch # 7055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010072479773953091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,772 INFO epoch # 7056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01172492604382569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,849 INFO epoch # 7057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009752468431543093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:29,925 INFO epoch # 7058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007697091918089427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,003 INFO epoch # 7059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010173200593271758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,080 INFO epoch # 7060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008633424069557805
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:30,080 INFO *** epoch 7060, rolling-avg-loss (window=10)= 0.009157756136846728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,157 INFO epoch # 7061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009090283274417743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,235 INFO epoch # 7062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007865052626584657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,316 INFO epoch # 7063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011279089630988892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,398 INFO epoch # 7064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009870327201497275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,480 INFO epoch # 7065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008331869830726646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,565 INFO epoch # 7066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009068516970728524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,647 INFO epoch # 7067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008435248702880926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,729 INFO epoch # 7068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009260228507628199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,810 INFO epoch # 7069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009774015372386202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,889 INFO epoch # 7070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00989510070940014
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:30,889 INFO *** epoch 7070, rolling-avg-loss (window=10)= 0.009286973282723921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:30,969 INFO epoch # 7071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008575689225835958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,050 INFO epoch # 7072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012056349267368205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,127 INFO epoch # 7073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008664314824272878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,204 INFO epoch # 7074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008336826678714715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,280 INFO epoch # 7075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00832230995001737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,357 INFO epoch # 7076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009187583265884314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,434 INFO epoch # 7077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010244830540614203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,510 INFO epoch # 7078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010916058796283323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,587 INFO epoch # 7079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007800178529578261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,666 INFO epoch # 7080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009746816540427972
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:31,666 INFO *** epoch 7080, rolling-avg-loss (window=10)= 0.00938509576189972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,742 INFO epoch # 7081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00864009496581275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,819 INFO epoch # 7082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007884743805334438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,896 INFO epoch # 7083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010462494858074933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:31,973 INFO epoch # 7084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010570394326350652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,050 INFO epoch # 7085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010412987874587998
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,126 INFO epoch # 7086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011562672749278136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,204 INFO epoch # 7087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01081165862706257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,281 INFO epoch # 7088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008442532838671468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,360 INFO epoch # 7089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009342495621240232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,438 INFO epoch # 7090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009756777319125831
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:32,438 INFO *** epoch 7090, rolling-avg-loss (window=10)= 0.0097886852985539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,530 INFO epoch # 7091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00848631560074864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,611 INFO epoch # 7092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008923130815674085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,689 INFO epoch # 7093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008794933368335478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,766 INFO epoch # 7094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008925912457925733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,843 INFO epoch # 7095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009571297385264188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,921 INFO epoch # 7096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011213312434847467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:32,997 INFO epoch # 7097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00840260335826315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,075 INFO epoch # 7098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008733573740755673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,153 INFO epoch # 7099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008975548611488193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,234 INFO epoch # 7100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009723349721753038
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:33,234 INFO *** epoch 7100, rolling-avg-loss (window=10)= 0.009174997749505564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,313 INFO epoch # 7101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008661337415105663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,390 INFO epoch # 7102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00957338689477183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,467 INFO epoch # 7103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009391180457896553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,545 INFO epoch # 7104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011083575591328554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,622 INFO epoch # 7105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009181267509120516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,699 INFO epoch # 7106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009522369349724613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,776 INFO epoch # 7107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008932806202210486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,852 INFO epoch # 7108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00848495273385197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:33,929 INFO epoch # 7109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007919158881122712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,006 INFO epoch # 7110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009748955868417397
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:34,006 INFO *** epoch 7110, rolling-avg-loss (window=10)= 0.00924989909035503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,087 INFO epoch # 7111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009954584573279135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,164 INFO epoch # 7112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008533703970897477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,241 INFO epoch # 7113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008071349933743477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,318 INFO epoch # 7114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009189127835270483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,395 INFO epoch # 7115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010569286176178139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,472 INFO epoch # 7116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008450501642073505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,549 INFO epoch # 7117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011829904127807822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,628 INFO epoch # 7118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011592431044846307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,705 INFO epoch # 7119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009118133471929468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,781 INFO epoch # 7120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009227570699295029
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:34,781 INFO *** epoch 7120, rolling-avg-loss (window=10)= 0.009653659347532085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,858 INFO epoch # 7121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011817066580988467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:34,934 INFO epoch # 7122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00897012405039277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,011 INFO epoch # 7123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009458970744162798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,088 INFO epoch # 7124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008829171420075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,164 INFO epoch # 7125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01001111077493988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,240 INFO epoch # 7126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009047169965924695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,317 INFO epoch # 7127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009599885903298855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,394 INFO epoch # 7128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010010774676629808
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,471 INFO epoch # 7129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009214254358084872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,546 INFO epoch # 7130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009345371232484467
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:35,547 INFO *** epoch 7130, rolling-avg-loss (window=10)= 0.00963038997069816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,623 INFO epoch # 7131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007839406367565971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,701 INFO epoch # 7132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00904823620658135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,777 INFO epoch # 7133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011004396146745421
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,856 INFO epoch # 7134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008851383565342985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:35,933 INFO epoch # 7135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010545540950261056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,010 INFO epoch # 7136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008234844382968731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,088 INFO epoch # 7137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008917154558730545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,163 INFO epoch # 7138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009646863814850803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,239 INFO epoch # 7139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009346442690002732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,316 INFO epoch # 7140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008839116650051437
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:36,317 INFO *** epoch 7140, rolling-avg-loss (window=10)= 0.009227338533310103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,394 INFO epoch # 7141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00856153664062731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,469 INFO epoch # 7142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010108982576639391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,546 INFO epoch # 7143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00886726965836715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,624 INFO epoch # 7144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010290812060702592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,702 INFO epoch # 7145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008060886553721502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,780 INFO epoch # 7146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009514062636299059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,857 INFO epoch # 7147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010419310008728644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:36,933 INFO epoch # 7148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00871848616952775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,013 INFO epoch # 7149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008753893249377143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,089 INFO epoch # 7150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009330157714430243
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:37,090 INFO *** epoch 7150, rolling-avg-loss (window=10)= 0.009262539726842078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,166 INFO epoch # 7151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00969652904313989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,243 INFO epoch # 7152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007984078212757595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,321 INFO epoch # 7153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008716892734810244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,399 INFO epoch # 7154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008258695452241227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,476 INFO epoch # 7155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008526844168954995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,552 INFO epoch # 7156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008542875206330791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,630 INFO epoch # 7157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010387392758275382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,708 INFO epoch # 7158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011833387703518383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,784 INFO epoch # 7159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008044231130043045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,861 INFO epoch # 7160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010114197721122764
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:37,861 INFO *** epoch 7160, rolling-avg-loss (window=10)= 0.009210512413119432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:37,938 INFO epoch # 7161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008841439892421477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,016 INFO epoch # 7162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007971342711243778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,093 INFO epoch # 7163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009138992136286106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,170 INFO epoch # 7164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008897907988284715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,247 INFO epoch # 7165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008177604257070925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,327 INFO epoch # 7166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00930294025602052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,403 INFO epoch # 7167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010148579909582622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,480 INFO epoch # 7168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009614937094738707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,558 INFO epoch # 7169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007705987671215553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,636 INFO epoch # 7170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007556434109574184
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:38,636 INFO *** epoch 7170, rolling-avg-loss (window=10)= 0.008735616602643858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,713 INFO epoch # 7171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008425650616118219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,790 INFO epoch # 7172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008680291874043178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,867 INFO epoch # 7173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008350594267540146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:38,944 INFO epoch # 7174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009270824382838327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,022 INFO epoch # 7175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009461358451517299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,100 INFO epoch # 7176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009177428109978791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,176 INFO epoch # 7177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008833677406073548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,253 INFO epoch # 7178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00881553410727065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,331 INFO epoch # 7179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01128051529667573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,408 INFO epoch # 7180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009321414763689972
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:39,408 INFO *** epoch 7180, rolling-avg-loss (window=10)= 0.009161728927574586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,484 INFO epoch # 7181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008865968316968065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,561 INFO epoch # 7182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012400065796100534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,639 INFO epoch # 7183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010054808721179143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,718 INFO epoch # 7184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008174419039278291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,799 INFO epoch # 7185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00813659762206953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,878 INFO epoch # 7186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007994146188138984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:39,955 INFO epoch # 7187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008343791028892156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,032 INFO epoch # 7188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008255223925516475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,109 INFO epoch # 7189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008527158395736478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,185 INFO epoch # 7190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008867663564160466
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:40,185 INFO *** epoch 7190, rolling-avg-loss (window=10)= 0.008961984259804013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,264 INFO epoch # 7191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00935703249706421
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,343 INFO epoch # 7192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009688487596577033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,423 INFO epoch # 7193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008968916554294992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,502 INFO epoch # 7194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008772142100497149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,580 INFO epoch # 7195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007704167815973051
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,655 INFO epoch # 7196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007834994830773212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,734 INFO epoch # 7197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009548386937240139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,812 INFO epoch # 7198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009995801534387283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,893 INFO epoch # 7199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008101544015516993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:40,970 INFO epoch # 7200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009440287132747471
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:40,970 INFO *** epoch 7200, rolling-avg-loss (window=10)= 0.008941176101507154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:41,048 INFO epoch # 7201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009435104417207185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:41,126 INFO epoch # 7202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010930249613011256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:41,204 INFO epoch # 7203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009645581005315762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:41,303 INFO epoch # 7204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077576586190843955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:41,390 INFO epoch # 7205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010049164004158229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:41,477 INFO epoch # 7206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011132349260151386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:41,567 INFO epoch # 7207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009134966516285203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:41,677 INFO epoch # 7208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008662951448059175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:41,775 INFO epoch # 7209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010292689097695984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:41,870 INFO epoch # 7210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009043040925462265
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:41,870 INFO *** epoch 7210, rolling-avg-loss (window=10)= 0.009608375490643084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:41,992 INFO epoch # 7211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010102245971211232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:42,098 INFO epoch # 7212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00937315114424564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:42,189 INFO epoch # 7213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01452053636603523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:42,364 INFO epoch # 7214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01107871274871286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:42,764 INFO epoch # 7215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009057361676241271
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:42,884 INFO epoch # 7216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007976632339705247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:42,979 INFO epoch # 7217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009294017625506967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,062 INFO epoch # 7218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009871898007986601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,139 INFO epoch # 7219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00790017352119321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,219 INFO epoch # 7220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008102314925054088
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:43,219 INFO *** epoch 7220, rolling-avg-loss (window=10)= 0.009727704432589235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,300 INFO epoch # 7221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011201799075934105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,385 INFO epoch # 7222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008985800159280188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,466 INFO epoch # 7223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009264998836442828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,547 INFO epoch # 7224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009184336311591323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,624 INFO epoch # 7225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010546612174948677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,706 INFO epoch # 7226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007802425563568249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,787 INFO epoch # 7227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008844424337439705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,866 INFO epoch # 7228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007905968013801612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:43,947 INFO epoch # 7229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008329864263942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,029 INFO epoch # 7230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008520603310898878
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:44,029 INFO *** epoch 7230, rolling-avg-loss (window=10)= 0.009058683204784757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,105 INFO epoch # 7231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010926761256996542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,187 INFO epoch # 7232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008645165427878965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,265 INFO epoch # 7233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009272812494600657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,343 INFO epoch # 7234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008030748816963751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,421 INFO epoch # 7235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010103892716870178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,501 INFO epoch # 7236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014747081979294308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,578 INFO epoch # 7237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008538060028513428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,659 INFO epoch # 7238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00859670002682833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,739 INFO epoch # 7239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008959563238022383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,821 INFO epoch # 7240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009187827119603753
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:44,821 INFO *** epoch 7240, rolling-avg-loss (window=10)= 0.009700861310557229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,901 INFO epoch # 7241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010383778717368841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:44,982 INFO epoch # 7242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008216902089770883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,059 INFO epoch # 7243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008357782367966138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,137 INFO epoch # 7244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01003054236934986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,215 INFO epoch # 7245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008709191577509046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,294 INFO epoch # 7246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008489807121804915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,372 INFO epoch # 7247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0108554058970185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,449 INFO epoch # 7248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009645160811487585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,527 INFO epoch # 7249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009558708596159704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,608 INFO epoch # 7250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009778492218174506
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:45,608 INFO *** epoch 7250, rolling-avg-loss (window=10)= 0.009402577176660997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,685 INFO epoch # 7251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008023758644412737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,764 INFO epoch # 7252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008099809216219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,842 INFO epoch # 7253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009228941475157626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,921 INFO epoch # 7254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01076789983198978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:45,998 INFO epoch # 7255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008259083318989724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,076 INFO epoch # 7256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00912398328364361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,154 INFO epoch # 7257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009330084372777492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,232 INFO epoch # 7258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009547056004521437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,313 INFO epoch # 7259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008187259925762191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,390 INFO epoch # 7260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007461819892341737
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:46,391 INFO *** epoch 7260, rolling-avg-loss (window=10)= 0.008802969596581533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,471 INFO epoch # 7261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00947980688943062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,551 INFO epoch # 7262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008925267407903448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,628 INFO epoch # 7263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009523638334940188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,710 INFO epoch # 7264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009613874841306824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,789 INFO epoch # 7265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00848621968907537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,870 INFO epoch # 7266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010156796604860574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:46,946 INFO epoch # 7267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009442265676625539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,029 INFO epoch # 7268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007727584648819175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,109 INFO epoch # 7269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011139130147057585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,205 INFO epoch # 7270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00922224081296008
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:47,205 INFO *** epoch 7270, rolling-avg-loss (window=10)= 0.00937168250529794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,319 INFO epoch # 7271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00854315492324531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,403 INFO epoch # 7272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008737404175917618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,487 INFO epoch # 7273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008430627807683777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,568 INFO epoch # 7274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008556894797948189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,649 INFO epoch # 7275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010802270844578743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,726 INFO epoch # 7276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01380014316237066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,815 INFO epoch # 7277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009700566020910628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,902 INFO epoch # 7278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011643507234111894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:47,987 INFO epoch # 7279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009557604280416854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:48,069 INFO epoch # 7280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013032054193899967
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:48,070 INFO *** epoch 7280, rolling-avg-loss (window=10)= 0.010280422744108364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:48,150 INFO epoch # 7281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008648544710013084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:48,264 INFO epoch # 7282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012920080916956067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:48,421 INFO epoch # 7283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01229094470909331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:48,517 INFO epoch # 7284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009646236489061266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:48,605 INFO epoch # 7285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008943521686887834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:48,689 INFO epoch # 7286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009421310627658386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:48,770 INFO epoch # 7287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009517473852611147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:48,854 INFO epoch # 7288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007671829000173602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:48,937 INFO epoch # 7289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007763402943965048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,022 INFO epoch # 7290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008144694053044077
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:49,022 INFO *** epoch 7290, rolling-avg-loss (window=10)= 0.009496803898946382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,103 INFO epoch # 7291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007449332857504487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,182 INFO epoch # 7292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008145994863298256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,258 INFO epoch # 7293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00813717172422912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,336 INFO epoch # 7294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008509293387760408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,417 INFO epoch # 7295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007898303425463382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,498 INFO epoch # 7296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007479066982341465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,577 INFO epoch # 7297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00899258231947897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,657 INFO epoch # 7298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008419659163337201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,734 INFO epoch # 7299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008757029121625237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,814 INFO epoch # 7300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008696910386788659
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:49,814 INFO *** epoch 7300, rolling-avg-loss (window=10)= 0.008248534423182718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,896 INFO epoch # 7301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009310740773798898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:49,975 INFO epoch # 7302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01008354142686585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,052 INFO epoch # 7303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008636640777694993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,129 INFO epoch # 7304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009182960457110312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,207 INFO epoch # 7305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008512187181622721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,284 INFO epoch # 7306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010119545258930884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,363 INFO epoch # 7307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008323498826939613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,440 INFO epoch # 7308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008478070369164925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,518 INFO epoch # 7309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010202061792369932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,595 INFO epoch # 7310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013049460190813988
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:50,595 INFO *** epoch 7310, rolling-avg-loss (window=10)= 0.009589870705531212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,673 INFO epoch # 7311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008161491408827715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,750 INFO epoch # 7312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009893047266814392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,833 INFO epoch # 7313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009182542038615793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,909 INFO epoch # 7314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009270398149965331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:50,987 INFO epoch # 7315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008158563752658665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,067 INFO epoch # 7316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011650530970655382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,147 INFO epoch # 7317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011501571869303007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,227 INFO epoch # 7318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00987202661781339
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,307 INFO epoch # 7319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00734249540255405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,383 INFO epoch # 7320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008070380732533522
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:51,384 INFO *** epoch 7320, rolling-avg-loss (window=10)= 0.009310304820974124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,461 INFO epoch # 7321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007836096330720466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,541 INFO epoch # 7322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008033666548726615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,619 INFO epoch # 7323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009220906853443012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,698 INFO epoch # 7324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008447037340374663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,777 INFO epoch # 7325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007995287080120761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,858 INFO epoch # 7326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009579939702234697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:51,935 INFO epoch # 7327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009857802680926397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,013 INFO epoch # 7328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010540129165747203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,091 INFO epoch # 7329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009466044772125315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,169 INFO epoch # 7330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007376549598120619
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:52,169 INFO *** epoch 7330, rolling-avg-loss (window=10)= 0.008835346007253974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,253 INFO epoch # 7331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008929901494411752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,332 INFO epoch # 7332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009161471047264058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,409 INFO epoch # 7333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009163509683276061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,488 INFO epoch # 7334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00828151073801564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,569 INFO epoch # 7335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007994176201464143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,649 INFO epoch # 7336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008144318795530125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,728 INFO epoch # 7337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007299922486708965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,809 INFO epoch # 7338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009125134776695631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,897 INFO epoch # 7339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00851726031396538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:52,980 INFO epoch # 7340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00872137100668624
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:52,981 INFO *** epoch 7340, rolling-avg-loss (window=10)= 0.008533857654401799
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,062 INFO epoch # 7341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009771016077138484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,141 INFO epoch # 7342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008476499351672828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,218 INFO epoch # 7343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009191033808747306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,294 INFO epoch # 7344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00995800659438828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,370 INFO epoch # 7345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011000400249031372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,446 INFO epoch # 7346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008235076020355336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,523 INFO epoch # 7347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008377487436519004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,598 INFO epoch # 7348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007798764963808935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,675 INFO epoch # 7349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00897359728696756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,753 INFO epoch # 7350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00988098252491909
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:53,753 INFO *** epoch 7350, rolling-avg-loss (window=10)= 0.009166286431354819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,831 INFO epoch # 7351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009118136658798903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,916 INFO epoch # 7352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008689763286383823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:53,994 INFO epoch # 7353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008619785836344818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,070 INFO epoch # 7354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008369682938791811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,148 INFO epoch # 7355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009488842239079531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,228 INFO epoch # 7356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008110848255455494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,304 INFO epoch # 7357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008927518632845022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,381 INFO epoch # 7358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00734275041759247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,459 INFO epoch # 7359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007590223125589546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,536 INFO epoch # 7360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008780020994890947
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:54,537 INFO *** epoch 7360, rolling-avg-loss (window=10)= 0.008503757238577237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,613 INFO epoch # 7361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008796896974672563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,689 INFO epoch # 7362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008958399608673062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,769 INFO epoch # 7363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008622426699730568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,850 INFO epoch # 7364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00900989387446316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:54,932 INFO epoch # 7365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01106836152030155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,010 INFO epoch # 7366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008765669284912292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,088 INFO epoch # 7367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008447741187410429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,164 INFO epoch # 7368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007950598446768709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,240 INFO epoch # 7369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008483208039251622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,317 INFO epoch # 7370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009803449509490747
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:55,317 INFO *** epoch 7370, rolling-avg-loss (window=10)= 0.008990664514567471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,394 INFO epoch # 7371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010325965005904436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,470 INFO epoch # 7372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007512839765695389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,546 INFO epoch # 7373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008060468782787211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,625 INFO epoch # 7374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008282007293018978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,702 INFO epoch # 7375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008779272029642016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,778 INFO epoch # 7376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00760927837836789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,854 INFO epoch # 7377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008845003860187717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:55,936 INFO epoch # 7378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008577027871069731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,012 INFO epoch # 7379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00860879234096501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,089 INFO epoch # 7380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008808550119283609
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:56,089 INFO *** epoch 7380, rolling-avg-loss (window=10)= 0.008540920544692198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,165 INFO epoch # 7381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009117573492403608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,241 INFO epoch # 7382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008002876216778532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,318 INFO epoch # 7383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007856027863454074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,395 INFO epoch # 7384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008748306529014371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,472 INFO epoch # 7385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011269771435763687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,548 INFO epoch # 7386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009627596438804176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,624 INFO epoch # 7387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011284799584245775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,700 INFO epoch # 7388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00884449745353777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,777 INFO epoch # 7389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008570471392886247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,853 INFO epoch # 7390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00843503152646008
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:56,853 INFO *** epoch 7390, rolling-avg-loss (window=10)= 0.009175695193334832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:56,932 INFO epoch # 7391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077335478126769885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,010 INFO epoch # 7392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00767172562336782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,086 INFO epoch # 7393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009029458429722581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,163 INFO epoch # 7394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00812276492069941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,240 INFO epoch # 7395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008066715745371766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,316 INFO epoch # 7396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007885455859650392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,392 INFO epoch # 7397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008744968356040772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,470 INFO epoch # 7398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008810662940959446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,547 INFO epoch # 7399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008151051253662445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,625 INFO epoch # 7400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009009215711557772
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:57,625 INFO *** epoch 7400, rolling-avg-loss (window=10)= 0.008322556665370939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,703 INFO epoch # 7401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007935182824439835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,780 INFO epoch # 7402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014839300507446751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,857 INFO epoch # 7403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00934146490180865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:57,941 INFO epoch # 7404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0080299920446123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,020 INFO epoch # 7405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009065762809768785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,097 INFO epoch # 7406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008273701969301328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,174 INFO epoch # 7407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009520159183011856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,252 INFO epoch # 7408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00815833641536301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,331 INFO epoch # 7409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010050677068647929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,408 INFO epoch # 7410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010519165094592609
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:58,408 INFO *** epoch 7410, rolling-avg-loss (window=10)= 0.009573374281899305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,488 INFO epoch # 7411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008396225472097285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,565 INFO epoch # 7412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007194905585492961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,641 INFO epoch # 7413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009500097803538665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,717 INFO epoch # 7414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0098365631129127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,793 INFO epoch # 7415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008979818514490034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,869 INFO epoch # 7416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00983930793881882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:58,952 INFO epoch # 7417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00957936434133444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,028 INFO epoch # 7418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008793095526925754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,104 INFO epoch # 7419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008708772787940688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,180 INFO epoch # 7420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00914231623755768
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:59,180 INFO *** epoch 7420, rolling-avg-loss (window=10)= 0.008997046732110903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,257 INFO epoch # 7421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008796313872153405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,335 INFO epoch # 7422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010129970229172613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,412 INFO epoch # 7423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008564436560845934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,490 INFO epoch # 7424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007924437115434557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,566 INFO epoch # 7425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00868999040540075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,643 INFO epoch # 7426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010182757549046073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,721 INFO epoch # 7427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011619896555203013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,798 INFO epoch # 7428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010743265200289898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,874 INFO epoch # 7429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01026267911947798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:23:59,952 INFO epoch # 7430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008829427872115048
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:23:59,953 INFO *** epoch 7430, rolling-avg-loss (window=10)= 0.009574317447913927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,031 INFO epoch # 7431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008972932759206742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,108 INFO epoch # 7432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008558146000723355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,186 INFO epoch # 7433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00798316847794922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,262 INFO epoch # 7434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007865739826229401
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,338 INFO epoch # 7435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00821252335299505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,415 INFO epoch # 7436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010483967867912725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,492 INFO epoch # 7437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008556570504879346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,568 INFO epoch # 7438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008004049908777233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,645 INFO epoch # 7439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009087655133043882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,722 INFO epoch # 7440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009705719974590465
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:00,722 INFO *** epoch 7440, rolling-avg-loss (window=10)= 0.008743047380630743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,798 INFO epoch # 7441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00967732346180128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,875 INFO epoch # 7442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008377764497708995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:00,951 INFO epoch # 7443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00808882109777187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,031 INFO epoch # 7444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011942070748773403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,108 INFO epoch # 7445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012717374695057515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,184 INFO epoch # 7446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008381608669878915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,261 INFO epoch # 7447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008610053431766573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,339 INFO epoch # 7448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0082682072170428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,415 INFO epoch # 7449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007880681041569915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,492 INFO epoch # 7450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007529753936978523
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:01,492 INFO *** epoch 7450, rolling-avg-loss (window=10)= 0.009147365879834978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,569 INFO epoch # 7451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008818150250590406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,645 INFO epoch # 7452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008559773632441647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,722 INFO epoch # 7453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015134296911128331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,800 INFO epoch # 7454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0097019154636655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,877 INFO epoch # 7455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00943806444411166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:01,953 INFO epoch # 7456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011882978782523423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,033 INFO epoch # 7457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010863406729185954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,109 INFO epoch # 7458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007845546693715733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,185 INFO epoch # 7459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00867017504060641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,261 INFO epoch # 7460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074537557156872936
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:02,262 INFO *** epoch 7460, rolling-avg-loss (window=10)= 0.009836806366365636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,338 INFO epoch # 7461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009100014387513511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,415 INFO epoch # 7462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008670171780977398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,492 INFO epoch # 7463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00984815537231043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,569 INFO epoch # 7464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009352498600492254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,649 INFO epoch # 7465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009411404389538802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,728 INFO epoch # 7466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00927730980765773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,808 INFO epoch # 7467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008833189735014457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,884 INFO epoch # 7468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010119643484358676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:02,961 INFO epoch # 7469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007262640705448575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,042 INFO epoch # 7470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007596161121909972
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:03,042 INFO *** epoch 7470, rolling-avg-loss (window=10)= 0.00894711893852218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,118 INFO epoch # 7471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008795687201200053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,197 INFO epoch # 7472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008607262796431314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,275 INFO epoch # 7473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008200190117349848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,351 INFO epoch # 7474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009176788684271742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,439 INFO epoch # 7475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008081248961389065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,533 INFO epoch # 7476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010314468519936781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,616 INFO epoch # 7477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008864993054885417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,706 INFO epoch # 7478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011804400681285188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,786 INFO epoch # 7479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008630597505543847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,865 INFO epoch # 7480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010828140104422346
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:03,866 INFO *** epoch 7480, rolling-avg-loss (window=10)= 0.00933037776267156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:03,945 INFO epoch # 7481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008274675958091393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,025 INFO epoch # 7482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010239343871944584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,104 INFO epoch # 7483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009297206954215653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,186 INFO epoch # 7484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008557766712328885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,268 INFO epoch # 7485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009915293616359122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,351 INFO epoch # 7486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009583263061358593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,431 INFO epoch # 7487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010610327663016506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,511 INFO epoch # 7488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00926116896152962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,592 INFO epoch # 7489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008618986452347599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,673 INFO epoch # 7490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00868374556011986
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:04,673 INFO *** epoch 7490, rolling-avg-loss (window=10)= 0.009304177881131182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,753 INFO epoch # 7491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00776203467103187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,832 INFO epoch # 7492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00928198111068923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,917 INFO epoch # 7493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009049941203556955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:04,998 INFO epoch # 7494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008479224052280188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,082 INFO epoch # 7495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009546222434437368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,162 INFO epoch # 7496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009280211561417673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,245 INFO epoch # 7497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010487675484910142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,330 INFO epoch # 7498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008768023581069428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,413 INFO epoch # 7499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009668280545156449
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,500 INFO epoch # 7500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009927182734827511
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:05,500 INFO *** epoch 7500, rolling-avg-loss (window=10)= 0.009225077737937681
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,578 INFO epoch # 7501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008761408462305553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,658 INFO epoch # 7502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008273803592601325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,736 INFO epoch # 7503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008485933671181556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,813 INFO epoch # 7504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008588022414187435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,890 INFO epoch # 7505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00794027990923496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:05,967 INFO epoch # 7506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010747037071269006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,047 INFO epoch # 7507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00896618423576001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,123 INFO epoch # 7508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008903233028831892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,200 INFO epoch # 7509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011701936891768128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,276 INFO epoch # 7510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00963513640454039
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:06,276 INFO *** epoch 7510, rolling-avg-loss (window=10)= 0.009200297568168026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,354 INFO epoch # 7511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008933925462770276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,433 INFO epoch # 7512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008288406286737882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,516 INFO epoch # 7513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010390740964794531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,597 INFO epoch # 7514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010405344204627909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,677 INFO epoch # 7515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008325850722030737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,756 INFO epoch # 7516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007952615302201593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,838 INFO epoch # 7517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007577890646643937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,917 INFO epoch # 7518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00803763976728078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:06,998 INFO epoch # 7519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01029957240098156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,084 INFO epoch # 7520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011209407326532528
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:07,084 INFO *** epoch 7520, rolling-avg-loss (window=10)= 0.009142139308460173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,162 INFO epoch # 7521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00756676281162072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,239 INFO epoch # 7522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01046885212417692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,315 INFO epoch # 7523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010887172844377346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,391 INFO epoch # 7524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009213946126692463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,471 INFO epoch # 7525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009164896640868392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,554 INFO epoch # 7526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010070289819850586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,632 INFO epoch # 7527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00929648855526466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,709 INFO epoch # 7528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008857811364578083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,785 INFO epoch # 7529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010033936930994969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,861 INFO epoch # 7530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009191077566356398
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:07,861 INFO *** epoch 7530, rolling-avg-loss (window=10)= 0.009475123478478054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:07,937 INFO epoch # 7531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00796686243120348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,014 INFO epoch # 7532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00839377412921749
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,091 INFO epoch # 7533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009541692459606566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,167 INFO epoch # 7534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007772824690619018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,245 INFO epoch # 7535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007607776220538653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,323 INFO epoch # 7536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009599115983291995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,401 INFO epoch # 7537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008592750877141953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,496 INFO epoch # 7538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008688601403264329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,597 INFO epoch # 7539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007892854715464637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,675 INFO epoch # 7540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008627624010841828
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:08,676 INFO *** epoch 7540, rolling-avg-loss (window=10)= 0.008468387692118995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,755 INFO epoch # 7541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008311491052154452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,837 INFO epoch # 7542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00910080800531432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,916 INFO epoch # 7543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008293794591736514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:08,996 INFO epoch # 7544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007479297310055699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,077 INFO epoch # 7545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007904799407697283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,157 INFO epoch # 7546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008850637896102853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,234 INFO epoch # 7547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008269761601695791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,310 INFO epoch # 7548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009066747181350365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,390 INFO epoch # 7549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008783121113083325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,471 INFO epoch # 7550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007566879336081911
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:09,471 INFO *** epoch 7550, rolling-avg-loss (window=10)= 0.008362733749527252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,550 INFO epoch # 7551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008296845029690303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,628 INFO epoch # 7552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007910779655503575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,704 INFO epoch # 7553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007363696160609834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,783 INFO epoch # 7554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008849610174365807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,859 INFO epoch # 7555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008850613558024634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:09,945 INFO epoch # 7556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007364057113591116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,025 INFO epoch # 7557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008332311314006802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,102 INFO epoch # 7558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007116647975635715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,178 INFO epoch # 7559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008445908795692958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,257 INFO epoch # 7560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008684243788593449
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:10,257 INFO *** epoch 7560, rolling-avg-loss (window=10)= 0.00812147135657142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,334 INFO epoch # 7561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007695951928326394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,420 INFO epoch # 7562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008001036345376633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,499 INFO epoch # 7563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01041149783122819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,580 INFO epoch # 7564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009474463426158763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,665 INFO epoch # 7565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010076794249471277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,745 INFO epoch # 7566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00937334214540897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,823 INFO epoch # 7567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0083544488879852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,909 INFO epoch # 7568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011187707495992072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:10,999 INFO epoch # 7569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007397689725621603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,080 INFO epoch # 7570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008059642925218213
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:11,080 INFO *** epoch 7570, rolling-avg-loss (window=10)= 0.00900325749607873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,161 INFO epoch # 7571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009574990908731706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,243 INFO epoch # 7572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008930191608669702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,325 INFO epoch # 7573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007880356301029678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,401 INFO epoch # 7574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01047145120537607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,479 INFO epoch # 7575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01034904558036942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,556 INFO epoch # 7576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008983862469904125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,633 INFO epoch # 7577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009413059029611759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,709 INFO epoch # 7578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008306672090839129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,804 INFO epoch # 7579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00790821404370945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,889 INFO epoch # 7580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008123272273223847
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:11,889 INFO *** epoch 7580, rolling-avg-loss (window=10)= 0.008994111551146489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:11,968 INFO epoch # 7581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008925209418521263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:12,044 INFO epoch # 7582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008483071316732094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:12,133 INFO epoch # 7583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009300657067797147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:12,285 INFO epoch # 7584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009422708346392028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:12,407 INFO epoch # 7585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008420763660978992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:12,534 INFO epoch # 7586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008219276060117409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:12,653 INFO epoch # 7587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008788896484475117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:12,739 INFO epoch # 7588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00755653659143718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:12,820 INFO epoch # 7589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00820475504588103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:12,897 INFO epoch # 7590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008177414412784856
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:12,897 INFO *** epoch 7590, rolling-avg-loss (window=10)= 0.00854992884051171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:12,974 INFO epoch # 7591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010082837405207101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,051 INFO epoch # 7592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008090109986369498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,128 INFO epoch # 7593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009726771015266422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,206 INFO epoch # 7594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010186118102865294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,282 INFO epoch # 7595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007642392927664332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,358 INFO epoch # 7596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00831132061284734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,435 INFO epoch # 7597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007873298709455412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,512 INFO epoch # 7598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009562746570736635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,588 INFO epoch # 7599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00730569772713352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,664 INFO epoch # 7600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010940234977169894
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:13,664 INFO *** epoch 7600, rolling-avg-loss (window=10)= 0.008972152803471545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,740 INFO epoch # 7601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010056157057988457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,817 INFO epoch # 7602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0081561651386437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,894 INFO epoch # 7603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008054680532950442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:13,970 INFO epoch # 7604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008601362198533025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,050 INFO epoch # 7605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007246416884299833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,126 INFO epoch # 7606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009364770630782004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,203 INFO epoch # 7607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010442605664138682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,282 INFO epoch # 7608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00916922349279048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,359 INFO epoch # 7609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009211803910147864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,438 INFO epoch # 7610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008947773872932885
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:14,438 INFO *** epoch 7610, rolling-avg-loss (window=10)= 0.008925095938320738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,519 INFO epoch # 7611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007415206651785411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,597 INFO epoch # 7612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011449767553131096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,674 INFO epoch # 7613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008569004283344839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,752 INFO epoch # 7614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008093605763860978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,831 INFO epoch # 7615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00825210107723251
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,910 INFO epoch # 7616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008085790956101846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:14,991 INFO epoch # 7617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007917685121356044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,071 INFO epoch # 7618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007848157947591972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,159 INFO epoch # 7619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009614417409466114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,247 INFO epoch # 7620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073617657835711725
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:15,247 INFO *** epoch 7620, rolling-avg-loss (window=10)= 0.008460750254744197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,333 INFO epoch # 7621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010186035033257212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,414 INFO epoch # 7622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00978654477512464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,499 INFO epoch # 7623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007905667087470647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,583 INFO epoch # 7624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007177921834227163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,665 INFO epoch # 7625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01174392265966162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,748 INFO epoch # 7626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009335715949418955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,827 INFO epoch # 7627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007918917886854615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,903 INFO epoch # 7628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008035167229536455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:15,984 INFO epoch # 7629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008101641957182437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,063 INFO epoch # 7630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00835264171473682
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:16,063 INFO *** epoch 7630, rolling-avg-loss (window=10)= 0.008854417612747056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,140 INFO epoch # 7631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011260471103014424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,218 INFO epoch # 7632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008045205977396108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,299 INFO epoch # 7633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009408801874087658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,380 INFO epoch # 7634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006971149334276561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,462 INFO epoch # 7635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00821388301847037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,542 INFO epoch # 7636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008072330398135819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,630 INFO epoch # 7637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010275281296344474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,718 INFO epoch # 7638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007986312106368132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,798 INFO epoch # 7639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00944524932128843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,876 INFO epoch # 7640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008385634209844284
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:16,876 INFO *** epoch 7640, rolling-avg-loss (window=10)= 0.008806431863922626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:16,957 INFO epoch # 7641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00882144742354285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,034 INFO epoch # 7642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008543998163077049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,111 INFO epoch # 7643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011773027988965623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,190 INFO epoch # 7644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014467283617705107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,267 INFO epoch # 7645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007185291404312011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,352 INFO epoch # 7646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010360340660554357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,440 INFO epoch # 7647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00980745280685369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,519 INFO epoch # 7648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008884104347089306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,598 INFO epoch # 7649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00773307666531764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,678 INFO epoch # 7650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008370971328986343
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:17,678 INFO *** epoch 7650, rolling-avg-loss (window=10)= 0.009594699440640397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,758 INFO epoch # 7651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009279350291762967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,836 INFO epoch # 7652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009400261224072892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:17,922 INFO epoch # 7653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007641047894139774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,006 INFO epoch # 7654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00843213695770828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,089 INFO epoch # 7655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008002129994565621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,179 INFO epoch # 7656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008188308464013971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,264 INFO epoch # 7657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007833744974050205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,348 INFO epoch # 7658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007891254914284218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,433 INFO epoch # 7659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008587105869082734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,515 INFO epoch # 7660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01124699751380831
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:18,516 INFO *** epoch 7660, rolling-avg-loss (window=10)= 0.008650233809748897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,595 INFO epoch # 7661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010815395900863223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,673 INFO epoch # 7662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009308534019510262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,749 INFO epoch # 7663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00775207977858372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,827 INFO epoch # 7664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007664364340598695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,903 INFO epoch # 7665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007482848479412496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:18,980 INFO epoch # 7666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009247685215086676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,056 INFO epoch # 7667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0092059167145635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,132 INFO epoch # 7668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010473352667759173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,208 INFO epoch # 7669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008818671085464302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,284 INFO epoch # 7670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00863410290912725
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:19,285 INFO *** epoch 7670, rolling-avg-loss (window=10)= 0.00894029511109693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,362 INFO epoch # 7671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009342818506411277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,442 INFO epoch # 7672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008143837447278202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,520 INFO epoch # 7673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00830297149514081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,596 INFO epoch # 7674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010140566868358292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,673 INFO epoch # 7675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077310830893111415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,756 INFO epoch # 7676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01018532910529757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,841 INFO epoch # 7677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007190446631284431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,917 INFO epoch # 7678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009588342378265224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:19,995 INFO epoch # 7679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00886798129795352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:20,108 INFO epoch # 7680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00858099867036799
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:20,109 INFO *** epoch 7680, rolling-avg-loss (window=10)= 0.008807437548966845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:20,215 INFO epoch # 7681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008539595211914275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:20,312 INFO epoch # 7682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008906091192329768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:20,407 INFO epoch # 7683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009211796750605572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:20,491 INFO epoch # 7684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007602512887388002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:20,574 INFO epoch # 7685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007404285053780768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:20,655 INFO epoch # 7686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007824481115676463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:20,741 INFO epoch # 7687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009651388936617877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:20,824 INFO epoch # 7688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008869719968060963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:20,907 INFO epoch # 7689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008144096922478639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:20,987 INFO epoch # 7690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008004056675417814
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:20,987 INFO *** epoch 7690, rolling-avg-loss (window=10)= 0.008415802471427014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,068 INFO epoch # 7691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00824305757851107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,152 INFO epoch # 7692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007629561092471704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,233 INFO epoch # 7693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010065724767628126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,316 INFO epoch # 7694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009819958955631591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,400 INFO epoch # 7695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008581265130487736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,485 INFO epoch # 7696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007957342873851303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,570 INFO epoch # 7697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00793881350546144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,653 INFO epoch # 7698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007510882103815675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,730 INFO epoch # 7699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007500079460442066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,811 INFO epoch # 7700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008456008807115722
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:21,812 INFO *** epoch 7700, rolling-avg-loss (window=10)= 0.008370269427541644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,895 INFO epoch # 7701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010655518824933097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:21,975 INFO epoch # 7702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010682612119126134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,057 INFO epoch # 7703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014250342603190802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,138 INFO epoch # 7704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007749738906568382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,221 INFO epoch # 7705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006806722034525592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,301 INFO epoch # 7706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00881251417013118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,378 INFO epoch # 7707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073663000875967555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,455 INFO epoch # 7708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00979047326836735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,548 INFO epoch # 7709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008656849531689659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,658 INFO epoch # 7710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006997234922891948
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:22,658 INFO *** epoch 7710, rolling-avg-loss (window=10)= 0.00917683064690209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,743 INFO epoch # 7711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011782785964896902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,823 INFO epoch # 7712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008337580438819714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,902 INFO epoch # 7713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008810103012365289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:22,982 INFO epoch # 7714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0085959104981157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,059 INFO epoch # 7715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010391982134024147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,141 INFO epoch # 7716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008881603884219658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,224 INFO epoch # 7717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00814403135154862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,303 INFO epoch # 7718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008017783620744012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,383 INFO epoch # 7719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0089182489173254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,460 INFO epoch # 7720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012229095445945859
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:23,461 INFO *** epoch 7720, rolling-avg-loss (window=10)= 0.00941091252680053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,538 INFO epoch # 7721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008382419931876939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,616 INFO epoch # 7722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007235094839415979
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,695 INFO epoch # 7723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008447428750514518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,789 INFO epoch # 7724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008304662835143972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,894 INFO epoch # 7725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008317300758790225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:23,985 INFO epoch # 7726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007460794862709008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,064 INFO epoch # 7727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008551098704629112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,142 INFO epoch # 7728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008034091690205969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,218 INFO epoch # 7729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00895050639519468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,296 INFO epoch # 7730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008811291598249227
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:24,296 INFO *** epoch 7730, rolling-avg-loss (window=10)= 0.008249469036672962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,373 INFO epoch # 7731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009272825562220532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,455 INFO epoch # 7732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008592517500801478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,533 INFO epoch # 7733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008537571571650915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,614 INFO epoch # 7734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007023815480351914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,740 INFO epoch # 7735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007124361596652307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,841 INFO epoch # 7736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007669418038858566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,919 INFO epoch # 7737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008015060564503074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:24,998 INFO epoch # 7738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008260016766143963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:25,078 INFO epoch # 7739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009100980976654682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:25,186 INFO epoch # 7740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007493577155400999
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:25,186 INFO *** epoch 7740, rolling-avg-loss (window=10)= 0.008109014521323843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:25,269 INFO epoch # 7741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007846477659768425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:25,350 INFO epoch # 7742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00958622110192664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:25,430 INFO epoch # 7743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008810673200059682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:25,509 INFO epoch # 7744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010382902677520178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:25,587 INFO epoch # 7745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008729858178412542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:25,667 INFO epoch # 7746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008437440577836242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:25,746 INFO epoch # 7747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009570857422659174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:25,822 INFO epoch # 7748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007613925430632662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:25,907 INFO epoch # 7749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00897123727190774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,022 INFO epoch # 7750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007073145359754562
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:26,022 INFO *** epoch 7750, rolling-avg-loss (window=10)= 0.008702273888047785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,111 INFO epoch # 7751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009329256587079726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,196 INFO epoch # 7752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00879873859230429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,283 INFO epoch # 7753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009048843225173187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,373 INFO epoch # 7754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00762589475925779
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,460 INFO epoch # 7755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00929589149018284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,546 INFO epoch # 7756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00868040668865433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,628 INFO epoch # 7757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007896049675764516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,714 INFO epoch # 7758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01064211004995741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,793 INFO epoch # 7759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007323824131162837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,870 INFO epoch # 7760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008249132712080609
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:26,870 INFO *** epoch 7760, rolling-avg-loss (window=10)= 0.008689014791161753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:26,947 INFO epoch # 7761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007860994104703423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:27,024 INFO epoch # 7762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008523681914084591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:27,103 INFO epoch # 7763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00997044501855271
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:27,181 INFO epoch # 7764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009317376803664956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:27,264 INFO epoch # 7765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008221965508710127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:27,344 INFO epoch # 7766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008062057619099505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:27,423 INFO epoch # 7767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009091111336601898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:27,506 INFO epoch # 7768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00794502126518637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:27,590 INFO epoch # 7769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007290655325050466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:27,673 INFO epoch # 7770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00852234302146826
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:27,674 INFO *** epoch 7770, rolling-avg-loss (window=10)= 0.00848056519171223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:27,756 INFO epoch # 7771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071995257821981795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:27,880 INFO epoch # 7772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00953891761309933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:28,040 INFO epoch # 7773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009767747644218616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:28,140 INFO epoch # 7774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008573552702728193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:28,246 INFO epoch # 7775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010924139933194965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:28,367 INFO epoch # 7776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008727444845135324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:28,462 INFO epoch # 7777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010037144515081309
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:28,542 INFO epoch # 7778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007814952230546623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:28,622 INFO epoch # 7779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00712154899520101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:28,701 INFO epoch # 7780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008811490188236348
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:28,701 INFO *** epoch 7780, rolling-avg-loss (window=10)= 0.00885164644496399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:28,780 INFO epoch # 7781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007575503550469875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:28,864 INFO epoch # 7782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01005400663416367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:28,944 INFO epoch # 7783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011855518983793445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,023 INFO epoch # 7784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00921099228435196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,104 INFO epoch # 7785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010101491570821963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,185 INFO epoch # 7786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008172534187906422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,262 INFO epoch # 7787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008700534504896495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,339 INFO epoch # 7788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010018496446718927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,415 INFO epoch # 7789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008467978193948511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,492 INFO epoch # 7790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009687194076832384
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:29,492 INFO *** epoch 7790, rolling-avg-loss (window=10)= 0.009384425043390366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,568 INFO epoch # 7791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007390816972474568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,644 INFO epoch # 7792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008032491998164915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,723 INFO epoch # 7793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00845586389186792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,803 INFO epoch # 7794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007625732418091502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,883 INFO epoch # 7795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075152487042942084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:29,963 INFO epoch # 7796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009668898113886826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,042 INFO epoch # 7797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00783003698597895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,121 INFO epoch # 7798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010188378444581758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,203 INFO epoch # 7799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010060358719783835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,280 INFO epoch # 7800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00781686975096818
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:30,281 INFO *** epoch 7800, rolling-avg-loss (window=10)= 0.008458469600009266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,360 INFO epoch # 7801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007473899095202796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,438 INFO epoch # 7802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006773496941605117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,520 INFO epoch # 7803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007418546796543524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,601 INFO epoch # 7804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007582429563626647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,684 INFO epoch # 7805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007753369602141902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,791 INFO epoch # 7806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009786656773940194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,885 INFO epoch # 7807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008009536300960463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:30,970 INFO epoch # 7808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01630276498326566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,053 INFO epoch # 7809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009868053450190928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,131 INFO epoch # 7810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007791451418597717
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:31,131 INFO *** epoch 7810, rolling-avg-loss (window=10)= 0.008876020492607495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,227 INFO epoch # 7811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008936430669564288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,322 INFO epoch # 7812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0078092259500408545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,409 INFO epoch # 7813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009137290180660784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,492 INFO epoch # 7814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008134314244671259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,581 INFO epoch # 7815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007811429051798768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,661 INFO epoch # 7816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007985507723788032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,740 INFO epoch # 7817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007883076228608843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,818 INFO epoch # 7818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007878525146225002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,908 INFO epoch # 7819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009172309102723375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:31,988 INFO epoch # 7820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01138932051253505
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:31,988 INFO *** epoch 7820, rolling-avg-loss (window=10)= 0.008613742881061625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,067 INFO epoch # 7821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011511118042108137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,144 INFO epoch # 7822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008797765527560841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,221 INFO epoch # 7823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008555623709980864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,301 INFO epoch # 7824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007632450317032635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,378 INFO epoch # 7825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00909416408103425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,454 INFO epoch # 7826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00847733672708273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,531 INFO epoch # 7827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010624028232996352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,617 INFO epoch # 7828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009231880540028214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,701 INFO epoch # 7829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008762106110225432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,794 INFO epoch # 7830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009575438380124979
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:32,794 INFO *** epoch 7830, rolling-avg-loss (window=10)= 0.009226191166817443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,878 INFO epoch # 7831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01117896162031684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:32,957 INFO epoch # 7832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009197402861900628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,037 INFO epoch # 7833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008702064413228072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,113 INFO epoch # 7834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00957884376111906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,190 INFO epoch # 7835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008975555334473029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,272 INFO epoch # 7836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008869672761647962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,351 INFO epoch # 7837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007881294295657426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,430 INFO epoch # 7838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010091481963172555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,514 INFO epoch # 7839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007854138486436568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,595 INFO epoch # 7840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00993351169017842
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:33,595 INFO *** epoch 7840, rolling-avg-loss (window=10)= 0.009226292718813056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,680 INFO epoch # 7841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009593395348929334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,763 INFO epoch # 7842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007905378108262084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,847 INFO epoch # 7843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008914436984923668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:33,934 INFO epoch # 7844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007905673497589305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:34,032 INFO epoch # 7845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008718136712559499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:34,131 INFO epoch # 7846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009098050693864934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:34,214 INFO epoch # 7847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010783355945022777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:34,290 INFO epoch # 7848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00775291310128523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:34,385 INFO epoch # 7849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007760002335999161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:34,506 INFO epoch # 7850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009419353307748679
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:34,506 INFO *** epoch 7850, rolling-avg-loss (window=10)= 0.008785069603618467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:34,590 INFO epoch # 7851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00952091442013625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:34,682 INFO epoch # 7852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00825985206029145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:34,769 INFO epoch # 7853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00999261497054249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:34,849 INFO epoch # 7854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007941700183437206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:34,926 INFO epoch # 7855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008682732004672289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,003 INFO epoch # 7856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008232845153543167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,079 INFO epoch # 7857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007650760286196601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,156 INFO epoch # 7858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00820737529284088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,232 INFO epoch # 7859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00786585937748896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,308 INFO epoch # 7860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009888017215416767
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:35,309 INFO *** epoch 7860, rolling-avg-loss (window=10)= 0.008624267096456606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,385 INFO epoch # 7861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008649709263409022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,463 INFO epoch # 7862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0087701292213751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,545 INFO epoch # 7863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009270510039641522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,624 INFO epoch # 7864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009900223834847566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,701 INFO epoch # 7865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009668959006376099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,777 INFO epoch # 7866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009304874009103514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,856 INFO epoch # 7867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00919292538674199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:35,933 INFO epoch # 7868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0080813910899451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,012 INFO epoch # 7869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008124719184706919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,093 INFO epoch # 7870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009687212797871325
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:36,093 INFO *** epoch 7870, rolling-avg-loss (window=10)= 0.009065065383401816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,178 INFO epoch # 7871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006732369831297547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,258 INFO epoch # 7872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008549190664780326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,334 INFO epoch # 7873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00801810939447023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,428 INFO epoch # 7874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00832745526713552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,543 INFO epoch # 7875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008275102925836109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,624 INFO epoch # 7876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009329988381068688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,703 INFO epoch # 7877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008726030835532583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,780 INFO epoch # 7878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008654421151732095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,858 INFO epoch # 7879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010224086989182979
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:36,935 INFO epoch # 7880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009010258181660902
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:36,935 INFO *** epoch 7880, rolling-avg-loss (window=10)= 0.008584701362269697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,011 INFO epoch # 7881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009667442602221854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,090 INFO epoch # 7882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011424780415836722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,167 INFO epoch # 7883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007746797287836671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,240 INFO epoch # 7884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007880725352151785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,314 INFO epoch # 7885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00902416578901466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,387 INFO epoch # 7886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007498204729927238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,465 INFO epoch # 7887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009650113599491306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,542 INFO epoch # 7888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00963998465158511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,624 INFO epoch # 7889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008650706244225148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,711 INFO epoch # 7890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008123504063405562
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:37,712 INFO *** epoch 7890, rolling-avg-loss (window=10)= 0.008930642473569606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,793 INFO epoch # 7891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008330895157996565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,873 INFO epoch # 7892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008237612200900912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:37,950 INFO epoch # 7893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010685291737900116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,028 INFO epoch # 7894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010534582776017487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,110 INFO epoch # 7895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013311292801517993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,186 INFO epoch # 7896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009374562607263215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,263 INFO epoch # 7897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007605197621160187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,346 INFO epoch # 7898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00869980226707412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,461 INFO epoch # 7899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008783806923020165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,546 INFO epoch # 7900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008521404553903267
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:38,547 INFO *** epoch 7900, rolling-avg-loss (window=10)= 0.009408444864675403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,646 INFO epoch # 7901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073099367145914584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,735 INFO epoch # 7902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007743916037725285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,815 INFO epoch # 7903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009854717689449899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,896 INFO epoch # 7904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010179026685364079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:38,973 INFO epoch # 7905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009448107914067805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,049 INFO epoch # 7906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010632836987497285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,129 INFO epoch # 7907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009300230703956913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,205 INFO epoch # 7908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010872401282540523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,281 INFO epoch # 7909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008021998321055435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,360 INFO epoch # 7910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007726821888354607
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:39,360 INFO *** epoch 7910, rolling-avg-loss (window=10)= 0.009108999422460328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,437 INFO epoch # 7911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00827317372750258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,514 INFO epoch # 7912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011088534622103907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,594 INFO epoch # 7913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009706212447781581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,671 INFO epoch # 7914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00898855904233642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,747 INFO epoch # 7915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010283288938808255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,827 INFO epoch # 7916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008701352569914889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,905 INFO epoch # 7917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008689925489306916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:39,983 INFO epoch # 7918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007148357377445791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:40,063 INFO epoch # 7919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007324212325329427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:40,150 INFO epoch # 7920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007411295460769907
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:40,150 INFO *** epoch 7920, rolling-avg-loss (window=10)= 0.008761491200129968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:40,227 INFO epoch # 7921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008298654771351721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:40,303 INFO epoch # 7922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009068296043551527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:40,388 INFO epoch # 7923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00836991124015185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:40,521 INFO epoch # 7924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008058462881308515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:40,654 INFO epoch # 7925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010873256120248698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:40,779 INFO epoch # 7926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074521269198157825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:40,870 INFO epoch # 7927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009918927462422289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:40,960 INFO epoch # 7928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00801198626140831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,049 INFO epoch # 7929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008449031840427779
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,129 INFO epoch # 7930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009478995008976199
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:41,129 INFO *** epoch 7930, rolling-avg-loss (window=10)= 0.008797964854966266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,206 INFO epoch # 7931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008717251737834886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,286 INFO epoch # 7932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007961632647493389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,366 INFO epoch # 7933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007782858607242815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,446 INFO epoch # 7934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009542776249872986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,528 INFO epoch # 7935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0098905405975529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,606 INFO epoch # 7936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008057271406869404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,686 INFO epoch # 7937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009794636047445238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,766 INFO epoch # 7938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008507072176143993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,845 INFO epoch # 7939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071072836144594476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:41,924 INFO epoch # 7940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009087902959436178
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:41,924 INFO *** epoch 7940, rolling-avg-loss (window=10)= 0.008644922604435123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,004 INFO epoch # 7941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007149903896788601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,083 INFO epoch # 7942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007932981476187706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,162 INFO epoch # 7943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068502569702104665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,242 INFO epoch # 7944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071703839566907845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,323 INFO epoch # 7945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008482531251502223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,401 INFO epoch # 7946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008109507951303385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,480 INFO epoch # 7947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006693577241094317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,563 INFO epoch # 7948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008107796347758267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,641 INFO epoch # 7949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007710589539783541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,721 INFO epoch # 7950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007948198217491154
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:42,721 INFO *** epoch 7950, rolling-avg-loss (window=10)= 0.007615572684881044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,801 INFO epoch # 7951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011651233246084303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,877 INFO epoch # 7952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008324749149323907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:42,962 INFO epoch # 7953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008178081996447872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,046 INFO epoch # 7954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009673740220023319
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,126 INFO epoch # 7955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007367704740318004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,203 INFO epoch # 7956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007753860081720632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,279 INFO epoch # 7957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006605568756640423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,355 INFO epoch # 7958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008489500753057655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,432 INFO epoch # 7959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009009526183945127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,508 INFO epoch # 7960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00695197973982431
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:43,509 INFO *** epoch 7960, rolling-avg-loss (window=10)= 0.008400594486738555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,590 INFO epoch # 7961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007809733288013376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,673 INFO epoch # 7962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008574456020141952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,750 INFO epoch # 7963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008425248990533873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,828 INFO epoch # 7964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009169989854854066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,907 INFO epoch # 7965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009261558014259208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:43,983 INFO epoch # 7966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008672005824337248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,061 INFO epoch # 7967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007802666958014015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,138 INFO epoch # 7968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0095081040635705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,216 INFO epoch # 7969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008125045853375923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,292 INFO epoch # 7970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006998678567470051
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:44,292 INFO *** epoch 7970, rolling-avg-loss (window=10)= 0.00843474874345702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,376 INFO epoch # 7971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007835257682017982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,453 INFO epoch # 7972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008010434299649205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,530 INFO epoch # 7973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074203253170708194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,617 INFO epoch # 7974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00711442249303218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,704 INFO epoch # 7975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010171495086979121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,784 INFO epoch # 7976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008223635100875981
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,867 INFO epoch # 7977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008868363249348477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:44,985 INFO epoch # 7978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00919228709244635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,077 INFO epoch # 7979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012987832087674178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,162 INFO epoch # 7980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010571467500994913
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:45,162 INFO *** epoch 7980, rolling-avg-loss (window=10)= 0.00903955199100892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,248 INFO epoch # 7981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00844860579672968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,328 INFO epoch # 7982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009768726464244537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,407 INFO epoch # 7983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00789576674287673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,483 INFO epoch # 7984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009003647610370535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,565 INFO epoch # 7985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007749246426101308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,642 INFO epoch # 7986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009502963948762044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,718 INFO epoch # 7987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008195116170099936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,797 INFO epoch # 7988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008904300382710062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,876 INFO epoch # 7989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009698709574877284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:45,952 INFO epoch # 7990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00823076371307252
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:45,953 INFO *** epoch 7990, rolling-avg-loss (window=10)= 0.008739784682984465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,034 INFO epoch # 7991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008155507901392411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,112 INFO epoch # 7992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01008533508866094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,188 INFO epoch # 7993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00858273954509059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,264 INFO epoch # 7994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0085471567363129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,341 INFO epoch # 7995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009040785938850604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,417 INFO epoch # 7996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01073158888175385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,494 INFO epoch # 7997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008753671834710985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,583 INFO epoch # 7998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00805431936896639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,661 INFO epoch # 7999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009721426584292203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,738 INFO epoch # 8000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009220254702086095
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:46,738 INFO *** epoch 8000, rolling-avg-loss (window=10)= 0.009089278658211696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,815 INFO epoch # 8001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076457843169919215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,891 INFO epoch # 8002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008082134685537312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:46,968 INFO epoch # 8003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007109668338671327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,044 INFO epoch # 8004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007052503904560581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,121 INFO epoch # 8005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00825049018021673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,198 INFO epoch # 8006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010547214566031471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,275 INFO epoch # 8007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008549475387553684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,351 INFO epoch # 8008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009528108450467698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,431 INFO epoch # 8009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008994416923087556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,507 INFO epoch # 8010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007592326546728145
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:47,507 INFO *** epoch 8010, rolling-avg-loss (window=10)= 0.008335212329984642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,584 INFO epoch # 8011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007406188058666885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,660 INFO epoch # 8012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00888395850051893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,737 INFO epoch # 8013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007939887058455497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,813 INFO epoch # 8014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009626794286305085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,889 INFO epoch # 8015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007248092631925829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:47,966 INFO epoch # 8016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009462624751904514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,043 INFO epoch # 8017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008002889662748203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,120 INFO epoch # 8018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008133437549986411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,197 INFO epoch # 8019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007888682004704606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,273 INFO epoch # 8020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009022666832606774
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:48,273 INFO *** epoch 8020, rolling-avg-loss (window=10)= 0.008361522133782273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,350 INFO epoch # 8021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007298207528947387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,427 INFO epoch # 8022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006892239267472178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,504 INFO epoch # 8023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009230602328898385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,584 INFO epoch # 8024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010095846737385727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,663 INFO epoch # 8025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007808489623130299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,740 INFO epoch # 8026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011685381294228137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,817 INFO epoch # 8027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008417722710873932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,894 INFO epoch # 8028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009493551522609778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:48,972 INFO epoch # 8029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010280381837219466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,050 INFO epoch # 8030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009390310231538024
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:49,050 INFO *** epoch 8030, rolling-avg-loss (window=10)= 0.009059273308230332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,127 INFO epoch # 8031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009161206660792232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,203 INFO epoch # 8032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009047612315043807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,281 INFO epoch # 8033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00760998681653291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,366 INFO epoch # 8034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007449675838870462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,447 INFO epoch # 8035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008937748163589276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,529 INFO epoch # 8036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076089853391749784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,607 INFO epoch # 8037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008598054482717998
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,683 INFO epoch # 8038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009070787018572446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,759 INFO epoch # 8039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007922009317553602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,843 INFO epoch # 8040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008464879996608943
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:49,844 INFO *** epoch 8040, rolling-avg-loss (window=10)= 0.008387094594945665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:49,925 INFO epoch # 8041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007519477163441479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,014 INFO epoch # 8042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008795670510153286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,100 INFO epoch # 8043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006765124613593798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,182 INFO epoch # 8044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008160123383277096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,271 INFO epoch # 8045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007567326661956031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,357 INFO epoch # 8046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011680656723910943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,445 INFO epoch # 8047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00847758243617136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,530 INFO epoch # 8048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008769056003075093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,610 INFO epoch # 8049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007379831615253352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,689 INFO epoch # 8050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008535610570106655
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:50,689 INFO *** epoch 8050, rolling-avg-loss (window=10)= 0.008365045968093909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,771 INFO epoch # 8051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010295109976141248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,850 INFO epoch # 8052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008462696372589562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:50,927 INFO epoch # 8053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007675134802411776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,004 INFO epoch # 8054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007735390281595755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,082 INFO epoch # 8055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007682824296352919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,158 INFO epoch # 8056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007862338527047541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,238 INFO epoch # 8057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00791358469723491
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,315 INFO epoch # 8058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006954065152967814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,392 INFO epoch # 8059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007743748843495268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,472 INFO epoch # 8060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007167747724452056
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:51,472 INFO *** epoch 8060, rolling-avg-loss (window=10)= 0.007949264067428885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,551 INFO epoch # 8061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008355804471648298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,629 INFO epoch # 8062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007911038628662936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,710 INFO epoch # 8063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007492558113881387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,789 INFO epoch # 8064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00903691064013401
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,865 INFO epoch # 8065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006638532751821913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:51,945 INFO epoch # 8066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008192160894395784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,022 INFO epoch # 8067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01011079055024311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,098 INFO epoch # 8068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008366768634004984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,176 INFO epoch # 8069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008339771899045445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,253 INFO epoch # 8070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007667868325370364
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:52,253 INFO *** epoch 8070, rolling-avg-loss (window=10)= 0.008211220490920823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,329 INFO epoch # 8071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007519211467297282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,407 INFO epoch # 8072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007820003360393457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,486 INFO epoch # 8073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009287359440349974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,563 INFO epoch # 8074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008348600327735767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,641 INFO epoch # 8075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007763393492496107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,718 INFO epoch # 8076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00930004940892104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,798 INFO epoch # 8077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007473749195924029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,875 INFO epoch # 8078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007228300069982652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:52,951 INFO epoch # 8079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009408879865077324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,028 INFO epoch # 8080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007459758424374741
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:53,028 INFO *** epoch 8080, rolling-avg-loss (window=10)= 0.008160930505255237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,108 INFO epoch # 8081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006292922145803459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,191 INFO epoch # 8082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008540536044165492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,276 INFO epoch # 8083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00890369475382613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,358 INFO epoch # 8084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008061413464020006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,439 INFO epoch # 8085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007019048527581617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,519 INFO epoch # 8086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007606743965880014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,600 INFO epoch # 8087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008915226368117146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,683 INFO epoch # 8088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01076628347800579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,764 INFO epoch # 8089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064753032274893485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,846 INFO epoch # 8090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008091739458905067
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:53,846 INFO *** epoch 8090, rolling-avg-loss (window=10)= 0.008067291143379406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:53,928 INFO epoch # 8091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007345106736465823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,007 INFO epoch # 8092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009415269938472193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,097 INFO epoch # 8093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007277164506376721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,214 INFO epoch # 8094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008268906021839939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,330 INFO epoch # 8095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010334875842090696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,419 INFO epoch # 8096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009700492984848097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,495 INFO epoch # 8097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008633947887574323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,574 INFO epoch # 8098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007727718133537564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,652 INFO epoch # 8099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00733294191013556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,729 INFO epoch # 8100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077923078497406095
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:54,730 INFO *** epoch 8100, rolling-avg-loss (window=10)= 0.008382873181108152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,808 INFO epoch # 8101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00700960243557347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,889 INFO epoch # 8102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008762592442508321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:54,968 INFO epoch # 8103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007542661391198635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,049 INFO epoch # 8104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007433598439092748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,126 INFO epoch # 8105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007764299421978649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,203 INFO epoch # 8106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008980599173810333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,282 INFO epoch # 8107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008603122783824801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,358 INFO epoch # 8108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00737127594766207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,435 INFO epoch # 8109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007400100279483013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,513 INFO epoch # 8110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008080229963525198
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:55,513 INFO *** epoch 8110, rolling-avg-loss (window=10)= 0.007894808227865724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,591 INFO epoch # 8111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009725022449856624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,668 INFO epoch # 8112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008141274673107546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,748 INFO epoch # 8113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012972369280760176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,826 INFO epoch # 8114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008055921447521541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,911 INFO epoch # 8115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009739267581608146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:55,994 INFO epoch # 8116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00964758702320978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,075 INFO epoch # 8117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009744619281264022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,155 INFO epoch # 8118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01060174370650202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,245 INFO epoch # 8119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008645296053146012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,352 INFO epoch # 8120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00733114449394634
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:56,352 INFO *** epoch 8120, rolling-avg-loss (window=10)= 0.00946042459909222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,434 INFO epoch # 8121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006779513605579268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,516 INFO epoch # 8122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008393927761062514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,595 INFO epoch # 8123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008876100269844756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,676 INFO epoch # 8124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010004216062952764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,755 INFO epoch # 8125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008478320603899192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,833 INFO epoch # 8126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007861868754844181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,913 INFO epoch # 8127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008475710521452129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:56,991 INFO epoch # 8128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072377435135422274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,074 INFO epoch # 8129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007891139524872415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,154 INFO epoch # 8130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007940125302411616
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:57,155 INFO *** epoch 8130, rolling-avg-loss (window=10)= 0.008193866592046106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,235 INFO epoch # 8131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007848810819268692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,314 INFO epoch # 8132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009281967795686796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,393 INFO epoch # 8133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009405551340023521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,470 INFO epoch # 8134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008415738622716162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,551 INFO epoch # 8135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007074141096381936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,630 INFO epoch # 8136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007104440344846807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,710 INFO epoch # 8137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007266283530043438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,789 INFO epoch # 8138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00803116473252885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,865 INFO epoch # 8139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009124893884290941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:57,943 INFO epoch # 8140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010262541080010124
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:57,943 INFO *** epoch 8140, rolling-avg-loss (window=10)= 0.008381553324579728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,022 INFO epoch # 8141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007797785765433218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,104 INFO epoch # 8142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008020639914320782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,183 INFO epoch # 8143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009272582712583244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,261 INFO epoch # 8144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008472922236251179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,337 INFO epoch # 8145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007094659493304789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,415 INFO epoch # 8146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007925085214083083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,492 INFO epoch # 8147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012658959807595238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,574 INFO epoch # 8148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015185897631454282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,661 INFO epoch # 8149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007594309070555028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,743 INFO epoch # 8150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007578633325465489
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:58,743 INFO *** epoch 8150, rolling-avg-loss (window=10)= 0.009160147517104634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,828 INFO epoch # 8151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007705504023761023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:58,917 INFO epoch # 8152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006411707938241307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,002 INFO epoch # 8153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008235662971856073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,083 INFO epoch # 8154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077868702210253105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,160 INFO epoch # 8155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007325144892092794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,237 INFO epoch # 8156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076208938407944515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,313 INFO epoch # 8157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007266050575708505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,390 INFO epoch # 8158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007599427262903191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,466 INFO epoch # 8159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072072579569066875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,545 INFO epoch # 8160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007994349747605156
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:24:59,545 INFO *** epoch 8160, rolling-avg-loss (window=10)= 0.00751528694308945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,623 INFO epoch # 8161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010813869870617054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,700 INFO epoch # 8162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00808308603882324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,777 INFO epoch # 8163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009839071615715511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,860 INFO epoch # 8164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007381411909591407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:24:59,958 INFO epoch # 8165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008246294892160222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:00,056 INFO epoch # 8166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008660768013214692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:00,153 INFO epoch # 8167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008149907924234867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:00,236 INFO epoch # 8168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007989128163899295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:00,325 INFO epoch # 8169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007710461795795709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:00,423 INFO epoch # 8170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007462855144694913
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:00,423 INFO *** epoch 8170, rolling-avg-loss (window=10)= 0.00843368553687469
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:00,507 INFO epoch # 8171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007719390676356852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:00,588 INFO epoch # 8172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008349734693183564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:00,670 INFO epoch # 8173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009614842587325256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:00,760 INFO epoch # 8174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007954470725962892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:00,840 INFO epoch # 8175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007696277083596215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:00,923 INFO epoch # 8176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008899657208530698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,003 INFO epoch # 8177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008862510505423415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,084 INFO epoch # 8178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007967464604007546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,162 INFO epoch # 8179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009769939781108405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,242 INFO epoch # 8180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009757843901752494
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:01,242 INFO *** epoch 8180, rolling-avg-loss (window=10)= 0.008659213176724734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,318 INFO epoch # 8181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008907808987714816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,414 INFO epoch # 8182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00873102898185607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,499 INFO epoch # 8183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007930774321721401
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,583 INFO epoch # 8184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006245113268960267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,665 INFO epoch # 8185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007807542817317881
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,743 INFO epoch # 8186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00907292558986228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,820 INFO epoch # 8187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008436195697868243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:01,919 INFO epoch # 8188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007541085236880463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,009 INFO epoch # 8189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006707651322358288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,092 INFO epoch # 8190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00782537659688387
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:02,092 INFO *** epoch 8190, rolling-avg-loss (window=10)= 0.007920550282142358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,171 INFO epoch # 8191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008526668047124986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,266 INFO epoch # 8192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00839999417803483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,369 INFO epoch # 8193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008255283391918056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,452 INFO epoch # 8194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006966348424612079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,533 INFO epoch # 8195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009181424728012644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,623 INFO epoch # 8196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008937326623708941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,729 INFO epoch # 8197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008988576955744065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,810 INFO epoch # 8198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00813798382296227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,887 INFO epoch # 8199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007613603134814184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:02,963 INFO epoch # 8200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009212299570208415
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:02,963 INFO *** epoch 8200, rolling-avg-loss (window=10)= 0.008421950887714047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,042 INFO epoch # 8201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009686644210887607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,121 INFO epoch # 8202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008541672767023556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,201 INFO epoch # 8203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008825837896438316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,278 INFO epoch # 8204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008306503281346522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,355 INFO epoch # 8205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00795751247642329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,432 INFO epoch # 8206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007301048361114226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,510 INFO epoch # 8207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007360395393334329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,587 INFO epoch # 8208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00713027392339427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,664 INFO epoch # 8209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008688024958246388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,741 INFO epoch # 8210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007755652186460793
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:03,741 INFO *** epoch 8210, rolling-avg-loss (window=10)= 0.00815535654546693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,818 INFO epoch # 8211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00885791332257213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,894 INFO epoch # 8212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009396629291586578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:03,970 INFO epoch # 8213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007758213519991841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,049 INFO epoch # 8214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007228436377772596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,129 INFO epoch # 8215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007848487119190395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,208 INFO epoch # 8216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072114108625100926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,287 INFO epoch # 8217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007055063142615836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,366 INFO epoch # 8218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008572731094318442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,447 INFO epoch # 8219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007886779705586378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,529 INFO epoch # 8220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011130374077765737
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:04,530 INFO *** epoch 8220, rolling-avg-loss (window=10)= 0.008294603851391002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,609 INFO epoch # 8221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00797236684593372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,686 INFO epoch # 8222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010943039378616959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,762 INFO epoch # 8223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008109895650704857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,842 INFO epoch # 8224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007331095679546706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,920 INFO epoch # 8225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007812003510480281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:04,999 INFO epoch # 8226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007577772099466529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,076 INFO epoch # 8227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008280852540337946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,155 INFO epoch # 8228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007648837570741307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,234 INFO epoch # 8229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011416953930165619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,314 INFO epoch # 8230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008468363012070768
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:05,314 INFO *** epoch 8230, rolling-avg-loss (window=10)= 0.00855611802180647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,405 INFO epoch # 8231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006843319584731944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,487 INFO epoch # 8232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006410755653632805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,573 INFO epoch # 8233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007541529281297699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,657 INFO epoch # 8234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007695895990764257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,737 INFO epoch # 8235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006943317406694405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,815 INFO epoch # 8236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00882774728233926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,894 INFO epoch # 8237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00801575297373347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:05,973 INFO epoch # 8238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007953267035190947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,055 INFO epoch # 8239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00784315136843361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,133 INFO epoch # 8240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00872636499843793
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:06,133 INFO *** epoch 8240, rolling-avg-loss (window=10)= 0.007680110157525633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,212 INFO epoch # 8241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007891483408457134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,292 INFO epoch # 8242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010854647589439992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,374 INFO epoch # 8243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012914586928673089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,454 INFO epoch # 8244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008465472848911304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,536 INFO epoch # 8245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008510814317560289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,616 INFO epoch # 8246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008016949097509496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,695 INFO epoch # 8247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075045724588562734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,773 INFO epoch # 8248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009379103190440219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,854 INFO epoch # 8249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008772625333222095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:06,938 INFO epoch # 8250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00885033257509349
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:06,939 INFO *** epoch 8250, rolling-avg-loss (window=10)= 0.009116058774816338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:07,021 INFO epoch # 8251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008377987178391777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:07,107 INFO epoch # 8252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008600510933320038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:07,190 INFO epoch # 8253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008077685350144748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:07,275 INFO epoch # 8254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075650034414138645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:07,382 INFO epoch # 8255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007300363700778689
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:07,492 INFO epoch # 8256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008130743983201683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:07,593 INFO epoch # 8257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008602743546362035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:07,691 INFO epoch # 8258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008931410302466247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:07,851 INFO epoch # 8259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008959444501670077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:07,990 INFO epoch # 8260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009423548457561992
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:07,990 INFO *** epoch 8260, rolling-avg-loss (window=10)= 0.008396944139531116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:08,081 INFO epoch # 8261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007739734814094845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:08,173 INFO epoch # 8262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006728407344780862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:08,258 INFO epoch # 8263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007699792804487515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:08,341 INFO epoch # 8264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007923620782094076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:08,425 INFO epoch # 8265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009300459976657294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:08,507 INFO epoch # 8266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009404642376466654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:08,596 INFO epoch # 8267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009693297542980872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:08,679 INFO epoch # 8268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00785208964953199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:08,760 INFO epoch # 8269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009058326970261987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:08,842 INFO epoch # 8270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008269613092124928
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:08,842 INFO *** epoch 8270, rolling-avg-loss (window=10)= 0.008366998535348103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:08,926 INFO epoch # 8271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00775165328377625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,008 INFO epoch # 8272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007108034129487351
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,096 INFO epoch # 8273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009477345272898674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,178 INFO epoch # 8274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008793590663117357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,260 INFO epoch # 8275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008553799336368684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,344 INFO epoch # 8276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0096245462045772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,426 INFO epoch # 8277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008051234115555417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,509 INFO epoch # 8278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074690770779852755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,597 INFO epoch # 8279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072804437222657725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,679 INFO epoch # 8280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007984020114236046
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:09,679 INFO *** epoch 8280, rolling-avg-loss (window=10)= 0.008209374392026802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,764 INFO epoch # 8281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007596541203383822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,846 INFO epoch # 8282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009266397413739469
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:09,931 INFO epoch # 8283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008723990424186923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,016 INFO epoch # 8284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009656120600993745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,104 INFO epoch # 8285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006945533234102186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,186 INFO epoch # 8286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008479063530103303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,273 INFO epoch # 8287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072077373042702675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,358 INFO epoch # 8288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008712788323464338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,442 INFO epoch # 8289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008164128303178586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,526 INFO epoch # 8290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008711903501534835
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:10,527 INFO *** epoch 8290, rolling-avg-loss (window=10)= 0.008346420383895747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,611 INFO epoch # 8291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008317842148244381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,693 INFO epoch # 8292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007607140767504461
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,775 INFO epoch # 8293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006981338148762006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,861 INFO epoch # 8294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011159226574818604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:10,947 INFO epoch # 8295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009633867673983332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,033 INFO epoch # 8296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008134348070598207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,122 INFO epoch # 8297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008453160015051253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,206 INFO epoch # 8298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006536635621159803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,289 INFO epoch # 8299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008821318733680528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,372 INFO epoch # 8300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00857093084778171
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:11,372 INFO *** epoch 8300, rolling-avg-loss (window=10)= 0.008421580860158429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,455 INFO epoch # 8301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007756335508020129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,537 INFO epoch # 8302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0084414452285273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,625 INFO epoch # 8303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008919576481275726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,708 INFO epoch # 8304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010784783757117111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,789 INFO epoch # 8305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007401721224596258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,872 INFO epoch # 8306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008131486400088761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:11,955 INFO epoch # 8307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013084874517517164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,037 INFO epoch # 8308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009580230864230543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,122 INFO epoch # 8309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009778844752872828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,205 INFO epoch # 8310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011008858076820616
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:12,205 INFO *** epoch 8310, rolling-avg-loss (window=10)= 0.009488815681106644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,288 INFO epoch # 8311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008062979650276247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,371 INFO epoch # 8312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008280839785584249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,455 INFO epoch # 8313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007939268289192114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,537 INFO epoch # 8314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009675686262198724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,622 INFO epoch # 8315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008341501546965446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,705 INFO epoch # 8316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006676819786662236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,789 INFO epoch # 8317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007725406438112259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,872 INFO epoch # 8318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010213352899882011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:12,955 INFO epoch # 8319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00679376697371481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,040 INFO epoch # 8320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007482281409465941
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:13,040 INFO *** epoch 8320, rolling-avg-loss (window=10)= 0.008119190304205404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,124 INFO epoch # 8321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00875758650363423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,205 INFO epoch # 8322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007380032489891164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,286 INFO epoch # 8323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008831168845063075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,369 INFO epoch # 8324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007715964689850807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,451 INFO epoch # 8325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008997151249786839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,535 INFO epoch # 8326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008894745376892388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,619 INFO epoch # 8327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007527312707679812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,702 INFO epoch # 8328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007860145247832406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,787 INFO epoch # 8329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00969702159636654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,874 INFO epoch # 8330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007150968063797336
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:13,874 INFO *** epoch 8330, rolling-avg-loss (window=10)= 0.00828120967707946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:13,959 INFO epoch # 8331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008794306602794677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,045 INFO epoch # 8332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008478950127027929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,131 INFO epoch # 8333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010729732312029228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,212 INFO epoch # 8334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075843102458748035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,296 INFO epoch # 8335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008450954723230097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,383 INFO epoch # 8336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007112518484063912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,469 INFO epoch # 8337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00701892921642866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,557 INFO epoch # 8338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007482239634555299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,644 INFO epoch # 8339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006892516146763228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,729 INFO epoch # 8340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077244169733603485
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:14,729 INFO *** epoch 8340, rolling-avg-loss (window=10)= 0.008026887446612818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,815 INFO epoch # 8341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007740101951640099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,900 INFO epoch # 8342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008953153090260457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:14,984 INFO epoch # 8343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007232594027300365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,068 INFO epoch # 8344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008707551161933225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,154 INFO epoch # 8345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009828625479713082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,239 INFO epoch # 8346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009677914400526788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,322 INFO epoch # 8347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006961306105949916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,406 INFO epoch # 8348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073690093413461
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,489 INFO epoch # 8349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007612792927830014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,574 INFO epoch # 8350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010012179853220005
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:15,574 INFO *** epoch 8350, rolling-avg-loss (window=10)= 0.008409522833972005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,662 INFO epoch # 8351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010810192456119694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,743 INFO epoch # 8352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006938117752724793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,825 INFO epoch # 8353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00766389488853747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,911 INFO epoch # 8354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007377608686510939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:15,998 INFO epoch # 8355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009308874570706394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:16,083 INFO epoch # 8356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008568742094212212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:16,168 INFO epoch # 8357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00971654541353928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:16,250 INFO epoch # 8358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007606915401993319
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:16,334 INFO epoch # 8359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007810928269464057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:16,418 INFO epoch # 8360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007534991607826669
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:16,419 INFO *** epoch 8360, rolling-avg-loss (window=10)= 0.008333681114163483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:16,502 INFO epoch # 8361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006759141026122961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:16,584 INFO epoch # 8362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009722460672492161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:16,691 INFO epoch # 8363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007931599589937832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:16,779 INFO epoch # 8364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011916690564248711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:16,879 INFO epoch # 8365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00688974438526202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:17,007 INFO epoch # 8366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007563564169686288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:17,138 INFO epoch # 8367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007537364945164882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:17,267 INFO epoch # 8368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007513695789384656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:17,395 INFO epoch # 8369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00739269291079836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:17,524 INFO epoch # 8370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008380109858990181
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:17,525 INFO *** epoch 8370, rolling-avg-loss (window=10)= 0.008160706391208805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:17,656 INFO epoch # 8371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00819078330823686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:17,786 INFO epoch # 8372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007673137755773496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:17,917 INFO epoch # 8373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00933069980237633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:18,053 INFO epoch # 8374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0106931607297156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:18,187 INFO epoch # 8375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008296375948702917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:18,319 INFO epoch # 8376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008843685245665256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:18,447 INFO epoch # 8377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00810125918360427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:18,578 INFO epoch # 8378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010174696173635311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:18,709 INFO epoch # 8379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007379282084002625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:18,838 INFO epoch # 8380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008698744379216805
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:18,838 INFO *** epoch 8380, rolling-avg-loss (window=10)= 0.008738182461092947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:18,968 INFO epoch # 8381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007463190398993902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:19,099 INFO epoch # 8382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007777296545100398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:19,228 INFO epoch # 8383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007759971740597393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:19,358 INFO epoch # 8384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006337133847409859
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:19,487 INFO epoch # 8385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010284410949680023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:19,618 INFO epoch # 8386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008571596510591917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:19,748 INFO epoch # 8387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009111887149629183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:19,877 INFO epoch # 8388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007782051296089776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:20,007 INFO epoch # 8389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00901176554907579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:20,139 INFO epoch # 8390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009871721646049991
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:20,140 INFO *** epoch 8390, rolling-avg-loss (window=10)= 0.008397102563321823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:20,268 INFO epoch # 8391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011301176302367821
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:20,400 INFO epoch # 8392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009353622604976408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:20,531 INFO epoch # 8393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00852905973806628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:20,662 INFO epoch # 8394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007479021056497004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:20,792 INFO epoch # 8395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006937478938198183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:20,923 INFO epoch # 8396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00861187107511796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:21,052 INFO epoch # 8397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007525523098593112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:21,183 INFO epoch # 8398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011357790819602087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:21,311 INFO epoch # 8399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007298680851818062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:21,439 INFO epoch # 8400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007846460051950999
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:21,440 INFO *** epoch 8400, rolling-avg-loss (window=10)= 0.008624068453718792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:21,570 INFO epoch # 8401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007546518150775228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:21,700 INFO epoch # 8402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007841836850275286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:21,829 INFO epoch # 8403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006998390294029377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:21,961 INFO epoch # 8404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007806659283232875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:22,092 INFO epoch # 8405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007456907085725106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:22,222 INFO epoch # 8406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008781745629676152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:22,350 INFO epoch # 8407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007612333305587526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:22,480 INFO epoch # 8408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00866679033060791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:22,610 INFO epoch # 8409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007874179405916948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:22,741 INFO epoch # 8410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007297630712855607
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:22,741 INFO *** epoch 8410, rolling-avg-loss (window=10)= 0.007788299104868201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:22,870 INFO epoch # 8411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010315482075384352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:22,998 INFO epoch # 8412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010341495522879995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:23,129 INFO epoch # 8413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007465857670467813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:23,258 INFO epoch # 8414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075757128579425626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:23,387 INFO epoch # 8415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006469638712587766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:23,518 INFO epoch # 8416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007079430833982769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:23,647 INFO epoch # 8417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008819077687803656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:23,777 INFO epoch # 8418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008004647737834603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:23,908 INFO epoch # 8419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007846256761695258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:24,038 INFO epoch # 8420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007331784610869363
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:24,038 INFO *** epoch 8420, rolling-avg-loss (window=10)= 0.008124938447144814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:24,169 INFO epoch # 8421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00897300284850644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:24,299 INFO epoch # 8422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006550873658852652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:24,429 INFO epoch # 8423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008319917396875098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:24,560 INFO epoch # 8424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009773954516276717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:24,689 INFO epoch # 8425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010902397028985433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:24,819 INFO epoch # 8426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009904832448228262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:24,950 INFO epoch # 8427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00693701185809914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:25,083 INFO epoch # 8428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008100973631371744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:25,214 INFO epoch # 8429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008163622995198239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:25,343 INFO epoch # 8430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006852741877082735
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:25,343 INFO *** epoch 8430, rolling-avg-loss (window=10)= 0.008447932825947645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:25,472 INFO epoch # 8431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006976338103413582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:25,602 INFO epoch # 8432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007529092566983309
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:25,733 INFO epoch # 8433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007721185953414533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:25,864 INFO epoch # 8434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007913011701020878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:25,993 INFO epoch # 8435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009319130054791458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:26,125 INFO epoch # 8436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008176479510439094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:26,254 INFO epoch # 8437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008844959869747981
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:26,383 INFO epoch # 8438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008594799488491844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:26,515 INFO epoch # 8439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009462883521337062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:26,647 INFO epoch # 8440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010632514371536672
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:26,647 INFO *** epoch 8440, rolling-avg-loss (window=10)= 0.008517039514117641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:26,778 INFO epoch # 8441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010170880050281994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:26,909 INFO epoch # 8442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00863356964691775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:27,039 INFO epoch # 8443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008076544880168512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:27,171 INFO epoch # 8444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007068429251376074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:27,299 INFO epoch # 8445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074546505202306435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:27,428 INFO epoch # 8446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008572422710130922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:27,559 INFO epoch # 8447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009823192725889385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:27,688 INFO epoch # 8448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009106878009333741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:27,818 INFO epoch # 8449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010666071946616285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:27,949 INFO epoch # 8450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008821748233458493
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:27,950 INFO *** epoch 8450, rolling-avg-loss (window=10)= 0.00883943879744038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:28,080 INFO epoch # 8451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009540967148495838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:28,209 INFO epoch # 8452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007406000389892142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:28,339 INFO epoch # 8453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067632911450346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:28,469 INFO epoch # 8454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008526169127435423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:28,600 INFO epoch # 8455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00759067757462617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:28,730 INFO epoch # 8456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006845521806098986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:28,860 INFO epoch # 8457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006752796070941258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:28,990 INFO epoch # 8458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010196635033935308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:29,122 INFO epoch # 8459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007365113931882661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:29,252 INFO epoch # 8460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007432105412590317
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:29,253 INFO *** epoch 8460, rolling-avg-loss (window=10)= 0.00784192776409327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:29,381 INFO epoch # 8461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007986729469848797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:29,510 INFO epoch # 8462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007688510188017972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:29,642 INFO epoch # 8463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006587077732547186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:29,777 INFO epoch # 8464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007166786934249103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:29,908 INFO epoch # 8465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007531365499744425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:30,037 INFO epoch # 8466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006865692754217889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:30,167 INFO epoch # 8467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006967749977775384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:30,295 INFO epoch # 8468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008538970221707132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:30,424 INFO epoch # 8469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007816305587766692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:30,555 INFO epoch # 8470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007639480150828604
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:30,556 INFO *** epoch 8470, rolling-avg-loss (window=10)= 0.007478866851670318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:30,687 INFO epoch # 8471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008286233292892575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:30,817 INFO epoch # 8472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010925178037723526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:30,948 INFO epoch # 8473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007633831708517391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:31,078 INFO epoch # 8474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008400126593187451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:31,208 INFO epoch # 8475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010435028525535017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:31,337 INFO epoch # 8476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010704137806897052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:31,465 INFO epoch # 8477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007852274669858161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:31,595 INFO epoch # 8478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007429579374729656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:31,723 INFO epoch # 8479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008489434105285909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:31,853 INFO epoch # 8480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007799506165611092
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:31,853 INFO *** epoch 8480, rolling-avg-loss (window=10)= 0.008795533028023782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:31,981 INFO epoch # 8481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00913264877453912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:32,113 INFO epoch # 8482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007716030566371046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:32,244 INFO epoch # 8483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074590546428225935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:32,372 INFO epoch # 8484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008240749761171173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:32,500 INFO epoch # 8485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007242596097057685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:32,631 INFO epoch # 8486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069948689779266715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:32,762 INFO epoch # 8487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008005799158127047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:32,894 INFO epoch # 8488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007540746606537141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:33,026 INFO epoch # 8489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008630945092590991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:33,159 INFO epoch # 8490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008938176732044667
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:33,159 INFO *** epoch 8490, rolling-avg-loss (window=10)= 0.007990161640918814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:33,289 INFO epoch # 8491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008268500248959754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:33,420 INFO epoch # 8492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008861210219038185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:33,550 INFO epoch # 8493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008784375531831756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:33,682 INFO epoch # 8494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006617484759772196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:33,810 INFO epoch # 8495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007612415982293896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:33,942 INFO epoch # 8496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007925924648588989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:34,072 INFO epoch # 8497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008245467586675659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:34,192 INFO epoch # 8498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00944175393669866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:34,273 INFO epoch # 8499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007862291429773904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:34,354 INFO epoch # 8500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009014923765789717
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:34,354 INFO *** epoch 8500, rolling-avg-loss (window=10)= 0.008263434810942272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:34,438 INFO epoch # 8501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008066686692473013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:34,519 INFO epoch # 8502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009103657430387102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:34,602 INFO epoch # 8503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008821336421533488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:34,687 INFO epoch # 8504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009542733183479868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:34,770 INFO epoch # 8505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00787141432374483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:34,851 INFO epoch # 8506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007383576747088227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:34,936 INFO epoch # 8507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006715365001582541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,021 INFO epoch # 8508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006770826788851991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,105 INFO epoch # 8509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008417112250754144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,188 INFO epoch # 8510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007461316810804419
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:35,188 INFO *** epoch 8510, rolling-avg-loss (window=10)= 0.008015402565069963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,268 INFO epoch # 8511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006623779874644242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,351 INFO epoch # 8512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007260671787662432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,432 INFO epoch # 8513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006134627794381231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,515 INFO epoch # 8514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007641657677595504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,600 INFO epoch # 8515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008575139116146602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,681 INFO epoch # 8516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007157866646593902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,763 INFO epoch # 8517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007284588791662827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,844 INFO epoch # 8518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006754578702384606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:35,925 INFO epoch # 8519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006897085368109401
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:36,018 INFO epoch # 8520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008314343154779635
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:36,018 INFO *** epoch 8520, rolling-avg-loss (window=10)= 0.007264433891396038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:36,109 INFO epoch # 8521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007025364822766278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:36,200 INFO epoch # 8522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007314036454772577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:36,282 INFO epoch # 8523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009058587093022652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:36,363 INFO epoch # 8524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007879732613218948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:36,446 INFO epoch # 8525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006937892088899389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:36,542 INFO epoch # 8526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008909142001357395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:36,667 INFO epoch # 8527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077130561985541135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:36,757 INFO epoch # 8528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008907048701075837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:36,852 INFO epoch # 8529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011180062487255782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:36,958 INFO epoch # 8530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007094424108800013
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:36,958 INFO *** epoch 8530, rolling-avg-loss (window=10)= 0.008201934656972299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,045 INFO epoch # 8531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00746656178671401
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,130 INFO epoch # 8532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008087870308372658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,230 INFO epoch # 8533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007058383162075188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,315 INFO epoch # 8534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011224447007407434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,397 INFO epoch # 8535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01097490853862837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,478 INFO epoch # 8536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010934619240288157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,561 INFO epoch # 8537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008223244934924878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,644 INFO epoch # 8538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00773827894590795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,726 INFO epoch # 8539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008556267275707796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,810 INFO epoch # 8540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009347980354505125
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:37,810 INFO *** epoch 8540, rolling-avg-loss (window=10)= 0.008961256155453156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,892 INFO epoch # 8541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007883649108407553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:37,984 INFO epoch # 8542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009070891406736337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:38,084 INFO epoch # 8543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007027912339253817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:38,170 INFO epoch # 8544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007428118056850508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:38,256 INFO epoch # 8545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008092399209999712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:38,340 INFO epoch # 8546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008547337711206637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:38,423 INFO epoch # 8547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007884691025537904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:38,505 INFO epoch # 8548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008406171145907138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:38,592 INFO epoch # 8549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008240310366090853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:38,677 INFO epoch # 8550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00758911397861084
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:38,677 INFO *** epoch 8550, rolling-avg-loss (window=10)= 0.00801705943486013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:38,762 INFO epoch # 8551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008106533568934537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:38,848 INFO epoch # 8552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007147393822378945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:38,932 INFO epoch # 8553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010489627755305264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,017 INFO epoch # 8554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010608281918393914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,103 INFO epoch # 8555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075994173748767935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,184 INFO epoch # 8556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01191457812092267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,268 INFO epoch # 8557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011241259024245664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,351 INFO epoch # 8558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008255654203821905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,432 INFO epoch # 8559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006989162582613062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,515 INFO epoch # 8560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0131749789143214
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:39,515 INFO *** epoch 8560, rolling-avg-loss (window=10)= 0.009552688728581416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,601 INFO epoch # 8561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006808773265220225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,683 INFO epoch # 8562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008178879332263023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,764 INFO epoch # 8563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068498738401103765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,847 INFO epoch # 8564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008205435398849659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:39,928 INFO epoch # 8565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008427427361311857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,011 INFO epoch # 8566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008031157820369117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,095 INFO epoch # 8567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008309813412779476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,177 INFO epoch # 8568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008604810587712564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,261 INFO epoch # 8569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012230714375618845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,343 INFO epoch # 8570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009175187151413411
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:40,343 INFO *** epoch 8570, rolling-avg-loss (window=10)= 0.008482207254564856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,426 INFO epoch # 8571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007615310467372183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,508 INFO epoch # 8572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007505519566620933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,592 INFO epoch # 8573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007112341816537082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,674 INFO epoch # 8574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006404161344107706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,756 INFO epoch # 8575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007547216635430232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,838 INFO epoch # 8576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074714888469316065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:40,920 INFO epoch # 8577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007688846599194221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,003 INFO epoch # 8578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007048929022857919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,090 INFO epoch # 8579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007264169566042256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,172 INFO epoch # 8580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008162435529811773
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:41,172 INFO *** epoch 8580, rolling-avg-loss (window=10)= 0.007382041939490591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,254 INFO epoch # 8581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007989794030436315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,338 INFO epoch # 8582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00852148578997003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,431 INFO epoch # 8583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00873615770979086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,514 INFO epoch # 8584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006977698525588494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,602 INFO epoch # 8585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007891740009654313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,684 INFO epoch # 8586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009936691028997302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,767 INFO epoch # 8587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009274435076804366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,849 INFO epoch # 8588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009676941997895483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:41,931 INFO epoch # 8589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009859232923190575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,012 INFO epoch # 8590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008685770502779633
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:42,012 INFO *** epoch 8590, rolling-avg-loss (window=10)= 0.008754994759510738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,098 INFO epoch # 8591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00732013206652482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,180 INFO epoch # 8592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009537578443996608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,261 INFO epoch # 8593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009310013432695996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,346 INFO epoch # 8594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007916059075796511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,427 INFO epoch # 8595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006949162318051094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,508 INFO epoch # 8596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007566021791717503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,595 INFO epoch # 8597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008513909560861066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,677 INFO epoch # 8598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007699569468968548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,764 INFO epoch # 8599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008956109275459312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,847 INFO epoch # 8600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011000246959156357
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:42,847 INFO *** epoch 8600, rolling-avg-loss (window=10)= 0.008476880239322781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:42,930 INFO epoch # 8601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008116021053865552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,012 INFO epoch # 8602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007625447848113254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,100 INFO epoch # 8603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009358094015624374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,181 INFO epoch # 8604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006535385928145843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,264 INFO epoch # 8605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006120175639807712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,346 INFO epoch # 8606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007304623562959023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,427 INFO epoch # 8607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009243798143870663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,509 INFO epoch # 8608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007748474235995673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,593 INFO epoch # 8609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007155235922255088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,674 INFO epoch # 8610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008655058321892284
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:43,674 INFO *** epoch 8610, rolling-avg-loss (window=10)= 0.007786231467252947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,757 INFO epoch # 8611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007351635700615589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,839 INFO epoch # 8612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007091743085766211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:43,921 INFO epoch # 8613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007376879453659058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,005 INFO epoch # 8614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007663460681214929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,089 INFO epoch # 8615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006706820902763866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,170 INFO epoch # 8616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008313896054460201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,252 INFO epoch # 8617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071710295051161665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,336 INFO epoch # 8618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007833146381017286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,420 INFO epoch # 8619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007319857642869465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,504 INFO epoch # 8620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007578727381769568
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:44,504 INFO *** epoch 8620, rolling-avg-loss (window=10)= 0.007440719678925234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,590 INFO epoch # 8621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008201074408134446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,673 INFO epoch # 8622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007259680787683465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,757 INFO epoch # 8623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008054210447880905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,840 INFO epoch # 8624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007824404347047675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:44,921 INFO epoch # 8625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007418274457450025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,004 INFO epoch # 8626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008573167579015717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,088 INFO epoch # 8627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006716555653838441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,169 INFO epoch # 8628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008069268267718144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,252 INFO epoch # 8629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01036440565076191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,336 INFO epoch # 8630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00878582097357139
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:45,336 INFO *** epoch 8630, rolling-avg-loss (window=10)= 0.008126686257310212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,418 INFO epoch # 8631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008022366586374119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,501 INFO epoch # 8632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007422572860377841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,587 INFO epoch # 8633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074915972363669425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,669 INFO epoch # 8634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008978771467809565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,752 INFO epoch # 8635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010075648708152585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,835 INFO epoch # 8636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007679784561332781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:45,916 INFO epoch # 8637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006929351067810785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,001 INFO epoch # 8638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006811846644268371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,087 INFO epoch # 8639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007582113023090642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,170 INFO epoch # 8640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007409724450553767
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:46,170 INFO *** epoch 8640, rolling-avg-loss (window=10)= 0.00784037766061374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,253 INFO epoch # 8641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006943278116523288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,337 INFO epoch # 8642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007720987661741674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,421 INFO epoch # 8643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006995612508035265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,503 INFO epoch # 8644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006735396156727802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,588 INFO epoch # 8645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008179739612387493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,670 INFO epoch # 8646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007142649104935117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,757 INFO epoch # 8647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00875031492614653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,839 INFO epoch # 8648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008898963089450262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:46,920 INFO epoch # 8649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007116839304217137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,001 INFO epoch # 8650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008995613505248912
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:47,002 INFO *** epoch 8650, rolling-avg-loss (window=10)= 0.007747939398541348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,087 INFO epoch # 8651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007547301203885581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,171 INFO epoch # 8652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007984016861882992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,254 INFO epoch # 8653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01014703897817526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,337 INFO epoch # 8654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008479720476316288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,420 INFO epoch # 8655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006910582800628617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,502 INFO epoch # 8656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008019039290957153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,587 INFO epoch # 8657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007377139525488019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,668 INFO epoch # 8658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008411709481151775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,751 INFO epoch # 8659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007183311339758802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,834 INFO epoch # 8660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068818374566035345
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:47,834 INFO *** epoch 8660, rolling-avg-loss (window=10)= 0.007894169741484802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,915 INFO epoch # 8661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007182892222772352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:47,997 INFO epoch # 8662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00870478109573014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:48,084 INFO epoch # 8663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008893499441910535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:48,167 INFO epoch # 8664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007709230987529736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:48,249 INFO epoch # 8665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007812573989212979
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:48,332 INFO epoch # 8666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007342331424297299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:48,416 INFO epoch # 8667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009408068828633986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:48,497 INFO epoch # 8668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014534237707266584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:48,583 INFO epoch # 8669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008696320954186376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:48,667 INFO epoch # 8670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007888712643762119
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:48,667 INFO *** epoch 8670, rolling-avg-loss (window=10)= 0.008817264929530211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:48,754 INFO epoch # 8671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007158043794333935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:48,840 INFO epoch # 8672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008266903321782593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:48,927 INFO epoch # 8673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008303114773298148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,014 INFO epoch # 8674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009572408482199535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,101 INFO epoch # 8675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00879037706181407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,183 INFO epoch # 8676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006954722513910383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,266 INFO epoch # 8677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00730090236174874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,349 INFO epoch # 8678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075268701402819715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,431 INFO epoch # 8679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075596186143229716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,513 INFO epoch # 8680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008548394253011793
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:49,513 INFO *** epoch 8680, rolling-avg-loss (window=10)= 0.007998135531670414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,602 INFO epoch # 8681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008712171096703969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,684 INFO epoch # 8682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006718644843203947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,770 INFO epoch # 8683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008344520079845097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,853 INFO epoch # 8684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00955067266477272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:49,935 INFO epoch # 8685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009690265498647932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,022 INFO epoch # 8686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00702813193493057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,109 INFO epoch # 8687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009543880354613066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,190 INFO epoch # 8688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007995125750312582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,275 INFO epoch # 8689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007982774855918251
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,361 INFO epoch # 8690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008328011608682573
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:50,361 INFO *** epoch 8690, rolling-avg-loss (window=10)= 0.00838941986876307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,441 INFO epoch # 8691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007743820206087548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,524 INFO epoch # 8692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073719515930861235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,613 INFO epoch # 8693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009161313530057669
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,697 INFO epoch # 8694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007804073597071692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,783 INFO epoch # 8695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009604012506315485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,867 INFO epoch # 8696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00868268210615497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:50,947 INFO epoch # 8697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008237778398324735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,030 INFO epoch # 8698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007046642720524687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,114 INFO epoch # 8699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007794022894813679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,198 INFO epoch # 8700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007542275081505068
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:51,199 INFO *** epoch 8700, rolling-avg-loss (window=10)= 0.008098857263394165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,284 INFO epoch # 8701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007174414757173508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,370 INFO epoch # 8702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011819537699921057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,454 INFO epoch # 8703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0086015744091128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,543 INFO epoch # 8704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007597719464683905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,628 INFO epoch # 8705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007958813126606401
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,712 INFO epoch # 8706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007759391453873832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,800 INFO epoch # 8707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00732768665329786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,884 INFO epoch # 8708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007897904237324838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:51,970 INFO epoch # 8709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006825397147622425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,056 INFO epoch # 8710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00732335358770797
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:52,057 INFO *** epoch 8710, rolling-avg-loss (window=10)= 0.00802857925373246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,141 INFO epoch # 8711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074301502281741705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,223 INFO epoch # 8712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006678797522909008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,306 INFO epoch # 8713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007403730327496305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,388 INFO epoch # 8714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007468906951544341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,470 INFO epoch # 8715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072054229967761785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,557 INFO epoch # 8716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006740182281646412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,640 INFO epoch # 8717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007270816364325583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,721 INFO epoch # 8718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007830104092136025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,807 INFO epoch # 8719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008981038416095544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,888 INFO epoch # 8720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007089598075253889
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:52,889 INFO *** epoch 8720, rolling-avg-loss (window=10)= 0.007409874725635745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:52,970 INFO epoch # 8721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006949055728910025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,058 INFO epoch # 8722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00848943443997996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,142 INFO epoch # 8723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006528799218358472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,224 INFO epoch # 8724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070837007951922715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,308 INFO epoch # 8725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007988516998011619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,394 INFO epoch # 8726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008396543584240135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,476 INFO epoch # 8727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007365525423665531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,562 INFO epoch # 8728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007434212107909843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,646 INFO epoch # 8729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075622068616212346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,728 INFO epoch # 8730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007790440951794153
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:53,728 INFO *** epoch 8730, rolling-avg-loss (window=10)= 0.0075588436109683245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,814 INFO epoch # 8731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007286556159669999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,897 INFO epoch # 8732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008332589241035748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:53,978 INFO epoch # 8733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006382200408552308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,064 INFO epoch # 8734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008930313066230156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,147 INFO epoch # 8735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008018324231670704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,228 INFO epoch # 8736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070694195019314066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,312 INFO epoch # 8737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008700300139025785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,393 INFO epoch # 8738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006836866152298171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,476 INFO epoch # 8739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006937307072803378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,563 INFO epoch # 8740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006505628200102365
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:54,563 INFO *** epoch 8740, rolling-avg-loss (window=10)= 0.007499950417332002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,646 INFO epoch # 8741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007271651047631167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,729 INFO epoch # 8742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076227040408411995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,816 INFO epoch # 8743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008935788900998887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,900 INFO epoch # 8744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008554013675166061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:54,986 INFO epoch # 8745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008066445028816815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,075 INFO epoch # 8746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007297949232452083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,164 INFO epoch # 8747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007569253160909284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,248 INFO epoch # 8748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007224843218864407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,331 INFO epoch # 8749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008249221449659672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,413 INFO epoch # 8750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008401334780501202
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:55,413 INFO *** epoch 8750, rolling-avg-loss (window=10)= 0.007919320453584077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,495 INFO epoch # 8751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008901578883524053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,580 INFO epoch # 8752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007734137871011626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,663 INFO epoch # 8753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008725616571609862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,746 INFO epoch # 8754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008383509986742865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,828 INFO epoch # 8755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006604617941775359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,909 INFO epoch # 8756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067241940123494714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:55,990 INFO epoch # 8757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007625096404808573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,074 INFO epoch # 8758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008268797551863827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,157 INFO epoch # 8759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009212767210556194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,238 INFO epoch # 8760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007781799613439944
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:56,238 INFO *** epoch 8760, rolling-avg-loss (window=10)= 0.007996211604768178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,328 INFO epoch # 8761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007420533635013271
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,413 INFO epoch # 8762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007610940658196341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,499 INFO epoch # 8763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007693430561630521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,583 INFO epoch # 8764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0085759358771611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,666 INFO epoch # 8765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009478026775468607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,749 INFO epoch # 8766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008253737250925042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,832 INFO epoch # 8767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007507065573008731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,914 INFO epoch # 8768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008306453230034094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:56,999 INFO epoch # 8769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007153338126954623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,084 INFO epoch # 8770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007022600453638006
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:57,084 INFO *** epoch 8770, rolling-avg-loss (window=10)= 0.007902206214203034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,169 INFO epoch # 8771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008629261472378857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,258 INFO epoch # 8772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008812696767563466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,344 INFO epoch # 8773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006812445375544485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,423 INFO epoch # 8774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006415795411157887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,501 INFO epoch # 8775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00788044693035772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,581 INFO epoch # 8776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00957171149275382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,660 INFO epoch # 8777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00712127083534142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,739 INFO epoch # 8778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007216586862341501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,819 INFO epoch # 8779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067057628475595266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,897 INFO epoch # 8780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006735635848599486
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:57,897 INFO *** epoch 8780, rolling-avg-loss (window=10)= 0.007590161384359817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:57,975 INFO epoch # 8781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00913397398107918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,055 INFO epoch # 8782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00832760820776457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,133 INFO epoch # 8783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009064714657142758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,212 INFO epoch # 8784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007948458987812046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,291 INFO epoch # 8785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068065615923842415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,371 INFO epoch # 8786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010012905600888189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,449 INFO epoch # 8787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008217037866415922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,528 INFO epoch # 8788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011931625791476108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,607 INFO epoch # 8789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009839483085670508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,686 INFO epoch # 8790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006146034902485553
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:58,686 INFO *** epoch 8790, rolling-avg-loss (window=10)= 0.008742840467311908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,767 INFO epoch # 8791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006558372260769829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,895 INFO epoch # 8792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010541940609982703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:58,973 INFO epoch # 8793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00903323957027169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,054 INFO epoch # 8794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076492575608426705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,132 INFO epoch # 8795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008602666384831537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,212 INFO epoch # 8796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007981016562553123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,291 INFO epoch # 8797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007724999399215449
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,370 INFO epoch # 8798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006694778669043444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,450 INFO epoch # 8799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007151961275667418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,532 INFO epoch # 8800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009707863187941257
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:25:59,532 INFO *** epoch 8800, rolling-avg-loss (window=10)= 0.008164609548111912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,611 INFO epoch # 8801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01024858411255991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,690 INFO epoch # 8802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008461041426926386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,769 INFO epoch # 8803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008414276730036363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,847 INFO epoch # 8804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006798941140004899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:25:59,925 INFO epoch # 8805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008064131659921259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,003 INFO epoch # 8806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00830975417920854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,082 INFO epoch # 8807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007644114579306915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,160 INFO epoch # 8808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007087736914400011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,238 INFO epoch # 8809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066340345510980114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,317 INFO epoch # 8810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071369918950949796
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:00,317 INFO *** epoch 8810, rolling-avg-loss (window=10)= 0.007879960718855727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,396 INFO epoch # 8811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00977700741350418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,473 INFO epoch # 8812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007208942224679049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,553 INFO epoch # 8813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075290394452167675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,631 INFO epoch # 8814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008271928338217549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,710 INFO epoch # 8815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007366739890130702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,789 INFO epoch # 8816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007214755285531282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,868 INFO epoch # 8817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008482841163640842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:00,946 INFO epoch # 8818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008152620401233435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,025 INFO epoch # 8819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008880851972207893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,104 INFO epoch # 8820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008150121277139988
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:01,104 INFO *** epoch 8820, rolling-avg-loss (window=10)= 0.008103484741150168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,182 INFO epoch # 8821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007661298579478171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,260 INFO epoch # 8822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077504407381638885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,338 INFO epoch # 8823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008623161738796625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,416 INFO epoch # 8824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009480681066634133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,495 INFO epoch # 8825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008045659589697607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,575 INFO epoch # 8826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006416388932848349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,653 INFO epoch # 8827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008360107836779207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,731 INFO epoch # 8828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011262841988354921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,809 INFO epoch # 8829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008832659063045867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,887 INFO epoch # 8830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068460034308373
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:01,887 INFO *** epoch 8830, rolling-avg-loss (window=10)= 0.008327924296463608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:01,965 INFO epoch # 8831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00927777451579459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,044 INFO epoch # 8832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007182993023889139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,123 INFO epoch # 8833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066633606984396465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,201 INFO epoch # 8834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007050734584481688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,280 INFO epoch # 8835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006820406066253781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,358 INFO epoch # 8836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008486866936436854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,437 INFO epoch # 8837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007887997053330764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,515 INFO epoch # 8838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008172868998371996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,608 INFO epoch # 8839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006287200914812274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,688 INFO epoch # 8840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00840404142945772
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:02,689 INFO *** epoch 8840, rolling-avg-loss (window=10)= 0.007623424422126846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,773 INFO epoch # 8841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007895459792052861
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,854 INFO epoch # 8842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007766322687530192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:02,935 INFO epoch # 8843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071948077966226265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,017 INFO epoch # 8844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00845795789064141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,098 INFO epoch # 8845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00631202196382219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,176 INFO epoch # 8846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007868695778597612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,255 INFO epoch # 8847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008328155861818232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,333 INFO epoch # 8848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007978441724844743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,412 INFO epoch # 8849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006505333301902283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,491 INFO epoch # 8850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00930268139927648
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:03,491 INFO *** epoch 8850, rolling-avg-loss (window=10)= 0.007760987819710863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,571 INFO epoch # 8851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00738646667014109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,649 INFO epoch # 8852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006694381729175802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,727 INFO epoch # 8853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068198173539713025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,806 INFO epoch # 8854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007623663368576672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,884 INFO epoch # 8855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007265946653205901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:03,962 INFO epoch # 8856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00658408154049539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,042 INFO epoch # 8857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006170098145958036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,121 INFO epoch # 8858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009944036712113302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,199 INFO epoch # 8859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009184509704937227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,278 INFO epoch # 8860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008604888782429043
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:04,278 INFO *** epoch 8860, rolling-avg-loss (window=10)= 0.007627789066100376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,357 INFO epoch # 8861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007754306170681957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,437 INFO epoch # 8862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007514873672334943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,515 INFO epoch # 8863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007211327647382859
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,594 INFO epoch # 8864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007486805217922665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,677 INFO epoch # 8865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009064216392289381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,759 INFO epoch # 8866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009749729120812844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,840 INFO epoch # 8867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009011068905238062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:04,922 INFO epoch # 8868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009380814721225761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,003 INFO epoch # 8869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008990342073957436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,085 INFO epoch # 8870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007800185274390969
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:05,086 INFO *** epoch 8870, rolling-avg-loss (window=10)= 0.008396366919623689
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,167 INFO epoch # 8871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009803887929592747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,255 INFO epoch # 8872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008136676500726026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,364 INFO epoch # 8873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006547585879161488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,444 INFO epoch # 8874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00782324600731954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,525 INFO epoch # 8875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008544470911147073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,607 INFO epoch # 8876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007250313909025863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,689 INFO epoch # 8877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008581192130804993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,770 INFO epoch # 8878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070137474467628635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,851 INFO epoch # 8879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008054873047512956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:05,932 INFO epoch # 8880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00976795845781453
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:05,932 INFO *** epoch 8880, rolling-avg-loss (window=10)= 0.008152395221986807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,013 INFO epoch # 8881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007419000852678437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,093 INFO epoch # 8882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006186933380377013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,171 INFO epoch # 8883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005849525405210443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,250 INFO epoch # 8884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007442997433827259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,328 INFO epoch # 8885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066934464339283295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,409 INFO epoch # 8886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006956134537176695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,487 INFO epoch # 8887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006513410633488093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,567 INFO epoch # 8888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006764935449609766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,649 INFO epoch # 8889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007804378361470299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,728 INFO epoch # 8890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006628270770306699
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:06,729 INFO *** epoch 8890, rolling-avg-loss (window=10)= 0.006825903325807303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,807 INFO epoch # 8891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007064457378874067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,888 INFO epoch # 8892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008998876633995678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:06,967 INFO epoch # 8893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00775912211247487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,047 INFO epoch # 8894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007286454136192333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,127 INFO epoch # 8895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006948710972210392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,205 INFO epoch # 8896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00853652945079375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,283 INFO epoch # 8897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008289821118523832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,361 INFO epoch # 8898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007566283573396504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,439 INFO epoch # 8899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011621635479968973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,517 INFO epoch # 8900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009997696004575118
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:07,517 INFO *** epoch 8900, rolling-avg-loss (window=10)= 0.008406958686100552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,598 INFO epoch # 8901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007296812909771688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,677 INFO epoch # 8902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007151861464080866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,756 INFO epoch # 8903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008402542342082597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,835 INFO epoch # 8904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008905150687496644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,913 INFO epoch # 8905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011599270786973648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:07,991 INFO epoch # 8906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006819990281655919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,071 INFO epoch # 8907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006663421074335929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,149 INFO epoch # 8908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065913957441807725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,227 INFO epoch # 8909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008903040587028954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,306 INFO epoch # 8910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00805833798222011
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:08,306 INFO *** epoch 8910, rolling-avg-loss (window=10)= 0.008039182385982713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,385 INFO epoch # 8911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007601776851515751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,463 INFO epoch # 8912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009145386284217238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,543 INFO epoch # 8913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006874840539239813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,625 INFO epoch # 8914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008251955281593837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,706 INFO epoch # 8915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008378622391319368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,785 INFO epoch # 8916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071670963952783495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,863 INFO epoch # 8917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007466640789061785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:08,942 INFO epoch # 8918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008554023537726607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,020 INFO epoch # 8919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006675294280285016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,102 INFO epoch # 8920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007327406885451637
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:09,103 INFO *** epoch 8920, rolling-avg-loss (window=10)= 0.00774430432356894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,183 INFO epoch # 8921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007726414623903111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,263 INFO epoch # 8922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008583613325754413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,341 INFO epoch # 8923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006674503754766192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,419 INFO epoch # 8924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006209987084730528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,497 INFO epoch # 8925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007319676486076787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,577 INFO epoch # 8926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00853394879231928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,655 INFO epoch # 8927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006570265446498524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,738 INFO epoch # 8928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01008504259516485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,816 INFO epoch # 8929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007136412954423577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,894 INFO epoch # 8930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008761474236962385
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:09,894 INFO *** epoch 8930, rolling-avg-loss (window=10)= 0.007760133930059965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:09,983 INFO epoch # 8931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008408575049543288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,080 INFO epoch # 8932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00915315760357771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,161 INFO epoch # 8933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008604733448009938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,240 INFO epoch # 8934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009156135725788772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,319 INFO epoch # 8935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007499872961489018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,397 INFO epoch # 8936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007335329362831544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,476 INFO epoch # 8937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007592245492560323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,556 INFO epoch # 8938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008417250202910509
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,634 INFO epoch # 8939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00884557694371324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,712 INFO epoch # 8940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008807666243228596
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:10,712 INFO *** epoch 8940, rolling-avg-loss (window=10)= 0.008382054303365295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,790 INFO epoch # 8941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008090374241874088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,867 INFO epoch # 8942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0084079173830105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:10,945 INFO epoch # 8943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009772656601853669
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,024 INFO epoch # 8944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006293758204265032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,102 INFO epoch # 8945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069314626744017005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,180 INFO epoch # 8946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00827495708654169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,258 INFO epoch # 8947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065580178634263575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,335 INFO epoch # 8948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00875227061624173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,413 INFO epoch # 8949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009704584983410314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,491 INFO epoch # 8950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009031224268255755
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:11,491 INFO *** epoch 8950, rolling-avg-loss (window=10)= 0.008181722392328084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,569 INFO epoch # 8951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006668645488389302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,647 INFO epoch # 8952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007606931212649215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,725 INFO epoch # 8953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006885355294798501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,803 INFO epoch # 8954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007115769913070835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,881 INFO epoch # 8955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007491370903153438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:11,958 INFO epoch # 8956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006751227694621775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,036 INFO epoch # 8957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00847256423003273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,115 INFO epoch # 8958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007406243727018591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,194 INFO epoch # 8959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006781524069083389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,272 INFO epoch # 8960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006793159263907
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:12,272 INFO *** epoch 8960, rolling-avg-loss (window=10)= 0.007197279179672477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,351 INFO epoch # 8961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009088558130315505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,429 INFO epoch # 8962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010134712902072351
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,507 INFO epoch # 8963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072867813214543276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,588 INFO epoch # 8964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009083675642614253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,668 INFO epoch # 8965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006394272451871075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,751 INFO epoch # 8966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074914373544743285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,847 INFO epoch # 8967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007556198237580247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:12,940 INFO epoch # 8968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006272940256167203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,033 INFO epoch # 8969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0059475917587406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,128 INFO epoch # 8970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007285765197593719
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:13,128 INFO *** epoch 8970, rolling-avg-loss (window=10)= 0.007654193325288361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,221 INFO epoch # 8971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00750128980143927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,313 INFO epoch # 8972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007086340854584705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,405 INFO epoch # 8973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007026211547781713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,485 INFO epoch # 8974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007571660178655293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,563 INFO epoch # 8975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007157926727813901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,643 INFO epoch # 8976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007158903150411788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,726 INFO epoch # 8977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007402426948829088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,808 INFO epoch # 8978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006682052131509408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,888 INFO epoch # 8979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00821661436202703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:13,966 INFO epoch # 8980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008288157885544933
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:13,967 INFO *** epoch 8980, rolling-avg-loss (window=10)= 0.0074091583588597135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,048 INFO epoch # 8981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007535388882388361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,126 INFO epoch # 8982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007612554865772836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,206 INFO epoch # 8983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008527593352482654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,284 INFO epoch # 8984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007866418818593957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,364 INFO epoch # 8985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008104936758172698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,444 INFO epoch # 8986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066841164079960436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,525 INFO epoch # 8987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007561962251202203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,607 INFO epoch # 8988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007206611306173727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,690 INFO epoch # 8989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008486487829941325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,772 INFO epoch # 8990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008587517644627951
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:14,773 INFO *** epoch 8990, rolling-avg-loss (window=10)= 0.007817358811735176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,854 INFO epoch # 8991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00709535078931367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:14,935 INFO epoch # 8992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009601929341442883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,014 INFO epoch # 8993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007587800719193183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,095 INFO epoch # 8994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007378348775091581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,173 INFO epoch # 8995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007659507195057813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,251 INFO epoch # 8996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007422248825605493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,330 INFO epoch # 8997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008097425248706713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,408 INFO epoch # 8998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007253198484249879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,486 INFO epoch # 8999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00877211315673776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,566 INFO epoch # 9000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007064078650728334
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:15,566 INFO *** epoch 9000, rolling-avg-loss (window=10)= 0.007793200118612731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,646 INFO epoch # 9001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006954204563953681
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,725 INFO epoch # 9002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006992314021772472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,807 INFO epoch # 9003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007154748451284831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,889 INFO epoch # 9004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007448663527611643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:15,969 INFO epoch # 9005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008169445784005802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,052 INFO epoch # 9006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007839150464860722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,132 INFO epoch # 9007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00876412008801708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,212 INFO epoch # 9008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007764524714730214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,294 INFO epoch # 9009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009970931903808378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,376 INFO epoch # 9010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008425686362897977
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:16,376 INFO *** epoch 9010, rolling-avg-loss (window=10)= 0.00794837898829428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,454 INFO epoch # 9011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008674313423398416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,534 INFO epoch # 9012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007099531329004094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,616 INFO epoch # 9013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007937575755931903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,718 INFO epoch # 9014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007506274567276705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,798 INFO epoch # 9015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008391336232307367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,874 INFO epoch # 9016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007588617474539205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:16,951 INFO epoch # 9017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00716220100002829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,030 INFO epoch # 9018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007242088628117926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,110 INFO epoch # 9019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066880356971523724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,186 INFO epoch # 9020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009362659278849605
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:17,186 INFO *** epoch 9020, rolling-avg-loss (window=10)= 0.007765263338660589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,263 INFO epoch # 9021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007971205617650412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,340 INFO epoch # 9022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007338643983530346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,416 INFO epoch # 9023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007073046930599958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,494 INFO epoch # 9024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009191156495944597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,574 INFO epoch # 9025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007525471082772128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,651 INFO epoch # 9026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006706307794956956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,730 INFO epoch # 9027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007968094556417782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,808 INFO epoch # 9028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009073362722119782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,885 INFO epoch # 9029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008433445793343708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:17,962 INFO epoch # 9030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009019637363962829
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:17,962 INFO *** epoch 9030, rolling-avg-loss (window=10)= 0.00803003723412985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,042 INFO epoch # 9031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007536619981692638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,120 INFO epoch # 9032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006086159741244046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,203 INFO epoch # 9033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00698251610447187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,279 INFO epoch # 9034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009761873217939865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,355 INFO epoch # 9035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007727405107289087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,432 INFO epoch # 9036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009023251324833836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,509 INFO epoch # 9037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006520974624436349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,588 INFO epoch # 9038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009129557642154396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,669 INFO epoch # 9039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01062371583248023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,752 INFO epoch # 9040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01359350114944391
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:18,752 INFO *** epoch 9040, rolling-avg-loss (window=10)= 0.008698557472598623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,832 INFO epoch # 9041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010728383502282668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,913 INFO epoch # 9042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009404582335264422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:18,994 INFO epoch # 9043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007574144627142232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,078 INFO epoch # 9044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00861469882511301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,161 INFO epoch # 9045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005946147386566736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,244 INFO epoch # 9046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008297467429656535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,326 INFO epoch # 9047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00949484222655883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,409 INFO epoch # 9048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007758645537251141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,490 INFO epoch # 9049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006556644810189027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,570 INFO epoch # 9050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006224071723408997
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:19,571 INFO *** epoch 9050, rolling-avg-loss (window=10)= 0.00805996284034336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,650 INFO epoch # 9051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006423021688533481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,727 INFO epoch # 9052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069160947314230725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,806 INFO epoch # 9053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007448366784956306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,886 INFO epoch # 9054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007742449844954535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:19,963 INFO epoch # 9055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007256049728312064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,039 INFO epoch # 9056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007376001623924822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,119 INFO epoch # 9057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070332884206436574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,197 INFO epoch # 9058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008007135671505239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,275 INFO epoch # 9059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009592123125912622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,357 INFO epoch # 9060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064935725458781235
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:20,357 INFO *** epoch 9060, rolling-avg-loss (window=10)= 0.007428810416604393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,444 INFO epoch # 9061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007392481646093074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,523 INFO epoch # 9062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007634616798895877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,602 INFO epoch # 9063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005857390766323078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,677 INFO epoch # 9064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007052170723909512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,796 INFO epoch # 9065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007538979887613095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,912 INFO epoch # 9066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007860752673877869
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:20,991 INFO epoch # 9067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007154496466682758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:21,068 INFO epoch # 9068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006005721865221858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:21,146 INFO epoch # 9069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008329848082212266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:21,276 INFO epoch # 9070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008831025348627008
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:21,277 INFO *** epoch 9070, rolling-avg-loss (window=10)= 0.00736574842594564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:21,394 INFO epoch # 9071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008028961492527742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:21,516 INFO epoch # 9072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006731076449796092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:21,618 INFO epoch # 9073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070039826750871725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:21,696 INFO epoch # 9074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006275853484112304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:21,780 INFO epoch # 9075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006833540821389761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:21,866 INFO epoch # 9076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006258946814341471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:21,944 INFO epoch # 9077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007696754422795493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:22,021 INFO epoch # 9078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008172586232831236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:22,142 INFO epoch # 9079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069448595932044555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:22,228 INFO epoch # 9080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006735661387210712
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:22,228 INFO *** epoch 9080, rolling-avg-loss (window=10)= 0.007068222337329644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:22,308 INFO epoch # 9081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008275604428490624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:22,392 INFO epoch # 9082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011288300709566101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:22,472 INFO epoch # 9083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062824228589306585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:22,552 INFO epoch # 9084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006612089593545534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:22,634 INFO epoch # 9085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00593545848823851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:22,735 INFO epoch # 9086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00700054503249703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:22,846 INFO epoch # 9087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007870429966715164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:22,956 INFO epoch # 9088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007649078688700683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:23,066 INFO epoch # 9089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007535510398156475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:23,176 INFO epoch # 9090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007095457935065497
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:23,177 INFO *** epoch 9090, rolling-avg-loss (window=10)= 0.007554489809990628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:23,285 INFO epoch # 9091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007240054597787093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:23,398 INFO epoch # 9092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009267326240660623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:23,520 INFO epoch # 9093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00869084580335766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:23,638 INFO epoch # 9094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006927483074832708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:23,742 INFO epoch # 9095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00823531857895432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:23,845 INFO epoch # 9096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006856928826891817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:23,998 INFO epoch # 9097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007081111645675264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:24,104 INFO epoch # 9098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008080233419605065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:24,205 INFO epoch # 9099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008007291929970961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:24,304 INFO epoch # 9100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006450588422012515
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:24,304 INFO *** epoch 9100, rolling-avg-loss (window=10)= 0.007683718253974803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:24,404 INFO epoch # 9101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006644427601713687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:24,503 INFO epoch # 9102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010397015917988028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:24,600 INFO epoch # 9103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010387856578745414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:24,701 INFO epoch # 9104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069325434233178385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:24,798 INFO epoch # 9105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006260816247959156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:24,897 INFO epoch # 9106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008297732056234963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:24,994 INFO epoch # 9107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006917842023540288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:25,093 INFO epoch # 9108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008117514084005961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:25,194 INFO epoch # 9109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067851736675947905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:25,294 INFO epoch # 9110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009186373761622235
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:25,294 INFO *** epoch 9110, rolling-avg-loss (window=10)= 0.007992729536272235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:25,392 INFO epoch # 9111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007471390963473823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:25,489 INFO epoch # 9112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008235384571889881
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:25,586 INFO epoch # 9113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008493873065162916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:25,684 INFO epoch # 9114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010086794303788338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:25,781 INFO epoch # 9115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00904949773394037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:25,878 INFO epoch # 9116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006493086875707377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:25,976 INFO epoch # 9117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006374075812345836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:26,073 INFO epoch # 9118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008034908976696897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:26,171 INFO epoch # 9119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008130398724460974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:26,268 INFO epoch # 9120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008315683779073879
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:26,268 INFO *** epoch 9120, rolling-avg-loss (window=10)= 0.008068509480654028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:26,365 INFO epoch # 9121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007825383814633824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:26,462 INFO epoch # 9122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008986836226540618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:26,560 INFO epoch # 9123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007059974646836054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:26,658 INFO epoch # 9124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006885767790663522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:26,757 INFO epoch # 9125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008304551898618229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:26,855 INFO epoch # 9126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074972924194298685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:26,952 INFO epoch # 9127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008907846829970367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:27,051 INFO epoch # 9128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008800892202998511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:27,148 INFO epoch # 9129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008207357896026224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:27,245 INFO epoch # 9130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00698064015887212
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:27,245 INFO *** epoch 9130, rolling-avg-loss (window=10)= 0.007945654388458933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:27,342 INFO epoch # 9131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00646884801244596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:27,439 INFO epoch # 9132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00778408617770765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:27,538 INFO epoch # 9133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0085428453530767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:27,637 INFO epoch # 9134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008031907775148284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:27,734 INFO epoch # 9135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007112645209417678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:27,831 INFO epoch # 9136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00802377589570824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:27,930 INFO epoch # 9137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007691596991207916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:28,027 INFO epoch # 9138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007671180152101442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:28,124 INFO epoch # 9139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076313338067848235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:28,223 INFO epoch # 9140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071656573345535435
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:28,223 INFO *** epoch 9140, rolling-avg-loss (window=10)= 0.007612387670815224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:28,319 INFO epoch # 9141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008997415003250353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:28,416 INFO epoch # 9142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00746221929875901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:28,513 INFO epoch # 9143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006776445887226146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:28,613 INFO epoch # 9144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008378754537261557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:28,709 INFO epoch # 9145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006848863173217978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:28,810 INFO epoch # 9146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009104598888370674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:28,910 INFO epoch # 9147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007959196213050745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:29,007 INFO epoch # 9148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007906228973297402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:29,105 INFO epoch # 9149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010500633652554825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:29,202 INFO epoch # 9150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007708481709414627
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:29,203 INFO *** epoch 9150, rolling-avg-loss (window=10)= 0.008164283733640332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:29,299 INFO epoch # 9151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067858646070817485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:29,396 INFO epoch # 9152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006804909098718781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:29,493 INFO epoch # 9153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007350012048846111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:29,590 INFO epoch # 9154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007401572664093692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:29,687 INFO epoch # 9155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007616121518367436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:29,783 INFO epoch # 9156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067436881581670605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:29,880 INFO epoch # 9157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006125576364865992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:29,979 INFO epoch # 9158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007487646260415204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:30,076 INFO epoch # 9159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009426612028619274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:30,175 INFO epoch # 9160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007875054609030485
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:30,175 INFO *** epoch 9160, rolling-avg-loss (window=10)= 0.007361705735820579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:30,272 INFO epoch # 9161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007256129865709227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:30,369 INFO epoch # 9162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006605599817703478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:30,465 INFO epoch # 9163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007556742602901068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:30,565 INFO epoch # 9164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068714185181306675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:30,662 INFO epoch # 9165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008016689898795448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:30,759 INFO epoch # 9166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012038685381412506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:30,857 INFO epoch # 9167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009219762225257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:30,954 INFO epoch # 9168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009992027953558136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:31,051 INFO epoch # 9169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008688811147294473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:31,147 INFO epoch # 9170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00867874403775204
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:31,148 INFO *** epoch 9170, rolling-avg-loss (window=10)= 0.008492461144851405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:31,244 INFO epoch # 9171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007844383944757283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:31,341 INFO epoch # 9172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007797316589858383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:31,438 INFO epoch # 9173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006975676165893674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:31,535 INFO epoch # 9174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006880259439640213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:31,634 INFO epoch # 9175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00751510176633019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:31,731 INFO epoch # 9176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007744523398287129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:31,829 INFO epoch # 9177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008714665964362212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:31,926 INFO epoch # 9178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008367513233679347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:32,023 INFO epoch # 9179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007049449392070528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:32,122 INFO epoch # 9180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008570355043048039
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:32,122 INFO *** epoch 9180, rolling-avg-loss (window=10)= 0.0077459244937927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:32,219 INFO epoch # 9181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00620151984912809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:32,316 INFO epoch # 9182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006682186547550373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:32,414 INFO epoch # 9183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007492044293030631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:32,511 INFO epoch # 9184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00732421509019332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:32,609 INFO epoch # 9185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006089348025852814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:32,706 INFO epoch # 9186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00763637315685628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:32,803 INFO epoch # 9187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007281276659341529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:32,901 INFO epoch # 9188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074290272932557855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:32,998 INFO epoch # 9189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008312736768857576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:33,097 INFO epoch # 9190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009071729058632627
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:33,097 INFO *** epoch 9190, rolling-avg-loss (window=10)= 0.007352045674269903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:33,194 INFO epoch # 9191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008355626589036547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:33,292 INFO epoch # 9192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007196624763309956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:33,389 INFO epoch # 9193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007750237287837081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:33,489 INFO epoch # 9194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006662891886662692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:33,586 INFO epoch # 9195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009035058108565863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:33,685 INFO epoch # 9196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006554569270520005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:33,782 INFO epoch # 9197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00794947021495318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:33,879 INFO epoch # 9198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006412680602807086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:33,977 INFO epoch # 9199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006426698557334021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:34,075 INFO epoch # 9200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006786548168747686
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:34,075 INFO *** epoch 9200, rolling-avg-loss (window=10)= 0.007313040544977412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:34,173 INFO epoch # 9201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007893498957855627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:34,270 INFO epoch # 9202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007523474625486415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:34,368 INFO epoch # 9203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00748213310725987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:34,465 INFO epoch # 9204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008272821898572147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:34,562 INFO epoch # 9205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00884756170125911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:34,660 INFO epoch # 9206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008041646557103377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:34,757 INFO epoch # 9207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060295445218798704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:34,854 INFO epoch # 9208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008167419349774718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:34,952 INFO epoch # 9209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009733810467878357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:35,050 INFO epoch # 9210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007535673757956829
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:35,050 INFO *** epoch 9210, rolling-avg-loss (window=10)= 0.007952758494502632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:35,149 INFO epoch # 9211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007301105433725752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:35,246 INFO epoch # 9212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007910315456683747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:35,343 INFO epoch # 9213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007423671406286303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:35,440 INFO epoch # 9214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007383238880720455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:35,536 INFO epoch # 9215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006455533191910945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:35,635 INFO epoch # 9216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007523868240241427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:35,732 INFO epoch # 9217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068204774797777645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:35,829 INFO epoch # 9218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006632838441873901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:35,925 INFO epoch # 9219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008133270297548734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:36,024 INFO epoch # 9220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076855691586388275
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:36,024 INFO *** epoch 9220, rolling-avg-loss (window=10)= 0.007326988798740786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:36,121 INFO epoch # 9221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007160897162975743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:36,218 INFO epoch # 9222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0098354988367646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:36,317 INFO epoch # 9223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008619457024906296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:36,413 INFO epoch # 9224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006788724960642867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:36,510 INFO epoch # 9225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006260364367335569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:36,609 INFO epoch # 9226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006929126298928168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:36,706 INFO epoch # 9227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073163943670806475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:36,803 INFO epoch # 9228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006728544882207643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:36,902 INFO epoch # 9229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076861590932821855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:36,998 INFO epoch # 9230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006407256732927635
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:36,999 INFO *** epoch 9230, rolling-avg-loss (window=10)= 0.007373242372705135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:37,098 INFO epoch # 9231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006233432381122839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:37,195 INFO epoch # 9232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006533403851790354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:37,293 INFO epoch # 9233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006507669248094317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:37,390 INFO epoch # 9234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006431450034142472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:37,487 INFO epoch # 9235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006650619659922086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:37,585 INFO epoch # 9236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006286730334977619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:37,681 INFO epoch # 9237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0081680400762707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:37,780 INFO epoch # 9238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008791252024821006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:37,877 INFO epoch # 9239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007821619212336373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:37,974 INFO epoch # 9240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007056973678118084
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:37,974 INFO *** epoch 9240, rolling-avg-loss (window=10)= 0.007048119050159585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:38,072 INFO epoch # 9241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007262160965183284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:38,170 INFO epoch # 9242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008664944703923538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:38,267 INFO epoch # 9243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009238967206329107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:38,364 INFO epoch # 9244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007005550985923037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:38,460 INFO epoch # 9245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007471267621440347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:38,558 INFO epoch # 9246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00773562109679915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:38,657 INFO epoch # 9247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007403450988931581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:38,754 INFO epoch # 9248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007057372204144485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:38,850 INFO epoch # 9249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008494295936543494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:38,949 INFO epoch # 9250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006649391332757659
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:38,949 INFO *** epoch 9250, rolling-avg-loss (window=10)= 0.007698302304197569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:39,046 INFO epoch # 9251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00734192160598468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:39,145 INFO epoch # 9252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007916818343801424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:39,243 INFO epoch # 9253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007624463891261257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:39,340 INFO epoch # 9254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066444533949834295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:39,439 INFO epoch # 9255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007281952686753357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:39,536 INFO epoch # 9256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006838787594460882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:39,634 INFO epoch # 9257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006922566542925779
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:39,732 INFO epoch # 9258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00713868375896709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:39,831 INFO epoch # 9259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008100137347355485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:39,928 INFO epoch # 9260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007833361421944574
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:39,928 INFO *** epoch 9260, rolling-avg-loss (window=10)= 0.007364314658843796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:40,025 INFO epoch # 9261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006020095657731872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:40,125 INFO epoch # 9262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013083276571705937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:40,222 INFO epoch # 9263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010523115292016882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:40,319 INFO epoch # 9264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006246205790375825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:40,416 INFO epoch # 9265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007146059760998469
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:40,512 INFO epoch # 9266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007155797240557149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:40,611 INFO epoch # 9267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010793710876896512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:40,709 INFO epoch # 9268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006876413070131093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:40,806 INFO epoch # 9269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008187815423298161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:40,902 INFO epoch # 9270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006736657494911924
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:40,903 INFO *** epoch 9270, rolling-avg-loss (window=10)= 0.008276914717862382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:40,999 INFO epoch # 9271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006614594127313467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:41,099 INFO epoch # 9272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006216824876901228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:41,196 INFO epoch # 9273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00732332964980742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:41,293 INFO epoch # 9274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007742542242340278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:41,389 INFO epoch # 9275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007381104915111791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:41,486 INFO epoch # 9276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006527317611471517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:41,586 INFO epoch # 9277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006396914817742072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:41,684 INFO epoch # 9278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007169620330387261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:41,780 INFO epoch # 9279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006968918445636518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:41,880 INFO epoch # 9280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00676114469388267
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:41,880 INFO *** epoch 9280, rolling-avg-loss (window=10)= 0.006910231171059422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:41,977 INFO epoch # 9281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006655664416030049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:42,073 INFO epoch # 9282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006670160393696278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:42,172 INFO epoch # 9283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008491902342939284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:42,269 INFO epoch # 9284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007178182779171038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:42,366 INFO epoch # 9285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008161673264112324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:42,464 INFO epoch # 9286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007690138430916704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:42,561 INFO epoch # 9287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007762256405840162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:42,660 INFO epoch # 9288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008718844896066003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:42,757 INFO epoch # 9289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00821638728666585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:42,854 INFO epoch # 9290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007958213493111543
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:42,854 INFO *** epoch 9290, rolling-avg-loss (window=10)= 0.0077503423708549235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:42,952 INFO epoch # 9291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008846978489600588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:43,050 INFO epoch # 9292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008801230440440122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:43,148 INFO epoch # 9293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008474523383483756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:43,245 INFO epoch # 9294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008316045947140083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:43,344 INFO epoch # 9295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00817375677434029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:43,443 INFO epoch # 9296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007916760951047763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:43,539 INFO epoch # 9297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00799008608009899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:43,636 INFO epoch # 9298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006611149932723492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:43,734 INFO epoch # 9299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006506393568997737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:43,831 INFO epoch # 9300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008042890120123047
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:43,832 INFO *** epoch 9300, rolling-avg-loss (window=10)= 0.007967981568799586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:43,928 INFO epoch # 9301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007193712270236574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:44,025 INFO epoch # 9302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006613266545173246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:44,124 INFO epoch # 9303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009570379370416049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:44,221 INFO epoch # 9304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070776061838842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:44,318 INFO epoch # 9305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070019562626839615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:44,415 INFO epoch # 9306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005539859994314611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:44,512 INFO epoch # 9307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00789013031317154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:44,610 INFO epoch # 9308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006815244705649093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:44,707 INFO epoch # 9309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00879675547912484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:44,806 INFO epoch # 9310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008595066676207352
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:44,806 INFO *** epoch 9310, rolling-avg-loss (window=10)= 0.007509397780086147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:44,903 INFO epoch # 9311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007081089170242194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:44,999 INFO epoch # 9312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009653933338995557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:45,099 INFO epoch # 9313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007682685733016115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:45,204 INFO epoch # 9314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007573713548481464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:45,304 INFO epoch # 9315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067574143831734546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:45,401 INFO epoch # 9316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0059834284547832794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:45,498 INFO epoch # 9317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076897838080185466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:45,595 INFO epoch # 9318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00597798521630466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:45,692 INFO epoch # 9319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006908203671628144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:45,789 INFO epoch # 9320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006549681842443533
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:45,790 INFO *** epoch 9320, rolling-avg-loss (window=10)= 0.007185791916708695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:45,887 INFO epoch # 9321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007604533479025122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:45,984 INFO epoch # 9322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006916805315995589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:46,081 INFO epoch # 9323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006788844664697535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:46,180 INFO epoch # 9324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008709540401468985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:46,277 INFO epoch # 9325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00721352959953947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:46,374 INFO epoch # 9326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00676231076795375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:46,471 INFO epoch # 9327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00668925151694566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:46,568 INFO epoch # 9328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00804746904032072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:46,665 INFO epoch # 9329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007756280334433541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:46,764 INFO epoch # 9330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072099675016943365
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:46,764 INFO *** epoch 9330, rolling-avg-loss (window=10)= 0.007369853262207471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:46,860 INFO epoch # 9331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007924972967884969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:46,957 INFO epoch # 9332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070197549648582935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:47,057 INFO epoch # 9333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008065571390034165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:47,154 INFO epoch # 9334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008587574237026274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:47,251 INFO epoch # 9335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006334929865261074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:47,347 INFO epoch # 9336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005668735415383708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:47,445 INFO epoch # 9337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006420123507268727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:47,542 INFO epoch # 9338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063454667761106975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:47,642 INFO epoch # 9339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007009856250078883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:47,739 INFO epoch # 9340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007291689766134368
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:47,739 INFO *** epoch 9340, rolling-avg-loss (window=10)= 0.007066867514004116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:47,836 INFO epoch # 9341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006955163473321591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:47,933 INFO epoch # 9342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0059015971928602085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:48,029 INFO epoch # 9343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005975378076982452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:48,129 INFO epoch # 9344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007600414166518021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:48,226 INFO epoch # 9345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071108408737927675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:48,322 INFO epoch # 9346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007305766448553186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:48,419 INFO epoch # 9347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012394607809255831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:48,519 INFO epoch # 9348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00877156293427106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:48,615 INFO epoch # 9349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009327134386694524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:48,712 INFO epoch # 9350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014146852983685676
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:48,713 INFO *** epoch 9350, rolling-avg-loss (window=10)= 0.008548931834593532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:48,812 INFO epoch # 9351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007527908630436286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:48,908 INFO epoch # 9352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006753707486495841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:49,005 INFO epoch # 9353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007233078722492792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:49,103 INFO epoch # 9354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064491412776988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:49,201 INFO epoch # 9355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070332652612705715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:49,298 INFO epoch # 9356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006375932251103222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:49,394 INFO epoch # 9357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006917734339367598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:49,491 INFO epoch # 9358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007880461613240186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:49,590 INFO epoch # 9359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065216403745580465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:49,687 INFO epoch # 9360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006875501549075125
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:49,687 INFO *** epoch 9360, rolling-avg-loss (window=10)= 0.006956837150573847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:49,783 INFO epoch # 9361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006707651184115093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:49,882 INFO epoch # 9362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00832629016804276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:49,979 INFO epoch # 9363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008643143839435652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:50,077 INFO epoch # 9364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007536754579632543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:50,174 INFO epoch # 9365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006745083454006817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:50,272 INFO epoch # 9366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070061907899798825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:50,370 INFO epoch # 9367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006695557298371568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:50,466 INFO epoch # 9368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075124377544852905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:50,564 INFO epoch # 9369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006341599073493853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:50,662 INFO epoch # 9370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073699321001186036
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:50,662 INFO *** epoch 9370, rolling-avg-loss (window=10)= 0.007288464024168206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:50,759 INFO epoch # 9371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008375186036573723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:50,856 INFO epoch # 9372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007348637249378953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:50,954 INFO epoch # 9373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008179267053492367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:51,051 INFO epoch # 9374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008658027887577191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:51,150 INFO epoch # 9375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064099287847056985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:51,246 INFO epoch # 9376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006593427715415601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:51,343 INFO epoch # 9377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007557107681350317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:51,440 INFO epoch # 9378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068995612891740166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:51,537 INFO epoch # 9379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008453012473182753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:51,636 INFO epoch # 9380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009529114307952113
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:51,636 INFO *** epoch 9380, rolling-avg-loss (window=10)= 0.007800327047880274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:51,733 INFO epoch # 9381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007395204156637192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:51,830 INFO epoch # 9382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008351605632924475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:51,927 INFO epoch # 9383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008489295803883579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:52,024 INFO epoch # 9384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007572964852442965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:52,121 INFO epoch # 9385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007513579024816863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:52,219 INFO epoch # 9386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008290345373097807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:52,316 INFO epoch # 9387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063278461311711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:52,413 INFO epoch # 9388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006426356638257857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:52,510 INFO epoch # 9389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007078715680108871
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:52,610 INFO epoch # 9390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00693188752484275
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:52,610 INFO *** epoch 9390, rolling-avg-loss (window=10)= 0.007437780081818346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:52,706 INFO epoch # 9391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006858831031422596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:52,803 INFO epoch # 9392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008151236186677124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:52,901 INFO epoch # 9393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008813002255919855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:52,998 INFO epoch # 9394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007840060570742935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:53,095 INFO epoch # 9395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008337023929925635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:53,195 INFO epoch # 9396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006685961350740399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:53,292 INFO epoch # 9397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00625730752653908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:53,388 INFO epoch # 9398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006566503281646874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:53,485 INFO epoch # 9399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00705039722379297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:53,583 INFO epoch # 9400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005801798091852106
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:53,584 INFO *** epoch 9400, rolling-avg-loss (window=10)= 0.007236212144925957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:53,681 INFO epoch # 9401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064672268272261135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:53,778 INFO epoch # 9402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008171340814442374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:53,876 INFO epoch # 9403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006701319805870298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:53,972 INFO epoch # 9404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008083047476247884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:54,071 INFO epoch # 9405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00642371521826135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:54,168 INFO epoch # 9406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01184936906065559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:54,264 INFO epoch # 9407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007890424567449372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:54,361 INFO epoch # 9408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00668142200447619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:54,458 INFO epoch # 9409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071243863421841525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:54,556 INFO epoch # 9410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006515001936350018
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:54,556 INFO *** epoch 9410, rolling-avg-loss (window=10)= 0.007590725405316334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:54,655 INFO epoch # 9411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007496823440305889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:54,752 INFO epoch # 9412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075497820871532895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:54,849 INFO epoch # 9413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073171224212273955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:54,946 INFO epoch # 9414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006898012870806269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:55,042 INFO epoch # 9415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006909069896209985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:55,141 INFO epoch # 9416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007022460747975856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:55,238 INFO epoch # 9417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007263891871843953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:55,335 INFO epoch # 9418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007110325739631662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:55,432 INFO epoch # 9419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006863166177936364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:55,530 INFO epoch # 9420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007249677131767385
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:55,530 INFO *** epoch 9420, rolling-avg-loss (window=10)= 0.007168033238485805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:55,627 INFO epoch # 9421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006746573926648125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:55,724 INFO epoch # 9422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007328634281293489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:55,822 INFO epoch # 9423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006542814189742785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:55,919 INFO epoch # 9424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007249127666000277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:56,015 INFO epoch # 9425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007179246473242529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:56,114 INFO epoch # 9426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006997563308686949
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:56,211 INFO epoch # 9427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064097608119482175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:56,308 INFO epoch # 9428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074211971295881085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:56,407 INFO epoch # 9429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007701699432800524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:56,504 INFO epoch # 9430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006428567161492538
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:56,504 INFO *** epoch 9430, rolling-avg-loss (window=10)= 0.007000518438144354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:56,602 INFO epoch # 9431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068566073387046345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:56,699 INFO epoch # 9432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005864795501111075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:56,796 INFO epoch # 9433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007256123979459517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:56,894 INFO epoch # 9434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007682105057028821
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:56,991 INFO epoch # 9435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006879712163936347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:57,089 INFO epoch # 9436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007739464192127343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:57,187 INFO epoch # 9437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008002913651580457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:57,284 INFO epoch # 9438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067781186298816465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:57,381 INFO epoch # 9439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007227473484817892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:57,477 INFO epoch # 9440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007000329136644723
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:57,478 INFO *** epoch 9440, rolling-avg-loss (window=10)= 0.007128764313529245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:57,577 INFO epoch # 9441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007655706547666341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:57,674 INFO epoch # 9442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006537681540066842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:57,773 INFO epoch # 9443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005956019820587244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:57,870 INFO epoch # 9444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008744868253415916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:57,967 INFO epoch # 9445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007667839265195653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:58,065 INFO epoch # 9446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006976504504564218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:58,163 INFO epoch # 9447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068698024115292355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:58,261 INFO epoch # 9448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064029369605123065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:58,357 INFO epoch # 9449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006784696284739766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:58,456 INFO epoch # 9450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0078887352719903
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:58,456 INFO *** epoch 9450, rolling-avg-loss (window=10)= 0.007148479086026782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:58,553 INFO epoch # 9451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010362566375988536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:58,650 INFO epoch # 9452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008394580763706472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:58,749 INFO epoch # 9453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008607056326582097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:58,845 INFO epoch # 9454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0083064513237332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:58,941 INFO epoch # 9455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005465286041726358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:59,039 INFO epoch # 9456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006646459638432134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:59,137 INFO epoch # 9457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008397258163313381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:59,234 INFO epoch # 9458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007234604250697885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:59,331 INFO epoch # 9459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007891921668488067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:59,429 INFO epoch # 9460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00931033098459011
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:26:59,430 INFO *** epoch 9460, rolling-avg-loss (window=10)= 0.008061651553725823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:59,527 INFO epoch # 9461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005962462193565443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:59,630 INFO epoch # 9462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007580289093311876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:59,727 INFO epoch # 9463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005919207927945536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:59,816 INFO epoch # 9464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006742934427165892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:59,897 INFO epoch # 9465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006797931622713804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:26:59,976 INFO epoch # 9466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007291858455573674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,056 INFO epoch # 9467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006745440594386309
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,137 INFO epoch # 9468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006808639151131501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,216 INFO epoch # 9469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006774892724934034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,294 INFO epoch # 9470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007218693215691019
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:00,295 INFO *** epoch 9470, rolling-avg-loss (window=10)= 0.006784234940641909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,376 INFO epoch # 9471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007077980175381526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,456 INFO epoch # 9472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006352537275233772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,535 INFO epoch # 9473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006461529068474192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,615 INFO epoch # 9474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007435038191033527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,695 INFO epoch # 9475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007119839465303812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,773 INFO epoch # 9476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006994064686296042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,851 INFO epoch # 9477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008004322029592004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:00,929 INFO epoch # 9478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007635206296981778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,007 INFO epoch # 9479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007205601592431776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,086 INFO epoch # 9480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008661313688207883
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:01,086 INFO *** epoch 9480, rolling-avg-loss (window=10)= 0.007294743246893632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,164 INFO epoch # 9481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064928668580250815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,242 INFO epoch # 9482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009293969385907985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,321 INFO epoch # 9483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007316092604014557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,399 INFO epoch # 9484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006968356145080179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,477 INFO epoch # 9485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006746944258338772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,557 INFO epoch # 9486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010867443626921158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,643 INFO epoch # 9487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006836310123617295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,733 INFO epoch # 9488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007495110905438196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,819 INFO epoch # 9489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005999252578476444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,903 INFO epoch # 9490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007569250250526238
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:01,903 INFO *** epoch 9490, rolling-avg-loss (window=10)= 0.00755855967363459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:01,981 INFO epoch # 9491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009920821205014363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,063 INFO epoch # 9492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007908154489996377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,143 INFO epoch # 9493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007396475557470694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,225 INFO epoch # 9494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008601966030255426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,304 INFO epoch # 9495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006571111265657237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,384 INFO epoch # 9496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006084386244765483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,461 INFO epoch # 9497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007046289480058476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,542 INFO epoch # 9498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006211457432073075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,622 INFO epoch # 9499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010207540457486175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,705 INFO epoch # 9500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009327961233793758
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:02,705 INFO *** epoch 9500, rolling-avg-loss (window=10)= 0.007927616339657106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,790 INFO epoch # 9501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00847280198649969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,874 INFO epoch # 9502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005958370478765573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:02,955 INFO epoch # 9503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006239378337340895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,035 INFO epoch # 9504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007691352970141452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,114 INFO epoch # 9505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007175963881309144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,193 INFO epoch # 9506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009644936239055824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,274 INFO epoch # 9507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007908302475698292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,353 INFO epoch # 9508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010540851777477656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,432 INFO epoch # 9509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007207337723230012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,510 INFO epoch # 9510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0059316645783837885
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:03,510 INFO *** epoch 9510, rolling-avg-loss (window=10)= 0.007677096044790233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,592 INFO epoch # 9511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005680804926669225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,675 INFO epoch # 9512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008964936932898127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,760 INFO epoch # 9513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011789370269980282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,842 INFO epoch # 9514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007199455598311033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:03,923 INFO epoch # 9515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006072305310226511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,005 INFO epoch # 9516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071057579552871175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,086 INFO epoch # 9517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007480498628865462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,168 INFO epoch # 9518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006514479362522252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,249 INFO epoch # 9519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006294292114034761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,330 INFO epoch # 9520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058406370553711895
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:04,330 INFO *** epoch 9520, rolling-avg-loss (window=10)= 0.007294253815416596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,408 INFO epoch # 9521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006427771833841689
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,489 INFO epoch # 9522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006450620952819008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,571 INFO epoch # 9523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007361403557297308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,651 INFO epoch # 9524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006756204107659869
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,729 INFO epoch # 9525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006491908177849837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,807 INFO epoch # 9526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008139642624882981
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,885 INFO epoch # 9527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007532212897785939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:04,964 INFO epoch # 9528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007215410478238482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,047 INFO epoch # 9529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006291606274317019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,128 INFO epoch # 9530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007493635464925319
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:05,129 INFO *** epoch 9530, rolling-avg-loss (window=10)= 0.007016041636961745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,209 INFO epoch # 9531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006751104570867028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,289 INFO epoch # 9532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006454739588662051
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,366 INFO epoch # 9533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073926218174165115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,444 INFO epoch # 9534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008446488209301606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,523 INFO epoch # 9535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006851054531580303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,602 INFO epoch # 9536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007667760670301504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,681 INFO epoch # 9537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007115628744941205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,759 INFO epoch # 9538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006936888341442682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,837 INFO epoch # 9539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006423268001526594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,915 INFO epoch # 9540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006852786711533554
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:05,915 INFO *** epoch 9540, rolling-avg-loss (window=10)= 0.007089234118757304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:05,994 INFO epoch # 9541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005673809369909577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,072 INFO epoch # 9542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007654588094737846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,152 INFO epoch # 9543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009052968722244259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,231 INFO epoch # 9544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006739379758073483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,310 INFO epoch # 9545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006198478673468344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,389 INFO epoch # 9546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00905486448755255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,467 INFO epoch # 9547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067510042354115285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,546 INFO epoch # 9548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007431493912008591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,627 INFO epoch # 9549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006101723836763995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,708 INFO epoch # 9550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006646083835221361
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:06,709 INFO *** epoch 9550, rolling-avg-loss (window=10)= 0.007130439492539153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,788 INFO epoch # 9551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00628530506946845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,867 INFO epoch # 9552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067707987982430495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:06,946 INFO epoch # 9553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006131320878921542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,024 INFO epoch # 9554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006785840872908011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,106 INFO epoch # 9555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007859730132622644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,186 INFO epoch # 9556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007103216623363551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,264 INFO epoch # 9557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009581648562743794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,343 INFO epoch # 9558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007004696271906141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,423 INFO epoch # 9559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006688701789244078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,504 INFO epoch # 9560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007500975589209702
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:07,504 INFO *** epoch 9560, rolling-avg-loss (window=10)= 0.007171223458863096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,585 INFO epoch # 9561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008224304270697758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,664 INFO epoch # 9562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009496195110841654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,743 INFO epoch # 9563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068667980303871445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,825 INFO epoch # 9564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008391070041398052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,907 INFO epoch # 9565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007454514488927089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:07,987 INFO epoch # 9566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071125442191259935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,067 INFO epoch # 9567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005717119791370351
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,146 INFO epoch # 9568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007205433168564923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,224 INFO epoch # 9569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007006866449955851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,303 INFO epoch # 9570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007572377558972221
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:08,303 INFO *** epoch 9570, rolling-avg-loss (window=10)= 0.007504722313024103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,382 INFO epoch # 9571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005515742534043966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,462 INFO epoch # 9572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074532904764055274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,542 INFO epoch # 9573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009351847365905996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,624 INFO epoch # 9574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007067025435389951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,704 INFO epoch # 9575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006458142830524594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,783 INFO epoch # 9576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008930922813306097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,861 INFO epoch # 9577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007149023294914514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:08,939 INFO epoch # 9578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007438115033437498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,017 INFO epoch # 9579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007745152379357023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,096 INFO epoch # 9580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007072203035932034
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:09,096 INFO *** epoch 9580, rolling-avg-loss (window=10)= 0.00741814651992172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,174 INFO epoch # 9581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006714424875099212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,252 INFO epoch # 9582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073404498034506105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,331 INFO epoch # 9583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007015696341113653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,409 INFO epoch # 9584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006283990784140769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,487 INFO epoch # 9585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006411918089725077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,565 INFO epoch # 9586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006132715832791291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,642 INFO epoch # 9587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007377925030596089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,720 INFO epoch # 9588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006976682678214274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,798 INFO epoch # 9589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006541154049045872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,876 INFO epoch # 9590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007315696682780981
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:09,877 INFO *** epoch 9590, rolling-avg-loss (window=10)= 0.006811065416695783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:09,955 INFO epoch # 9591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007639568946615327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:10,035 INFO epoch # 9592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008830530670820735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:10,114 INFO epoch # 9593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007211259318864904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:10,193 INFO epoch # 9594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006849127996247262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:10,270 INFO epoch # 9595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006670427646895405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:10,348 INFO epoch # 9596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007653982116607949
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:10,435 INFO epoch # 9597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068351807713042945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:10,516 INFO epoch # 9598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073637218156363815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:10,594 INFO epoch # 9599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007369885530351894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:10,693 INFO epoch # 9600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008599420310929418
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:10,694 INFO *** epoch 9600, rolling-avg-loss (window=10)= 0.007502310512427357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:10,815 INFO epoch # 9601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008459304401185364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:10,927 INFO epoch # 9602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006571528218046296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,005 INFO epoch # 9603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00695872594951652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,083 INFO epoch # 9604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007716100320976693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,161 INFO epoch # 9605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007426658688928001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,239 INFO epoch # 9606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006204786557646003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,316 INFO epoch # 9607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00782539379724767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,395 INFO epoch # 9608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067858453912776895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,476 INFO epoch # 9609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006866187235573307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,554 INFO epoch # 9610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007619492127560079
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:11,554 INFO *** epoch 9610, rolling-avg-loss (window=10)= 0.007243402268795762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,632 INFO epoch # 9611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006728114538418595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,710 INFO epoch # 9612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006954940661671571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,787 INFO epoch # 9613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006065280467737466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,865 INFO epoch # 9614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00699713418725878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:11,943 INFO epoch # 9615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006637887781835161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,021 INFO epoch # 9616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008502381373546086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,101 INFO epoch # 9617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073592865592218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,180 INFO epoch # 9618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077491513366112486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,258 INFO epoch # 9619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00651587349420879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,338 INFO epoch # 9620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006683438972686417
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:12,338 INFO *** epoch 9620, rolling-avg-loss (window=10)= 0.007019348937319592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,417 INFO epoch # 9621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007827835499483626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,496 INFO epoch # 9622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007535770135291386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,578 INFO epoch # 9623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008169538297806866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,657 INFO epoch # 9624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007210689538624138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,735 INFO epoch # 9625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007722226211626548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,815 INFO epoch # 9626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006882356290589087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,894 INFO epoch # 9627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006187941769894678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:12,972 INFO epoch # 9628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008192902620066889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,052 INFO epoch # 9629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007504548993892968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,131 INFO epoch # 9630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006002743466524407
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:13,131 INFO *** epoch 9630, rolling-avg-loss (window=10)= 0.0073236552823800595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,209 INFO epoch # 9631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007530511371442117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,290 INFO epoch # 9632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007037387331365608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,368 INFO epoch # 9633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007155380291806068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,447 INFO epoch # 9634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066012398528982885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,527 INFO epoch # 9635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006515541681437753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,608 INFO epoch # 9636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061080920495442115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,688 INFO epoch # 9637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008727141284907702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,767 INFO epoch # 9638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008348934396053664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,846 INFO epoch # 9639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009168168893666007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:13,926 INFO epoch # 9640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006146174040623009
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:13,926 INFO *** epoch 9640, rolling-avg-loss (window=10)= 0.007333857119374443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,007 INFO epoch # 9641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006554008949024137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,088 INFO epoch # 9642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006753048983227927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,166 INFO epoch # 9643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007287067302968353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,244 INFO epoch # 9644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006588552052562591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,323 INFO epoch # 9645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007678993184526917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,401 INFO epoch # 9646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008250670776760671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,480 INFO epoch # 9647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007265245185408276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,561 INFO epoch # 9648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005818480007292237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,645 INFO epoch # 9649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006215199056896381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,723 INFO epoch # 9650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006650090923358221
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:14,723 INFO *** epoch 9650, rolling-avg-loss (window=10)= 0.006906135642202571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,802 INFO epoch # 9651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007813384181645233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,880 INFO epoch # 9652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00624885981233092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:14,959 INFO epoch # 9653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006026659328199457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,039 INFO epoch # 9654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006561654568940867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,117 INFO epoch # 9655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00660997270460939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,196 INFO epoch # 9656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007769432821078226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,274 INFO epoch # 9657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007819361933798064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,352 INFO epoch # 9658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007367940699623432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,430 INFO epoch # 9659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007505612462409772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,509 INFO epoch # 9660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008455797651549801
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:15,509 INFO *** epoch 9660, rolling-avg-loss (window=10)= 0.007217867616418516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,591 INFO epoch # 9661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007089803577400744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,675 INFO epoch # 9662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006999475248449016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,754 INFO epoch # 9663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006880977460241411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,832 INFO epoch # 9664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006791141364374198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,912 INFO epoch # 9665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007174813268647995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:15,991 INFO epoch # 9666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007836828794097528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,072 INFO epoch # 9667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006220860268513206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,154 INFO epoch # 9668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007000919467827771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,233 INFO epoch # 9669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007455777784343809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,312 INFO epoch # 9670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007038968848064542
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:16,312 INFO *** epoch 9670, rolling-avg-loss (window=10)= 0.007048956608196022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,392 INFO epoch # 9671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005598812949756393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,473 INFO epoch # 9672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006660250081040431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,556 INFO epoch # 9673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008068082366662566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,653 INFO epoch # 9674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008527383004548028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,744 INFO epoch # 9675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007757376537483651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,826 INFO epoch # 9676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006711957685183734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,910 INFO epoch # 9677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007907997664005961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:16,989 INFO epoch # 9678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006633338038227521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,070 INFO epoch # 9679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007088105499860831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,152 INFO epoch # 9680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061588511016452685
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:17,152 INFO *** epoch 9680, rolling-avg-loss (window=10)= 0.007111215492841438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,231 INFO epoch # 9681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006842123846581671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,309 INFO epoch # 9682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0083045624778606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,387 INFO epoch # 9683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008611291166744195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,467 INFO epoch # 9684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006740717813954689
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,546 INFO epoch # 9685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007359894654655363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,626 INFO epoch # 9686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011005657317582518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,707 INFO epoch # 9687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006028742362104822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,787 INFO epoch # 9688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007634835652424954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,867 INFO epoch # 9689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006063245309633203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:17,948 INFO epoch # 9690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005835979362018406
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:17,949 INFO *** epoch 9690, rolling-avg-loss (window=10)= 0.0074427049963560424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,030 INFO epoch # 9691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008249014150351286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,113 INFO epoch # 9692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006017934450937901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,195 INFO epoch # 9693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007223692678962834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,275 INFO epoch # 9694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00832776099559851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,360 INFO epoch # 9695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007422578193654772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,438 INFO epoch # 9696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007293030415894464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,519 INFO epoch # 9697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00820425358688226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,603 INFO epoch # 9698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00829856694326736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,684 INFO epoch # 9699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00799493588419864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,768 INFO epoch # 9700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007943295153381769
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:18,768 INFO *** epoch 9700, rolling-avg-loss (window=10)= 0.00769750624531298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,849 INFO epoch # 9701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008762366836890578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:18,930 INFO epoch # 9702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006677895573375281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,012 INFO epoch # 9703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007917415656265803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,093 INFO epoch # 9704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00742866921791574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,175 INFO epoch # 9705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007082909542077687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,257 INFO epoch # 9706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007231609590235166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,338 INFO epoch # 9707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008208725739677902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,418 INFO epoch # 9708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006376342738803942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,497 INFO epoch # 9709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007729406039288733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,585 INFO epoch # 9710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007808363327058032
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:19,585 INFO *** epoch 9710, rolling-avg-loss (window=10)= 0.007522370426158886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,668 INFO epoch # 9711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007772470031341072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,750 INFO epoch # 9712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006592407640710007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,832 INFO epoch # 9713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00665039784507826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,914 INFO epoch # 9714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007662290139705874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:19,996 INFO epoch # 9715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00634990928665502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,079 INFO epoch # 9716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073739120562095195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,163 INFO epoch # 9717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005954471926088445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,247 INFO epoch # 9718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006227747435332276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,329 INFO epoch # 9719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006314927544735838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,411 INFO epoch # 9720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069190562862786464
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:20,411 INFO *** epoch 9720, rolling-avg-loss (window=10)= 0.006781759019213495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,503 INFO epoch # 9721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006065763162041549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,592 INFO epoch # 9722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071613143518334255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,674 INFO epoch # 9723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006660060862486716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,756 INFO epoch # 9724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006658170837908983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,839 INFO epoch # 9725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005787697889900301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,917 INFO epoch # 9726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009015808936965186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:20,997 INFO epoch # 9727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006067560265364591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:21,084 INFO epoch # 9728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006536506429256406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:21,169 INFO epoch # 9729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008030898694414645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:21,252 INFO epoch # 9730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007167784751800355
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:21,252 INFO *** epoch 9730, rolling-avg-loss (window=10)= 0.006915156618197216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:21,334 INFO epoch # 9731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006652293195656966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:21,413 INFO epoch # 9732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007027744715742301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:21,492 INFO epoch # 9733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008816024695988744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:21,578 INFO epoch # 9734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010817014263011515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:21,659 INFO epoch # 9735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009689659906143788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:21,738 INFO epoch # 9736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00646787989535369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:21,821 INFO epoch # 9737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007032087138213683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:21,916 INFO epoch # 9738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006720441037032288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,004 INFO epoch # 9739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009690822043921798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,083 INFO epoch # 9740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007816968311090022
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:22,083 INFO *** epoch 9740, rolling-avg-loss (window=10)= 0.00807309352021548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,163 INFO epoch # 9741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008977243138360791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,245 INFO epoch # 9742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007740008048131131
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,334 INFO epoch # 9743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007229334507428575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,415 INFO epoch # 9744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008424758314504288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,496 INFO epoch # 9745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006565415998920798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,585 INFO epoch # 9746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008322980400407687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,664 INFO epoch # 9747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007891523440775927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,744 INFO epoch # 9748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007537719851825386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,824 INFO epoch # 9749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007145230883907061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,908 INFO epoch # 9750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006340170293697156
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:22,908 INFO *** epoch 9750, rolling-avg-loss (window=10)= 0.00761743848779588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:22,990 INFO epoch # 9751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006310674776614178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,075 INFO epoch # 9752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007933915192552377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,156 INFO epoch # 9753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006450490502174944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,234 INFO epoch # 9754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008330263233801816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,313 INFO epoch # 9755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008334728619956877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,399 INFO epoch # 9756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007144030882045627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,485 INFO epoch # 9757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007846015490940772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,570 INFO epoch # 9758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007400211921776645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,652 INFO epoch # 9759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007180779415648431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,732 INFO epoch # 9760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007099456190189812
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:23,732 INFO *** epoch 9760, rolling-avg-loss (window=10)= 0.007403056622570148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,815 INFO epoch # 9761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006107808272645343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,894 INFO epoch # 9762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006975763652008027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:23,972 INFO epoch # 9763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006679201731458306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,053 INFO epoch # 9764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006356846810376737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,135 INFO epoch # 9765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006342986554955132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,215 INFO epoch # 9766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007397256238618866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,295 INFO epoch # 9767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062721002032049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,376 INFO epoch # 9768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008693490191944875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,454 INFO epoch # 9769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006001144756737631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,535 INFO epoch # 9770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006906971968419384
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:24,536 INFO *** epoch 9770, rolling-avg-loss (window=10)= 0.00677335703803692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,617 INFO epoch # 9771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008600833956734277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,711 INFO epoch # 9772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00800877949950518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,798 INFO epoch # 9773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007893936468462925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,875 INFO epoch # 9774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008977611192676704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:24,952 INFO epoch # 9775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077906211372464895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,030 INFO epoch # 9776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006427571417589206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,109 INFO epoch # 9777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008877258449501824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,187 INFO epoch # 9778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005970034922938794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,265 INFO epoch # 9779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00749020000512246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,343 INFO epoch # 9780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006155767521704547
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:25,343 INFO *** epoch 9780, rolling-avg-loss (window=10)= 0.007619261457148241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,421 INFO epoch # 9781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006732748603099026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,499 INFO epoch # 9782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00811921595595777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,579 INFO epoch # 9783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063797431284911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,658 INFO epoch # 9784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00977760207024403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,739 INFO epoch # 9785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012041236921504606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,820 INFO epoch # 9786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006434781455027405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,899 INFO epoch # 9787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007928935854579322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:25,976 INFO epoch # 9788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007459887572622392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,055 INFO epoch # 9789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009036953240865842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,134 INFO epoch # 9790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006449407279433217
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:26,134 INFO *** epoch 9790, rolling-avg-loss (window=10)= 0.00803605120818247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,212 INFO epoch # 9791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00694415665930137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,290 INFO epoch # 9792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006344939363771118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,370 INFO epoch # 9793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006326754599285778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,451 INFO epoch # 9794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062846192449796945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,533 INFO epoch # 9795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006609292526263744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,614 INFO epoch # 9796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007900686134235002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,695 INFO epoch # 9797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005976024913252331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,774 INFO epoch # 9798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007080978502926882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,852 INFO epoch # 9799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006151833054900635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:26,933 INFO epoch # 9800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007767062910716049
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:26,934 INFO *** epoch 9800, rolling-avg-loss (window=10)= 0.0067386347909632605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,017 INFO epoch # 9801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008585784678871278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,099 INFO epoch # 9802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008707945140486117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,180 INFO epoch # 9803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00730316289991606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,270 INFO epoch # 9804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007407670665998012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,357 INFO epoch # 9805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006306709612545092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,441 INFO epoch # 9806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006153447808173951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,521 INFO epoch # 9807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007321312419662718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,600 INFO epoch # 9808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069369629709399305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,678 INFO epoch # 9809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006325987516902387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,758 INFO epoch # 9810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007814517281076405
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:27,758 INFO *** epoch 9810, rolling-avg-loss (window=10)= 0.007286350099457195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,837 INFO epoch # 9811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005664350370352622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,914 INFO epoch # 9812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006215149296622258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:27,992 INFO epoch # 9813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007621346725500189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,070 INFO epoch # 9814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006427403328416403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,148 INFO epoch # 9815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00918980026472127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,225 INFO epoch # 9816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00627155540860258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,302 INFO epoch # 9817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009416036424227059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,383 INFO epoch # 9818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008643215587653685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,460 INFO epoch # 9819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00876897931448184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,537 INFO epoch # 9820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00712583411950618
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:28,537 INFO *** epoch 9820, rolling-avg-loss (window=10)= 0.007534367084008409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,616 INFO epoch # 9821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01045819235878298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,694 INFO epoch # 9822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008546051023586188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,772 INFO epoch # 9823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073299611249240115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,852 INFO epoch # 9824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006044761656085029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:28,933 INFO epoch # 9825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067513334506656975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,010 INFO epoch # 9826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006693005201668711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,092 INFO epoch # 9827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007856423224438913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,169 INFO epoch # 9828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007167467825638596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,247 INFO epoch # 9829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007417204469675198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,330 INFO epoch # 9830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00686105163913453
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:29,330 INFO *** epoch 9830, rolling-avg-loss (window=10)= 0.007512545197459986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,410 INFO epoch # 9831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073870427659130655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,488 INFO epoch # 9832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006822814095357899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,569 INFO epoch # 9833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006477347487816587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,651 INFO epoch # 9834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007314656846574508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,734 INFO epoch # 9835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006188958461279981
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,812 INFO epoch # 9836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064988589292624965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,892 INFO epoch # 9837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075723641566582955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:29,973 INFO epoch # 9838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00877211605256889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,063 INFO epoch # 9839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006838500547019066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,144 INFO epoch # 9840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056742412234598305
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:30,145 INFO *** epoch 9840, rolling-avg-loss (window=10)= 0.006954690056591062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,225 INFO epoch # 9841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006771571686840616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,304 INFO epoch # 9842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007579131779493764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,384 INFO epoch # 9843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00715454229793977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,463 INFO epoch # 9844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006456201335822698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,552 INFO epoch # 9845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009310187182563823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,632 INFO epoch # 9846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011390165047487244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,710 INFO epoch # 9847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008658928927616216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,788 INFO epoch # 9848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008146908941853326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,870 INFO epoch # 9849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006953294592676684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:30,950 INFO epoch # 9850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006369782982801553
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:30,950 INFO *** epoch 9850, rolling-avg-loss (window=10)= 0.00787907147750957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,035 INFO epoch # 9851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066738391178660095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,115 INFO epoch # 9852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008405312895774841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,194 INFO epoch # 9853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008368630544282496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,274 INFO epoch # 9854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007972325853188522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,354 INFO epoch # 9855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008150765977916308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,432 INFO epoch # 9856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007175290673330892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,513 INFO epoch # 9857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007371992702246644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,597 INFO epoch # 9858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007671000879781786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,677 INFO epoch # 9859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010387865811935626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,761 INFO epoch # 9860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068340484431246296
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:31,761 INFO *** epoch 9860, rolling-avg-loss (window=10)= 0.007901107289944776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,839 INFO epoch # 9861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00706195584643865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:31,919 INFO epoch # 9862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007585522442241199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,001 INFO epoch # 9863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0080403151223436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,087 INFO epoch # 9864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007569009620056022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,169 INFO epoch # 9865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007014513110334519
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,256 INFO epoch # 9866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010014695959398523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,339 INFO epoch # 9867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007342536395299248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,421 INFO epoch # 9868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007107951947546098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,502 INFO epoch # 9869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007559895311715081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,582 INFO epoch # 9870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006158251024316996
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:32,583 INFO *** epoch 9870, rolling-avg-loss (window=10)= 0.0075454646779689934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,662 INFO epoch # 9871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005840367382916156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,746 INFO epoch # 9872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007007625958067365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,828 INFO epoch # 9873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007446846226230264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,907 INFO epoch # 9874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00776769922231324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:32,989 INFO epoch # 9875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007138413348002359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,069 INFO epoch # 9876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005776828795205802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,150 INFO epoch # 9877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005939823098742636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,231 INFO epoch # 9878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063317870735772885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,312 INFO epoch # 9879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006238805326574948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,390 INFO epoch # 9880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006015893013682216
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:33,390 INFO *** epoch 9880, rolling-avg-loss (window=10)= 0.006550408944531227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,472 INFO epoch # 9881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067541383250500076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,553 INFO epoch # 9882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007385693825199269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,632 INFO epoch # 9883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006443861573643517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,711 INFO epoch # 9884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007008034517639317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,799 INFO epoch # 9885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00680354461655952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,897 INFO epoch # 9886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006922390959516633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:33,976 INFO epoch # 9887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009011720918351784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,054 INFO epoch # 9888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007662387983145891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,132 INFO epoch # 9889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005882480400032364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,211 INFO epoch # 9890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007149735349230468
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:34,211 INFO *** epoch 9890, rolling-avg-loss (window=10)= 0.007102398846836877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,292 INFO epoch # 9891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006785202887840569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,369 INFO epoch # 9892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007159553264500573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,449 INFO epoch # 9893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006473985893535428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,530 INFO epoch # 9894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066951535700354725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,609 INFO epoch # 9895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006445494771469384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,689 INFO epoch # 9896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006024537142366171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,771 INFO epoch # 9897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006003852220601402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,849 INFO epoch # 9898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007376836896582972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:34,930 INFO epoch # 9899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007121566028217785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,008 INFO epoch # 9900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008027289717574604
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:35,008 INFO *** epoch 9900, rolling-avg-loss (window=10)= 0.006811347239272436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,093 INFO epoch # 9901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006375260330969468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,178 INFO epoch # 9902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009557689947541803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,260 INFO epoch # 9903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008554666208510753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,341 INFO epoch # 9904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010001269467466045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,423 INFO epoch # 9905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008169029781129211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,510 INFO epoch # 9906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006844891271612141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,594 INFO epoch # 9907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006248503756069113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,675 INFO epoch # 9908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005654308071825653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,759 INFO epoch # 9909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072518479864811525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,847 INFO epoch # 9910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009555389595334418
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:35,847 INFO *** epoch 9910, rolling-avg-loss (window=10)= 0.007821285641693976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:35,927 INFO epoch # 9911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007820297025318723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,007 INFO epoch # 9912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007362649543210864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,087 INFO epoch # 9913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006621012129471637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,167 INFO epoch # 9914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008972886767878663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,245 INFO epoch # 9915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074293731595389545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,326 INFO epoch # 9916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008184443096979521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,410 INFO epoch # 9917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006723497856000904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,489 INFO epoch # 9918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00712436579487985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,570 INFO epoch # 9919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008225748206314165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,652 INFO epoch # 9920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006840224872576073
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:36,652 INFO *** epoch 9920, rolling-avg-loss (window=10)= 0.007530449845216935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,731 INFO epoch # 9921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006561366917594569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,809 INFO epoch # 9922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006034859725332353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,886 INFO epoch # 9923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008039094798732549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:36,967 INFO epoch # 9924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008692954848811496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,050 INFO epoch # 9925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008012682177650277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,132 INFO epoch # 9926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006837600732978899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,211 INFO epoch # 9927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007343649689573795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,289 INFO epoch # 9928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063673417607788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,368 INFO epoch # 9929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009138858236838132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,448 INFO epoch # 9930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00982465179549763
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:37,448 INFO *** epoch 9930, rolling-avg-loss (window=10)= 0.00768530606837885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,527 INFO epoch # 9931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065642437839414924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,610 INFO epoch # 9932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061303157490328886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,691 INFO epoch # 9933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006101047642005142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,772 INFO epoch # 9934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063650315860286355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,853 INFO epoch # 9935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075761150364996865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:37,933 INFO epoch # 9936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00787441050488269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,013 INFO epoch # 9937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006400537116860505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,094 INFO epoch # 9938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006061747408239171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,176 INFO epoch # 9939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00654772762209177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,257 INFO epoch # 9940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009961498188204132
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:38,257 INFO *** epoch 9940, rolling-avg-loss (window=10)= 0.006958267463778612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,344 INFO epoch # 9941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007764230664179195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,424 INFO epoch # 9942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008055325874011032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,502 INFO epoch # 9943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008886000046913978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,582 INFO epoch # 9944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006427731896110345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,660 INFO epoch # 9945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005651858591591008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,745 INFO epoch # 9946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007772510303766467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,829 INFO epoch # 9947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006006371113471687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,909 INFO epoch # 9948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00736570551089244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:38,995 INFO epoch # 9949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062185390561353415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:39,083 INFO epoch # 9950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007772002376441378
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:39,083 INFO *** epoch 9950, rolling-avg-loss (window=10)= 0.007192027543351287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:39,169 INFO epoch # 9951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007662887262995355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:39,253 INFO epoch # 9952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006048739596735686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:39,337 INFO epoch # 9953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007719701508904109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:39,422 INFO epoch # 9954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068544399764505215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:39,505 INFO epoch # 9955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007118310968508013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:39,587 INFO epoch # 9956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007451289908203762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:39,743 INFO epoch # 9957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006538930829265155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:39,865 INFO epoch # 9958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010886728778132237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,005 INFO epoch # 9959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006345645604596939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,115 INFO epoch # 9960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007611220367834903
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:40,115 INFO *** epoch 9960, rolling-avg-loss (window=10)= 0.007423789480162668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,206 INFO epoch # 9961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00741627184470417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,300 INFO epoch # 9962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00753184513450833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,421 INFO epoch # 9963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008685753040481359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,504 INFO epoch # 9964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008764101563428994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,585 INFO epoch # 9965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007724624789261725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,666 INFO epoch # 9966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00700633733504219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,746 INFO epoch # 9967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006823077404987998
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,825 INFO epoch # 9968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008392349802306853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,906 INFO epoch # 9969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075274092887411825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:40,986 INFO epoch # 9970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008413921277679037
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:40,986 INFO *** epoch 9970, rolling-avg-loss (window=10)= 0.007828569148114185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:41,067 INFO epoch # 9971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005249850739346584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:41,148 INFO epoch # 9972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007390062099148054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:41,227 INFO epoch # 9973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00870913038670551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:41,317 INFO epoch # 9974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008459415017568972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:41,396 INFO epoch # 9975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006417908480216283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:41,473 INFO epoch # 9976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006612512835999951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:41,552 INFO epoch # 9977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005983764611301012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:41,669 INFO epoch # 9978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006950848503038287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:41,759 INFO epoch # 9979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007756302489724476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:41,844 INFO epoch # 9980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008395710334298201
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:41,845 INFO *** epoch 9980, rolling-avg-loss (window=10)= 0.007192550549734733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:41,932 INFO epoch # 9981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007574920113256667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,017 INFO epoch # 9982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007668055848625954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,097 INFO epoch # 9983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007234043376229238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,175 INFO epoch # 9984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006103811294451589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,255 INFO epoch # 9985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058724198752315715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,333 INFO epoch # 9986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00649958159920061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,413 INFO epoch # 9987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005876967494259588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,491 INFO epoch # 9988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007007576423347928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,572 INFO epoch # 9989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006454319474869408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,653 INFO epoch # 9990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0059592955076368526
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:42,653 INFO *** epoch 9990, rolling-avg-loss (window=10)= 0.00662509910071094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,731 INFO epoch # 9991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007302737234567758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,813 INFO epoch # 9992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072253407997777686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,895 INFO epoch # 9993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006468791820225306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:42,974 INFO epoch # 9994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064204361697193235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:43,052 INFO epoch # 9995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009172698810289148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:43,134 INFO epoch # 9996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066066484796465375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:43,215 INFO epoch # 9997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005550047073484166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:43,293 INFO epoch # 9998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006345199741190299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:43,373 INFO epoch # 9999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007074063803884201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 15:27:43,500 INFO epoch # 10000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006947608155314811
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 15:27:43,501 INFO *** epoch 10000, rolling-avg-loss (window=10)= 0.006911357208809932
[experiments_sandbox.py:919 -   <module>()] 2023-04-25 15:27:43,501 INFO training time in seconds = 827
[experiments_sandbox.py:923 -   <module>()] 2023-04-25 15:27:43,510 INFO train-epochs-loss curve df :
[experiments_sandbox.py:924 -   <module>()] 2023-04-25 15:27:43,559 INFO 
     epochs  rolling-avg-loss
0        10         45.189522
1        20         22.715772
2        30         24.411103
3        40         14.862254
4        50         14.278416
5        60         13.754903
6        70         11.725922
7        80         10.090449
8        90          8.315463
9       100          5.258398
10      110          6.035889
11      120          3.832964
12      130          3.629292
13      140          3.060882
14      150          2.803113
15      160          1.922548
16      170          1.647434
17      180          1.570228
18      190          1.490427
19      200          1.402581
20      210          1.168525
21      220          1.213661
22      230          0.870351
23      240          0.818010
24      250          0.794492
25      260          0.744212
26      270          0.687646
27      280          0.663614
28      290          0.574114
29      300          0.582644
30      310          0.526841
31      320          0.456157
32      330          0.429021
33      340          0.434959
34      350          0.390814
35      360          0.366457
36      370          0.363012
37      380          0.321701
38      390          0.313594
39      400          0.312437
40      410          0.291655
41      420          0.283406
42      430          0.293522
43      440          0.267768
44      450          0.265061
45      460          0.253396
46      470          0.252403
47      480          0.239946
48      490          0.237962
49      500          0.232278
50      510          0.227105
51      520          0.222156
52      530          0.223622
53      540          0.219647
54      550          0.216925
55      560          0.217804
56      570          0.217386
57      580          0.213345
58      590          0.213371
59      600          0.211856
60      610          0.212605
61      620          0.208410
62      630          0.206647
63      640          0.206558
64      650          0.204252
65      660          0.204792
66      670          0.203349
67      680          0.201691
68      690          0.202183
69      700          0.201958
70      710          0.198750
71      720          0.198463
72      730          0.202360
73      740          0.200316
74      750          0.196908
75      760          0.194746
76      770          0.194271
77      780          0.192550
78      790          0.193791
79      800          0.190120
80      810          0.190269
81      820          0.189653
82      830          0.191703
83      840          0.190909
84      850          0.188073
85      860          0.184611
86      870          0.186345
87      880          0.184318
88      890          0.182113
89      900          0.179630
90      910          0.179569
91      920          0.176060
92      930          0.174964
93      940          0.175214
94      950          0.167370
95      960          0.163142
96      970          0.157134
97      980          0.151591
98      990          0.147861
99     1000          0.144506
100    1010          0.141212
101    1020          0.142492
102    1030          0.135586
103    1040          0.133356
104    1050          0.134587
105    1060          0.131009
106    1070          0.130500
107    1080          0.127691
108    1090          0.125717
109    1100          0.124461
110    1110          0.123839
111    1120          0.121288
112    1130          0.116936
113    1140          0.117761
114    1150          0.118246
115    1160          0.111250
116    1170          0.112704
117    1180          0.109069
118    1190          0.104933
119    1200          0.104580
120    1210          0.100850
121    1220          0.098128
122    1230          0.094072
123    1240          0.091999
124    1250          0.085314
125    1260          0.082782
126    1270          0.079215
127    1280          0.074812
128    1290          0.074625
129    1300          0.070157
130    1310          0.065868
131    1320          0.064931
132    1330          0.063829
133    1340          0.062001
134    1350          0.059362
135    1360          0.060723
136    1370          0.057515
137    1380          0.057288
138    1390          0.056092
139    1400          0.055646
140    1410          0.056001
141    1420          0.054650
142    1430          0.053378
143    1440          0.053276
144    1450          0.052090
145    1460          0.051205
146    1470          0.052208
147    1480          0.050442
148    1490          0.050452
149    1500          0.049945
150    1510          0.048894
151    1520          0.049054
152    1530          0.049237
153    1540          0.048068
154    1550          0.048610
155    1560          0.047990
156    1570          0.047757
157    1580          0.046040
158    1590          0.046303
159    1600          0.045953
160    1610          0.044219
161    1620          0.046012
162    1630          0.045551
163    1640          0.045828
164    1650          0.045010
165    1660          0.046294
166    1670          0.044235
167    1680          0.043294
168    1690          0.043588
169    1700          0.043688
170    1710          0.042302
171    1720          0.041345
172    1730          0.041373
173    1740          0.040835
174    1750          0.040537
175    1760          0.042260
176    1770          0.040228
177    1780          0.040739
178    1790          0.040065
179    1800          0.039847
180    1810          0.040367
181    1820          0.038729
182    1830          0.039069
183    1840          0.038472
184    1850          0.038998
185    1860          0.038258
186    1870          0.038550
187    1880          0.037404
188    1890          0.036962
189    1900          0.037670
190    1910          0.036464
191    1920          0.036770
192    1930          0.035480
193    1940          0.035683
194    1950          0.034878
195    1960          0.034741
196    1970          0.035814
197    1980          0.033631
198    1990          0.033631
199    2000          0.034160
200    2010          0.033310
201    2020          0.034248
202    2030          0.033137
203    2040          0.033495
204    2050          0.032736
205    2060          0.033307
206    2070          0.031771
207    2080          0.033083
208    2090          0.033521
209    2100          0.031101
210    2110          0.032135
211    2120          0.032460
212    2130          0.030872
213    2140          0.031412
214    2150          0.032022
215    2160          0.030983
216    2170          0.030628
217    2180          0.031520
218    2190          0.030789
219    2200          0.030206
220    2210          0.029783
221    2220          0.029442
222    2230          0.029390
223    2240          0.029302
224    2250          0.029352
225    2260          0.029711
226    2270          0.029302
227    2280          0.028190
228    2290          0.026959
229    2300          0.027966
230    2310          0.028891
231    2320          0.028163
232    2330          0.027103
233    2340          0.027753
234    2350          0.027704
235    2360          0.026692
236    2370          0.027790
237    2380          0.026604
238    2390          0.027757
239    2400          0.026888
240    2410          0.027214
241    2420          0.025463
242    2430          0.026511
243    2440          0.027260
244    2450          0.025203
245    2460          0.026242
246    2470          0.024900
247    2480          0.025346
248    2490          0.024950
249    2500          0.025373
250    2510          0.026294
251    2520          0.023612
252    2530          0.025627
253    2540          0.024468
254    2550          0.022979
255    2560          0.024107
256    2570          0.023476
257    2580          0.024103
258    2590          0.024216
259    2600          0.023976
260    2610          0.023899
261    2620          0.023333
262    2630          0.023519
263    2640          0.023472
264    2650          0.023157
265    2660          0.023216
266    2670          0.023367
267    2680          0.022247
268    2690          0.022534
269    2700          0.022050
270    2710          0.022601
271    2720          0.021616
272    2730          0.021392
273    2740          0.022161
274    2750          0.021614
275    2760          0.021811
276    2770          0.021419
277    2780          0.020855
278    2790          0.021340
279    2800          0.020498
280    2810          0.020971
281    2820          0.021583
282    2830          0.020330
283    2840          0.020632
284    2850          0.020979
285    2860          0.020006
286    2870          0.020464
287    2880          0.020276
288    2890          0.019180
289    2900          0.019421
290    2910          0.020116
291    2920          0.019747
292    2930          0.019256
293    2940          0.018382
294    2950          0.018602
295    2960          0.019388
296    2970          0.019265
297    2980          0.018706
298    2990          0.019315
299    3000          0.019255
300    3010          0.018019
301    3020          0.020274
302    3030          0.018979
303    3040          0.017790
304    3050          0.018191
305    3060          0.018142
306    3070          0.018530
307    3080          0.017549
308    3090          0.018171
309    3100          0.017798
310    3110          0.017118
311    3120          0.017611
312    3130          0.018508
313    3140          0.016559
314    3150          0.017424
315    3160          0.017617
316    3170          0.017153
317    3180          0.017427
318    3190          0.018353
319    3200          0.017160
320    3210          0.018352
321    3220          0.016797
322    3230          0.017785
323    3240          0.016438
324    3250          0.016531
325    3260          0.017556
326    3270          0.017694
327    3280          0.017230
328    3290          0.017338
329    3300          0.015595
330    3310          0.017018
331    3320          0.016268
332    3330          0.017304
333    3340          0.016939
334    3350          0.016005
335    3360          0.016997
336    3370          0.016750
337    3380          0.016046
338    3390          0.016549
339    3400          0.016178
340    3410          0.015764
341    3420          0.016934
342    3430          0.015151
343    3440          0.015779
344    3450          0.016028
345    3460          0.016640
346    3470          0.015817
347    3480          0.015886
348    3490          0.015781
349    3500          0.015780
350    3510          0.015819
351    3520          0.015252
352    3530          0.015381
353    3540          0.017062
354    3550          0.016210
355    3560          0.015125
356    3570          0.015947
357    3580          0.014686
358    3590          0.015385
359    3600          0.016142
360    3610          0.015219
361    3620          0.015452
362    3630          0.014805
363    3640          0.015378
364    3650          0.015583
365    3660          0.015308
366    3670          0.014866
367    3680          0.014747
368    3690          0.015233
369    3700          0.015197
370    3710          0.015961
371    3720          0.015645
372    3730          0.015162
373    3740          0.014763
374    3750          0.014675
375    3760          0.015319
376    3770          0.014818
377    3780          0.015246
378    3790          0.015526
379    3800          0.015552
380    3810          0.013878
381    3820          0.014252
382    3830          0.014994
383    3840          0.014177
384    3850          0.015776
385    3860          0.014079
386    3870          0.015074
387    3880          0.014550
388    3890          0.014572
389    3900          0.014112
390    3910          0.014815
391    3920          0.014183
392    3930          0.014379
393    3940          0.013862
394    3950          0.014727
395    3960          0.014284
396    3970          0.014467
397    3980          0.014283
398    3990          0.014509
399    4000          0.014535
400    4010          0.013493
401    4020          0.013046
402    4030          0.014913
403    4040          0.014524
404    4050          0.014454
405    4060          0.013894
406    4070          0.015033
407    4080          0.014704
408    4090          0.014429
409    4100          0.013686
410    4110          0.013430
411    4120          0.013097
412    4130          0.015105
413    4140          0.013886
414    4150          0.013304
415    4160          0.013522
416    4170          0.013478
417    4180          0.014105
418    4190          0.014367
419    4200          0.013693
420    4210          0.014190
421    4220          0.014022
422    4230          0.013113
423    4240          0.012683
424    4250          0.013238
425    4260          0.013625
426    4270          0.015294
427    4280          0.013941
428    4290          0.013643
429    4300          0.013149
430    4310          0.013266
431    4320          0.013197
432    4330          0.012689
433    4340          0.013526
434    4350          0.012765
435    4360          0.013567
436    4370          0.013802
437    4380          0.012774
438    4390          0.013273
439    4400          0.013578
440    4410          0.013183
441    4420          0.013265
442    4430          0.012686
443    4440          0.012693
444    4450          0.012800
445    4460          0.012267
446    4470          0.013270
447    4480          0.015304
448    4490          0.013572
449    4500          0.012788
450    4510          0.013075
451    4520          0.012895
452    4530          0.012796
453    4540          0.013258
454    4550          0.012512
455    4560          0.013264
456    4570          0.013018
457    4580          0.013234
458    4590          0.013443
459    4600          0.013053
460    4610          0.014007
461    4620          0.012251
462    4630          0.012611
463    4640          0.013947
464    4650          0.012045
465    4660          0.012432
466    4670          0.012255
467    4680          0.013380
468    4690          0.012835
469    4700          0.012708
470    4710          0.013510
471    4720          0.013009
472    4730          0.012415
473    4740          0.012326
474    4750          0.012364
475    4760          0.011870
476    4770          0.012911
477    4780          0.012641
478    4790          0.012217
479    4800          0.012820
480    4810          0.012442
481    4820          0.011620
482    4830          0.012576
483    4840          0.012384
484    4850          0.012411
485    4860          0.012578
486    4870          0.011470
487    4880          0.011842
488    4890          0.011690
489    4900          0.012361
490    4910          0.011988
491    4920          0.012337
492    4930          0.013010
493    4940          0.012502
494    4950          0.012143
495    4960          0.012316
496    4970          0.011856
497    4980          0.012527
498    4990          0.012559
499    5000          0.012707
500    5010          0.011220
501    5020          0.012084
502    5030          0.012305
503    5040          0.011789
504    5050          0.012990
505    5060          0.011679
506    5070          0.012455
507    5080          0.011644
508    5090          0.011789
509    5100          0.011687
510    5110          0.011747
511    5120          0.011621
512    5130          0.011465
513    5140          0.011845
514    5150          0.012072
515    5160          0.011220
516    5170          0.011692
517    5180          0.011414
518    5190          0.012149
519    5200          0.011453
520    5210          0.013253
521    5220          0.011767
522    5230          0.012228
523    5240          0.011222
524    5250          0.011672
525    5260          0.011585
526    5270          0.011456
527    5280          0.011204
528    5290          0.012315
529    5300          0.011241
530    5310          0.011310
531    5320          0.011574
532    5330          0.010657
533    5340          0.011438
534    5350          0.011731
535    5360          0.012119
536    5370          0.010860
537    5380          0.010646
538    5390          0.011642
539    5400          0.012509
540    5410          0.012003
541    5420          0.011508
542    5430          0.011290
543    5440          0.011503
544    5450          0.010881
545    5460          0.010364
546    5470          0.011366
547    5480          0.011724
548    5490          0.011793
549    5500          0.011851
550    5510          0.012606
551    5520          0.011132
552    5530          0.011560
553    5540          0.011847
554    5550          0.011263
555    5560          0.011006
556    5570          0.010905
557    5580          0.010803
558    5590          0.011572
559    5600          0.011577
560    5610          0.011318
561    5620          0.010777
562    5630          0.011080
563    5640          0.011187
564    5650          0.011606
565    5660          0.010078
566    5670          0.011630
567    5680          0.010733
568    5690          0.011047
569    5700          0.011337
570    5710          0.010256
571    5720          0.010982
572    5730          0.010656
573    5740          0.010421
574    5750          0.011235
575    5760          0.010953
576    5770          0.010903
577    5780          0.010895
578    5790          0.010620
579    5800          0.010660
580    5810          0.011563
581    5820          0.011123
582    5830          0.010468
583    5840          0.012183
584    5850          0.010865
585    5860          0.011736
586    5870          0.010697
587    5880          0.010882
588    5890          0.010190
589    5900          0.010078
590    5910          0.010800
591    5920          0.010566
592    5930          0.010306
593    5940          0.010543
594    5950          0.010688
595    5960          0.010850
596    5970          0.011084
597    5980          0.010705
598    5990          0.010659
599    6000          0.011103
600    6010          0.010221
601    6020          0.011085
602    6030          0.010275
603    6040          0.010382
604    6050          0.011281
605    6060          0.011207
606    6070          0.009989
607    6080          0.010364
608    6090          0.010593
609    6100          0.010707
610    6110          0.010705
611    6120          0.010708
612    6130          0.010437
613    6140          0.010503
614    6150          0.010171
615    6160          0.010105
616    6170          0.010737
617    6180          0.010338
618    6190          0.010331
619    6200          0.009935
620    6210          0.010045
621    6220          0.010731
622    6230          0.010895
623    6240          0.009850
624    6250          0.010220
625    6260          0.010672
626    6270          0.010402
627    6280          0.009733
628    6290          0.010042
629    6300          0.010435
630    6310          0.010461
631    6320          0.009833
632    6330          0.010153
633    6340          0.009772
634    6350          0.010709
635    6360          0.010616
636    6370          0.009959
637    6380          0.009393
638    6390          0.011045
639    6400          0.010263
640    6410          0.009865
641    6420          0.010947
642    6430          0.009916
643    6440          0.010845
644    6450          0.009904
645    6460          0.009882
646    6470          0.009650
647    6480          0.010381
648    6490          0.009902
649    6500          0.009664
650    6510          0.009660
651    6520          0.010276
652    6530          0.009699
653    6540          0.009959
654    6550          0.010279
655    6560          0.010132
656    6570          0.010598
657    6580          0.009884
658    6590          0.010065
659    6600          0.009506
660    6610          0.009942
661    6620          0.010172
662    6630          0.009757
663    6640          0.010181
664    6650          0.009574
665    6660          0.009561
666    6670          0.010043
667    6680          0.009547
668    6690          0.009479
669    6700          0.009832
670    6710          0.009849
671    6720          0.009638
672    6730          0.009689
673    6740          0.009820
674    6750          0.009740
675    6760          0.009139
676    6770          0.009183
677    6780          0.009733
678    6790          0.009559
679    6800          0.009925
680    6810          0.009691
681    6820          0.009608
682    6830          0.009349
683    6840          0.009132
684    6850          0.009616
685    6860          0.009622
686    6870          0.011819
687    6880          0.010152
688    6890          0.008790
689    6900          0.009089
690    6910          0.009948
691    6920          0.010251
692    6930          0.010031
693    6940          0.008626
694    6950          0.009290
695    6960          0.009172
696    6970          0.010223
697    6980          0.009851
698    6990          0.009673
699    7000          0.009388
700    7010          0.009174
701    7020          0.009819
702    7030          0.010148
703    7040          0.009403
704    7050          0.009313
705    7060          0.009158
706    7070          0.009287
707    7080          0.009385
708    7090          0.009789
709    7100          0.009175
710    7110          0.009250
711    7120          0.009654
712    7130          0.009630
713    7140          0.009227
714    7150          0.009263
715    7160          0.009211
716    7170          0.008736
717    7180          0.009162
718    7190          0.008962
719    7200          0.008941
720    7210          0.009608
721    7220          0.009728
722    7230          0.009059
723    7240          0.009701
724    7250          0.009403
725    7260          0.008803
726    7270          0.009372
727    7280          0.010280
728    7290          0.009497
729    7300          0.008249
730    7310          0.009590
731    7320          0.009310
732    7330          0.008835
733    7340          0.008534
734    7350          0.009166
735    7360          0.008504
736    7370          0.008991
737    7380          0.008541
738    7390          0.009176
739    7400          0.008323
740    7410          0.009573
741    7420          0.008997
742    7430          0.009574
743    7440          0.008743
744    7450          0.009147
745    7460          0.009837
746    7470          0.008947
747    7480          0.009330
748    7490          0.009304
749    7500          0.009225
750    7510          0.009200
751    7520          0.009142
752    7530          0.009475
753    7540          0.008468
754    7550          0.008363
755    7560          0.008121
756    7570          0.009003
757    7580          0.008994
758    7590          0.008550
759    7600          0.008972
760    7610          0.008925
761    7620          0.008461
762    7630          0.008854
763    7640          0.008806
764    7650          0.009595
765    7660          0.008650
766    7670          0.008940
767    7680          0.008807
768    7690          0.008416
769    7700          0.008370
770    7710          0.009177
771    7720          0.009411
772    7730          0.008249
773    7740          0.008109
774    7750          0.008702
775    7760          0.008689
776    7770          0.008481
777    7780          0.008852
778    7790          0.009384
779    7800          0.008458
780    7810          0.008876
781    7820          0.008614
782    7830          0.009226
783    7840          0.009226
784    7850          0.008785
785    7860          0.008624
786    7870          0.009065
787    7880          0.008585
788    7890          0.008931
789    7900          0.009408
790    7910          0.009109
791    7920          0.008761
792    7930          0.008798
793    7940          0.008645
794    7950          0.007616
795    7960          0.008401
796    7970          0.008435
797    7980          0.009040
798    7990          0.008740
799    8000          0.009089
800    8010          0.008335
801    8020          0.008362
802    8030          0.009059
803    8040          0.008387
804    8050          0.008365
805    8060          0.007949
806    8070          0.008211
807    8080          0.008161
808    8090          0.008067
809    8100          0.008383
810    8110          0.007895
811    8120          0.009460
812    8130          0.008194
813    8140          0.008382
814    8150          0.009160
815    8160          0.007515
816    8170          0.008434
817    8180          0.008659
818    8190          0.007921
819    8200          0.008422
820    8210          0.008155
821    8220          0.008295
822    8230          0.008556
823    8240          0.007680
824    8250          0.009116
825    8260          0.008397
826    8270          0.008367
827    8280          0.008209
828    8290          0.008346
829    8300          0.008422
830    8310          0.009489
831    8320          0.008119
832    8330          0.008281
833    8340          0.008027
834    8350          0.008410
835    8360          0.008334
836    8370          0.008161
837    8380          0.008738
838    8390          0.008397
839    8400          0.008624
840    8410          0.007788
841    8420          0.008125
842    8430          0.008448
843    8440          0.008517
844    8450          0.008839
845    8460          0.007842
846    8470          0.007479
847    8480          0.008796
848    8490          0.007990
849    8500          0.008263
850    8510          0.008015
851    8520          0.007264
852    8530          0.008202
853    8540          0.008961
854    8550          0.008017
855    8560          0.009553
856    8570          0.008482
857    8580          0.007382
858    8590          0.008755
859    8600          0.008477
860    8610          0.007786
861    8620          0.007441
862    8630          0.008127
863    8640          0.007840
864    8650          0.007748
865    8660          0.007894
866    8670          0.008817
867    8680          0.007998
868    8690          0.008389
869    8700          0.008099
870    8710          0.008029
871    8720          0.007410
872    8730          0.007559
873    8740          0.007500
874    8750          0.007919
875    8760          0.007996
876    8770          0.007902
877    8780          0.007590
878    8790          0.008743
879    8800          0.008165
880    8810          0.007880
881    8820          0.008103
882    8830          0.008328
883    8840          0.007623
884    8850          0.007761
885    8860          0.007628
886    8870          0.008396
887    8880          0.008152
888    8890          0.006826
889    8900          0.008407
890    8910          0.008039
891    8920          0.007744
892    8930          0.007760
893    8940          0.008382
894    8950          0.008182
895    8960          0.007197
896    8970          0.007654
897    8980          0.007409
898    8990          0.007817
899    9000          0.007793
900    9010          0.007948
901    9020          0.007765
902    9030          0.008030
903    9040          0.008699
904    9050          0.008060
905    9060          0.007429
906    9070          0.007366
907    9080          0.007068
908    9090          0.007554
909    9100          0.007684
910    9110          0.007993
911    9120          0.008069
912    9130          0.007946
913    9140          0.007612
914    9150          0.008164
915    9160          0.007362
916    9170          0.008492
917    9180          0.007746
918    9190          0.007352
919    9200          0.007313
920    9210          0.007953
921    9220          0.007327
922    9230          0.007373
923    9240          0.007048
924    9250          0.007698
925    9260          0.007364
926    9270          0.008277
927    9280          0.006910
928    9290          0.007750
929    9300          0.007968
930    9310          0.007509
931    9320          0.007186
932    9330          0.007370
933    9340          0.007067
934    9350          0.008549
935    9360          0.006957
936    9370          0.007288
937    9380          0.007800
938    9390          0.007438
939    9400          0.007236
940    9410          0.007591
941    9420          0.007168
942    9430          0.007001
943    9440          0.007129
944    9450          0.007148
945    9460          0.008062
946    9470          0.006784
947    9480          0.007295
948    9490          0.007559
949    9500          0.007928
950    9510          0.007677
951    9520          0.007294
952    9530          0.007016
953    9540          0.007089
954    9550          0.007130
955    9560          0.007171
956    9570          0.007505
957    9580          0.007418
958    9590          0.006811
959    9600          0.007502
960    9610          0.007243
961    9620          0.007019
962    9630          0.007324
963    9640          0.007334
964    9650          0.006906
965    9660          0.007218
966    9670          0.007049
967    9680          0.007111
968    9690          0.007443
969    9700          0.007698
970    9710          0.007522
971    9720          0.006782
972    9730          0.006915
973    9740          0.008073
974    9750          0.007617
975    9760          0.007403
976    9770          0.006773
977    9780          0.007619
978    9790          0.008036
979    9800          0.006739
980    9810          0.007286
981    9820          0.007534
982    9830          0.007513
983    9840          0.006955
984    9850          0.007879
985    9860          0.007901
986    9870          0.007545
987    9880          0.006550
988    9890          0.007102
989    9900          0.006811
990    9910          0.007821
991    9920          0.007530
992    9930          0.007685
993    9940          0.006958
994    9950          0.007192
995    9960          0.007424
996    9970          0.007829
997    9980          0.007193
998    9990          0.006625
999   10000          0.006911
[experiments_sandbox.py:926 -   <module>()] 2023-04-25 15:27:43,560 INFO Model parameters after training
[experiments_sandbox.py:927 -   <module>()] 2023-04-25 15:27:43,560 INFO Model = TTRBF
[experiments_sandbox.py:929 -   <module>()] 2023-04-25 15:27:43,561 INFO G0 = Parameter containing:
tensor([[ 0.5190, -0.1416, -0.6272],
        [-2.7797, -0.1946,  3.1733],
        [-2.0854,  4.4699,  0.9506],
        [ 0.8473, -0.2996, -1.0414]], requires_grad=True)
[experiments_sandbox.py:929 -   <module>()] 2023-04-25 15:27:43,562 INFO G1 = Parameter containing:
tensor([[[-1.5646, -0.2755,  3.7627],
         [ 0.1270,  3.1592,  0.0296],
         [ 1.3295,  2.0403,  0.1529],
         [ 2.1777, -0.0863, -3.3056]],

        [[ 0.2376, -0.0581,  0.4101],
         [ 1.0156, -3.8272, -2.1496],
         [-2.0258,  2.6596, -2.7399],
         [ 0.1898, -0.7570,  0.3239]],

        [[ 1.9310,  0.3882, -5.7928],
         [-0.9378, -3.3755, -0.3219],
         [-0.9253, -1.7828, -0.0235],
         [-2.6396, -0.1568,  2.4663]]], requires_grad=True)
[experiments_sandbox.py:929 -   <module>()] 2023-04-25 15:27:43,563 INFO G2 = Parameter containing:
tensor([[[-1.4606,  4.0628,  1.5194],
         [ 1.9040,  0.4196,  1.3209],
         [-3.2326, -2.0161, -1.8234],
         [ 2.9115, -2.3297, -1.6895]],

        [[-0.2768,  0.7794, -0.0158],
         [ 0.1496,  0.5371,  0.4604],
         [ 0.0417,  0.0837, -0.3543],
         [-0.2434, -0.6073,  0.4154]],

        [[ 4.7299, -8.7605, -1.5384],
         [-7.9258,  1.5977, -5.5701],
         [ 1.6669,  2.7024, -0.2091],
         [ 2.8576,  3.2627,  5.6641]]], requires_grad=True)
[experiments_sandbox.py:929 -   <module>()] 2023-04-25 15:27:43,564 INFO G3 = Parameter containing:
tensor([[[-3.1124e+00,  6.1627e+00,  6.9897e-01],
         [ 2.1848e-01,  1.6572e+00, -6.4966e-01],
         [-3.5397e-02, -1.6548e+00, -7.3403e-01],
         [ 3.3728e+00, -6.9638e+00,  4.8605e-01]],

        [[-2.1832e+00,  4.2126e+00,  8.3943e-01],
         [ 8.6593e-02,  9.4156e-01, -6.1457e-01],
         [-1.5318e-01, -8.0123e+00, -5.5704e-01],
         [ 2.3581e+00,  2.9960e+00,  3.5788e-01]],

        [[ 3.9396e+00,  2.6398e+00, -9.3610e-02],
         [-3.9488e-01, -1.8751e-02,  1.7178e-01],
         [ 4.6944e-03,  6.1480e+00,  5.9960e-01],
         [-3.4817e+00, -5.8315e+00, -5.6730e-02]]], requires_grad=True)
[experiments_sandbox.py:929 -   <module>()] 2023-04-25 15:27:43,565 INFO rbf_module.centres = Parameter containing:
tensor([[ 3.5261,  3.0219,  1.7982],
        [-1.3899,  1.1583, -3.2242],
        [-3.7407, -3.8371, -3.7859],
        [ 2.9119,  0.0881, -3.2380],
        [-1.9723,  0.3638, -2.4085],
        [-3.5754,  4.1778, -2.9102],
        [-0.2789, -3.1079, -3.2736],
        [ 0.3752, -0.3967,  0.4739],
        [ 2.7338,  1.0137,  4.4597],
        [ 1.0202,  1.2309, -4.1884],
        [-2.3632, -2.4291, -0.0522],
        [-0.7225, -1.8276,  2.5337],
        [-2.2125,  2.7788,  1.6321],
        [-4.1073, -3.2333,  2.9612],
        [-4.0780, -1.3790, -3.4378],
        [ 4.0701, -1.5756, -1.0030]], requires_grad=True)
[experiments_sandbox.py:929 -   <module>()] 2023-04-25 15:27:43,566 INFO rbf_module.log_sigmas = Parameter containing:
tensor([-1.5422, 11.1861,  0.6777, -1.4508,  8.9326,  1.3802,  0.9898,  2.0949,
         2.0547,  2.2837,  2.7892,  2.2490,  2.2337,  2.1443,  2.0392,  2.3009],
       requires_grad=True)
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 15:27:43,566 INFO Out-of sample batch-test
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,568 INFO test-batch  # 0 => test-loss = 0.018621347844600677
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,570 INFO test-batch  # 1 => test-loss = 0.05069055035710335
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,571 INFO test-batch  # 2 => test-loss = 0.008805464021861553
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,573 INFO test-batch  # 3 => test-loss = 0.006995272357016802
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,575 INFO test-batch  # 4 => test-loss = 0.013935406692326069
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,577 INFO test-batch  # 5 => test-loss = 0.0364098846912384
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,579 INFO test-batch  # 6 => test-loss = 0.010776509530842304
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,580 INFO test-batch  # 7 => test-loss = 0.009567277505993843
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,582 INFO test-batch  # 8 => test-loss = 0.005023712757974863
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,584 INFO test-batch  # 9 => test-loss = 0.010152765549719334
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,586 INFO test-batch  # 10 => test-loss = 0.008201044984161854
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,587 INFO test-batch  # 11 => test-loss = 0.00661206990480423
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,589 INFO test-batch  # 12 => test-loss = 0.029363812878727913
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,591 INFO test-batch  # 13 => test-loss = 0.04415386542677879
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,593 INFO test-batch  # 14 => test-loss = 0.004259330686181784
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,595 INFO test-batch  # 15 => test-loss = 0.12858574092388153
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,596 INFO test-batch  # 16 => test-loss = 0.006297719199210405
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,598 INFO test-batch  # 17 => test-loss = 0.009057070128619671
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,600 INFO test-batch  # 18 => test-loss = 0.02019622176885605
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,602 INFO test-batch  # 19 => test-loss = 0.009283908642828465
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,603 INFO test-batch  # 20 => test-loss = 0.006815458182245493
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,605 INFO test-batch  # 21 => test-loss = 0.009771733544766903
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,607 INFO test-batch  # 22 => test-loss = 0.00826635304838419
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,609 INFO test-batch  # 23 => test-loss = 0.013053699396550655
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,610 INFO test-batch  # 24 => test-loss = 0.00972281489521265
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,613 INFO test-batch  # 25 => test-loss = 0.287326842546463
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,615 INFO test-batch  # 26 => test-loss = 0.008310052566230297
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,617 INFO test-batch  # 27 => test-loss = 0.006688667926937342
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,619 INFO test-batch  # 28 => test-loss = 0.009207186289131641
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,621 INFO test-batch  # 29 => test-loss = 0.006553439889103174
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,622 INFO test-batch  # 30 => test-loss = 0.006528701167553663
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 15:27:43,624 INFO test-batch  # 31 => test-loss = 0.004168101120740175
