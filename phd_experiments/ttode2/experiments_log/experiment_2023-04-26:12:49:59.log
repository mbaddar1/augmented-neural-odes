[experiments_sandbox.py:768 -   <module>()] 2023-04-26 12:49:59,094 INFO SEED = 42
[experiments_sandbox.py:848 -   <module>()] 2023-04-26 12:49:59,095 INFO model = ***
TTRBF
order = 4
num_rbf_centers= 24
tt_rank = 3
dim = 6
learnable_numel = 276
***

[experiments_sandbox.py:849 -   <module>()] 2023-04-26 12:49:59,095 INFO optimizer  = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.05
    maximize: False
    weight_decay: 0
)
[experiments_sandbox.py:857 -   <module>()] 2023-04-26 12:49:59,095 INFO lr_scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faabc2a0400>
[experiments_sandbox.py:883 -   <module>()] 2023-04-26 12:49:59,096 INFO train-dataset = 
***
Lorenz-System
N = 1100rho = 28
sigma = 10
beta = 2.6666666666666665
****

[experiments_sandbox.py:884 -   <module>()] 2023-04-26 12:49:59,096 INFO test-dataset = 
***
Lorenz-System
N = 1100rho = 28
sigma = 10
beta = 2.6666666666666665
****

[experiments_sandbox.py:885 -   <module>()] 2023-04-26 12:49:59,097 INFO train-epochs = 2000
[experiments_sandbox.py:890 -   <module>()] 2023-04-26 12:49:59,097 INFO Output Normalization = None
[experiments_sandbox.py:891 -   <module>()] 2023-04-26 12:49:59,097 INFO Gradient-clipping max-norm = 10
[experiments_sandbox.py:893 -   <module>()] 2023-04-26 12:49:59,097 INFO epochs_losses_window = 10
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:00,750 INFO epoch # 0 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 311.27230290004184
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:00,815 INFO epoch # 1 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 225.19071829659597
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:00,881 INFO epoch # 2 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 129.6787239074707
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:00,950 INFO epoch # 3 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 71.05145481654576
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,018 INFO epoch # 4 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 42.63412619999477
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,084 INFO epoch # 5 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 30.641889299665177
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,153 INFO epoch # 6 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 23.100290216718403
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,219 INFO epoch # 7 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 26.43646424157279
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,284 INFO epoch # 8 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 18.716389179229736
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,354 INFO epoch # 9 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 14.179701266969953
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,420 INFO epoch # 10 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 14.950397205352782
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:01,420 INFO *** epoch 10, rolling-avg-loss (window=10)= 59.65801546301161
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,491 INFO epoch # 11 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 10.355877556119646
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,560 INFO epoch # 12 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 9.957989263534547
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,627 INFO epoch # 13 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 10.93510263647352
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,692 INFO epoch # 14 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 8.40257033279964
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,760 INFO epoch # 15 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 10.360458816800799
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,827 INFO epoch # 16 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 9.584093832969666
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,893 INFO epoch # 17 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 12.919085332325526
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:01,962 INFO epoch # 18 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 7.453118555886405
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,029 INFO epoch # 19 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 5.4239577974591935
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,095 INFO epoch # 20 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 4.5921281712395805
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:02,095 INFO *** epoch 20, rolling-avg-loss (window=10)= 8.998438229560852
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,163 INFO epoch # 21 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 5.46973055771419
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,230 INFO epoch # 22 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 8.088347945894514
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,296 INFO epoch # 23 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 6.888669569151742
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,366 INFO epoch # 24 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 5.187758667128427
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,432 INFO epoch # 25 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 5.714133541924613
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,499 INFO epoch # 26 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 5.244958748136248
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,568 INFO epoch # 27 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 4.909649695668902
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,635 INFO epoch # 28 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 6.28545902797154
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,700 INFO epoch # 29 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 4.200065619604928
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,771 INFO epoch # 30 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 4.51039970602308
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:02,771 INFO *** epoch 30, rolling-avg-loss (window=10)= 5.649917307921818
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,837 INFO epoch # 31 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 4.4145284278052195
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,903 INFO epoch # 32 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 5.078398861203875
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:02,975 INFO epoch # 33 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 3.3827412860734123
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,041 INFO epoch # 34 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 4.407285676683698
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,106 INFO epoch # 35 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 4.605995447295053
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,174 INFO epoch # 36 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 5.1885650907244
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,240 INFO epoch # 37 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 2.6601560728890554
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,305 INFO epoch # 38 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 3.2999092510768344
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,373 INFO epoch # 39 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 4.07228171314512
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,440 INFO epoch # 40 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 6.718328012738909
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:03,440 INFO *** epoch 40, rolling-avg-loss (window=10)= 4.382818983963557
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,507 INFO epoch # 41 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 3.282990482875279
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,572 INFO epoch # 42 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 3.0595190490995137
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,643 INFO epoch # 43 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 5.057825544902257
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,709 INFO epoch # 44 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 4.004235822813851
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,774 INFO epoch # 45 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 4.197196497235979
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,839 INFO epoch # 46 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 2.229085952895028
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,910 INFO epoch # 47 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 3.901885951416833
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:03,977 INFO epoch # 48 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 3.212447670527867
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,042 INFO epoch # 49 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 2.8380057777677266
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,107 INFO epoch # 50 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 2.873716580867767
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:04,107 INFO *** epoch 50, rolling-avg-loss (window=10)= 3.46569093304021
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,178 INFO epoch # 51 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 3.7408054079328266
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,244 INFO epoch # 52 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 2.2858646971838814
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,310 INFO epoch # 53 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 2.744217497961862
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,375 INFO epoch # 54 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 2.6286796620913915
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,444 INFO epoch # 55 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 2.6616074749401637
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,512 INFO epoch # 56 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.05 -loss = 2.8260357550212314
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,578 INFO epoch # 57 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05-> 0.04 -loss = 3.4656854476247516
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,646 INFO epoch # 58 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 2.0732482024601526
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,712 INFO epoch # 59 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 1.8876125710351126
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,777 INFO epoch # 60 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 2.3735676952770777
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:04,777 INFO *** epoch 60, rolling-avg-loss (window=10)= 2.6687324411528452
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,846 INFO epoch # 61 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 2.2843898330415997
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,912 INFO epoch # 62 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 1.8467292449304036
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:04,979 INFO epoch # 63 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 2.2736498364380426
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,049 INFO epoch # 64 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 2.338931247166225
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,115 INFO epoch # 65 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 1.9701369796480452
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,179 INFO epoch # 66 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 2.2768742850848604
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,247 INFO epoch # 67 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 2.4928538322448732
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,313 INFO epoch # 68 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 1.256308685881751
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,378 INFO epoch # 69 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 1.6180343466145652
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,453 INFO epoch # 70 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 2.2810698355947223
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:05,454 INFO *** epoch 70, rolling-avg-loss (window=10)= 2.0638978126645084
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,522 INFO epoch # 71 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 1.8069112956523896
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,587 INFO epoch # 72 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 1.882412462575095
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,656 INFO epoch # 73 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 1.7981028173651015
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,722 INFO epoch # 74 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 1.9930873206683568
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,786 INFO epoch # 75 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 2.529986321926117
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,855 INFO epoch # 76 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 1.9977884914193835
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,924 INFO epoch # 77 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 2.1526780247688295
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:05,990 INFO epoch # 78 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.04 -loss = 1.6348960178239005
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,059 INFO epoch # 79 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04-> 0.032 -loss = 1.3511678116662162
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,126 INFO epoch # 80 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.9777313164302281
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:06,126 INFO *** epoch 80, rolling-avg-loss (window=10)= 1.9124761880295615
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,191 INFO epoch # 81 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 2.6462658762931826
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,260 INFO epoch # 82 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.2762382132666452
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,326 INFO epoch # 83 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.4294105316911425
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,391 INFO epoch # 84 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.2495172036545616
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,460 INFO epoch # 85 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.3463634703840528
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,528 INFO epoch # 86 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.4135082108633858
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,594 INFO epoch # 87 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.056056879247938
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,663 INFO epoch # 88 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.5328256130218505
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,730 INFO epoch # 89 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.1856696265084403
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,795 INFO epoch # 90 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 2.6962316078799113
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:06,795 INFO *** epoch 90, rolling-avg-loss (window=10)= 1.583208723281111
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,863 INFO epoch # 91 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.6992379111903055
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,929 INFO epoch # 92 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.9820780213390078
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:06,995 INFO epoch # 93 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.5340194063527244
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,065 INFO epoch # 94 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.5000896964754378
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,132 INFO epoch # 95 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.500285267829895
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,196 INFO epoch # 96 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.1622760721615382
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,265 INFO epoch # 97 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.5679278186389378
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,332 INFO epoch # 98 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.3630457895142691
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,398 INFO epoch # 99 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.9262391026530947
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,468 INFO epoch # 100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.9191460345472608
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:07,468 INFO *** epoch 100, rolling-avg-loss (window=10)= 1.3154345120702469
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,535 INFO epoch # 101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 2.1375887087413243
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,600 INFO epoch # 102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.00023401762758
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,670 INFO epoch # 103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.0205526769161224
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,736 INFO epoch # 104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.2770171965871537
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,801 INFO epoch # 105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.3596030822822025
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,870 INFO epoch # 106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.275649846451623
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:07,937 INFO epoch # 107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.2558864997965948
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,003 INFO epoch # 108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.968124577828816
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,073 INFO epoch # 109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 1.4141753443649836
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,139 INFO epoch # 110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.032 -loss = 0.9309164979628154
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:08,139 INFO *** epoch 110, rolling-avg-loss (window=10)= 1.2639748448559216
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,205 INFO epoch # 111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032-> 0.0256 -loss = 1.5631093876702444
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,274 INFO epoch # 112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 1.1309172425951277
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,341 INFO epoch # 113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.8810475213187081
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,407 INFO epoch # 114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.8699479051998683
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,479 INFO epoch # 115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.9615807252270835
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,545 INFO epoch # 116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 1.3668225952557156
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,612 INFO epoch # 117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.7530921161174774
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,680 INFO epoch # 118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.7435132656778608
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,748 INFO epoch # 119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.6580539034945624
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,815 INFO epoch # 120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.8872170712266649
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:08,815 INFO *** epoch 120, rolling-avg-loss (window=10)= 0.9815301733783313
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,883 INFO epoch # 121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.7585226437875203
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:08,950 INFO epoch # 122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.6223849781921932
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,019 INFO epoch # 123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.6328700069870268
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,090 INFO epoch # 124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 1.638956406712532
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,157 INFO epoch # 125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 1.2422687858343124
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,224 INFO epoch # 126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.7588044796671186
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,294 INFO epoch # 127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 1.0650505227702005
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,362 INFO epoch # 128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.893063912647111
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,430 INFO epoch # 129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.7888669269425529
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,505 INFO epoch # 130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.5862256722790854
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:09,505 INFO *** epoch 130, rolling-avg-loss (window=10)= 0.8987014335819653
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,573 INFO epoch # 131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 1.3646782755851745
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,642 INFO epoch # 132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.9232323518821172
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,712 INFO epoch # 133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.7321970318044935
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,779 INFO epoch # 134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.48329818823507853
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,848 INFO epoch # 135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.694292995759419
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,918 INFO epoch # 136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.6265445628336498
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:09,987 INFO epoch # 137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 1.0819628566503525
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,055 INFO epoch # 138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.7598759553262165
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,125 INFO epoch # 139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.9141440451145172
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,194 INFO epoch # 140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 1.1127712871347155
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:10,194 INFO *** epoch 140, rolling-avg-loss (window=10)= 0.8692997550325735
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,263 INFO epoch # 141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.7976310661860875
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,333 INFO epoch # 142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.5517088485615594
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,401 INFO epoch # 143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.5221043122666222
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,471 INFO epoch # 144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.0256 -loss = 0.8730283622230802
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,541 INFO epoch # 145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0256-> 0.02048 -loss = 0.9202331015041896
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,610 INFO epoch # 146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.5494458901030677
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,678 INFO epoch # 147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.43786278175456184
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,748 INFO epoch # 148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.516692216694355
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,816 INFO epoch # 149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 1.1172068008354732
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,885 INFO epoch # 150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.44571811514241355
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:10,885 INFO *** epoch 150, rolling-avg-loss (window=10)= 0.673163149527141
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:10,955 INFO epoch # 151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.40500125672136034
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,026 INFO epoch # 152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.3469839074781963
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,095 INFO epoch # 153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.3517518588474819
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,166 INFO epoch # 154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.37341450303792956
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,235 INFO epoch # 155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.40642003055129733
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,303 INFO epoch # 156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.4744226055485862
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,374 INFO epoch # 157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.4623602811779295
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,443 INFO epoch # 158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.5335986167192459
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,513 INFO epoch # 159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 1.0094959152596337
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,584 INFO epoch # 160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.6691440160785402
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:11,584 INFO *** epoch 160, rolling-avg-loss (window=10)= 0.5032592991420202
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,653 INFO epoch # 161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.4494880531515394
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,722 INFO epoch # 162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.02048 -loss = 0.45910382100514
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,794 INFO epoch # 163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02048-> 0.016384 -loss = 0.35997399921928136
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,864 INFO epoch # 164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.3681952548878534
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:11,934 INFO epoch # 165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.3250312905226435
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,006 INFO epoch # 166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.33074366109711784
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,076 INFO epoch # 167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.4821574441024235
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,145 INFO epoch # 168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.32237613392727715
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,214 INFO epoch # 169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.2887715069311006
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,283 INFO epoch # 170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.32977879962750845
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:12,283 INFO *** epoch 170, rolling-avg-loss (window=10)= 0.3715619964471885
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,353 INFO epoch # 171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.3772102830665452
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,426 INFO epoch # 172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.26112724372318813
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,495 INFO epoch # 173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.3307935142091342
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,565 INFO epoch # 174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.2718003387962069
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,634 INFO epoch # 175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.2949091579232897
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,703 INFO epoch # 176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.4020311679158892
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,772 INFO epoch # 177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.4677331051656178
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,844 INFO epoch # 178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.2752953054649489
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,913 INFO epoch # 179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.28069113822919983
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:12,984 INFO epoch # 180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.3943108643804278
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:12,984 INFO *** epoch 180, rolling-avg-loss (window=10)= 0.33559021188744476
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,054 INFO epoch # 181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.39670084480728424
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,123 INFO epoch # 182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.016384 -loss = 0.32305604985782077
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,192 INFO epoch # 183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.016384-> 0.0131072 -loss = 0.2719888631786619
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,263 INFO epoch # 184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.2857475640518325
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,332 INFO epoch # 185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.1618552281920399
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,401 INFO epoch # 186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.2709614809070315
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,475 INFO epoch # 187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.2327232114970684
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,544 INFO epoch # 188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.1657419000353132
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,614 INFO epoch # 189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.16745074944836752
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,684 INFO epoch # 190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.2390499391726085
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:13,684 INFO *** epoch 190, rolling-avg-loss (window=10)= 0.25152758311480283
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,756 INFO epoch # 191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.18321570104786328
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,825 INFO epoch # 192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.26648791198219574
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,897 INFO epoch # 193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.17592998455677714
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:13,966 INFO epoch # 194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.48107836438076834
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,036 INFO epoch # 195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.0131072 -loss = 0.24905830419489317
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,106 INFO epoch # 196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0131072-> 0.01048576 -loss = 0.22014171119247164
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,175 INFO epoch # 197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.18955051813806806
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,244 INFO epoch # 198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.12282166853547097
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,316 INFO epoch # 199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.19696716996175903
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,385 INFO epoch # 200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.13718629639063562
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:14,385 INFO *** epoch 200, rolling-avg-loss (window=10)= 0.22224376303809032
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,455 INFO epoch # 201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.17155240987028395
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,534 INFO epoch # 202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.13085989701960768
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,603 INFO epoch # 203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.0944621453327792
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,673 INFO epoch # 204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.1604907831975392
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,743 INFO epoch # 205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.1793363360421998
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,810 INFO epoch # 206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.25551311182124276
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,879 INFO epoch # 207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.14222121685743333
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:14,950 INFO epoch # 208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.17143160636935914
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,020 INFO epoch # 209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.19628415895359858
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,088 INFO epoch # 210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.15908975388322558
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:15,088 INFO *** epoch 210, rolling-avg-loss (window=10)= 0.16612414193472697
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,159 INFO epoch # 211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.2937963834830693
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,228 INFO epoch # 212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.22044853823525565
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,297 INFO epoch # 213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.01048576 -loss = 0.17301355579069683
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,369 INFO epoch # 214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01048576-> 0.00838861 -loss = 0.23652413210698536
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,439 INFO epoch # 215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.17445333940642221
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,514 INFO epoch # 216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.1163607627685581
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,583 INFO epoch # 217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0863654438406229
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,653 INFO epoch # 218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0896184110215732
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,721 INFO epoch # 219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.11393759953124183
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,792 INFO epoch # 220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.134512083871024
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:15,792 INFO *** epoch 220, rolling-avg-loss (window=10)= 0.16390302500554493
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,861 INFO epoch # 221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.10850373698132379
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:15,930 INFO epoch # 222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.17029880647148404
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,004 INFO epoch # 223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.13090485962373868
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,074 INFO epoch # 224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.0995178120476859
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,144 INFO epoch # 225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.12274139470287732
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,213 INFO epoch # 226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.11489956602454185
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,282 INFO epoch # 227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00838861 -loss = 0.11480095333286694
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,351 INFO epoch # 228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00838861-> 0.00671089 -loss = 0.08975669805492674
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,422 INFO epoch # 229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.0706427663564682
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,492 INFO epoch # 230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.06304723849253994
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:16,492 INFO *** epoch 230, rolling-avg-loss (window=10)= 0.10851138320884535
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,566 INFO epoch # 231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.07005261949130467
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,652 INFO epoch # 232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.08484756595322064
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,721 INFO epoch # 233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.09494682188544955
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,791 INFO epoch # 234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.10507065866674696
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,861 INFO epoch # 235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.07276143452950887
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:16,930 INFO epoch # 236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.07168197977755751
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,000 INFO epoch # 237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.08158213799553259
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,070 INFO epoch # 238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.11550905284072671
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,139 INFO epoch # 239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.07415957126234259
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,207 INFO epoch # 240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00671089 -loss = 0.11676021242248161
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:17,207 INFO *** epoch 240, rolling-avg-loss (window=10)= 0.08873720548248717
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,278 INFO epoch # 241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00671089-> 0.00536871 -loss = 0.13009665587118693
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,346 INFO epoch # 242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.09690310880541801
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,415 INFO epoch # 243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.06520863100886345
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,489 INFO epoch # 244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.048741126991808416
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,561 INFO epoch # 245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.05230379721948079
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,631 INFO epoch # 246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.052152838318475656
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,702 INFO epoch # 247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.07317658747945513
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,771 INFO epoch # 248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.0593404617692743
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,840 INFO epoch # 249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.05199316367506981
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,911 INFO epoch # 250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.05355012986276831
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:17,911 INFO *** epoch 250, rolling-avg-loss (window=10)= 0.06834665010018007
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:17,980 INFO epoch # 251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.07333973890968731
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,052 INFO epoch # 252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.05512591419475419
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,123 INFO epoch # 253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.07779035956731864
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,192 INFO epoch # 254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00536871 -loss = 0.07443696430751255
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,260 INFO epoch # 255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536871-> 0.00429497 -loss = 0.05738503304975373
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,333 INFO epoch # 256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.051403228619268965
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,402 INFO epoch # 257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.041739482432603836
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,474 INFO epoch # 258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.061367308348417283
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,548 INFO epoch # 259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.049044386403901234
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,618 INFO epoch # 260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.037869151149477275
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:18,618 INFO *** epoch 260, rolling-avg-loss (window=10)= 0.057950156698269494
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,689 INFO epoch # 261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.04752140465591635
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,759 INFO epoch # 262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.051970485411584375
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,828 INFO epoch # 263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.04938822697315897
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,897 INFO epoch # 264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.05186748972960881
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:18,969 INFO epoch # 265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.04275353942066431
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,039 INFO epoch # 266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.05144053085574082
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,112 INFO epoch # 267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.060017102397978306
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,184 INFO epoch # 268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.05916420625788825
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,254 INFO epoch # 269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.06229897524629321
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,326 INFO epoch # 270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00429497 -loss = 0.04669443161359855
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:19,327 INFO *** epoch 270, rolling-avg-loss (window=10)= 0.05231163925624319
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,401 INFO epoch # 271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00429497-> 0.00343597 -loss = 0.04779904782772064
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,475 INFO epoch # 272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.045308806534324376
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,550 INFO epoch # 273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.03915749415755272
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,622 INFO epoch # 274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.036673636894140924
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,693 INFO epoch # 275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.037016394042543005
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,765 INFO epoch # 276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.04326824319681951
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,838 INFO epoch # 277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.04693032649478742
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,909 INFO epoch # 278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.053477193628038676
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:19,984 INFO epoch # 279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.04982865521950381
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,055 INFO epoch # 280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.041088000498712064
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:20,055 INFO *** epoch 280, rolling-avg-loss (window=10)= 0.04405477984941432
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,126 INFO epoch # 281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.03598500160234315
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,196 INFO epoch # 282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.039862464368343356
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,272 INFO epoch # 283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.03922531860215323
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,346 INFO epoch # 284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.04061863613980157
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,419 INFO epoch # 285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.04301125284816538
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,490 INFO epoch # 286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.05399384203233889
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,565 INFO epoch # 287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.04495656351958002
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,636 INFO epoch # 288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.047943389415740965
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,707 INFO epoch # 289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.05770255917949336
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,778 INFO epoch # 290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.04621635550366981
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:20,778 INFO *** epoch 290, rolling-avg-loss (window=10)= 0.04495153832116298
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,850 INFO epoch # 291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00343597 -loss = 0.05712223601128374
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:20,926 INFO epoch # 292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00343597-> 0.00274878 -loss = 0.04147478799734797
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,002 INFO epoch # 293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.034768316016665524
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,074 INFO epoch # 294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.03514199326080936
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,144 INFO epoch # 295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.03321344440004655
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,214 INFO epoch # 296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.0332556207797357
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,284 INFO epoch # 297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.043966571667364666
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,360 INFO epoch # 298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.04276517408766917
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,432 INFO epoch # 299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.03248447562967028
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,506 INFO epoch # 300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.02994193961577756
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:21,506 INFO *** epoch 300, rolling-avg-loss (window=10)= 0.038413455946637046
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,582 INFO epoch # 301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.02879288393471922
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,653 INFO epoch # 302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.03502973898180894
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,724 INFO epoch # 303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.027005833573639393
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,799 INFO epoch # 304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.031741825863718987
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,871 INFO epoch # 305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.03209719181592975
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:21,943 INFO epoch # 306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.033545711237405025
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,016 INFO epoch # 307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.03889992801206452
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,088 INFO epoch # 308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.03510318978556565
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,160 INFO epoch # 309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.03303705448550837
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,233 INFO epoch # 310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.03999255855700799
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:22,234 INFO *** epoch 310, rolling-avg-loss (window=10)= 0.03352459162473678
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,305 INFO epoch # 311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.03955923401351486
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,375 INFO epoch # 312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.03141349402389356
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,448 INFO epoch # 313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00274878 -loss = 0.029851671414715904
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,520 INFO epoch # 314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00274878-> 0.00219902 -loss = 0.041161065362393855
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,601 INFO epoch # 315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.033887192660144394
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,673 INFO epoch # 316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.03243715720517295
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,744 INFO epoch # 317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.029427298291453293
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,815 INFO epoch # 318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.027991258273167268
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,886 INFO epoch # 319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.027720915712416173
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:22,955 INFO epoch # 320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.027407444774040154
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:22,955 INFO *** epoch 320, rolling-avg-loss (window=10)= 0.032085673173091245
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,028 INFO epoch # 321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.029655730883990015
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,101 INFO epoch # 322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.032533868854599336
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,172 INFO epoch # 323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.03667425284428256
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,244 INFO epoch # 324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00219902 -loss = 0.028712732797222477
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,315 INFO epoch # 325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00219902-> 0.00175922 -loss = 0.03187269382178783
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,385 INFO epoch # 326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.02546265596257789
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,458 INFO epoch # 327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.02891535047175629
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,532 INFO epoch # 328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.025866822100111417
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,609 INFO epoch # 329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.022946211935154028
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,683 INFO epoch # 330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.023571646213531493
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:23,683 INFO *** epoch 330, rolling-avg-loss (window=10)= 0.02862119658850133
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,755 INFO epoch # 331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.024027446191757916
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,827 INFO epoch # 332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.022868293443960803
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,900 INFO epoch # 333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.026816877057509764
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:23,976 INFO epoch # 334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.023396851015942438
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,046 INFO epoch # 335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.023986007911818367
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,117 INFO epoch # 336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.02474549931607076
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,189 INFO epoch # 337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.03345636132040194
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,260 INFO epoch # 338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.025615412928164005
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,334 INFO epoch # 339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.02717370561191014
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,405 INFO epoch # 340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.02725367069776569
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:24,405 INFO *** epoch 340, rolling-avg-loss (window=10)= 0.025934012549530185
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,477 INFO epoch # 341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.02502555490604469
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,548 INFO epoch # 342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00175922 -loss = 0.024452126611556324
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,625 INFO epoch # 343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00175922-> 0.00140737 -loss = 0.02739283349364996
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,696 INFO epoch # 344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.023801949167890208
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,772 INFO epoch # 345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.025036808289587497
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,846 INFO epoch # 346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.021352492831647397
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,916 INFO epoch # 347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.021413380546229228
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:24,987 INFO epoch # 348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.026383488864770956
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,059 INFO epoch # 349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.02666569163224527
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,128 INFO epoch # 350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.025276070992861475
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:25,129 INFO *** epoch 350, rolling-avg-loss (window=10)= 0.024680039733648303
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,199 INFO epoch # 351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.021062775860939707
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,272 INFO epoch # 352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.023952474764415196
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,344 INFO epoch # 353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.02568352970161608
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,416 INFO epoch # 354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.02594012530254466
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,489 INFO epoch # 355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.024582899920642377
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,559 INFO epoch # 356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.023119056171604566
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,634 INFO epoch # 357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.020209231400596245
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,706 INFO epoch # 358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.021702703567487852
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,776 INFO epoch # 359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.025076034505452427
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,845 INFO epoch # 360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.024112785660794802
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:25,845 INFO *** epoch 360, rolling-avg-loss (window=10)= 0.02354416168560939
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,916 INFO epoch # 361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.021421421079763345
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:25,985 INFO epoch # 362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.022823387730334488
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,055 INFO epoch # 363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.02273508246455874
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,128 INFO epoch # 364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.020888397323765923
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,197 INFO epoch # 365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.020357659139803478
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,267 INFO epoch # 366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.02256598166589226
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,337 INFO epoch # 367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.00140737 -loss = 0.023524574882217816
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,410 INFO epoch # 368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00140737-> 0.0011259 -loss = 0.02470363908048187
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,483 INFO epoch # 369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.02654949078070266
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,554 INFO epoch # 370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.019452139043382236
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:26,554 INFO *** epoch 370, rolling-avg-loss (window=10)= 0.022502177319090284
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,626 INFO epoch # 371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.01941638926842383
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,699 INFO epoch # 372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.019256917852908374
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,774 INFO epoch # 373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.025522391857313257
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,844 INFO epoch # 374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.0206487844565085
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,918 INFO epoch # 375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.020124086059097733
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:26,992 INFO epoch # 376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.020282871755106107
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,062 INFO epoch # 377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.02055796632277114
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,133 INFO epoch # 378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.021644620570753302
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,206 INFO epoch # 379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.021312480486397233
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,278 INFO epoch # 380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.019005406900708163
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:27,278 INFO *** epoch 380, rolling-avg-loss (window=10)= 0.020777191552998765
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,349 INFO epoch # 381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.02003346612410886
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,422 INFO epoch # 382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.02495770092521395
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,494 INFO epoch # 383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.023770676220634154
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,565 INFO epoch # 384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.021543615657304015
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,640 INFO epoch # 385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.02180593458137342
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,712 INFO epoch # 386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.02054483141484005
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,784 INFO epoch # 387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.022398877489779676
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,855 INFO epoch # 388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.018424329840179
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,925 INFO epoch # 389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.01881151535947408
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:27,995 INFO epoch # 390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.018711373382913215
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:27,995 INFO *** epoch 390, rolling-avg-loss (window=10)= 0.02110023209958204
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,068 INFO epoch # 391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.020658154280057975
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,137 INFO epoch # 392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.019969801418483257
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,207 INFO epoch # 393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.02214224466255733
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,279 INFO epoch # 394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.021974560751446656
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,349 INFO epoch # 395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.022914730278509005
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,419 INFO epoch # 396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.020981884614697525
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,491 INFO epoch # 397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.020094190777412483
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,560 INFO epoch # 398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.0011259 -loss = 0.018944431974419525
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,630 INFO epoch # 399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0011259-> 0.00090072 -loss = 0.01847572937341673
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,706 INFO epoch # 400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.018312307779810258
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:28,707 INFO *** epoch 400, rolling-avg-loss (window=10)= 0.020446803591081076
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,776 INFO epoch # 401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.01883788855214204
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,848 INFO epoch # 402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.018377130372183664
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,922 INFO epoch # 403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.02067259012588433
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:28,995 INFO epoch # 404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.02012017587465899
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,065 INFO epoch # 405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.01788062985454287
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,137 INFO epoch # 406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.01974008556987558
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,209 INFO epoch # 407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.019614503772131035
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,281 INFO epoch # 408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.0184895537261452
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,352 INFO epoch # 409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.018849920987018518
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,422 INFO epoch # 410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.018985081450747593
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:29,422 INFO *** epoch 410, rolling-avg-loss (window=10)= 0.019156756028532985
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,493 INFO epoch # 411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.02007559925051672
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,565 INFO epoch # 412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.022508404896195447
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,636 INFO epoch # 413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.019223748120878424
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,714 INFO epoch # 414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.018098962706114566
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,784 INFO epoch # 415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.017596906223999603
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,853 INFO epoch # 416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.02087412734649011
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,924 INFO epoch # 417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.018110797620777573
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:29,999 INFO epoch # 418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.018760796929044383
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,070 INFO epoch # 419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.017995552998036145
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,143 INFO epoch # 420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.018052978148417814
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:30,144 INFO *** epoch 420, rolling-avg-loss (window=10)= 0.019129787424047077
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,215 INFO epoch # 421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.019881994583244834
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,286 INFO epoch # 422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.017998106604708092
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,368 INFO epoch # 423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.018956965474145753
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,437 INFO epoch # 424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.019101681879588536
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,508 INFO epoch # 425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00090072 -loss = 0.018624193726905755
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,577 INFO epoch # 426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00090072-> 0.00072058 -loss = 0.01932419783302716
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,652 INFO epoch # 427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.01744053985125252
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,731 INFO epoch # 428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.017409830567027842
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,805 INFO epoch # 429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.016994785530758755
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,879 INFO epoch # 430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.01982979593532426
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:30,879 INFO *** epoch 430, rolling-avg-loss (window=10)= 0.01855620919859835
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:30,968 INFO epoch # 431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.01724758017808199
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,095 INFO epoch # 432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.01779112346204264
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,171 INFO epoch # 433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.016658931358584334
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,241 INFO epoch # 434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.01714204696140119
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,310 INFO epoch # 435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.0179084947598832
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,380 INFO epoch # 436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.01927532193117908
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,448 INFO epoch # 437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.01903656031936407
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,524 INFO epoch # 438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.017783903370478322
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,602 INFO epoch # 439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.01810212092740195
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,676 INFO epoch # 440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.017387298507882017
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:31,676 INFO *** epoch 440, rolling-avg-loss (window=10)= 0.01783333817762988
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,756 INFO epoch # 441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.017027879812355554
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,830 INFO epoch # 442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.01745259901508689
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,902 INFO epoch # 443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00072058 -loss = 0.01875572180641549
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:31,978 INFO epoch # 444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00072058-> 0.00057646 -loss = 0.01705688573420048
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,050 INFO epoch # 445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.016724948106067523
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,124 INFO epoch # 446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.016007307996707302
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,198 INFO epoch # 447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.01784584934690169
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,272 INFO epoch # 448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.016838494342352663
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,345 INFO epoch # 449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.016703657805919647
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,421 INFO epoch # 450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.01572151720257742
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:32,421 INFO *** epoch 450, rolling-avg-loss (window=10)= 0.01701348611685847
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,493 INFO epoch # 451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.017886566024805817
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,564 INFO epoch # 452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.016729400173894
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,639 INFO epoch # 453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.016618675858314547
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,718 INFO epoch # 454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.017046087408172234
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,791 INFO epoch # 455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.017081811678196702
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,867 INFO epoch # 456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.017753265159470694
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:32,940 INFO epoch # 457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.017124218972665924
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,013 INFO epoch # 458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.015965292403208358
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,086 INFO epoch # 459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.016034456142889602
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,160 INFO epoch # 460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.017646054471177713
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:33,160 INFO *** epoch 460, rolling-avg-loss (window=10)= 0.016988582829279557
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,232 INFO epoch # 461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.015439231866704567
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,307 INFO epoch # 462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.01631316357691373
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,381 INFO epoch # 463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.015851056495947497
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,453 INFO epoch # 464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.015572789377932038
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,528 INFO epoch # 465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.017219598138970987
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,601 INFO epoch # 466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.017976611360375372
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,674 INFO epoch # 467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.016327673316534078
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,753 INFO epoch # 468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.017541517689824104
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,825 INFO epoch # 469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.017121115659496613
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,893 INFO epoch # 470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.01667564543230193
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:33,894 INFO *** epoch 470, rolling-avg-loss (window=10)= 0.01660384029150009
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:33,963 INFO epoch # 471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00057646 -loss = 0.0165268316332783
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,033 INFO epoch # 472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00057646-> 0.00046117 -loss = 0.01684584923620735
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,102 INFO epoch # 473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.018135680883590664
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,170 INFO epoch # 474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.015775410192353383
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,241 INFO epoch # 475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.015803971554019622
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,313 INFO epoch # 476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.015826626760619026
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,387 INFO epoch # 477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.015418580640107394
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,455 INFO epoch # 478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.01667857961729169
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,524 INFO epoch # 479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.016065378613503916
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,593 INFO epoch # 480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.015802685650331633
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:34,593 INFO *** epoch 480, rolling-avg-loss (window=10)= 0.016287959478130297
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,663 INFO epoch # 481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.015662496643407003
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,736 INFO epoch # 482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.015131313619869096
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,805 INFO epoch # 483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.01548440425789782
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,875 INFO epoch # 484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.014832117767738445
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:34,945 INFO epoch # 485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.015831458289176224
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,013 INFO epoch # 486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.014785219236676183
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,083 INFO epoch # 487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.01528545409174902
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,152 INFO epoch # 488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.01531415358185768
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,221 INFO epoch # 489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.015648707494671857
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,290 INFO epoch # 490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.016281878761947154
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:35,290 INFO *** epoch 490, rolling-avg-loss (window=10)= 0.015425720374499047
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,359 INFO epoch # 491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.015831520035862922
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,429 INFO epoch # 492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.01594281233847141
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,501 INFO epoch # 493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.015409024498824562
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,569 INFO epoch # 494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.016216612181493215
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,638 INFO epoch # 495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.016081023309379817
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,708 INFO epoch # 496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00046117 -loss = 0.01495899044509445
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,783 INFO epoch # 497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00046117-> 0.00036893 -loss = 0.015170346479862928
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,851 INFO epoch # 498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.015290745640439647
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,920 INFO epoch # 499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.014850639019693647
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:35,989 INFO epoch # 500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.015932649626795733
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:35,989 INFO *** epoch 500, rolling-avg-loss (window=10)= 0.015568436357591831
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,060 INFO epoch # 501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.015202308778784104
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,131 INFO epoch # 502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.015206196238952023
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,201 INFO epoch # 503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.014988855764802014
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,270 INFO epoch # 504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.014949536762599434
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,340 INFO epoch # 505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.015744723459439617
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,411 INFO epoch # 506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.01520136912752475
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,482 INFO epoch # 507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00036893 -loss = 0.015538474624710424
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,553 INFO epoch # 508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00036893-> 0.00029515 -loss = 0.01507106153294444
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,624 INFO epoch # 509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014554529610489095
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,694 INFO epoch # 510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014666450768709182
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:36,694 INFO *** epoch 510, rolling-avg-loss (window=10)= 0.015112350666895508
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,770 INFO epoch # 511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014654896834066936
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,840 INFO epoch # 512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.01417964869844062
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,911 INFO epoch # 513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.01554683947137424
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:36,984 INFO epoch # 514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.01589614778224911
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:37,055 INFO epoch # 515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.015109089349529573
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:37,125 INFO epoch # 516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014480010526520865
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:37,194 INFO epoch # 517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014786596516413349
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:37,268 INFO epoch # 518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014860487622874124
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:37,342 INFO epoch # 519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014183946352984225
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:37,410 INFO epoch # 520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.015066158931170191
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:37,411 INFO *** epoch 520, rolling-avg-loss (window=10)= 0.014876382208562322
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:37,480 INFO epoch # 521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014741069098402346
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:37,552 INFO epoch # 522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014092071394303015
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:37,841 INFO epoch # 523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.015246545576623508
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:37,986 INFO epoch # 524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014690842253289052
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,058 INFO epoch # 525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.015288245172372886
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,127 INFO epoch # 526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014352017933768885
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,194 INFO epoch # 527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014035808667540551
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,268 INFO epoch # 528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014742764670933996
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,343 INFO epoch # 529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.015112203652305263
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,410 INFO epoch # 530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014386029514883245
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:38,410 INFO *** epoch 530, rolling-avg-loss (window=10)= 0.014668759793442274
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,482 INFO epoch # 531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014454086696995156
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,552 INFO epoch # 532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014290927124342748
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,621 INFO epoch # 533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014149894552039249
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,690 INFO epoch # 534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.01446860520435231
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,759 INFO epoch # 535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014418965032590287
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,835 INFO epoch # 536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.014760474315179246
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,905 INFO epoch # 537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00029515 -loss = 0.015325742986585412
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:38,976 INFO epoch # 538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00029515-> 0.00023612 -loss = 0.01429828859067389
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,045 INFO epoch # 539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.015140396542847157
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,113 INFO epoch # 540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014020353848380702
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:39,113 INFO *** epoch 540, rolling-avg-loss (window=10)= 0.014532773489398618
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,182 INFO epoch # 541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014301319806171315
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,251 INFO epoch # 542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.01423020821862987
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,320 INFO epoch # 543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014159501809626818
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,391 INFO epoch # 544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.013905091237808977
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,461 INFO epoch # 545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.013890204099672182
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,529 INFO epoch # 546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014116492321980851
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,597 INFO epoch # 547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014244020583906345
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,665 INFO epoch # 548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.013882670817630631
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,734 INFO epoch # 549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014299147882099662
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,809 INFO epoch # 550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.013833227926599127
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:39,809 INFO *** epoch 550, rolling-avg-loss (window=10)= 0.014086188470412578
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,877 INFO epoch # 551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014478842049304929
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:39,947 INFO epoch # 552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014804117767406361
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,016 INFO epoch # 553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014002298803201743
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,085 INFO epoch # 554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.01376187117504222
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,153 INFO epoch # 555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.01405927940670933
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,222 INFO epoch # 556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.0141593525053135
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,290 INFO epoch # 557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014410935967628444
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,359 INFO epoch # 558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.013912961751754795
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,429 INFO epoch # 559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014971456849681479
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,498 INFO epoch # 560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014379490539431572
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:40,498 INFO *** epoch 560, rolling-avg-loss (window=10)= 0.014294060681547437
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,567 INFO epoch # 561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014494985275502716
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,636 INFO epoch # 562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.013786330060767276
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,705 INFO epoch # 563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014030071946659258
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,774 INFO epoch # 564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014027089944907598
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,848 INFO epoch # 565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.013581980991044214
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,918 INFO epoch # 566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.013800106437078545
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:40,989 INFO epoch # 567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.01385037981505905
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,059 INFO epoch # 568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.01400779794369425
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,128 INFO epoch # 569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.01389273949233549
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,197 INFO epoch # 570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.01395631999309574
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:41,197 INFO *** epoch 570, rolling-avg-loss (window=10)= 0.013942780190014414
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,267 INFO epoch # 571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.014142011252364943
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,336 INFO epoch # 572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.01412436497796859
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,406 INFO epoch # 573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.01591643068407263
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,479 INFO epoch # 574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.013868176551269634
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,548 INFO epoch # 575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00023612 -loss = 0.013730243008051599
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,617 INFO epoch # 576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00023612-> 0.00018889 -loss = 0.014032162473137889
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,687 INFO epoch # 577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.013999102524082575
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,755 INFO epoch # 578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.013449110363477042
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,830 INFO epoch # 579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.01375895088006343
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,900 INFO epoch # 580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.013778135446565492
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:41,900 INFO *** epoch 580, rolling-avg-loss (window=10)= 0.014079868816105384
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:41,969 INFO epoch # 581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.013723214928592955
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,037 INFO epoch # 582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.013551063103867429
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,107 INFO epoch # 583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.013808728461819035
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,176 INFO epoch # 584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.013474854454398156
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,245 INFO epoch # 585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.013605836792183773
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,313 INFO epoch # 586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.0140850028422262
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,382 INFO epoch # 587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.013488803085471903
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,452 INFO epoch # 588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00018889 -loss = 0.013836868439934083
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,523 INFO epoch # 589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00018889-> 0.00015112 -loss = 0.01413550509938172
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,592 INFO epoch # 590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.014016867455627238
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:42,592 INFO *** epoch 590, rolling-avg-loss (window=10)= 0.013772674466350249
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,661 INFO epoch # 591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.0155679381718593
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,731 INFO epoch # 592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.014202341623604297
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,801 INFO epoch # 593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013548980985901186
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,874 INFO epoch # 594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.01342953604512981
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:42,944 INFO epoch # 595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013305249051856143
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,013 INFO epoch # 596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.0133610524370202
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,082 INFO epoch # 597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013380495538668973
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,151 INFO epoch # 598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013193586668265717
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,220 INFO epoch # 599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013929294621837991
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,289 INFO epoch # 600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.014445677532681398
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:43,289 INFO *** epoch 600, rolling-avg-loss (window=10)= 0.013836415267682502
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,359 INFO epoch # 601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013461477881563561
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,427 INFO epoch # 602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013569221259760005
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,497 INFO epoch # 603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013764437860144037
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,567 INFO epoch # 604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013313334515052183
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,636 INFO epoch # 605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013408003335020372
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,705 INFO epoch # 606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013390414177307061
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,774 INFO epoch # 607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.01322390123137406
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,848 INFO epoch # 608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00015112 -loss = 0.013990335472460305
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,917 INFO epoch # 609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00015112-> 0.00012089 -loss = 0.013736088813415596
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:43,987 INFO epoch # 610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.013213279856634992
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:43,987 INFO *** epoch 610, rolling-avg-loss (window=10)= 0.013507049440273217
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,056 INFO epoch # 611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.01349856590053865
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,124 INFO epoch # 612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.013351224522505487
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,194 INFO epoch # 613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.01400967211063419
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,263 INFO epoch # 614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.013393119829041617
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,333 INFO epoch # 615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.013198710379323788
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,402 INFO epoch # 616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.013449190171169384
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,472 INFO epoch # 617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.01370522045929517
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,541 INFO epoch # 618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.013404170809579747
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,611 INFO epoch # 619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.00012089 -loss = 0.013310635156397308
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,680 INFO epoch # 620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00012089-> 0.0001 -loss = 0.013246732497853892
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:44,680 INFO *** epoch 620, rolling-avg-loss (window=10)= 0.013456724183633926
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,749 INFO epoch # 621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013240630818264825
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,818 INFO epoch # 622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013274425800357545
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,892 INFO epoch # 623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013293986727616617
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:44,963 INFO epoch # 624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012853069590138537
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,033 INFO epoch # 625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013291770638898016
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,102 INFO epoch # 626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013100524699049336
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,171 INFO epoch # 627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013114944113684551
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,241 INFO epoch # 628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013534735861633505
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,310 INFO epoch # 629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013124342182917255
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,379 INFO epoch # 630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01319782135209867
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:45,380 INFO *** epoch 630, rolling-avg-loss (window=10)= 0.013202625178465887
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,451 INFO epoch # 631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013062377886048386
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,521 INFO epoch # 632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012963869835117034
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,593 INFO epoch # 633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013116729725152255
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,662 INFO epoch # 634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013258153067103455
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,731 INFO epoch # 635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013265918607690505
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,800 INFO epoch # 636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013352281374058554
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,875 INFO epoch # 637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01303472526238433
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:45,945 INFO epoch # 638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01301205236730831
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,014 INFO epoch # 639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01290843438889299
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,084 INFO epoch # 640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01304214181644576
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:46,084 INFO *** epoch 640, rolling-avg-loss (window=10)= 0.013101668433020158
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,153 INFO epoch # 641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01309009449822562
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,223 INFO epoch # 642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01311013690595116
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,293 INFO epoch # 643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013363989083362476
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,362 INFO epoch # 644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013197723776102066
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,431 INFO epoch # 645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01327847871663315
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,503 INFO epoch # 646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013372246082872153
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,572 INFO epoch # 647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013180848849671228
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,642 INFO epoch # 648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012954935132126724
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,713 INFO epoch # 649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012990325423223632
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,782 INFO epoch # 650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013207564463040658
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:46,782 INFO *** epoch 650, rolling-avg-loss (window=10)= 0.013174634293120888
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,851 INFO epoch # 651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012973940798214503
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,924 INFO epoch # 652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013200085756501981
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:46,995 INFO epoch # 653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013493305697504962
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,065 INFO epoch # 654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013130639772862196
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,136 INFO epoch # 655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012887306617838996
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,205 INFO epoch # 656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013181157144052642
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,275 INFO epoch # 657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013216787802853755
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,345 INFO epoch # 658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013333562043096337
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,415 INFO epoch # 659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013149284079138722
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,487 INFO epoch # 660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012996903021952935
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:47,487 INFO *** epoch 660, rolling-avg-loss (window=10)= 0.013156297273401705
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,557 INFO epoch # 661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01339925779029727
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,626 INFO epoch # 662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01300777392461896
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,696 INFO epoch # 663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013083473553082772
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,769 INFO epoch # 664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013093733747622797
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,838 INFO epoch # 665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013277315960398743
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,912 INFO epoch # 666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012827894810054983
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:47,982 INFO epoch # 667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013037674834153481
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,051 INFO epoch # 668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01297642347802009
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,120 INFO epoch # 669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012823078036308288
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,189 INFO epoch # 670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012913302039461477
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:48,190 INFO *** epoch 670, rolling-avg-loss (window=10)= 0.013043992817401887
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,259 INFO epoch # 671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013439798408320972
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,328 INFO epoch # 672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012922423093446664
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,398 INFO epoch # 673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013242908833282335
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,468 INFO epoch # 674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013190781990332263
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,538 INFO epoch # 675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013249131365280066
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,609 INFO epoch # 676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01314620225291167
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,679 INFO epoch # 677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013241986624364342
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,749 INFO epoch # 678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01291910173105342
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,819 INFO epoch # 679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01290236847209079
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,889 INFO epoch # 680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013613199256360531
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:48,889 INFO *** epoch 680, rolling-avg-loss (window=10)= 0.013186790202744306
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:48,968 INFO epoch # 681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013103936000594072
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,038 INFO epoch # 682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013057979648666722
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,109 INFO epoch # 683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012777285929769278
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,178 INFO epoch # 684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013414685002395085
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,248 INFO epoch # 685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013515097834169865
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,319 INFO epoch # 686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013588076723473412
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,388 INFO epoch # 687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01348052408014025
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,458 INFO epoch # 688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01342548465888415
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,528 INFO epoch # 689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012932790935571705
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,597 INFO epoch # 690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012975742694522653
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:49,598 INFO *** epoch 690, rolling-avg-loss (window=10)= 0.013227160350818718
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,668 INFO epoch # 691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012801955866494349
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,738 INFO epoch # 692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012993668591869728
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,807 INFO epoch # 693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012772746849805117
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,884 INFO epoch # 694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012919902788209064
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:49,960 INFO epoch # 695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012915098214788097
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,030 INFO epoch # 696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013088825664349964
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,100 INFO epoch # 697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012840568221041135
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,170 INFO epoch # 698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013117130672825234
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,240 INFO epoch # 699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013137793926788228
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,310 INFO epoch # 700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012844462493168457
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:50,310 INFO *** epoch 700, rolling-avg-loss (window=10)= 0.012943215328933938
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,380 INFO epoch # 701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012738167760627611
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,454 INFO epoch # 702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01310037061838167
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,526 INFO epoch # 703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012920691990958792
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,597 INFO epoch # 704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013016717661438243
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,668 INFO epoch # 705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01293271747417748
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,739 INFO epoch # 706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0132880809184696
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,810 INFO epoch # 707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012897880215729986
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,881 INFO epoch # 708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013041593985898154
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:50,961 INFO epoch # 709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012944974564015865
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,031 INFO epoch # 710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01296060247612851
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:51,031 INFO *** epoch 710, rolling-avg-loss (window=10)= 0.012984179766582591
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,103 INFO epoch # 711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013040498471153635
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,175 INFO epoch # 712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01265693986788392
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,244 INFO epoch # 713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013280769677034445
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,315 INFO epoch # 714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012910998798906803
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,387 INFO epoch # 715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01285813988319465
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,460 INFO epoch # 716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013178942086441176
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,532 INFO epoch # 717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012851571065506764
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,603 INFO epoch # 718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01281328753435186
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,673 INFO epoch # 719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013130641276282924
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,745 INFO epoch # 720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012838073380823647
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:51,745 INFO *** epoch 720, rolling-avg-loss (window=10)= 0.012955986204157981
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,816 INFO epoch # 721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01285509399271437
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,886 INFO epoch # 722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012762740600321974
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:51,962 INFO epoch # 723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01297244021136846
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,033 INFO epoch # 724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012776285529668842
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,103 INFO epoch # 725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012948555273136922
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,173 INFO epoch # 726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012895423759307181
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,244 INFO epoch # 727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012846201086150748
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,314 INFO epoch # 728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013212487886526755
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,385 INFO epoch # 729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012901009326534612
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,457 INFO epoch # 730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012980899095003095
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:52,457 INFO *** epoch 730, rolling-avg-loss (window=10)= 0.012915113676073297
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,527 INFO epoch # 731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013177215600652353
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,597 INFO epoch # 732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012816152588597366
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,667 INFO epoch # 733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012945223333580154
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,738 INFO epoch # 734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012905477838856833
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,807 INFO epoch # 735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012725026506398405
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,878 INFO epoch # 736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012587206730885165
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:52,951 INFO epoch # 737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013101855186479432
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,022 INFO epoch # 738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013702446687966585
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,092 INFO epoch # 739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012923959562821047
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,162 INFO epoch # 740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012870612900171961
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:53,162 INFO *** epoch 740, rolling-avg-loss (window=10)= 0.012975517693640932
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,232 INFO epoch # 741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01277410765843732
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,302 INFO epoch # 742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01263406795582601
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,372 INFO epoch # 743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01310857106000185
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,443 INFO epoch # 744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012801580676542862
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,513 INFO epoch # 745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012709379315908466
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,583 INFO epoch # 746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012769045773893595
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,653 INFO epoch # 747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012824088228600365
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,724 INFO epoch # 748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012892822083085776
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,794 INFO epoch # 749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012978384963103703
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,864 INFO epoch # 750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012665881442704372
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:53,864 INFO *** epoch 750, rolling-avg-loss (window=10)= 0.01281579291581043
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:53,935 INFO epoch # 751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012838924676179886
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,009 INFO epoch # 752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012587481284780162
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,079 INFO epoch # 753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012788025728826011
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,149 INFO epoch # 754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012912859820893833
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,219 INFO epoch # 755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013218088062214
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,290 INFO epoch # 756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012809253377573831
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,364 INFO epoch # 757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012786133999803237
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,433 INFO epoch # 758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01313849248524223
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,504 INFO epoch # 759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012930030774857318
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,575 INFO epoch # 760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013060831424913237
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:54,575 INFO *** epoch 760, rolling-avg-loss (window=10)= 0.012907012163528372
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,644 INFO epoch # 761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012752928584814072
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,714 INFO epoch # 762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012618972321173974
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,785 INFO epoch # 763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012766918247299535
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,855 INFO epoch # 764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01290536608014788
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:54,926 INFO epoch # 765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013032740447670221
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,003 INFO epoch # 766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012690043283094253
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,072 INFO epoch # 767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012991739277328765
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,143 INFO epoch # 768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012973408414317029
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,213 INFO epoch # 769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01262613988614508
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,283 INFO epoch # 770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012786462211183139
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:55,283 INFO *** epoch 770, rolling-avg-loss (window=10)= 0.012814471875317396
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,353 INFO epoch # 771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012695847797606673
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,424 INFO epoch # 772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012619937384235008
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,495 INFO epoch # 773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012703512768660273
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,567 INFO epoch # 774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012768847827932665
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,644 INFO epoch # 775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012978763585644109
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,714 INFO epoch # 776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012886460657630648
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,785 INFO epoch # 777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012590377824380994
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,856 INFO epoch # 778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01259977426379919
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:55,927 INFO epoch # 779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013167670023228441
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,004 INFO epoch # 780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013499362181339946
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:56,004 INFO *** epoch 780, rolling-avg-loss (window=10)= 0.012851055431445796
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,074 INFO epoch # 781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012737477903387376
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,144 INFO epoch # 782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01254460331318634
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,214 INFO epoch # 783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012743017875722476
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,285 INFO epoch # 784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012634707654693296
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,356 INFO epoch # 785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012966795092714685
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,426 INFO epoch # 786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012982667457046253
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,497 INFO epoch # 787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012795141778354134
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,567 INFO epoch # 788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01287231156602502
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,638 INFO epoch # 789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01303116584728871
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,708 INFO epoch # 790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012881310230919292
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:56,708 INFO *** epoch 790, rolling-avg-loss (window=10)= 0.01281891987193376
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,779 INFO epoch # 791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012546835400696312
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,851 INFO epoch # 792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012914734134184463
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,922 INFO epoch # 793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012517950963228941
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:56,995 INFO epoch # 794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012581068077789886
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,069 INFO epoch # 795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012926364650151559
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,140 INFO epoch # 796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012650573586246798
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,210 INFO epoch # 797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012498127336480787
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,281 INFO epoch # 798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012870249657758645
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,353 INFO epoch # 799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013037461268582514
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,424 INFO epoch # 800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012606099979685885
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:57,425 INFO *** epoch 800, rolling-avg-loss (window=10)= 0.012714946505480582
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,499 INFO epoch # 801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.014656971355101892
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,571 INFO epoch # 802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012743946297892503
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,642 INFO epoch # 803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012675337120890617
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,714 INFO epoch # 804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012555748676615103
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,787 INFO epoch # 805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012743474489876202
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,857 INFO epoch # 806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012727493925818375
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:57,929 INFO epoch # 807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012731865154845375
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,001 INFO epoch # 808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013040589966944285
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,077 INFO epoch # 809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012457129678555898
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,148 INFO epoch # 810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012660130499196905
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:58,148 INFO *** epoch 810, rolling-avg-loss (window=10)= 0.012899268716573717
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,218 INFO epoch # 811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012673055699893406
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,289 INFO epoch # 812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012742338263030563
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,359 INFO epoch # 813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012550600697951657
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,431 INFO epoch # 814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012656022794544697
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,502 INFO epoch # 815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012566906047452774
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,572 INFO epoch # 816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012467469527785267
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,643 INFO epoch # 817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012369663308241538
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,713 INFO epoch # 818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013301923871040344
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,783 INFO epoch # 819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012462706717529467
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,854 INFO epoch # 820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012818054016679525
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:58,854 INFO *** epoch 820, rolling-avg-loss (window=10)= 0.012660874094414923
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,925 INFO epoch # 821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012563739357782261
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:58,996 INFO epoch # 822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012438393690224205
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,071 INFO epoch # 823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012477005672241961
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,142 INFO epoch # 824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01266417426190206
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,212 INFO epoch # 825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012616142737013954
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,282 INFO epoch # 826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01280750155981098
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,355 INFO epoch # 827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01306342405400106
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,425 INFO epoch # 828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01278891329254423
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,497 INFO epoch # 829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012582085913579379
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,570 INFO epoch # 830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012426084479583162
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:50:59,570 INFO *** epoch 830, rolling-avg-loss (window=10)= 0.012642746501868325
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,641 INFO epoch # 831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012991644615041358
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,712 INFO epoch # 832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012356852793267795
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,782 INFO epoch # 833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012518988243703332
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,853 INFO epoch # 834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012659686231719596
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,923 INFO epoch # 835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012450844588290369
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:50:59,995 INFO epoch # 836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012449366625930582
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,073 INFO epoch # 837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012960370842899594
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,144 INFO epoch # 838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012787191356931413
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,215 INFO epoch # 839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012444782722741365
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,286 INFO epoch # 840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012785477683480297
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:00,286 INFO *** epoch 840, rolling-avg-loss (window=10)= 0.012640520570400571
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,375 INFO epoch # 841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012595793059361832
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,454 INFO epoch # 842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012539667409977742
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,528 INFO epoch # 843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012565264079187597
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,603 INFO epoch # 844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012516836090279477
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,677 INFO epoch # 845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012438531007085528
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,752 INFO epoch # 846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012643986428156496
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,827 INFO epoch # 847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01313508754330022
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,908 INFO epoch # 848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01300799414249403
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:00,996 INFO epoch # 849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012305790970900229
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:01,072 INFO epoch # 850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012371933846069234
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:01,072 INFO *** epoch 850, rolling-avg-loss (window=10)= 0.012612088457681239
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:01,149 INFO epoch # 851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012546788448733942
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:01,226 INFO epoch # 852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012773653345980815
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:01,302 INFO epoch # 853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012760229076125793
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:01,378 INFO epoch # 854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01262720035655158
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:01,455 INFO epoch # 855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012501437589526177
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:01,582 INFO epoch # 856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01253555484914354
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:01,694 INFO epoch # 857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012566015915945172
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:01,768 INFO epoch # 858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012389311912868704
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:01,847 INFO epoch # 859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012455744203180075
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:01,924 INFO epoch # 860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012693065092233675
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:01,924 INFO *** epoch 860, rolling-avg-loss (window=10)= 0.01258490007902895
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,003 INFO epoch # 861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013134814386389086
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,083 INFO epoch # 862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013200479346726622
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,174 INFO epoch # 863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01263631841699992
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,267 INFO epoch # 864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012843960776392903
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,342 INFO epoch # 865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012403921390484487
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,417 INFO epoch # 866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012341801822185517
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,496 INFO epoch # 867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012566307386649506
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,577 INFO epoch # 868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012433323017986758
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,655 INFO epoch # 869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012678235742662634
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,732 INFO epoch # 870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012431786927793707
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:02,732 INFO *** epoch 870, rolling-avg-loss (window=10)= 0.012667094921427114
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,806 INFO epoch # 871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012547857088169882
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,880 INFO epoch # 872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012393743757690702
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:02,954 INFO epoch # 873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012394336171980415
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,029 INFO epoch # 874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012343864209417786
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,108 INFO epoch # 875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01274273764741208
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,201 INFO epoch # 876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012523155506434186
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,277 INFO epoch # 877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012516357337257691
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,352 INFO epoch # 878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012433344365230628
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,426 INFO epoch # 879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012438096891024283
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,500 INFO epoch # 880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012439024647963898
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:03,500 INFO *** epoch 880, rolling-avg-loss (window=10)= 0.012477251762258156
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,574 INFO epoch # 881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012289406212845019
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,651 INFO epoch # 882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012734958862087557
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,727 INFO epoch # 883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012654773638184581
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,804 INFO epoch # 884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012580910830625467
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,879 INFO epoch # 885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012541530440960612
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:03,954 INFO epoch # 886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012103391791294727
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,030 INFO epoch # 887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012252688687294722
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,105 INFO epoch # 888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01246021737211517
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,180 INFO epoch # 889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012435963563621043
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,256 INFO epoch # 890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012448217440396547
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:04,256 INFO *** epoch 890, rolling-avg-loss (window=10)= 0.012450205883942545
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,341 INFO epoch # 891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012776272118623767
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,431 INFO epoch # 892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012169800366141967
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,510 INFO epoch # 893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012369883639205779
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,586 INFO epoch # 894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012874974643013307
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,661 INFO epoch # 895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01279845087389861
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,736 INFO epoch # 896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012195909648601498
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,812 INFO epoch # 897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012558746723724264
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,887 INFO epoch # 898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012637024053505488
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:04,965 INFO epoch # 899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012412892188876867
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,041 INFO epoch # 900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012551957102758544
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:05,041 INFO *** epoch 900, rolling-avg-loss (window=10)= 0.012534591135835011
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,123 INFO epoch # 901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01264297239748495
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,197 INFO epoch # 902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012262158561497927
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,272 INFO epoch # 903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012151350772806576
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,346 INFO epoch # 904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012386175817144768
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,422 INFO epoch # 905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012735736117299114
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,497 INFO epoch # 906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01258196999718036
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,572 INFO epoch # 907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012008666985535196
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,648 INFO epoch # 908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012670475111476013
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,726 INFO epoch # 909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0124021220287042
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,801 INFO epoch # 910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01225933072024158
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:05,801 INFO *** epoch 910, rolling-avg-loss (window=10)= 0.012410095850937067
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,875 INFO epoch # 911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012588873053235666
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:05,950 INFO epoch # 912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012418907015983548
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,026 INFO epoch # 913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01230111587792635
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,106 INFO epoch # 914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012297442342553819
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,181 INFO epoch # 915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012431248383862632
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,256 INFO epoch # 916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012075347340266619
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,331 INFO epoch # 917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01238731287552842
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,411 INFO epoch # 918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012359353793518884
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,497 INFO epoch # 919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012339872487687639
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,586 INFO epoch # 920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012362224914665734
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:06,586 INFO *** epoch 920, rolling-avg-loss (window=10)= 0.012356169808522933
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,677 INFO epoch # 921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012130921187677553
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,767 INFO epoch # 922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012446872457595808
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,856 INFO epoch # 923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01223727517894336
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:06,946 INFO epoch # 924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012409779908401626
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:07,034 INFO epoch # 925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012412984935300691
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:07,128 INFO epoch # 926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01251250458881259
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:07,227 INFO epoch # 927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012289459644151585
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:07,317 INFO epoch # 928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012360777253551142
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:07,411 INFO epoch # 929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012376037639166627
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:07,500 INFO epoch # 930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012490798572876624
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:07,500 INFO *** epoch 930, rolling-avg-loss (window=10)= 0.01236674113664776
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:07,596 INFO epoch # 931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012092401606163808
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:07,694 INFO epoch # 932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012234906918768372
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:07,788 INFO epoch # 933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012305823460753476
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:07,890 INFO epoch # 934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012207548027592045
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:07,987 INFO epoch # 935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012324740551412106
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:08,081 INFO epoch # 936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012578357769442456
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:08,173 INFO epoch # 937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012374649602653725
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:08,417 INFO epoch # 938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012424887922991599
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:08,579 INFO epoch # 939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012358288586671864
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:08,703 INFO epoch # 940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012072010032300438
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:08,703 INFO *** epoch 940, rolling-avg-loss (window=10)= 0.012297361447874989
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:08,815 INFO epoch # 941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012181764535073723
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:08,927 INFO epoch # 942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012164431371326957
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:09,036 INFO epoch # 943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012200469403926816
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:09,143 INFO epoch # 944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012291542374129806
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:09,235 INFO epoch # 945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012463776441290975
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:09,324 INFO epoch # 946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012457667690302645
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:09,414 INFO epoch # 947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012144161255231925
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:09,504 INFO epoch # 948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012240503834826605
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:09,593 INFO epoch # 949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012373339610972575
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:09,681 INFO epoch # 950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012856770266911814
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:09,681 INFO *** epoch 950, rolling-avg-loss (window=10)= 0.012337442678399384
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:09,770 INFO epoch # 951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012707593770963805
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:09,860 INFO epoch # 952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012076783406415156
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:09,948 INFO epoch # 953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012112995782600982
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:10,034 INFO epoch # 954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012237339839339256
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:10,118 INFO epoch # 955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012154518573411873
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:10,217 INFO epoch # 956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012109456370983806
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:10,311 INFO epoch # 957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012773741349311812
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:10,403 INFO epoch # 958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01254375610234482
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:10,490 INFO epoch # 959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012157007280204976
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:10,582 INFO epoch # 960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012352469084518296
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:10,582 INFO *** epoch 960, rolling-avg-loss (window=10)= 0.01232256615600948
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:10,683 INFO epoch # 961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012092145758547954
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:10,778 INFO epoch # 962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012389687515263046
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:10,866 INFO epoch # 963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012194354273378848
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:10,952 INFO epoch # 964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012325293025267976
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:11,042 INFO epoch # 965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012265080426420485
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:11,131 INFO epoch # 966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012406791161213602
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:11,222 INFO epoch # 967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012491326207028967
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:11,313 INFO epoch # 968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013934240836117948
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:11,401 INFO epoch # 969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012238216320318835
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:11,487 INFO epoch # 970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012306996142225607
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:11,487 INFO *** epoch 970, rolling-avg-loss (window=10)= 0.012464413166578326
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:11,574 INFO epoch # 971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012226156823869263
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:11,661 INFO epoch # 972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012074530949550015
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:11,746 INFO epoch # 973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.014380646669971091
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:11,834 INFO epoch # 974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012525818922689983
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:11,920 INFO epoch # 975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012265893058585269
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,004 INFO epoch # 976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012423281078892096
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,092 INFO epoch # 977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012128210686413305
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,182 INFO epoch # 978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012404444747205291
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,268 INFO epoch # 979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012092996302193829
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,353 INFO epoch # 980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012296368527625287
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:12,353 INFO *** epoch 980, rolling-avg-loss (window=10)= 0.012481834776699544
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,442 INFO epoch # 981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012180047162941524
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,531 INFO epoch # 982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01205895741337112
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,621 INFO epoch # 983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011827579985505769
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,711 INFO epoch # 984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012152683761503016
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,799 INFO epoch # 985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012213161494582892
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,888 INFO epoch # 986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01240931066817471
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:12,978 INFO epoch # 987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012585342475878341
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:13,075 INFO epoch # 988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012311351458941187
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:13,162 INFO epoch # 989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01232400031627289
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:13,251 INFO epoch # 990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01204394961574248
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:13,251 INFO *** epoch 990, rolling-avg-loss (window=10)= 0.012210638435291393
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:13,344 INFO epoch # 991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012224823881738952
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:13,434 INFO epoch # 992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012630345459495271
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:13,520 INFO epoch # 993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012425614773694959
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:13,607 INFO epoch # 994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012143151967653207
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:13,698 INFO epoch # 995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012145082679178033
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:13,785 INFO epoch # 996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012058189058942454
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:13,870 INFO epoch # 997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012192756163754634
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:13,970 INFO epoch # 998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012381218879350593
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:14,058 INFO epoch # 999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012197606012757335
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:14,142 INFO epoch # 1000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011945426816652928
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:14,142 INFO *** epoch 1000, rolling-avg-loss (window=10)= 0.012234421569321838
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:14,227 INFO epoch # 1001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012022634954856976
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:14,315 INFO epoch # 1002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012169462029955216
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:14,402 INFO epoch # 1003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01196582680568099
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:14,488 INFO epoch # 1004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01212342046201229
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:14,577 INFO epoch # 1005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012141346678669963
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:14,661 INFO epoch # 1006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012091062803353583
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:14,748 INFO epoch # 1007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012223094409065587
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:14,835 INFO epoch # 1008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012144725091223206
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:14,919 INFO epoch # 1009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012033712757485254
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,006 INFO epoch # 1010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011949804012796708
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:15,007 INFO *** epoch 1010, rolling-avg-loss (window=10)= 0.012086509000509976
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,093 INFO epoch # 1011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012578926009259054
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,177 INFO epoch # 1012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012063705428902591
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,266 INFO epoch # 1013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012077724221827728
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,352 INFO epoch # 1014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011979479901492595
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,436 INFO epoch # 1015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01215969506385071
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,523 INFO epoch # 1016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011985429755545089
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,610 INFO epoch # 1017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012324546230956912
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,695 INFO epoch # 1018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012463371994506036
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,781 INFO epoch # 1019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012118428373443228
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,866 INFO epoch # 1020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012195197466228689
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:15,866 INFO *** epoch 1020, rolling-avg-loss (window=10)= 0.012194650444601263
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:15,951 INFO epoch # 1021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012243131348597152
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,040 INFO epoch # 1022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012118632027081081
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,127 INFO epoch # 1023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011978193731712444
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,212 INFO epoch # 1024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012178687511810235
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,298 INFO epoch # 1025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01258239219231265
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,384 INFO epoch # 1026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012282214806016002
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,468 INFO epoch # 1027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012158042930864862
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,554 INFO epoch # 1028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012195194685565574
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,641 INFO epoch # 1029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01238167252657669
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,728 INFO epoch # 1030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012166278024337122
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:16,728 INFO *** epoch 1030, rolling-avg-loss (window=10)= 0.012228443978487382
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,813 INFO epoch # 1031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012101481016725302
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,898 INFO epoch # 1032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011988546273538044
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:16,982 INFO epoch # 1033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012070447778595346
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:17,070 INFO epoch # 1034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011797032771365984
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:17,155 INFO epoch # 1035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01229171682415264
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:17,239 INFO epoch # 1036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012052247114479542
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:17,326 INFO epoch # 1037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01395151097593563
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:17,412 INFO epoch # 1038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012328063643404415
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:17,496 INFO epoch # 1039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012257965015513557
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:17,585 INFO epoch # 1040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012316832718040262
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:17,585 INFO *** epoch 1040, rolling-avg-loss (window=10)= 0.012315584413175071
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:17,670 INFO epoch # 1041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011912524128066641
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:17,753 INFO epoch # 1042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01191198853775859
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:17,840 INFO epoch # 1043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012131129750715834
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:17,925 INFO epoch # 1044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012188062191541706
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,010 INFO epoch # 1045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012091836386493275
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,098 INFO epoch # 1046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011896824158195938
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,182 INFO epoch # 1047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011945881880819798
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,266 INFO epoch # 1048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011852271469043834
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,352 INFO epoch # 1049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012132784883890833
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,443 INFO epoch # 1050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012016675448311226
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:18,443 INFO *** epoch 1050, rolling-avg-loss (window=10)= 0.012007997883483768
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,534 INFO epoch # 1051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011970896486725125
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,625 INFO epoch # 1052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013914917781949043
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,709 INFO epoch # 1053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01219862632985626
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,795 INFO epoch # 1054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0121437615315829
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,886 INFO epoch # 1055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011960937827825546
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:18,976 INFO epoch # 1056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012164437810757331
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:19,074 INFO epoch # 1057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01202027297445706
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:19,161 INFO epoch # 1058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011874220586780991
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:19,244 INFO epoch # 1059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011966931021639279
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:19,331 INFO epoch # 1060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012121479332979237
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:19,331 INFO *** epoch 1060, rolling-avg-loss (window=10)= 0.012233648168455276
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:19,418 INFO epoch # 1061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012035499506496957
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:19,503 INFO epoch # 1062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012317491761807884
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:19,589 INFO epoch # 1063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011892028957871455
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:19,674 INFO epoch # 1064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011924517680225628
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:19,761 INFO epoch # 1065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012093499555651631
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:19,845 INFO epoch # 1066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012072573761854853
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:19,932 INFO epoch # 1067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01221593656976308
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:20,016 INFO epoch # 1068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012630790232547692
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:20,103 INFO epoch # 1069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01190595954789647
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:20,189 INFO epoch # 1070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011951822261991246
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:20,189 INFO *** epoch 1070, rolling-avg-loss (window=10)= 0.012104011983610689
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:20,276 INFO epoch # 1071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012031610429819142
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:20,360 INFO epoch # 1072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011988721794581839
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:20,451 INFO epoch # 1073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012125734106770583
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:20,543 INFO epoch # 1074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012134025052988103
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:20,630 INFO epoch # 1075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012065890418099506
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:20,718 INFO epoch # 1076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011981390403317554
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:20,809 INFO epoch # 1077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012163187190890313
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:20,920 INFO epoch # 1078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012337571582091707
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:21,011 INFO epoch # 1079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012363060390842812
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:21,133 INFO epoch # 1080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011969232213284288
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:21,133 INFO *** epoch 1080, rolling-avg-loss (window=10)= 0.012116042358268585
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:21,441 INFO epoch # 1081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011789789596306426
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:21,603 INFO epoch # 1082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01187852006405592
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:21,696 INFO epoch # 1083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011989749276212283
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:21,780 INFO epoch # 1084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011926512034343821
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:21,898 INFO epoch # 1085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012085645579333816
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:21,988 INFO epoch # 1086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012089845844145332
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:22,075 INFO epoch # 1087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011867485607841186
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:22,195 INFO epoch # 1088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011770760893289533
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:22,309 INFO epoch # 1089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011795070035649197
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:22,396 INFO epoch # 1090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01229815747854965
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:22,396 INFO *** epoch 1090, rolling-avg-loss (window=10)= 0.011949153640972716
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:22,486 INFO epoch # 1091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011811610470925058
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:22,583 INFO epoch # 1092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01197436725321625
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:22,670 INFO epoch # 1093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011852321028709412
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:22,792 INFO epoch # 1094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011616347264498473
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:22,883 INFO epoch # 1095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012101260359798159
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:22,970 INFO epoch # 1096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012017616682818958
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:23,058 INFO epoch # 1097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012295399380049536
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:23,146 INFO epoch # 1098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01187409283593297
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:23,233 INFO epoch # 1099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012049760869038957
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:23,320 INFO epoch # 1100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01193948902988008
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:23,320 INFO *** epoch 1100, rolling-avg-loss (window=10)= 0.011953226517486785
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:23,413 INFO epoch # 1101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013596702393676554
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:23,513 INFO epoch # 1102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012133672700396605
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:23,620 INFO epoch # 1103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012141833653939622
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:23,708 INFO epoch # 1104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011953488258378846
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:23,793 INFO epoch # 1105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011737533086644752
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:23,877 INFO epoch # 1106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012012388410844973
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:23,961 INFO epoch # 1107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011813518511397498
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:24,047 INFO epoch # 1108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01232263471798173
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:24,132 INFO epoch # 1109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013374820710825068
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:24,215 INFO epoch # 1110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012336793901132686
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:24,215 INFO *** epoch 1110, rolling-avg-loss (window=10)= 0.012342338634521832
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:24,306 INFO epoch # 1111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011914813704788686
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:24,405 INFO epoch # 1112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011831873934715987
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:24,492 INFO epoch # 1113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011863206060869352
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:24,576 INFO epoch # 1114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011802960060802954
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:24,660 INFO epoch # 1115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011917677614837885
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:24,744 INFO epoch # 1116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011925897281616925
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:24,832 INFO epoch # 1117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013516451139003039
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:24,917 INFO epoch # 1118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011865610236834202
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,001 INFO epoch # 1119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012028638167040688
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,085 INFO epoch # 1120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011773537458585842
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:25,086 INFO *** epoch 1120, rolling-avg-loss (window=10)= 0.012044066565909556
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,171 INFO epoch # 1121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01175262045913509
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,256 INFO epoch # 1122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011817571081753289
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,358 INFO epoch # 1123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011579302977770567
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,444 INFO epoch # 1124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011725934940789428
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,536 INFO epoch # 1125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011788738306079592
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,636 INFO epoch # 1126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01216378097555467
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,725 INFO epoch # 1127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011944789652313504
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,815 INFO epoch # 1128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011723421966390951
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,902 INFO epoch # 1129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011804076057991811
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:25,990 INFO epoch # 1130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011787142937204667
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:25,990 INFO *** epoch 1130, rolling-avg-loss (window=10)= 0.011808737935498357
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:26,078 INFO epoch # 1131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011664301875446524
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:26,164 INFO epoch # 1132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012380087202680962
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:26,248 INFO epoch # 1133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011488871662212269
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:26,333 INFO epoch # 1134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011727065553090402
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:26,417 INFO epoch # 1135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01168353296816349
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:26,503 INFO epoch # 1136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011795676978571075
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:26,590 INFO epoch # 1137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011721479480287858
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:26,678 INFO epoch # 1138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012000094407371112
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:26,763 INFO epoch # 1139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011732511315494775
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:26,849 INFO epoch # 1140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011915470526686737
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:26,849 INFO *** epoch 1140, rolling-avg-loss (window=10)= 0.01181090919700052
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:26,933 INFO epoch # 1141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0116533361574901
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,019 INFO epoch # 1142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011875367563750064
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,106 INFO epoch # 1143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012134123633482626
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,192 INFO epoch # 1144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012215831609708922
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,276 INFO epoch # 1145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012042617318885666
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,371 INFO epoch # 1146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012094093659626586
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,457 INFO epoch # 1147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011740056352157678
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,543 INFO epoch # 1148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011791761991168772
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,628 INFO epoch # 1149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011979502772114107
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,714 INFO epoch # 1150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01178741232891168
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:27,714 INFO *** epoch 1150, rolling-avg-loss (window=10)= 0.01193141033872962
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,798 INFO epoch # 1151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011566079647413323
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,884 INFO epoch # 1152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011639341698693378
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:27,969 INFO epoch # 1153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011873456303562437
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:28,053 INFO epoch # 1154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011686163621821575
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:28,137 INFO epoch # 1155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.013305402174592019
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:28,223 INFO epoch # 1156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011635581463841456
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:28,308 INFO epoch # 1157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011728759868336575
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:28,404 INFO epoch # 1158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01158644557664437
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:28,494 INFO epoch # 1159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01166884247213602
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:28,583 INFO epoch # 1160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011795161158910819
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:28,583 INFO *** epoch 1160, rolling-avg-loss (window=10)= 0.011848523398595196
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:28,722 INFO epoch # 1161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011562691615628345
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:28,870 INFO epoch # 1162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011635546306414264
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:29,013 INFO epoch # 1163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011872798423948033
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:29,165 INFO epoch # 1164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012192855442741088
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:29,298 INFO epoch # 1165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01190148983815951
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:29,404 INFO epoch # 1166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011927485213215861
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:29,516 INFO epoch # 1167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011551678366959094
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:29,605 INFO epoch # 1168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011983895341732673
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:29,700 INFO epoch # 1169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011684425015534674
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:29,790 INFO epoch # 1170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01159815877409918
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:29,790 INFO *** epoch 1170, rolling-avg-loss (window=10)= 0.011791102433843273
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:29,875 INFO epoch # 1171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011746649244534118
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:29,959 INFO epoch # 1172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011664853850379586
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:30,065 INFO epoch # 1173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01200007637962699
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:30,158 INFO epoch # 1174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011712353955954314
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:30,254 INFO epoch # 1175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011927175668201275
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:30,345 INFO epoch # 1176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01167129849615906
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:30,428 INFO epoch # 1177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011716851818242243
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:30,513 INFO epoch # 1178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011805519301976477
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:30,597 INFO epoch # 1179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011887953763029405
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:30,681 INFO epoch # 1180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011443379468151502
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:30,681 INFO *** epoch 1180, rolling-avg-loss (window=10)= 0.011757611194625497
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:30,767 INFO epoch # 1181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01147372992709279
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:30,851 INFO epoch # 1182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011879704639847789
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:30,935 INFO epoch # 1183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011914485772805555
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,019 INFO epoch # 1184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011783304032204407
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,104 INFO epoch # 1185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01183849811287863
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,189 INFO epoch # 1186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011545768486601965
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,273 INFO epoch # 1187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011953463631549052
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,358 INFO epoch # 1188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011740381137600966
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,442 INFO epoch # 1189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01151394437599395
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,526 INFO epoch # 1190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011866297885509474
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:31,526 INFO *** epoch 1190, rolling-avg-loss (window=10)= 0.011750957800208458
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,611 INFO epoch # 1191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011767125755016292
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,695 INFO epoch # 1192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011440283099987677
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,778 INFO epoch # 1193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011687512655875512
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,865 INFO epoch # 1194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01149317328152912
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:31,948 INFO epoch # 1195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011644355866259762
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,034 INFO epoch # 1196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01164702247562153
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,117 INFO epoch # 1197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011429689152698432
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,202 INFO epoch # 1198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011679876489298684
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,286 INFO epoch # 1199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011678219320518629
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,380 INFO epoch # 1200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012161096557974815
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:32,380 INFO *** epoch 1200, rolling-avg-loss (window=10)= 0.011662835465478045
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,464 INFO epoch # 1201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011567037339721408
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,548 INFO epoch # 1202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011694816393511637
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,632 INFO epoch # 1203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011643777448417885
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,716 INFO epoch # 1204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011698955057987145
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,800 INFO epoch # 1205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011527793741385852
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,885 INFO epoch # 1206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011704612596492683
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:32,968 INFO epoch # 1207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01195053931192628
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,053 INFO epoch # 1208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012215837337342756
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,137 INFO epoch # 1209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011951041075267963
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,221 INFO epoch # 1210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01150795257916408
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:33,221 INFO *** epoch 1210, rolling-avg-loss (window=10)= 0.01174623628812177
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,306 INFO epoch # 1211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011591773333826235
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,391 INFO epoch # 1212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011489821624542986
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,479 INFO epoch # 1213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011700306101036922
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,564 INFO epoch # 1214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011640158096062286
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,649 INFO epoch # 1215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011671893378453596
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,732 INFO epoch # 1216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011602816251771791
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,821 INFO epoch # 1217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011895805510825344
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,905 INFO epoch # 1218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01160013009128826
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:33,989 INFO epoch # 1219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011695103573479823
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:34,073 INFO epoch # 1220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01163809715903231
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:34,074 INFO *** epoch 1220, rolling-avg-loss (window=10)= 0.011652590512031956
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:34,157 INFO epoch # 1221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011727687829573239
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:34,241 INFO epoch # 1222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01152273661323956
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:34,326 INFO epoch # 1223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01162024335935712
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:34,410 INFO epoch # 1224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01185473000098552
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:34,494 INFO epoch # 1225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01237057125461953
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:34,577 INFO epoch # 1226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011664539568924479
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:34,662 INFO epoch # 1227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011665822731863175
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:34,770 INFO epoch # 1228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011345838901719876
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:34,867 INFO epoch # 1229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01138719154654869
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:34,951 INFO epoch # 1230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011365283014518874
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:34,951 INFO *** epoch 1230, rolling-avg-loss (window=10)= 0.011652464482135007
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,034 INFO epoch # 1231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011602031785462584
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,118 INFO epoch # 1232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011621118629617351
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,201 INFO epoch # 1233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011737711834056037
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,284 INFO epoch # 1234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011458155353154455
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,369 INFO epoch # 1235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011771980999037623
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,453 INFO epoch # 1236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01161671323435647
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,539 INFO epoch # 1237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01170995351858437
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,626 INFO epoch # 1238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01152643682435155
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,710 INFO epoch # 1239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011379633444760527
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,794 INFO epoch # 1240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011893166082778147
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:35,794 INFO *** epoch 1240, rolling-avg-loss (window=10)= 0.011631690170615912
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,882 INFO epoch # 1241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011354290501081518
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:35,966 INFO epoch # 1242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011402256241334336
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,050 INFO epoch # 1243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011561755277216434
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,135 INFO epoch # 1244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011308833970023052
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,219 INFO epoch # 1245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011490589978971651
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,303 INFO epoch # 1246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011588061773883445
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,388 INFO epoch # 1247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01166090556819524
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,472 INFO epoch # 1248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011806469370744059
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,557 INFO epoch # 1249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011837899458727666
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,641 INFO epoch # 1250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011598399960036788
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:36,641 INFO *** epoch 1250, rolling-avg-loss (window=10)= 0.01156094621002142
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,725 INFO epoch # 1251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011444540774183614
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,811 INFO epoch # 1252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011548202271972384
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,895 INFO epoch # 1253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011408902159226792
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:36,979 INFO epoch # 1254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0117763781254845
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,063 INFO epoch # 1255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011536550920988832
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,148 INFO epoch # 1256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011414033187819379
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,233 INFO epoch # 1257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01155544560668724
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,319 INFO epoch # 1258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011574881736721311
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,403 INFO epoch # 1259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01158296363428235
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,487 INFO epoch # 1260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011825628365789142
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:37,488 INFO *** epoch 1260, rolling-avg-loss (window=10)= 0.011566752678315554
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,573 INFO epoch # 1261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011628544889390469
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,657 INFO epoch # 1262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011575645447841712
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,741 INFO epoch # 1263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011593378814203399
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,826 INFO epoch # 1264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011311727063730358
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,910 INFO epoch # 1265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011352349864318967
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:37,994 INFO epoch # 1266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01147360441141895
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:38,078 INFO epoch # 1267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011647796391376428
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:38,163 INFO epoch # 1268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01186924816242286
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:38,247 INFO epoch # 1269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01167582644016615
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:38,332 INFO epoch # 1270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01148347341056381
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:38,332 INFO *** epoch 1270, rolling-avg-loss (window=10)= 0.01156115948954331
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:38,416 INFO epoch # 1271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011713723898200051
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:38,500 INFO epoch # 1272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011649322682725533
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:38,584 INFO epoch # 1273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011527561057092888
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:38,669 INFO epoch # 1274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011459574795195035
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:38,753 INFO epoch # 1275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011374610276626689
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:38,839 INFO epoch # 1276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01191492531714695
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:38,924 INFO epoch # 1277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011616806007389511
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,007 INFO epoch # 1278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011747254763862916
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,091 INFO epoch # 1279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011697294316919786
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,175 INFO epoch # 1280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011632910490568195
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:39,175 INFO *** epoch 1280, rolling-avg-loss (window=10)= 0.011633398360572755
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,260 INFO epoch # 1281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011668901364984257
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,345 INFO epoch # 1282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011477936272110257
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,429 INFO epoch # 1283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011344514787197113
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,512 INFO epoch # 1284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011547481920570136
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,597 INFO epoch # 1285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01170435549159135
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,682 INFO epoch # 1286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011493697090606604
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,766 INFO epoch # 1287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011335840435432537
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,851 INFO epoch # 1288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011412957004670586
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:39,934 INFO epoch # 1289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01126187936003719
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,018 INFO epoch # 1290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011441258208027908
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:40,018 INFO *** epoch 1290, rolling-avg-loss (window=10)= 0.011468882193522795
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,102 INFO epoch # 1291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011542324163019656
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,186 INFO epoch # 1292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011782726625512753
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,270 INFO epoch # 1293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011326725900705372
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,357 INFO epoch # 1294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011451857670077255
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,441 INFO epoch # 1295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011687470227479934
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,525 INFO epoch # 1296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011597895409379687
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,618 INFO epoch # 1297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011222689811672484
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,701 INFO epoch # 1298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011221921117976309
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,786 INFO epoch # 1299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011375041159668139
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,871 INFO epoch # 1300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011686420846464379
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:40,871 INFO *** epoch 1300, rolling-avg-loss (window=10)= 0.011489507293195598
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:40,955 INFO epoch # 1301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011234063701704145
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,039 INFO epoch # 1302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011519322304853372
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,123 INFO epoch # 1303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011531357527045268
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,207 INFO epoch # 1304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011624471231230667
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,291 INFO epoch # 1305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01121732419622796
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,376 INFO epoch # 1306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011332463113857168
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,460 INFO epoch # 1307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012102475616016559
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,544 INFO epoch # 1308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011300255132040807
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,628 INFO epoch # 1309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011297735199332238
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,717 INFO epoch # 1310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011512197886726685
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:41,717 INFO *** epoch 1310, rolling-avg-loss (window=10)= 0.011467166590903485
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,801 INFO epoch # 1311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011316827172413469
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,887 INFO epoch # 1312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011378747836819718
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:41,973 INFO epoch # 1313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011724199141774859
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,057 INFO epoch # 1314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011915649965937648
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,141 INFO epoch # 1315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011856657466185944
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,224 INFO epoch # 1316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011280629464558193
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,308 INFO epoch # 1317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011267528909125498
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,392 INFO epoch # 1318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011225305204944951
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,478 INFO epoch # 1319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011326606298929879
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,563 INFO epoch # 1320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011459297320938536
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:42,563 INFO *** epoch 1320, rolling-avg-loss (window=10)= 0.01147514487816287
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,648 INFO epoch # 1321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011450585603181804
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,732 INFO epoch # 1322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01150153631876622
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,820 INFO epoch # 1323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011309960018843412
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,904 INFO epoch # 1324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011183153598436288
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:42,988 INFO epoch # 1325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011173354860927378
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:43,071 INFO epoch # 1326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011485858288194453
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:43,155 INFO epoch # 1327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011565637814679315
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:43,239 INFO epoch # 1328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011275558639317751
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:43,326 INFO epoch # 1329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011317776728953633
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:43,410 INFO epoch # 1330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011473201833931463
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:43,410 INFO *** epoch 1330, rolling-avg-loss (window=10)= 0.011373662370523173
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:43,494 INFO epoch # 1331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011203258418078933
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:43,578 INFO epoch # 1332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011197767187176006
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:43,663 INFO epoch # 1333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01142775509506464
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:43,747 INFO epoch # 1334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01127772977841752
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:43,834 INFO epoch # 1335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011533513691808496
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:43,918 INFO epoch # 1336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011275837384164333
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,002 INFO epoch # 1337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011301252325730664
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,086 INFO epoch # 1338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011351342293034706
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,169 INFO epoch # 1339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011585632971088802
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,253 INFO epoch # 1340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011236169787922076
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:44,253 INFO *** epoch 1340, rolling-avg-loss (window=10)= 0.01133902589324862
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,340 INFO epoch # 1341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011426918966961758
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,424 INFO epoch # 1342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01118514487253768
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,508 INFO epoch # 1343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01132369932851621
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,592 INFO epoch # 1344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011632525229028292
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,677 INFO epoch # 1345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011596429507647242
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,761 INFO epoch # 1346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011319811602256128
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,847 INFO epoch # 1347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011464188234614474
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:44,931 INFO epoch # 1348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011028900895533817
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,015 INFO epoch # 1349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011215744240741645
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,099 INFO epoch # 1350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011200508940964938
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:45,099 INFO *** epoch 1350, rolling-avg-loss (window=10)= 0.011339387181880219
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,182 INFO epoch # 1351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011422554363629647
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,266 INFO epoch # 1352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011289365642837116
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,352 INFO epoch # 1353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01128145611978003
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,436 INFO epoch # 1354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011236493276166064
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,519 INFO epoch # 1355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011308572348207235
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,603 INFO epoch # 1356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011127168871462345
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,687 INFO epoch # 1357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011065019154921174
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,770 INFO epoch # 1358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012718453564281975
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,855 INFO epoch # 1359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011329400166869164
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:45,938 INFO epoch # 1360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011195696265037572
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:45,938 INFO *** epoch 1360, rolling-avg-loss (window=10)= 0.01139741797731923
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,022 INFO epoch # 1361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01282089947323714
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,105 INFO epoch # 1362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011565920098551683
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,188 INFO epoch # 1363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011528128611722163
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,271 INFO epoch # 1364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011454258752720697
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,356 INFO epoch # 1365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01128209193370172
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,440 INFO epoch # 1366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011480387938874109
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,524 INFO epoch # 1367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011734347863655006
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,607 INFO epoch # 1368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011473165785095522
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,691 INFO epoch # 1369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011317408803318227
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,775 INFO epoch # 1370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011337530912299242
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:46,775 INFO *** epoch 1370, rolling-avg-loss (window=10)= 0.011599414017317549
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,864 INFO epoch # 1371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011325259865926845
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:46,947 INFO epoch # 1372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011052819580904075
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,031 INFO epoch # 1373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011381193157285452
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,114 INFO epoch # 1374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010964513590027179
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,197 INFO epoch # 1375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011262391387884105
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,281 INFO epoch # 1376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01177397686322885
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,366 INFO epoch # 1377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011593755933323078
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,450 INFO epoch # 1378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011164957258318151
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,534 INFO epoch # 1379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01100666235600199
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,617 INFO epoch # 1380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01121173787063786
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:47,617 INFO *** epoch 1380, rolling-avg-loss (window=10)= 0.011273726786353758
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,703 INFO epoch # 1381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011302034011376756
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,786 INFO epoch # 1382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011182610131800174
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,872 INFO epoch # 1383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01113400352852685
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:47,960 INFO epoch # 1384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011289390841765064
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,044 INFO epoch # 1385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011215622177613634
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,128 INFO epoch # 1386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011217230917619808
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,211 INFO epoch # 1387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011443937822644199
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,297 INFO epoch # 1388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011332496628165245
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,381 INFO epoch # 1389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01123912933149508
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,465 INFO epoch # 1390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011212294562054533
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:48,465 INFO *** epoch 1390, rolling-avg-loss (window=10)= 0.011256874995306134
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,549 INFO epoch # 1391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01102266399455922
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,634 INFO epoch # 1392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011353132887078184
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,719 INFO epoch # 1393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011327815907342094
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,804 INFO epoch # 1394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011228053564471858
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,889 INFO epoch # 1395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01105714315282447
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:48,973 INFO epoch # 1396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011114200776708978
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:49,058 INFO epoch # 1397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011153629355664765
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:49,143 INFO epoch # 1398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011444135981478861
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:49,225 INFO epoch # 1399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011228544437991721
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:49,310 INFO epoch # 1400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0110810865914183
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:49,310 INFO *** epoch 1400, rolling-avg-loss (window=10)= 0.011201040664953846
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:49,394 INFO epoch # 1401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010933421977928707
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:49,478 INFO epoch # 1402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01117546305592571
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:49,564 INFO epoch # 1403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01130218992142805
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:49,666 INFO epoch # 1404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01105122540944389
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:49,755 INFO epoch # 1405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011164213796811444
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:49,841 INFO epoch # 1406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011350772928978715
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:49,927 INFO epoch # 1407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01118122058521424
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,013 INFO epoch # 1408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011026529169508388
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,101 INFO epoch # 1409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011012275556900672
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,187 INFO epoch # 1410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011257698307079927
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:50,187 INFO *** epoch 1410, rolling-avg-loss (window=10)= 0.011145501070921975
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,273 INFO epoch # 1411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011232605656342848
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,359 INFO epoch # 1412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011002495379320213
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,445 INFO epoch # 1413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011175402493349144
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,531 INFO epoch # 1414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011471323602433716
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,617 INFO epoch # 1415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011116990360564419
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,707 INFO epoch # 1416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011269260424056224
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,792 INFO epoch # 1417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011091266525909305
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,878 INFO epoch # 1418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011286354823304074
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:50,964 INFO epoch # 1419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011237765436193772
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:51,050 INFO epoch # 1420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01147023190611175
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:51,050 INFO *** epoch 1420, rolling-avg-loss (window=10)= 0.011235369660758547
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:51,136 INFO epoch # 1421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011110965521740061
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:51,222 INFO epoch # 1422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010933755072099823
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:51,308 INFO epoch # 1423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011228694480710797
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:51,395 INFO epoch # 1424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011498575890436768
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:51,482 INFO epoch # 1425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011294235980936458
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:51,566 INFO epoch # 1426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011255529802292585
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:51,654 INFO epoch # 1427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011039713916501829
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:51,743 INFO epoch # 1428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011549811238156897
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:51,829 INFO epoch # 1429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01106910295119243
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:51,916 INFO epoch # 1430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011060960497707129
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:51,917 INFO *** epoch 1430, rolling-avg-loss (window=10)= 0.011204134535177478
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,003 INFO epoch # 1431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011308373923280409
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,093 INFO epoch # 1432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011086287522422416
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,182 INFO epoch # 1433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01109492498050843
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,268 INFO epoch # 1434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01112618524847286
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,356 INFO epoch # 1435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011203983512574008
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,445 INFO epoch # 1436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010875766604606594
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,532 INFO epoch # 1437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010968968498387507
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,618 INFO epoch # 1438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011115939462823528
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,705 INFO epoch # 1439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011251810884901455
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,790 INFO epoch # 1440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011063254491559097
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:52,791 INFO *** epoch 1440, rolling-avg-loss (window=10)= 0.01110954951295363
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,877 INFO epoch # 1441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011202405552778925
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:52,963 INFO epoch # 1442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011113322273428951
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:53,048 INFO epoch # 1443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011089090803372009
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:53,134 INFO epoch # 1444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011068985837378672
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:53,220 INFO epoch # 1445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011028478919927562
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:53,307 INFO epoch # 1446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011005397393767323
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:53,393 INFO epoch # 1447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011169769149273635
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:53,481 INFO epoch # 1448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011077501624822617
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:53,580 INFO epoch # 1449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011165099630930594
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:53,668 INFO epoch # 1450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011038822268268892
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:53,669 INFO *** epoch 1450, rolling-avg-loss (window=10)= 0.011095887345394918
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:53,756 INFO epoch # 1451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011076068698561618
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:53,842 INFO epoch # 1452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011207128568951572
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:53,926 INFO epoch # 1453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011009756196290254
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,012 INFO epoch # 1454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0110026765614748
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,098 INFO epoch # 1455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010919139168358275
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,184 INFO epoch # 1456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01097372615975993
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,269 INFO epoch # 1457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011168702917971781
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,357 INFO epoch # 1458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010924627884690251
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,443 INFO epoch # 1459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011174275440030865
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,529 INFO epoch # 1460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011126864648291043
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:54,529 INFO *** epoch 1460, rolling-avg-loss (window=10)= 0.01105829662443804
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,614 INFO epoch # 1461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010989391205034086
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,700 INFO epoch # 1462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010954454148720419
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,787 INFO epoch # 1463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011220483761280774
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,875 INFO epoch # 1464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011384893621184996
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:54,963 INFO epoch # 1465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011319515787597214
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:55,049 INFO epoch # 1466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011287819654015558
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:55,134 INFO epoch # 1467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011006839899346232
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:55,220 INFO epoch # 1468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01097994353622198
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:55,309 INFO epoch # 1469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011248531831162317
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:55,396 INFO epoch # 1470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011171221174299717
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:55,396 INFO *** epoch 1470, rolling-avg-loss (window=10)= 0.01115630946188633
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:55,482 INFO epoch # 1471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010941003682091832
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:55,568 INFO epoch # 1472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011373333459986108
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:55,654 INFO epoch # 1473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01145870815962553
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:55,741 INFO epoch # 1474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01157683784674321
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:55,831 INFO epoch # 1475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011021775991788932
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:55,918 INFO epoch # 1476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011050543541620885
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,004 INFO epoch # 1477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010951398818620613
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,090 INFO epoch # 1478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011076385767332145
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,176 INFO epoch # 1479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010970416492117302
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,262 INFO epoch # 1480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010883888196466225
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:56,262 INFO *** epoch 1480, rolling-avg-loss (window=10)= 0.011130429195639278
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,350 INFO epoch # 1481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010896123307091849
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,437 INFO epoch # 1482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011097667805318321
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,523 INFO epoch # 1483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011287707955177341
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,610 INFO epoch # 1484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011184402541922671
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,696 INFO epoch # 1485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01091451617623014
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,783 INFO epoch # 1486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011403964000887104
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,872 INFO epoch # 1487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010844062009294117
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:56,958 INFO epoch # 1488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01078004545665213
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,044 INFO epoch # 1489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010834027733653783
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,130 INFO epoch # 1490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0109925794548222
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:57,130 INFO *** epoch 1490, rolling-avg-loss (window=10)= 0.011023509644104966
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,216 INFO epoch # 1491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010900383815169334
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,303 INFO epoch # 1492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011044213202382837
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,389 INFO epoch # 1493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010856790082263095
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,475 INFO epoch # 1494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010967819379376513
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,562 INFO epoch # 1495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01120540112523096
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,648 INFO epoch # 1496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011168072359370333
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,734 INFO epoch # 1497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01111651717552117
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,821 INFO epoch # 1498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010843612918896335
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,908 INFO epoch # 1499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010874619960252728
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:57,993 INFO epoch # 1500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010931170359253884
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:57,993 INFO *** epoch 1500, rolling-avg-loss (window=10)= 0.010990860037771718
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:58,079 INFO epoch # 1501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011142199912241527
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:58,165 INFO epoch # 1502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01096189939416945
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:58,252 INFO epoch # 1503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011054277007601091
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:58,339 INFO epoch # 1504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010754905295159135
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:58,426 INFO epoch # 1505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01089546806178987
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:58,513 INFO epoch # 1506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010800538478153092
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:58,600 INFO epoch # 1507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011708606074431112
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:58,687 INFO epoch # 1508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01126281080235328
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:58,773 INFO epoch # 1509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01096784421908004
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:58,859 INFO epoch # 1510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01080769281834364
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:58,859 INFO *** epoch 1510, rolling-avg-loss (window=10)= 0.011035624206332221
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:58,945 INFO epoch # 1511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010911927891096898
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,030 INFO epoch # 1512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011184248235076666
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,114 INFO epoch # 1513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011126856465956994
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,200 INFO epoch # 1514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011189699851508651
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,287 INFO epoch # 1515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011176566699785846
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,374 INFO epoch # 1516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011037800101829427
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,461 INFO epoch # 1517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011008987296372652
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,547 INFO epoch # 1518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010980603764099733
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,633 INFO epoch # 1519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011024618694292648
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,719 INFO epoch # 1520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011196562501468828
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:51:59,719 INFO *** epoch 1520, rolling-avg-loss (window=10)= 0.011083787150148834
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,808 INFO epoch # 1521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01098301654149379
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,894 INFO epoch # 1522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011070108027862652
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:51:59,981 INFO epoch # 1523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011123524133914283
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:00,069 INFO epoch # 1524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010945414578808205
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:00,157 INFO epoch # 1525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010879575900201286
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:00,244 INFO epoch # 1526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011018404344628965
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:00,334 INFO epoch # 1527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011011340921478612
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:00,421 INFO epoch # 1528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010728972777724267
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:00,507 INFO epoch # 1529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010882503665717585
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:00,602 INFO epoch # 1530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010699845358197178
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:00,602 INFO *** epoch 1530, rolling-avg-loss (window=10)= 0.010934270625002681
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:00,691 INFO epoch # 1531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010941057738714984
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:00,781 INFO epoch # 1532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010930595946099077
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:00,875 INFO epoch # 1533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010791331263525146
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:00,980 INFO epoch # 1534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011269791251314538
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:01,075 INFO epoch # 1535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0110289744500603
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:01,167 INFO epoch # 1536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010981203176613365
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:01,296 INFO epoch # 1537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010718965949490667
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:01,388 INFO epoch # 1538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011335487790139659
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:01,477 INFO epoch # 1539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010710559725495322
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:01,573 INFO epoch # 1540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010861108651650804
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:01,573 INFO *** epoch 1540, rolling-avg-loss (window=10)= 0.010956907594310386
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:01,660 INFO epoch # 1541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010926763633532184
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:01,746 INFO epoch # 1542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011101161655304687
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:01,834 INFO epoch # 1543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010817382045622383
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:01,919 INFO epoch # 1544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01086116289453847
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,008 INFO epoch # 1545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01128816468907254
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,097 INFO epoch # 1546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01079487425408193
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,183 INFO epoch # 1547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010976686368563346
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,269 INFO epoch # 1548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011131435526268822
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,355 INFO epoch # 1549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010810987065945353
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,442 INFO epoch # 1550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010810403126691069
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:02,442 INFO *** epoch 1550, rolling-avg-loss (window=10)= 0.010951902125962079
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,528 INFO epoch # 1551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010830866758312498
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,616 INFO epoch # 1552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01100961584597826
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,703 INFO epoch # 1553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010848653569285358
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,790 INFO epoch # 1554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010948380162673337
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,877 INFO epoch # 1555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010834240341292961
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:02,963 INFO epoch # 1556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010858879584286893
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:03,049 INFO epoch # 1557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010816855828410812
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:03,137 INFO epoch # 1558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010705880062388522
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:03,224 INFO epoch # 1559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010977608311389173
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:03,310 INFO epoch # 1560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010801953722589783
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:03,310 INFO *** epoch 1560, rolling-avg-loss (window=10)= 0.01086329341866076
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:03,397 INFO epoch # 1561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010808407781379564
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:03,483 INFO epoch # 1562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010754807852208614
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:03,571 INFO epoch # 1563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010826042194717696
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:03,658 INFO epoch # 1564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011019607327346291
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:03,744 INFO epoch # 1565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010822207241186073
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:03,830 INFO epoch # 1566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010877102513664536
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:03,920 INFO epoch # 1567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010798638021307332
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,006 INFO epoch # 1568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010702956946832792
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,094 INFO epoch # 1569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01088303653523326
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,182 INFO epoch # 1570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01072945431806147
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:04,182 INFO *** epoch 1570, rolling-avg-loss (window=10)= 0.010822226073193763
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,268 INFO epoch # 1571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011342152806797198
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,354 INFO epoch # 1572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.012460160608004247
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,441 INFO epoch # 1573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010875516052224806
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,527 INFO epoch # 1574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010640294982918672
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,615 INFO epoch # 1575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0108075376466981
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,702 INFO epoch # 1576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010816182575321623
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,788 INFO epoch # 1577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0110827060682433
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,877 INFO epoch # 1578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010676931722887925
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:04,964 INFO epoch # 1579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010849488752761057
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:05,051 INFO epoch # 1580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01108763324362891
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:05,051 INFO *** epoch 1580, rolling-avg-loss (window=10)= 0.011063860445948582
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:05,137 INFO epoch # 1581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010843807252656136
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:05,225 INFO epoch # 1582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010790588174547469
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:05,310 INFO epoch # 1583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011059150325932673
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:05,397 INFO epoch # 1584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010919194834839021
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:05,484 INFO epoch # 1585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010913107018651707
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:05,577 INFO epoch # 1586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01076190924005849
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:05,663 INFO epoch # 1587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010906110783772809
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:05,752 INFO epoch # 1588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01116640731426222
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:05,839 INFO epoch # 1589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011067761653768165
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:05,928 INFO epoch # 1590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010710061847099236
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:05,928 INFO *** epoch 1590, rolling-avg-loss (window=10)= 0.010913809844558793
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,015 INFO epoch # 1591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01115512488675969
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,102 INFO epoch # 1592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010724782677633422
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,188 INFO epoch # 1593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010649696491392595
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,274 INFO epoch # 1594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010795621001826865
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,360 INFO epoch # 1595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010998931221131767
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,447 INFO epoch # 1596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010952680564618537
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,533 INFO epoch # 1597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01083813195249864
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,620 INFO epoch # 1598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010684115586004087
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,706 INFO epoch # 1599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010673433442467026
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,794 INFO epoch # 1600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010800933531884636
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:06,794 INFO *** epoch 1600, rolling-avg-loss (window=10)= 0.010827345135621728
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,882 INFO epoch # 1601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010631414928606578
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:06,970 INFO epoch # 1602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010600043380899088
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:07,059 INFO epoch # 1603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010672761699450868
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:07,147 INFO epoch # 1604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011096136338476623
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:07,232 INFO epoch # 1605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011079112440347672
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:07,319 INFO epoch # 1606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011070854795564498
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:07,405 INFO epoch # 1607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010819051135331392
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:07,491 INFO epoch # 1608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010857374886316912
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:07,579 INFO epoch # 1609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010573312207790357
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:07,667 INFO epoch # 1610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01072188743523189
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:07,667 INFO *** epoch 1610, rolling-avg-loss (window=10)= 0.010812194924801588
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:07,753 INFO epoch # 1611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010926724491374834
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:07,841 INFO epoch # 1612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01070730903052858
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:07,930 INFO epoch # 1613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011546758309538876
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,016 INFO epoch # 1614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010712649766355753
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,104 INFO epoch # 1615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010536168316113098
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,189 INFO epoch # 1616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010477909791682448
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,275 INFO epoch # 1617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010505939594336919
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,363 INFO epoch # 1618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010753897298127413
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,449 INFO epoch # 1619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010723052493163517
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,535 INFO epoch # 1620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010754726760621582
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:08,535 INFO *** epoch 1620, rolling-avg-loss (window=10)= 0.010764513585184302
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,629 INFO epoch # 1621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010726820970220225
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,715 INFO epoch # 1622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010715617625308888
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,801 INFO epoch # 1623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010792348460693444
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,891 INFO epoch # 1624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010680279009310262
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:08,980 INFO epoch # 1625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010675271374306508
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:09,069 INFO epoch # 1626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011985256749072245
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:09,159 INFO epoch # 1627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010689519019797445
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:09,245 INFO epoch # 1628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010573228701416935
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:09,331 INFO epoch # 1629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010674703347363642
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:09,420 INFO epoch # 1630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010664257766412838
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:09,420 INFO *** epoch 1630, rolling-avg-loss (window=10)= 0.010817730302390242
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:09,506 INFO epoch # 1631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010696271327989441
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:09,595 INFO epoch # 1632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010501551069319248
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:09,686 INFO epoch # 1633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01089866619024958
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:09,773 INFO epoch # 1634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010843889847663896
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:09,859 INFO epoch # 1635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010587103851139545
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:09,951 INFO epoch # 1636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010668734527592148
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,038 INFO epoch # 1637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010663950722664595
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,128 INFO epoch # 1638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010579244360061628
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,214 INFO epoch # 1639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010526173415460758
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,300 INFO epoch # 1640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010732267697208694
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:10,300 INFO *** epoch 1640, rolling-avg-loss (window=10)= 0.010669785300934955
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,386 INFO epoch # 1641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010645474985774074
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,473 INFO epoch # 1642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010514976297106063
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,558 INFO epoch # 1643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01056976452735918
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,644 INFO epoch # 1644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010596343568925347
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,731 INFO epoch # 1645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010811144392937422
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,818 INFO epoch # 1646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01067760081828705
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,903 INFO epoch # 1647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010925352047862751
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:10,993 INFO epoch # 1648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010842035684202398
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:11,080 INFO epoch # 1649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010440396371164493
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:11,166 INFO epoch # 1650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010771735997072289
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:11,166 INFO *** epoch 1650, rolling-avg-loss (window=10)= 0.010679482469069105
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:11,252 INFO epoch # 1651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010659996579800332
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:11,339 INFO epoch # 1652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010688188592238086
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:11,425 INFO epoch # 1653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01058631128232394
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:11,511 INFO epoch # 1654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010647037651922021
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:11,597 INFO epoch # 1655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01049769033810922
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:11,683 INFO epoch # 1656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010381633974611759
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:11,770 INFO epoch # 1657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010626105565045561
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:11,856 INFO epoch # 1658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010791385240320648
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:11,942 INFO epoch # 1659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010661454991038356
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,032 INFO epoch # 1660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010564417844372136
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:12,032 INFO *** epoch 1660, rolling-avg-loss (window=10)= 0.010610422205978205
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,120 INFO epoch # 1661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010614755723093236
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,206 INFO epoch # 1662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010704577647681747
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,292 INFO epoch # 1663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010663008357265165
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,378 INFO epoch # 1664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010646828836096184
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,464 INFO epoch # 1665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010584745462983847
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,552 INFO epoch # 1666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010519235907122493
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,640 INFO epoch # 1667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010608274496293493
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,727 INFO epoch # 1668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010757128814501422
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,813 INFO epoch # 1669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010679089384419577
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,901 INFO epoch # 1670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010498162638396025
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:12,901 INFO *** epoch 1670, rolling-avg-loss (window=10)= 0.01062758072678532
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:12,992 INFO epoch # 1671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010550358918096338
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:13,084 INFO epoch # 1672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010445761281464782
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:13,174 INFO epoch # 1673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010449320011373078
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:13,260 INFO epoch # 1674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010681638840053763
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:13,348 INFO epoch # 1675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010847799626312086
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:13,435 INFO epoch # 1676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010611937660723925
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:13,522 INFO epoch # 1677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010632933929030385
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:13,612 INFO epoch # 1678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010582557915976004
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:13,700 INFO epoch # 1679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010443798040172883
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:13,787 INFO epoch # 1680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011104024773729699
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:13,787 INFO *** epoch 1680, rolling-avg-loss (window=10)= 0.010635013099693294
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:13,875 INFO epoch # 1681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010506023306931768
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:13,962 INFO epoch # 1682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010471093375235796
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:14,055 INFO epoch # 1683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010475222460393395
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:14,145 INFO epoch # 1684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010541386968855346
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:14,233 INFO epoch # 1685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010570544895849058
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:14,323 INFO epoch # 1686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010583955528480666
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:14,415 INFO epoch # 1687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010470937744581273
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:14,506 INFO epoch # 1688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010781445553792374
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:14,599 INFO epoch # 1689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010424609202891589
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:14,689 INFO epoch # 1690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010502932533355696
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:14,689 INFO *** epoch 1690, rolling-avg-loss (window=10)= 0.010532815157036697
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:14,776 INFO epoch # 1691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010569605996300068
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:14,863 INFO epoch # 1692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010378821846097707
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:14,953 INFO epoch # 1693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010473860507564886
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:15,044 INFO epoch # 1694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010706675105861254
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:15,134 INFO epoch # 1695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010587018076330423
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:15,225 INFO epoch # 1696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010507116413542203
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:15,311 INFO epoch # 1697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010816160655979599
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:15,397 INFO epoch # 1698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01104241581633687
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:15,486 INFO epoch # 1699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0106514898860561
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:15,575 INFO epoch # 1700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010355042972202811
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:15,575 INFO *** epoch 1700, rolling-avg-loss (window=10)= 0.010608820727627193
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:15,664 INFO epoch # 1701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010422875812011106
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:15,752 INFO epoch # 1702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010415885850254979
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:15,841 INFO epoch # 1703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010408289291496788
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:15,927 INFO epoch # 1704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010548447085810559
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,016 INFO epoch # 1705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010388958833313413
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,106 INFO epoch # 1706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01043408727273345
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,192 INFO epoch # 1707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010582340828010014
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,279 INFO epoch # 1708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010471562874902572
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,366 INFO epoch # 1709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010529791018260376
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,452 INFO epoch # 1710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010676151514053345
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:16,452 INFO *** epoch 1710, rolling-avg-loss (window=10)= 0.010487839038084663
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,539 INFO epoch # 1711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010513583484238812
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,627 INFO epoch # 1712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010685108488957797
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,716 INFO epoch # 1713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01050560644694737
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,802 INFO epoch # 1714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010883604302736265
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,887 INFO epoch # 1715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010435120575129986
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:16,973 INFO epoch # 1716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010561688916225519
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:17,060 INFO epoch # 1717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010598406328686647
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:17,147 INFO epoch # 1718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010465001514447587
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:17,233 INFO epoch # 1719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010370321039642607
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:17,322 INFO epoch # 1720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010711314449352877
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:17,322 INFO *** epoch 1720, rolling-avg-loss (window=10)= 0.010572975554636547
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:17,408 INFO epoch # 1721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011159480442958218
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:17,494 INFO epoch # 1722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010485313407012394
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:17,589 INFO epoch # 1723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01063632790797523
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:17,676 INFO epoch # 1724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0103286983844425
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:17,762 INFO epoch # 1725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01067436562318887
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:17,849 INFO epoch # 1726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010967099047931177
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:17,935 INFO epoch # 1727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01054241676548762
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,021 INFO epoch # 1728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010661110854042428
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,111 INFO epoch # 1729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010415226221084595
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,198 INFO epoch # 1730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010630740810717855
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:18,199 INFO *** epoch 1730, rolling-avg-loss (window=10)= 0.01065007794648409
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,284 INFO epoch # 1731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010561625100672245
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,373 INFO epoch # 1732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010456635071230786
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,461 INFO epoch # 1733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010516962209450347
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,549 INFO epoch # 1734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010716253958110298
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,642 INFO epoch # 1735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010610768957329647
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,728 INFO epoch # 1736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01041699553440724
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,815 INFO epoch # 1737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010349884289982063
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,904 INFO epoch # 1738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010480241983064584
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:18,991 INFO epoch # 1739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010483364667743444
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:19,081 INFO epoch # 1740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010465658509305545
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:19,081 INFO *** epoch 1740, rolling-avg-loss (window=10)= 0.01050583902812962
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:19,174 INFO epoch # 1741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010503391502425075
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:19,262 INFO epoch # 1742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010230033232697418
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:19,348 INFO epoch # 1743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010391101680163826
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:19,435 INFO epoch # 1744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010539467140500034
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:19,522 INFO epoch # 1745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010562946474445717
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:19,610 INFO epoch # 1746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010763333112533604
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:19,697 INFO epoch # 1747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010398476624063083
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:19,785 INFO epoch # 1748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011082259638767158
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:19,871 INFO epoch # 1749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010512797547770398
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:19,959 INFO epoch # 1750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01097922238654324
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:19,959 INFO *** epoch 1750, rolling-avg-loss (window=10)= 0.010596302933990954
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:20,047 INFO epoch # 1751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01051570519006678
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:20,138 INFO epoch # 1752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010546361495341574
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:20,225 INFO epoch # 1753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010369235210652862
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:20,311 INFO epoch # 1754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010510828851589134
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:20,397 INFO epoch # 1755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010673645597749522
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:20,485 INFO epoch # 1756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010382910664858563
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:20,627 INFO epoch # 1757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010473839246800967
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:20,718 INFO epoch # 1758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010781289051686014
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:20,805 INFO epoch # 1759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010362667084804603
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:20,893 INFO epoch # 1760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010470251266711524
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:20,894 INFO *** epoch 1760, rolling-avg-loss (window=10)= 0.010508673366026155
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:20,980 INFO epoch # 1761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010383225684719426
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:21,068 INFO epoch # 1762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010704668644549591
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:21,155 INFO epoch # 1763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010537099073241863
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:21,244 INFO epoch # 1764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01029123015967863
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:21,331 INFO epoch # 1765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010318637160318238
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:21,436 INFO epoch # 1766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0106420193732317
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:21,524 INFO epoch # 1767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010340867504211409
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:21,611 INFO epoch # 1768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010274458290742977
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:21,697 INFO epoch # 1769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010238492229421223
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:21,785 INFO epoch # 1770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010506723302283458
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:21,785 INFO *** epoch 1770, rolling-avg-loss (window=10)= 0.010423742142239852
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:21,873 INFO epoch # 1771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010586392613393919
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:21,963 INFO epoch # 1772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010386466693931392
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:22,050 INFO epoch # 1773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010518409797389593
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:22,139 INFO epoch # 1774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010538825326200043
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:22,238 INFO epoch # 1775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010410954297653267
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:22,327 INFO epoch # 1776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01036068576255015
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:22,415 INFO epoch # 1777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010518760540123497
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:22,502 INFO epoch # 1778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010196485596575907
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:22,590 INFO epoch # 1779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010136711597442626
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:22,679 INFO epoch # 1780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01047322903094547
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:22,679 INFO *** epoch 1780, rolling-avg-loss (window=10)= 0.010412692125620586
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:22,768 INFO epoch # 1781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010285829360197698
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:22,858 INFO epoch # 1782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011772533918597868
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:22,944 INFO epoch # 1783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010393631731026939
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,033 INFO epoch # 1784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010123445878603629
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,122 INFO epoch # 1785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010651808730991823
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,209 INFO epoch # 1786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010471919744408557
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,295 INFO epoch # 1787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010185419169387647
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,381 INFO epoch # 1788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010606799327901432
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,466 INFO epoch # 1789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010591083924685206
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,552 INFO epoch # 1790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01087178838705378
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:23,552 INFO *** epoch 1790, rolling-avg-loss (window=10)= 0.01059542601728546
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,641 INFO epoch # 1791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010644665586629085
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,729 INFO epoch # 1792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010061159962788224
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,816 INFO epoch # 1793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010357855333547506
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,903 INFO epoch # 1794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010535636969975063
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:23,992 INFO epoch # 1795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010210429231769272
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:24,083 INFO epoch # 1796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010375174627240216
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:24,173 INFO epoch # 1797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010489111034465688
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:24,260 INFO epoch # 1798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010318455718723791
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:24,347 INFO epoch # 1799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010439290458868657
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:24,437 INFO epoch # 1800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010468061294938836
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:24,437 INFO *** epoch 1800, rolling-avg-loss (window=10)= 0.010389984021894633
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:24,524 INFO epoch # 1801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010424779854448778
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:24,611 INFO epoch # 1802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01051117278901594
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:24,698 INFO epoch # 1803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01030366239803178
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:24,785 INFO epoch # 1804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010378944514585393
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:24,873 INFO epoch # 1805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010725657735019923
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:24,965 INFO epoch # 1806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010438060547624315
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:25,056 INFO epoch # 1807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01022057106851467
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:25,145 INFO epoch # 1808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010334602942956345
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:25,233 INFO epoch # 1809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010303280875086784
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:25,320 INFO epoch # 1810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0103047914603459
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:25,320 INFO *** epoch 1810, rolling-avg-loss (window=10)= 0.01039455241856298
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:25,406 INFO epoch # 1811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010441656636872462
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:25,496 INFO epoch # 1812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010506947578064033
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:25,584 INFO epoch # 1813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010296298403825078
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:25,672 INFO epoch # 1814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01050679948447006
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:25,760 INFO epoch # 1815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010177312465384603
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:25,850 INFO epoch # 1816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01034600199200213
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:25,937 INFO epoch # 1817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010329918017876999
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,024 INFO epoch # 1818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010675327799149922
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,110 INFO epoch # 1819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010465950166274393
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,197 INFO epoch # 1820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010782042119119848
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:26,197 INFO *** epoch 1820, rolling-avg-loss (window=10)= 0.010452825466303953
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,285 INFO epoch # 1821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010552413429000549
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,372 INFO epoch # 1822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010188092796930245
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,458 INFO epoch # 1823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01022033628209361
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,549 INFO epoch # 1824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010193083668127656
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,637 INFO epoch # 1825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010293025190808944
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,724 INFO epoch # 1826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010362070306603398
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,814 INFO epoch # 1827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010279894886272295
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,901 INFO epoch # 1828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010278581308999233
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:26,987 INFO epoch # 1829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010331277948405061
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:27,073 INFO epoch # 1830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010218279409621443
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:27,074 INFO *** epoch 1830, rolling-avg-loss (window=10)= 0.010291705522686243
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:27,160 INFO epoch # 1831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010196294063436133
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:27,246 INFO epoch # 1832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010233084564762455
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:27,333 INFO epoch # 1833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010101442730852535
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:27,421 INFO epoch # 1834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010147546158571328
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:27,510 INFO epoch # 1835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010227109185819115
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:27,604 INFO epoch # 1836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010077957955322096
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:27,691 INFO epoch # 1837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010126159227054034
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:27,779 INFO epoch # 1838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010676374512591532
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:27,868 INFO epoch # 1839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0101630021485367
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:27,955 INFO epoch # 1840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010281474848410913
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:27,955 INFO *** epoch 1840, rolling-avg-loss (window=10)= 0.010223044539535684
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:28,041 INFO epoch # 1841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010251693467476538
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:28,136 INFO epoch # 1842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010179677312927586
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:28,222 INFO epoch # 1843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01048921369947493
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:28,309 INFO epoch # 1844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010563670384830663
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:28,398 INFO epoch # 1845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010299726302868554
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:28,485 INFO epoch # 1846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010125353879162243
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:28,575 INFO epoch # 1847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01042228801442044
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:28,664 INFO epoch # 1848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010224773156057512
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:28,750 INFO epoch # 1849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010324297393006937
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:28,836 INFO epoch # 1850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010194408986717463
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:28,836 INFO *** epoch 1850, rolling-avg-loss (window=10)= 0.010307510259694288
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:28,927 INFO epoch # 1851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010274924963180507
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,013 INFO epoch # 1852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010397318151912518
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,102 INFO epoch # 1853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010317923215084842
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,192 INFO epoch # 1854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010393594138856444
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,279 INFO epoch # 1855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010975190152281097
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,365 INFO epoch # 1856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011028209015993135
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,454 INFO epoch # 1857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010465913013155971
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,540 INFO epoch # 1858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010144911566749215
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,630 INFO epoch # 1859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01024519464533244
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,719 INFO epoch # 1860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010073148419282266
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:29,719 INFO *** epoch 1860, rolling-avg-loss (window=10)= 0.010431632728182842
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,805 INFO epoch # 1861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010187521164438555
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,892 INFO epoch # 1862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010075288186115878
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:29,982 INFO epoch # 1863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010212204618645566
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:30,069 INFO epoch # 1864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010135337956515805
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:30,156 INFO epoch # 1865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010208744555711746
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:30,245 INFO epoch # 1866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010366957341986043
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:30,332 INFO epoch # 1867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010137031360396316
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:30,419 INFO epoch # 1868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010274529091215559
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:30,508 INFO epoch # 1869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010237880969153984
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:30,598 INFO epoch # 1870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010217519077871526
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:30,598 INFO *** epoch 1870, rolling-avg-loss (window=10)= 0.010205301432205096
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:30,684 INFO epoch # 1871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01025568994560412
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:30,771 INFO epoch # 1872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010143889440223574
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:30,858 INFO epoch # 1873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010260141481246267
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:30,944 INFO epoch # 1874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010304851989660944
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,030 INFO epoch # 1875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010198019737643855
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,117 INFO epoch # 1876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010092406254261731
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,204 INFO epoch # 1877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010304299302931343
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,290 INFO epoch # 1878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010173460355560694
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,376 INFO epoch # 1879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010160592530987092
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,462 INFO epoch # 1880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010112280139167394
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:31,462 INFO *** epoch 1880, rolling-avg-loss (window=10)= 0.010200563117728703
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,548 INFO epoch # 1881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010318904130586556
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,636 INFO epoch # 1882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010646849338497435
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,722 INFO epoch # 1883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010220869989799602
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,810 INFO epoch # 1884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010397547377007349
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,896 INFO epoch # 1885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00998614172318152
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:31,982 INFO epoch # 1886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01003080615773797
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:32,068 INFO epoch # 1887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010280180722475052
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:32,155 INFO epoch # 1888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010029999406210013
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:32,241 INFO epoch # 1889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01024446768153991
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:32,329 INFO epoch # 1890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010068591531100018
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:32,329 INFO *** epoch 1890, rolling-avg-loss (window=10)= 0.010222435805813543
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:32,415 INFO epoch # 1891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010456802177109889
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:32,502 INFO epoch # 1892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010521943295108421
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:32,590 INFO epoch # 1893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01017136807287378
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:32,676 INFO epoch # 1894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010293674641954048
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:32,762 INFO epoch # 1895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010177635893757855
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:32,848 INFO epoch # 1896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01009489887260965
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:32,934 INFO epoch # 1897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01002663069936846
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,020 INFO epoch # 1898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010097404150292278
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,107 INFO epoch # 1899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010196787092302527
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,193 INFO epoch # 1900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010277272375034435
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:33,194 INFO *** epoch 1900, rolling-avg-loss (window=10)= 0.010231441727041135
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,280 INFO epoch # 1901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010277028082470809
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,366 INFO epoch # 1902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010140002758375236
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,452 INFO epoch # 1903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00997711777953165
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,537 INFO epoch # 1904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010099085021231856
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,625 INFO epoch # 1905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010169685565467391
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,711 INFO epoch # 1906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010184443283027837
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,797 INFO epoch # 1907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010183053975924849
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,883 INFO epoch # 1908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010050732575889145
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:33,969 INFO epoch # 1909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010147646003003632
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:34,055 INFO epoch # 1910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010118110690798078
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:34,055 INFO *** epoch 1910, rolling-avg-loss (window=10)= 0.010134690573572048
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:34,144 INFO epoch # 1911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009982415667868087
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:34,231 INFO epoch # 1912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010113672193671977
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:34,317 INFO epoch # 1913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010067382548004388
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:34,403 INFO epoch # 1914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010228991867708308
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:34,489 INFO epoch # 1915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01041545302474073
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:34,576 INFO epoch # 1916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010402648124311652
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:34,663 INFO epoch # 1917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010467840611402478
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:34,749 INFO epoch # 1918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00995839041923838
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:34,834 INFO epoch # 1919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010176150314509868
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:34,921 INFO epoch # 1920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010071776368256126
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:34,921 INFO *** epoch 1920, rolling-avg-loss (window=10)= 0.0101884721139712
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,007 INFO epoch # 1921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010512507893145085
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,095 INFO epoch # 1922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010245315610830273
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,181 INFO epoch # 1923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010222196891637786
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,268 INFO epoch # 1924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010031343697171126
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,354 INFO epoch # 1925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010360859547342572
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,440 INFO epoch # 1926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01051064749647464
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,526 INFO epoch # 1927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009965358102428061
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,613 INFO epoch # 1928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010179087519645691
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,702 INFO epoch # 1929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010439064646405833
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,788 INFO epoch # 1930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010348911304026842
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:35,788 INFO *** epoch 1930, rolling-avg-loss (window=10)= 0.01028152927091079
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,876 INFO epoch # 1931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01018662600378905
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:35,965 INFO epoch # 1932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010012725507840515
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:36,052 INFO epoch # 1933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010412806139460632
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:36,139 INFO epoch # 1934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010018642872039761
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:36,227 INFO epoch # 1935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011623673633273159
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:36,314 INFO epoch # 1936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010170693083533218
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:36,399 INFO epoch # 1937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009987514466047287
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:36,487 INFO epoch # 1938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010334033665380308
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:36,574 INFO epoch # 1939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010246967570856214
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:36,660 INFO epoch # 1940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010125880036503077
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:36,660 INFO *** epoch 1940, rolling-avg-loss (window=10)= 0.010311956297872325
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:36,747 INFO epoch # 1941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010228057312113898
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:36,833 INFO epoch # 1942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010474794743848699
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:36,919 INFO epoch # 1943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010132787888869644
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,008 INFO epoch # 1944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010149786122409361
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,094 INFO epoch # 1945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010257677993338023
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,180 INFO epoch # 1946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010015162705842937
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,265 INFO epoch # 1947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009945240091266377
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,351 INFO epoch # 1948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009891883117545928
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,437 INFO epoch # 1949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009971898654475808
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,523 INFO epoch # 1950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010017380071803927
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:37,523 INFO *** epoch 1950, rolling-avg-loss (window=10)= 0.01010846687015146
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,609 INFO epoch # 1951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010075927898287772
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,695 INFO epoch # 1952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010424766802628126
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,781 INFO epoch # 1953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010113665967115335
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,867 INFO epoch # 1954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010298483359760472
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:37,952 INFO epoch # 1955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01015574336051941
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,038 INFO epoch # 1956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01015420369138675
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,124 INFO epoch # 1957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009895531447338207
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,210 INFO epoch # 1958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009965444010283266
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,296 INFO epoch # 1959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010325296036899089
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,382 INFO epoch # 1960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01001330568854298
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:38,382 INFO *** epoch 1960, rolling-avg-loss (window=10)= 0.01014223682627614
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,468 INFO epoch # 1961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010260906429695231
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,554 INFO epoch # 1962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009888557670637965
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,641 INFO epoch # 1963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01001503700390458
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,727 INFO epoch # 1964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010075113124081067
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,813 INFO epoch # 1965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010396224260330201
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,899 INFO epoch # 1966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010345175330127989
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:38,985 INFO epoch # 1967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009978758157896145
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:39,073 INFO epoch # 1968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.011231747402676514
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:39,160 INFO epoch # 1969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01052091789564916
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:39,248 INFO epoch # 1970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01013395955919155
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:39,248 INFO *** epoch 1970, rolling-avg-loss (window=10)= 0.01028463968341904
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:39,334 INFO epoch # 1971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010219269412170563
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:39,420 INFO epoch # 1972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009861864096352032
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:39,506 INFO epoch # 1973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009896303433924914
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:39,597 INFO epoch # 1974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009890410024672747
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:39,683 INFO epoch # 1975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010000304704798119
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:39,768 INFO epoch # 1976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010089035944214888
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:39,857 INFO epoch # 1977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010018722885953528
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:39,943 INFO epoch # 1978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010008024823452746
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,029 INFO epoch # 1979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010355589831514017
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,117 INFO epoch # 1980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01022173708437809
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:40,117 INFO *** epoch 1980, rolling-avg-loss (window=10)= 0.010056126224143166
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,203 INFO epoch # 1981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010094987614346402
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,289 INFO epoch # 1982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010020470905250737
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,380 INFO epoch # 1983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0102383566953774
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,466 INFO epoch # 1984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010127592060182775
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,552 INFO epoch # 1985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009950167692399451
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,645 INFO epoch # 1986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010250633156725338
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,731 INFO epoch # 1987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010079735876726253
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,818 INFO epoch # 1988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009911448122667414
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,907 INFO epoch # 1989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009934154505442296
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:40,994 INFO epoch # 1990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010027965637190002
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:40,994 INFO *** epoch 1990, rolling-avg-loss (window=10)= 0.010063551226630807
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:41,080 INFO epoch # 1991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.0104724324640951
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:41,169 INFO epoch # 1992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009947924502193927
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:41,255 INFO epoch # 1993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010317583994141647
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:41,341 INFO epoch # 1994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01034775259239333
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:41,427 INFO epoch # 1995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010093766756887947
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:41,516 INFO epoch # 1996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.00988023207922067
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:41,605 INFO epoch # 1997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010029125007401619
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:41,692 INFO epoch # 1998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.009829874536288636
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:41,779 INFO epoch # 1999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.01112756741100124
[experiments_sandbox.py:920 -   <module>()] 2023-04-26 12:52:41,865 INFO epoch # 2000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0001-> 0.0001 -loss = 0.010086656627910478
[experiments_sandbox.py:932 -   <module>()] 2023-04-26 12:52:41,865 INFO *** epoch 2000, rolling-avg-loss (window=10)= 0.01021329159715346
[experiments_sandbox.py:940 -   <module>()] 2023-04-26 12:52:41,866 INFO training time in seconds = 162
[experiments_sandbox.py:957 -   <module>()] 2023-04-26 12:52:42,021 INFO train-epochs-loss curve df :
[experiments_sandbox.py:958 -   <module>()] 2023-04-26 12:52:42,029 INFO 
     epochs       loss
0        10  59.658015
1        20   8.998438
2        30   5.649917
3        40   4.382819
4        50   3.465691
5        60   2.668732
6        70   2.063898
7        80   1.912476
8        90   1.583209
9       100   1.315435
10      110   1.263975
11      120   0.981530
12      130   0.898701
13      140   0.869300
14      150   0.673163
15      160   0.503259
16      170   0.371562
17      180   0.335590
18      190   0.251528
19      200   0.222244
20      210   0.166124
21      220   0.163903
22      230   0.108511
23      240   0.088737
24      250   0.068347
25      260   0.057950
26      270   0.052312
27      280   0.044055
28      290   0.044952
29      300   0.038413
30      310   0.033525
31      320   0.032086
32      330   0.028621
33      340   0.025934
34      350   0.024680
35      360   0.023544
36      370   0.022502
37      380   0.020777
38      390   0.021100
39      400   0.020447
40      410   0.019157
41      420   0.019130
42      430   0.018556
43      440   0.017833
44      450   0.017013
45      460   0.016989
46      470   0.016604
47      480   0.016288
48      490   0.015426
49      500   0.015568
50      510   0.015112
51      520   0.014876
52      530   0.014669
53      540   0.014533
54      550   0.014086
55      560   0.014294
56      570   0.013943
57      580   0.014080
58      590   0.013773
59      600   0.013836
60      610   0.013507
61      620   0.013457
62      630   0.013203
63      640   0.013102
64      650   0.013175
65      660   0.013156
66      670   0.013044
67      680   0.013187
68      690   0.013227
69      700   0.012943
70      710   0.012984
71      720   0.012956
72      730   0.012915
73      740   0.012976
74      750   0.012816
75      760   0.012907
76      770   0.012814
77      780   0.012851
78      790   0.012819
79      800   0.012715
80      810   0.012899
81      820   0.012661
82      830   0.012643
83      840   0.012641
84      850   0.012612
85      860   0.012585
86      870   0.012667
87      880   0.012477
88      890   0.012450
89      900   0.012535
90      910   0.012410
91      920   0.012356
92      930   0.012367
93      940   0.012297
94      950   0.012337
95      960   0.012323
96      970   0.012464
97      980   0.012482
98      990   0.012211
99     1000   0.012234
100    1010   0.012087
101    1020   0.012195
102    1030   0.012228
103    1040   0.012316
104    1050   0.012008
105    1060   0.012234
106    1070   0.012104
107    1080   0.012116
108    1090   0.011949
109    1100   0.011953
110    1110   0.012342
111    1120   0.012044
112    1130   0.011809
113    1140   0.011811
114    1150   0.011931
115    1160   0.011849
116    1170   0.011791
117    1180   0.011758
118    1190   0.011751
119    1200   0.011663
120    1210   0.011746
121    1220   0.011653
122    1230   0.011652
123    1240   0.011632
124    1250   0.011561
125    1260   0.011567
126    1270   0.011561
127    1280   0.011633
128    1290   0.011469
129    1300   0.011490
130    1310   0.011467
131    1320   0.011475
132    1330   0.011374
133    1340   0.011339
134    1350   0.011339
135    1360   0.011397
136    1370   0.011599
137    1380   0.011274
138    1390   0.011257
139    1400   0.011201
140    1410   0.011146
141    1420   0.011235
142    1430   0.011204
143    1440   0.011110
144    1450   0.011096
145    1460   0.011058
146    1470   0.011156
147    1480   0.011130
148    1490   0.011024
149    1500   0.010991
150    1510   0.011036
151    1520   0.011084
152    1530   0.010934
153    1540   0.010957
154    1550   0.010952
155    1560   0.010863
156    1570   0.010822
157    1580   0.011064
158    1590   0.010914
159    1600   0.010827
160    1610   0.010812
161    1620   0.010765
162    1630   0.010818
163    1640   0.010670
164    1650   0.010679
165    1660   0.010610
166    1670   0.010628
167    1680   0.010635
168    1690   0.010533
169    1700   0.010609
170    1710   0.010488
171    1720   0.010573
172    1730   0.010650
173    1740   0.010506
174    1750   0.010596
175    1760   0.010509
176    1770   0.010424
177    1780   0.010413
178    1790   0.010595
179    1800   0.010390
180    1810   0.010395
181    1820   0.010453
182    1830   0.010292
183    1840   0.010223
184    1850   0.010308
185    1860   0.010432
186    1870   0.010205
187    1880   0.010201
188    1890   0.010222
189    1900   0.010231
190    1910   0.010135
191    1920   0.010188
192    1930   0.010282
193    1940   0.010312
194    1950   0.010108
195    1960   0.010142
196    1970   0.010285
197    1980   0.010056
198    1990   0.010064
199    2000   0.010213
[experiments_sandbox.py:960 -   <module>()] 2023-04-26 12:52:42,029 INFO Model parameters after training
[experiments_sandbox.py:961 -   <module>()] 2023-04-26 12:52:42,030 INFO Model = TTRBF
[experiments_sandbox.py:963 -   <module>()] 2023-04-26 12:52:42,030 INFO G0 = Parameter containing:
tensor([[ 1.9983, -1.0028, -0.6164],
        [ 0.2717,  2.4715,  1.2567],
        [ 0.0873,  1.0980,  0.9739],
        [-4.5718, -3.5367,  1.6734],
        [-0.2264,  1.4152,  1.8899],
        [-0.4448,  0.3181, -0.4201]], requires_grad=True)
[experiments_sandbox.py:963 -   <module>()] 2023-04-26 12:52:42,031 INFO G1 = Parameter containing:
tensor([[[ 0.4588,  0.0757, -0.6561],
         [ 1.0391,  1.0889, -1.1263],
         [ 0.9587, -1.3925, -1.1453],
         [ 0.4560, -1.3651, -0.6051],
         [ 0.6223, -0.7310, -0.8631],
         [-3.3615,  2.8377,  3.5729]],

        [[ 1.6269, -1.9342, -1.9170],
         [-0.9556, -1.7800,  0.7796],
         [-2.4169,  2.3889,  2.6015],
         [-0.3377, -0.1651,  0.2658],
         [ 0.7092, -0.2922, -1.0136],
         [-0.6312,  0.4863,  0.6280]],

        [[ 1.5090,  0.3518, -1.8872],
         [-0.9464,  0.7662,  0.4732],
         [-2.6710,  2.1455,  2.4210],
         [-0.6140,  0.5922,  0.4964],
         [ 0.2932, -0.1946, -0.5029],
         [ 0.8501, -0.4553, -0.7880]]], requires_grad=True)
[experiments_sandbox.py:963 -   <module>()] 2023-04-26 12:52:42,032 INFO G2 = Parameter containing:
tensor([[[-0.1183,  2.3677,  0.6453],
         [ 2.3357,  3.7672,  1.6392],
         [-1.2093, -0.6195, -1.1412],
         [ 1.6044,  1.4353,  2.0951],
         [-0.9503, -0.2958, -0.7480],
         [ 0.6289, -0.5899, -1.3020]],

        [[-1.3984, -1.4608, -0.1785],
         [ 4.1230,  3.2009,  2.3603],
         [ 1.6547,  0.9353,  0.4129],
         [ 3.4224,  3.2917,  3.3416],
         [ 1.5515,  0.9514,  0.5965],
         [ 1.3747, -1.0589, -1.3292]],

        [[-0.8420, -2.6668, -0.4669],
         [-1.6683, -2.6878, -0.2107],
         [ 1.2417,  0.9616,  1.6103],
         [-0.9285,  0.1189, -1.8167],
         [ 0.8903,  0.9234,  1.3032],
         [-0.5428,  0.5161,  1.6233]]], requires_grad=True)
[experiments_sandbox.py:963 -   <module>()] 2023-04-26 12:52:42,032 INFO G3 = Parameter containing:
tensor([[[-3.1004e-01, -2.9699e+00,  9.3614e-01],
         [-7.1158e-01,  1.2717e+00,  5.5992e-01],
         [-2.0363e+00, -7.4438e-01, -9.3168e-01],
         [ 3.2780e+00, -1.4558e-03, -1.6322e+00],
         [ 2.2222e+00,  8.0319e-01, -9.5172e-02],
         [-1.4135e+00,  2.3781e+00, -1.6662e+00]],

        [[ 7.7454e-01, -2.9500e+00, -1.9857e-01],
         [-1.3720e-01,  3.2725e+00, -2.3066e-01],
         [-6.3448e-02,  1.2112e+00,  3.2366e-01],
         [-9.1991e-01, -9.1618e-02,  2.8676e-01],
         [-6.2853e-01,  4.8602e-01,  4.1731e-01],
         [ 2.4132e-01, -7.9434e-01,  5.8720e-01]],

        [[-1.1524e-01, -2.5391e+00, -1.0122e+00],
         [ 9.0495e-01,  1.2331e+00, -3.9751e-01],
         [ 2.0365e+00, -2.2821e-01,  8.1670e-01],
         [-2.9389e+00,  2.2850e-02,  1.6216e+00],
         [-1.9543e+00, -5.6621e-01,  3.5763e-02],
         [ 1.4676e+00, -2.2550e+00,  1.4476e+00]]], requires_grad=True)
[experiments_sandbox.py:963 -   <module>()] 2023-04-26 12:52:42,033 INFO rbf_module.centres = Parameter containing:
tensor([[ 2.2353,  0.0592,  1.8821],
        [-3.4460,  1.4659, -3.5875],
        [-3.1698, -3.1216, -2.8503],
        [ 3.3596,  0.9561, -3.2972],
        [-2.6132,  0.2210, -2.7271],
        [ 1.7598,  0.2928, -0.2319],
        [-1.6540,  0.2453, -4.6941],
        [ 4.3798, -1.5283,  3.0185],
        [ 3.0517,  0.8950,  2.5618],
        [ 3.8346, -2.4599,  3.4662],
        [-3.3754,  1.2335, -3.9074],
        [-2.6621,  0.7029,  2.6739],
        [ 0.1746, -0.3615,  0.6857],
        [-3.4355, -2.2000,  3.2869],
        [-2.0331,  0.7197, -2.6933],
        [ 2.7762, -2.6348, -3.2427],
        [-2.0918,  0.7043, -1.5280],
        [ 2.4596, -0.4329,  1.6809],
        [-1.6065,  0.4452, -1.6958],
        [ 0.5726, -0.1051, -0.2431],
        [ 2.5503, -4.1725, -1.6236],
        [-3.1940,  3.2552,  1.8690],
        [-3.0847,  3.0079, -2.2951],
        [ 3.0972, -1.7024,  3.9125]], requires_grad=True)
[experiments_sandbox.py:963 -   <module>()] 2023-04-26 12:52:42,034 INFO rbf_module.log_sigmas = Parameter containing:
tensor([4.5782, 1.4239, 1.1093, 1.3902, 1.4685, 1.6714, 2.6566, 2.8434, 1.3771,
        1.0484, 0.0738, 1.3742, 1.3647, 1.1771, 4.8051, 0.9906, 4.9062, 4.8612,
        5.2689, 1.5324, 1.6004, 1.6862, 1.4803, 1.5171], requires_grad=True)
[experiments_sandbox.py:966 -   <module>()] 2023-04-26 12:52:42,034 INFO Out-of sample batch-test
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,035 INFO test-batch  # 0 => test-loss = 0.010578177869319916
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,036 INFO test-batch  # 1 => test-loss = 0.007920301519334316
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,037 INFO test-batch  # 2 => test-loss = 0.024792147800326347
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,038 INFO test-batch  # 3 => test-loss = 0.019843855872750282
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,040 INFO test-batch  # 4 => test-loss = 0.007504025008529425
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,041 INFO test-batch  # 5 => test-loss = 0.004778721835464239
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,042 INFO test-batch  # 6 => test-loss = 0.0036882925778627396
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,043 INFO test-batch  # 7 => test-loss = 0.005967646837234497
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,044 INFO test-batch  # 8 => test-loss = 0.006261528003960848
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,045 INFO test-batch  # 9 => test-loss = 0.3375914394855499
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,046 INFO test-batch  # 10 => test-loss = 0.03207780420780182
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,047 INFO test-batch  # 11 => test-loss = 0.009523943066596985
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,049 INFO test-batch  # 12 => test-loss = 0.011826802976429462
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,050 INFO test-batch  # 13 => test-loss = 0.009767160750925541
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,051 INFO test-batch  # 14 => test-loss = 0.006608293857425451
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,052 INFO test-batch  # 15 => test-loss = 0.011218422092497349
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,053 INFO test-batch  # 16 => test-loss = 0.007429702673107386
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,054 INFO test-batch  # 17 => test-loss = 0.007544256281107664
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,055 INFO test-batch  # 18 => test-loss = 0.013401683419942856
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,057 INFO test-batch  # 19 => test-loss = 0.042365480214357376
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,058 INFO test-batch  # 20 => test-loss = 0.021834999322891235
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,059 INFO test-batch  # 21 => test-loss = 0.008379374630749226
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,060 INFO test-batch  # 22 => test-loss = 0.011452724225819111
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,061 INFO test-batch  # 23 => test-loss = 0.09416124224662781
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,062 INFO test-batch  # 24 => test-loss = 0.04291506111621857
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,063 INFO test-batch  # 25 => test-loss = 0.020284315571188927
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,065 INFO test-batch  # 26 => test-loss = 0.022067174315452576
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,066 INFO test-batch  # 27 => test-loss = 0.01912841759622097
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,067 INFO test-batch  # 28 => test-loss = 0.006735323462635279
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,068 INFO test-batch  # 29 => test-loss = 0.021426638588309288
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,069 INFO test-batch  # 30 => test-loss = 0.05479978397488594
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,070 INFO test-batch  # 31 => test-loss = 0.003874675603583455
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,071 INFO test-batch  # 32 => test-loss = 0.004980859812349081
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,073 INFO test-batch  # 33 => test-loss = 0.010160892270505428
[experiments_sandbox.py:971 -   <module>()] 2023-04-26 12:52:42,074 INFO test-batch  # 34 => test-loss = 0.010148739442229271
