[experiments_sandbox.py:737 -   <module>()] 2023-04-25 20:57:35,727 INFO SEED = 42
[experiments_sandbox.py:818 -   <module>()] 2023-04-25 20:57:35,728 INFO model = 
***
NN-Model 
Sequential(
  (0): Linear(in_features=2, out_features=50, bias=True)
  (1): Identity()
  (2): Tanh()
  (3): Linear(in_features=50, out_features=2, bias=True)
)
numel_learnable = 252
***
[experiments_sandbox.py:819 -   <module>()] 2023-04-25 20:57:35,728 INFO optimizer  = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.1
    maximize: False
    weight_decay: 0
)
[experiments_sandbox.py:827 -   <module>()] 2023-04-25 20:57:35,728 INFO lr_scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f189245b790>
[experiments_sandbox.py:830 -   <module>()] 2023-04-25 20:57:35,728 INFO Normalize-Data-source-X-train = False
[experiments_sandbox.py:831 -   <module>()] 2023-04-25 20:57:35,728 INFO Normalize-Data-source-Y-train = False
[experiments_sandbox.py:832 -   <module>()] 2023-04-25 20:57:35,728 INFO Normalize-Data-source-X-test = False
[experiments_sandbox.py:833 -   <module>()] 2023-04-25 20:57:35,728 INFO Normalize-Data-source-Y-test = False
[experiments_sandbox.py:862 -   <module>()] 2023-04-25 20:57:35,729 INFO train-dataset = ***
VDP Dataset
N=1000
mio = 0.5
x_gen_norm_mean = 0
x_gen_norm_std = 1
normalize_X = False
normalize_Y = False
train-or-test = train
***
[experiments_sandbox.py:863 -   <module>()] 2023-04-25 20:57:35,729 INFO test-dataset = ***
VDP Dataset
N=1000
mio = 0.5
x_gen_norm_mean = 0
x_gen_norm_std = 1
normalize_X = False
normalize_Y = False
train-or-test = test
***
[experiments_sandbox.py:864 -   <module>()] 2023-04-25 20:57:35,729 INFO train-epochs = 1000
[experiments_sandbox.py:868 -   <module>()] 2023-04-25 20:57:35,729 INFO Input batch normalization = False
[experiments_sandbox.py:869 -   <module>()] 2023-04-25 20:57:35,729 INFO Output Normalization = None
[experiments_sandbox.py:870 -   <module>()] 2023-04-25 20:57:35,729 INFO Gradient-clipping max-norm = 10
[experiments_sandbox.py:872 -   <module>()] 2023-04-25 20:57:35,729 INFO epochs_losses_window = 10
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,331 INFO epoch # 0 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 2.86020139278844
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,355 INFO epoch # 1 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.2169766811421141
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,379 INFO epoch # 2 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.07488266460131854
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,403 INFO epoch # 3 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.07327235481352545
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,427 INFO epoch # 4 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.054418143699876964
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,451 INFO epoch # 5 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.06965608446625993
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,475 INFO epoch # 6 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.12355287419632077
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,500 INFO epoch # 7 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.08945309335831553
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,525 INFO epoch # 8 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.04872824938502163
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,549 INFO epoch # 9 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.08570620504906401
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,573 INFO epoch # 10 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.038282585999695584
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:37,573 INFO *** epoch 10, rolling-avg-loss (window=10)= 0.08749289367115125
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,597 INFO epoch # 11 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.04361550768953748
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,622 INFO epoch # 12 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.06337037414778024
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,645 INFO epoch # 13 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.04938517650589347
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,669 INFO epoch # 14 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.05015444051241502
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,693 INFO epoch # 15 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.0428880097460933
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,717 INFO epoch # 16 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.0709409574046731
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,741 INFO epoch # 17 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.053963374011800624
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,765 INFO epoch # 18 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.04694955094601028
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,790 INFO epoch # 19 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.06598524714354426
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,814 INFO epoch # 20 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.03879835770931095
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:37,814 INFO *** epoch 20, rolling-avg-loss (window=10)= 0.05260509958170587
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,838 INFO epoch # 21 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.08 -loss = 0.03830426326021552
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,863 INFO epoch # 22 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.05141386477043852
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,887 INFO epoch # 23 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.039468305389164016
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,911 INFO epoch # 24 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.025603271962609142
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,934 INFO epoch # 25 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.029172095091780648
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,958 INFO epoch # 26 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.023096027987776324
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:37,983 INFO epoch # 27 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.03925255991634913
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,007 INFO epoch # 28 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.07190134620759636
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,032 INFO epoch # 29 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.06821577419759706
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,056 INFO epoch # 30 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.042597156803822145
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:38,056 INFO *** epoch 30, rolling-avg-loss (window=10)= 0.042902466558734885
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,080 INFO epoch # 31 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.032914302399149165
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,104 INFO epoch # 32 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.03693984486744739
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,128 INFO epoch # 33 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.02425491537724156
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,152 INFO epoch # 34 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.03155154389969539
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,177 INFO epoch # 35 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.021751650172518566
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,203 INFO epoch # 36 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.031522667180979624
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,228 INFO epoch # 37 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.07553231867495924
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,254 INFO epoch # 38 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.029121182014932856
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,278 INFO epoch # 39 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.016890791379410075
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,302 INFO epoch # 40 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.024805864901281893
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:38,302 INFO *** epoch 40, rolling-avg-loss (window=10)= 0.032528508086761575
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,327 INFO epoch # 41 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.026312308182241395
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,351 INFO epoch # 42 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.03430142599972896
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,375 INFO epoch # 43 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.04148482158780098
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,399 INFO epoch # 44 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.019335556818987243
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,424 INFO epoch # 45 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.01955802099837456
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,450 INFO epoch # 46 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.02529265923658386
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,475 INFO epoch # 47 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.040800876275170594
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,500 INFO epoch # 48 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.027736379503039643
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,523 INFO epoch # 49 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.027281552655040286
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,547 INFO epoch # 50 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.064 -loss = 0.04280133271822706
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:38,547 INFO *** epoch 50, rolling-avg-loss (window=10)= 0.03049049339751946
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,571 INFO epoch # 51 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.013406913618382532
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,595 INFO epoch # 52 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.02885462829726748
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,620 INFO epoch # 53 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.022301232325844467
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,645 INFO epoch # 54 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.02096026595972944
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,671 INFO epoch # 55 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.043821024300996214
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,696 INFO epoch # 56 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.024413443461526185
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,720 INFO epoch # 57 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.025698229823319707
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,744 INFO epoch # 58 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.01686689404596109
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,768 INFO epoch # 59 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.07846053660614416
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,792 INFO epoch # 60 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.04641406546579674
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:38,792 INFO *** epoch 60, rolling-avg-loss (window=10)= 0.032119723390496804
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,816 INFO epoch # 61 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.026573687442578375
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,841 INFO epoch # 62 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.0512 -loss = 0.022261197358602658
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,867 INFO epoch # 63 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.019281206594314426
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,892 INFO epoch # 64 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.01831658687297022
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,916 INFO epoch # 65 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.01470247173710959
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,940 INFO epoch # 66 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.018521261314162984
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,964 INFO epoch # 67 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.01804015677771531
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:38,987 INFO epoch # 68 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.015439757524291053
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,011 INFO epoch # 69 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0115387160985847
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,035 INFO epoch # 70 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.019382333535759244
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:39,035 INFO *** epoch 70, rolling-avg-loss (window=10)= 0.018405737525608857
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,059 INFO epoch # 71 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.02520833986636717
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,082 INFO epoch # 72 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.01740591759153176
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,106 INFO epoch # 73 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.010877651890041307
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,130 INFO epoch # 74 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.016426914480689447
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,154 INFO epoch # 75 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.01273581171699334
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,178 INFO epoch # 76 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.02072850364493206
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,201 INFO epoch # 77 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.027215217560296878
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,225 INFO epoch # 78 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.021104737475980073
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,248 INFO epoch # 79 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.019503802264807746
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,272 INFO epoch # 80 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.014560128860466648
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:39,272 INFO *** epoch 80, rolling-avg-loss (window=10)= 0.018576702535210644
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,297 INFO epoch # 81 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.015761349815875292
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,321 INFO epoch # 82 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.021097607634146698
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,345 INFO epoch # 83 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.015609224399668165
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,369 INFO epoch # 84 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.04096 -loss = 0.013006138302444015
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,392 INFO epoch # 85 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.014603369199903682
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,416 INFO epoch # 86 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.007568099703348707
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,439 INFO epoch # 87 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.009826856090512592
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,463 INFO epoch # 88 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.015601860963215586
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,487 INFO epoch # 89 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.011409838050894905
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,511 INFO epoch # 90 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.008810883497062605
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:39,511 INFO *** epoch 90, rolling-avg-loss (window=10)= 0.013329522765707225
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,534 INFO epoch # 91 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.006789700706576696
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,558 INFO epoch # 92 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.007167861775087658
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,582 INFO epoch # 93 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.016685666239936836
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,606 INFO epoch # 94 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.01098834072763566
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,630 INFO epoch # 95 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.008450587447441649
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,654 INFO epoch # 96 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.008034305112232687
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,678 INFO epoch # 97 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.012153425836004317
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,702 INFO epoch # 98 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.008657637314172462
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,726 INFO epoch # 99 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.012311560036323499
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,749 INFO epoch # 100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.006456924264057307
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:39,749 INFO *** epoch 100, rolling-avg-loss (window=10)= 0.009769600945946876
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,773 INFO epoch # 101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.009843021151027642
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,798 INFO epoch # 102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.010255857196170837
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,822 INFO epoch # 103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.005115111074701417
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,846 INFO epoch # 104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.007339600917475764
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,869 INFO epoch # 105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.004961844599165488
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,893 INFO epoch # 106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.007239187805680558
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,917 INFO epoch # 107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.010567944220383652
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,941 INFO epoch # 108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.01524658128619194
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,965 INFO epoch # 109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.013768872602668125
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:39,988 INFO epoch # 110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.010254570333927404
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:39,989 INFO *** epoch 110, rolling-avg-loss (window=10)= 0.009459259118739283
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,012 INFO epoch # 111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.00919746628642315
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,036 INFO epoch # 112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.010686285655538086
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,060 INFO epoch # 113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.016872858308488503
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,084 INFO epoch # 114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.011225628804822918
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,108 INFO epoch # 115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.008166641368006822
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,132 INFO epoch # 116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.032768 -loss = 0.01683115381456446
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,156 INFO epoch # 117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.011835434852400795
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,180 INFO epoch # 118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.008335161215654807
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,204 INFO epoch # 119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.005912819267905434
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,227 INFO epoch # 120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.0051866200628865045
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:40,227 INFO *** epoch 120, rolling-avg-loss (window=10)= 0.010425006963669149
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,251 INFO epoch # 121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.004832135175092844
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,275 INFO epoch # 122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.004982009588275105
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,299 INFO epoch # 123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.007863951725084917
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,323 INFO epoch # 124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.005534862113563577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,346 INFO epoch # 125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.007094958025845699
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,370 INFO epoch # 126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.005975144351396011
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,394 INFO epoch # 127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.00824140618351521
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,418 INFO epoch # 128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.00823401591333095
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,442 INFO epoch # 129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.010825457138707861
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,465 INFO epoch # 130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.015339858699007891
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:40,466 INFO *** epoch 130, rolling-avg-loss (window=10)= 0.007892379891382006
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,489 INFO epoch # 131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.004494590653848718
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,513 INFO epoch # 132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.006573599457624368
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,536 INFO epoch # 133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.006871597288409248
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,560 INFO epoch # 134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.006596086979698157
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,584 INFO epoch # 135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.017172619416669477
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,608 INFO epoch # 136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.013355067014344968
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,633 INFO epoch # 137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.01672138791764155
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,657 INFO epoch # 138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.009020242025144398
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,681 INFO epoch # 139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.006933247597771697
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,705 INFO epoch # 140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.010679349630663637
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:40,705 INFO *** epoch 140, rolling-avg-loss (window=10)= 0.009841778798181622
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,729 INFO epoch # 141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.007923362230940256
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,753 INFO epoch # 142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.0262144 -loss = 0.008371950760192703
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,777 INFO epoch # 143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.00507986658885784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,801 INFO epoch # 144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.004068703143275343
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,825 INFO epoch # 145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.004421219746291172
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,849 INFO epoch # 146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.004253666636941489
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,873 INFO epoch # 147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.003036239337234292
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,896 INFO epoch # 148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.003384711813851027
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,921 INFO epoch # 149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.004023628578579519
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,945 INFO epoch # 150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.003266621264629066
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:40,945 INFO *** epoch 150, rolling-avg-loss (window=10)= 0.0047829970100792705
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,968 INFO epoch # 151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0031901658039714675
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:40,992 INFO epoch # 152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0051499394758138806
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,016 INFO epoch # 153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.008849175810610177
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,040 INFO epoch # 154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.006202122940521804
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,064 INFO epoch # 155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0037562562902166974
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,088 INFO epoch # 156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.004108072436793009
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,111 INFO epoch # 157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.003918173886631848
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,135 INFO epoch # 158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.02097152 -loss = 0.0050880038979812525
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,158 INFO epoch # 159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.004630173394616577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,182 INFO epoch # 160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.003350699524162337
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:41,182 INFO *** epoch 160, rolling-avg-loss (window=10)= 0.004824278346131905
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,206 INFO epoch # 161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0030679236297146417
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,230 INFO epoch # 162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0027444654042483307
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,253 INFO epoch # 163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0025968615900637815
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,277 INFO epoch # 164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0027434119019744685
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,302 INFO epoch # 165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.002957237325972528
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,325 INFO epoch # 166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.002945830919998116
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,349 INFO epoch # 167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0028374665562296286
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,373 INFO epoch # 168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.002231621831015218
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,397 INFO epoch # 169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.003896221751347184
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,421 INFO epoch # 170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.008562737471947912
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:41,421 INFO *** epoch 170, rolling-avg-loss (window=10)= 0.003458377838251181
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,445 INFO epoch # 171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0033661269735603128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,468 INFO epoch # 172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.004957534711138578
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,492 INFO epoch # 173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0039017599301587325
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,516 INFO epoch # 174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.006246077085961588
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,540 INFO epoch # 175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.005016530532884644
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,565 INFO epoch # 176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.002605831450637197
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,589 INFO epoch # 177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0033492195943836123
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,613 INFO epoch # 178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.004097575565538136
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,637 INFO epoch # 179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.01677722 -loss = 0.01067500801582355
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,661 INFO epoch # 180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.006171278946567327
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:41,662 INFO *** epoch 180, rolling-avg-loss (window=10)= 0.0050386942806653675
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,686 INFO epoch # 181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0030809659110673238
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,710 INFO epoch # 182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.00276655161724193
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,735 INFO epoch # 183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.002215881175288814
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,759 INFO epoch # 184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0018501805379855796
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,783 INFO epoch # 185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.002015017336816527
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,807 INFO epoch # 186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0019878759230778087
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,832 INFO epoch # 187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0019709483640326653
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,856 INFO epoch # 188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.002856850802345434
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,881 INFO epoch # 189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.005866139610589016
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,905 INFO epoch # 190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.004719987235148437
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:41,906 INFO *** epoch 190, rolling-avg-loss (window=10)= 0.002933039851359354
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,930 INFO epoch # 191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.002452971883030841
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,955 INFO epoch # 192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0024790730567474384
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:41,980 INFO epoch # 193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.002241549958853284
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,004 INFO epoch # 194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.003060391607505153
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,029 INFO epoch # 195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01342177 -loss = 0.002800296075292863
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,053 INFO epoch # 196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.003113921709882561
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,078 INFO epoch # 197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0025979383535741363
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,103 INFO epoch # 198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.003430507722441689
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,127 INFO epoch # 199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.002333221538719954
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,152 INFO epoch # 200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0015427593007188989
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:42,152 INFO *** epoch 200, rolling-avg-loss (window=10)= 0.0026052631206766818
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,177 INFO epoch # 201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0019287443947177962
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,201 INFO epoch # 202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.002124454582371982
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,225 INFO epoch # 203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0024032148794503883
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,249 INFO epoch # 204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0021035006666352274
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,274 INFO epoch # 205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.002012823037148337
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,299 INFO epoch # 206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0024311413453688147
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,324 INFO epoch # 207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0025470562341070035
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,349 INFO epoch # 208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.001963196962606162
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,373 INFO epoch # 209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0017071100082830526
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,398 INFO epoch # 210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.002273174799483968
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:42,398 INFO *** epoch 210, rolling-avg-loss (window=10)= 0.002149441691017273
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,422 INFO epoch # 211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01073742 -loss = 0.002090963167574955
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,447 INFO epoch # 212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.002485860428350861
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,471 INFO epoch # 213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.001711914776933554
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,495 INFO epoch # 214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.00224144210369559
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,519 INFO epoch # 215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0030672115481138462
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,544 INFO epoch # 216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0014335219475469785
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,569 INFO epoch # 217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0015860123239690438
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,593 INFO epoch # 218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.002161038109989022
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,618 INFO epoch # 219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0018390831828583032
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,643 INFO epoch # 220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0014224482574718422
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:42,643 INFO *** epoch 220, rolling-avg-loss (window=10)= 0.0020039495846504
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,667 INFO epoch # 221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0015846040987526067
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,691 INFO epoch # 222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0013214353111834498
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,715 INFO epoch # 223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.001819336152948381
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,740 INFO epoch # 224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0014278264907261473
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,764 INFO epoch # 225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0022798264435550664
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,790 INFO epoch # 226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0018478378406143747
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,816 INFO epoch # 227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0014388245863301563
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,841 INFO epoch # 228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0016847493698151084
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,865 INFO epoch # 229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0015472456398128998
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,890 INFO epoch # 230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0016973895908449776
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:42,890 INFO *** epoch 230, rolling-avg-loss (window=10)= 0.0016649075524583167
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,914 INFO epoch # 231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.002162003838293458
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,939 INFO epoch # 232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.002212362016507541
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,963 INFO epoch # 233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.00858993 -loss = 0.0018771844133880222
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:42,988 INFO epoch # 234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.002292933062562952
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,012 INFO epoch # 235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0015568915077892598
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,037 INFO epoch # 236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0014315432308649179
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,062 INFO epoch # 237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0012596712567756185
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,086 INFO epoch # 238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0012275488152226899
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,110 INFO epoch # 239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0012958672086824663
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,135 INFO epoch # 240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.001076470551197417
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:43,135 INFO *** epoch 240, rolling-avg-loss (window=10)= 0.0016392475901284343
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,159 INFO epoch # 241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0009732713592711661
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,184 INFO epoch # 242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.001011142991046654
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,209 INFO epoch # 243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0016783526834842633
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,233 INFO epoch # 244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0013807683817503857
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,258 INFO epoch # 245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0020194280077703297
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,283 INFO epoch # 246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.002725752750848187
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,309 INFO epoch # 247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0013565455528805614
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,334 INFO epoch # 248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.002049689546765876
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,358 INFO epoch # 249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0016783258588475292
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,383 INFO epoch # 250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0017383250706188846
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:43,383 INFO *** epoch 250, rolling-avg-loss (window=10)= 0.0016611602203283836
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,408 INFO epoch # 251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0016507251539223944
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,432 INFO epoch # 252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00687195 -loss = 0.0025017423104145564
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,457 INFO epoch # 253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0015657902131351875
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,481 INFO epoch # 254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0012931976452819072
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,506 INFO epoch # 255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0013874734104319941
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,530 INFO epoch # 256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0011496577080833958
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,554 INFO epoch # 257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0011059162288802327
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,579 INFO epoch # 258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0009633805411795038
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,603 INFO epoch # 259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0010479125967322034
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,627 INFO epoch # 260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0010409987216917216
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:43,627 INFO *** epoch 260, rolling-avg-loss (window=10)= 0.0013706794529753097
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,651 INFO epoch # 261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0012272411167941755
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,676 INFO epoch # 262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0018170912971982034
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,700 INFO epoch # 263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.00110475136898458
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,724 INFO epoch # 264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0010876334717977443
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,749 INFO epoch # 265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0011477826856207685
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,773 INFO epoch # 266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0010825151439348701
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,799 INFO epoch # 267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0010384463002992561
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,824 INFO epoch # 268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0012244940508026048
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,849 INFO epoch # 269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00549756 -loss = 0.0011246266603848198
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,873 INFO epoch # 270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0009053815974766621
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:43,873 INFO *** epoch 270, rolling-avg-loss (window=10)= 0.0011759963693293684
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,898 INFO epoch # 271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.001049311793394736
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,923 INFO epoch # 272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0009098827222260297
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,947 INFO epoch # 273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0008953581723289972
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,972 INFO epoch # 274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.000928593190110405
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:43,996 INFO epoch # 275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0010075619620693033
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,021 INFO epoch # 276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0014752317811144167
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,045 INFO epoch # 277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.001019374314637389
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,069 INFO epoch # 278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0011830337352876086
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,093 INFO epoch # 279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0008775564215284248
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,118 INFO epoch # 280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0008030740409594728
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:44,118 INFO *** epoch 280, rolling-avg-loss (window=10)= 0.0010148978133656783
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,143 INFO epoch # 281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0007719475788690033
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,167 INFO epoch # 282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0009123638164965087
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,192 INFO epoch # 283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0011350148706696928
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,216 INFO epoch # 284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0009004798994283192
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,240 INFO epoch # 285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.001088207948669151
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,264 INFO epoch # 286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0008174719364433258
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,289 INFO epoch # 287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0011297287655906985
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,314 INFO epoch # 288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0008885662618922652
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,338 INFO epoch # 289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0008895850214685197
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,363 INFO epoch # 290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0008502158179908292
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:44,363 INFO *** epoch 290, rolling-avg-loss (window=10)= 0.0009383581917518313
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,388 INFO epoch # 291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0011672713480948005
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,412 INFO epoch # 292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00439805 -loss = 0.0012782471012542374
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,436 INFO epoch # 293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0010944557816401357
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,460 INFO epoch # 294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0009086066393138026
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,484 INFO epoch # 295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0007866816686146194
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,509 INFO epoch # 296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.000851335534662212
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,534 INFO epoch # 297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.001022193504468305
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,559 INFO epoch # 298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0007841365927561128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,583 INFO epoch # 299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.001044485105012427
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,608 INFO epoch # 300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0007223854481708258
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:44,608 INFO *** epoch 300, rolling-avg-loss (window=10)= 0.0009659798723987478
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,633 INFO epoch # 301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.000652179847293155
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,658 INFO epoch # 302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.000695742398420407
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,682 INFO epoch # 303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0007564468910459254
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,706 INFO epoch # 304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0009838651371865126
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,731 INFO epoch # 305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0008300462454826629
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,756 INFO epoch # 306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0009025191648106556
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,780 INFO epoch # 307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0007028681434348982
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,806 INFO epoch # 308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0007836530176064116
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,831 INFO epoch # 309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0009108620424740366
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,856 INFO epoch # 310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0006357681720601249
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:44,856 INFO *** epoch 310, rolling-avg-loss (window=10)= 0.000785395105981479
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,880 INFO epoch # 311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0006700872022520343
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,905 INFO epoch # 312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0008699046411493327
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,929 INFO epoch # 313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0007110843562259106
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,954 INFO epoch # 314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0010817086531460518
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:44,979 INFO epoch # 315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0009103870843318873
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,004 INFO epoch # 316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0007140359075492597
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,029 INFO epoch # 317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.000857051661114383
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,053 INFO epoch # 318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0008802818601907347
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,078 INFO epoch # 319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0007583781020912284
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,102 INFO epoch # 320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0009847344745139708
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:45,102 INFO *** epoch 320, rolling-avg-loss (window=10)= 0.0008437653942564794
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,127 INFO epoch # 321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00351844 -loss = 0.0008102903047984
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,151 INFO epoch # 322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0009811367708607577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,176 INFO epoch # 323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0010903308111664956
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,201 INFO epoch # 324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.000811724769846478
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,226 INFO epoch # 325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0005822067996632541
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,251 INFO epoch # 326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0006880677246954292
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,276 INFO epoch # 327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0005970793426968157
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,303 INFO epoch # 328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0006299791907622421
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,328 INFO epoch # 329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.001005543101200601
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,353 INFO epoch # 330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.000652705670290743
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:45,353 INFO *** epoch 330, rolling-avg-loss (window=10)= 0.0007849064485981217
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,378 INFO epoch # 331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0006753894958819728
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,404 INFO epoch # 332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0006931810125934135
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,429 INFO epoch # 333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0006861848314656527
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,454 INFO epoch # 334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0006651253106610966
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,479 INFO epoch # 335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0006059288816686603
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,505 INFO epoch # 336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00281475 -loss = 0.0007796193658577977
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,530 INFO epoch # 337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.000573926422475779
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,555 INFO epoch # 338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0005822504031129938
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,581 INFO epoch # 339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0005612648328678915
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,606 INFO epoch # 340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.00055137538447525
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:45,606 INFO *** epoch 340, rolling-avg-loss (window=10)= 0.0006374245941060507
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,632 INFO epoch # 341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0006369185975927394
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,657 INFO epoch # 342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0005395820458033995
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,682 INFO epoch # 343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0006878115564177278
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,707 INFO epoch # 344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0005698065756405413
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,732 INFO epoch # 345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0005194317059249443
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,756 INFO epoch # 346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0005369519908526854
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,782 INFO epoch # 347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0004887382783635985
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,808 INFO epoch # 348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0006653888976870803
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,833 INFO epoch # 349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0006604407008126145
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,858 INFO epoch # 350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0005573696398641914
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:45,858 INFO *** epoch 350, rolling-avg-loss (window=10)= 0.0005862439988959522
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,883 INFO epoch # 351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0005780786832474405
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,908 INFO epoch # 352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0006490547775683808
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,933 INFO epoch # 353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0005675973879988305
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,957 INFO epoch # 354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0006571775684278691
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:45,982 INFO epoch # 355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0006347206081045442
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,008 INFO epoch # 356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0005424216467417864
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,033 INFO epoch # 357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0005934101736784214
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,058 INFO epoch # 358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.0022518 -loss = 0.0005053520508226939
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,083 INFO epoch # 359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0004915119102406607
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,108 INFO epoch # 360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0004596003780079627
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:46,108 INFO *** epoch 360, rolling-avg-loss (window=10)= 0.000567892518483859
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,132 INFO epoch # 361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00045803204579897283
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,157 INFO epoch # 362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.000461867299691221
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,182 INFO epoch # 363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0005541607324630604
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,207 INFO epoch # 364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0006231650190784421
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,232 INFO epoch # 365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00047531829227409617
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,257 INFO epoch # 366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0005145167569935438
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,282 INFO epoch # 367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0005575913723987469
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,307 INFO epoch # 368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0005406385467949804
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,332 INFO epoch # 369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0005201840986046591
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,357 INFO epoch # 370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0005172505088921753
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:46,357 INFO *** epoch 370, rolling-avg-loss (window=10)= 0.0005222724672989898
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,382 INFO epoch # 371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.000685473643216028
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,408 INFO epoch # 372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.00180144 -loss = 0.0004842847279178386
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,433 INFO epoch # 373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.00046784261303400854
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,458 INFO epoch # 374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004629231752915075
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,483 INFO epoch # 375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004633767707673542
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,508 INFO epoch # 376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0006005837590237206
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,533 INFO epoch # 377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0006093164115554828
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,558 INFO epoch # 378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.000506470792061009
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,582 INFO epoch # 379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.000524598029869594
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,608 INFO epoch # 380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004468202669158927
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:46,608 INFO *** epoch 380, rolling-avg-loss (window=10)= 0.0005251690189652436
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,633 INFO epoch # 381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.00046271938208519714
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,659 INFO epoch # 382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0005939573857176583
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,684 INFO epoch # 383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004943911862937966
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,709 INFO epoch # 384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004896348227703129
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,734 INFO epoch # 385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0005081015447103709
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,759 INFO epoch # 386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0006082244258323044
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,783 INFO epoch # 387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004692653783422429
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,810 INFO epoch # 388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.00040807967502587417
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,835 INFO epoch # 389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004245055527007935
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,860 INFO epoch # 390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.000493998852562072
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:46,860 INFO *** epoch 390, rolling-avg-loss (window=10)= 0.0004952878206040623
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,885 INFO epoch # 391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004544682280993584
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,910 INFO epoch # 392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004733757436952146
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,935 INFO epoch # 393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004179835914328578
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,960 INFO epoch # 394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004958957438248035
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:46,984 INFO epoch # 395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0006748722289557918
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,009 INFO epoch # 396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0004891787016276794
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,034 INFO epoch # 397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.00045045734077575617
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,060 INFO epoch # 398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.00048118193763002637
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,085 INFO epoch # 399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00144115 -loss = 0.0004341818876127945
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,110 INFO epoch # 400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0004700988633885572
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:47,110 INFO *** epoch 400, rolling-avg-loss (window=10)= 0.000484169426704284
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,135 INFO epoch # 401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0004075379561072623
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,160 INFO epoch # 402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0003958696640893322
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,185 INFO epoch # 403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00039482580245930876
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,209 INFO epoch # 404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0004083880223788583
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,234 INFO epoch # 405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0004147581817051105
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,260 INFO epoch # 406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00043353806040613563
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,285 INFO epoch # 407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0004912996869279596
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,311 INFO epoch # 408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0004001926799901412
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,336 INFO epoch # 409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00039905898802317097
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,361 INFO epoch # 410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0004496030869631795
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:47,361 INFO *** epoch 410, rolling-avg-loss (window=10)= 0.00041950721290504587
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,386 INFO epoch # 411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00040711993574404914
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,411 INFO epoch # 412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0004979663112862909
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,436 INFO epoch # 413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0004598146997523145
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,462 INFO epoch # 414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00115292 -loss = 0.00041616161843194277
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,487 INFO epoch # 415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0003801170678343624
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,512 INFO epoch # 416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0003854591052459
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,537 INFO epoch # 417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00039350403744720097
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,562 INFO epoch # 418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.000379153140102062
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,587 INFO epoch # 419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00043130661333634635
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,612 INFO epoch # 420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0003984301724813122
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:47,612 INFO *** epoch 420, rolling-avg-loss (window=10)= 0.0004149032701661781
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,637 INFO epoch # 421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00038832133168398286
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,662 INFO epoch # 422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0003616088283706631
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,687 INFO epoch # 423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0003726001214090502
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,712 INFO epoch # 424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0004871850155723223
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,737 INFO epoch # 425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0004281993547010643
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,762 INFO epoch # 426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0004662161138639931
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,786 INFO epoch # 427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0004230855781770515
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,811 INFO epoch # 428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0004094978899047419
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,837 INFO epoch # 429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0003845566343443352
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,862 INFO epoch # 430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00042073762529071246
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:47,862 INFO *** epoch 430, rolling-avg-loss (window=10)= 0.0004142008493317917
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,888 INFO epoch # 431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0003933198447612085
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,912 INFO epoch # 432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0003642204505922564
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,937 INFO epoch # 433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0003483992136352754
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,962 INFO epoch # 434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0003860087654175004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:47,987 INFO epoch # 435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00040306626010533364
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,011 INFO epoch # 436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00036840737880083907
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,036 INFO epoch # 437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0003725080384811008
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,061 INFO epoch # 438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00037863980264774
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,086 INFO epoch # 439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00037576485419776873
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,111 INFO epoch # 440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00040126504313775513
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:48,111 INFO *** epoch 440, rolling-avg-loss (window=10)= 0.0003791599651776778
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,136 INFO epoch # 441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00037642185884578794
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,161 INFO epoch # 442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00040874373462429503
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,186 INFO epoch # 443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0004701656848737912
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,211 INFO epoch # 444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.001 -loss = 0.0004041911702188372
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,236 INFO epoch # 445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00035424413454165915
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,260 INFO epoch # 446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00035507886605046224
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,285 INFO epoch # 447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003513119904710038
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,311 INFO epoch # 448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00036083023269384285
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,336 INFO epoch # 449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003644085804808128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,361 INFO epoch # 450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0005227651731729566
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:48,361 INFO *** epoch 450, rolling-avg-loss (window=10)= 0.0003968161425973449
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,386 INFO epoch # 451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0005657652598074492
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,411 INFO epoch # 452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00034253080065127506
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,436 INFO epoch # 453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003415929654693173
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,460 INFO epoch # 454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003461858079845115
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,485 INFO epoch # 455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00042719855127870687
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,510 INFO epoch # 456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00037241106747387676
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,535 INFO epoch # 457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003701957559769653
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,561 INFO epoch # 458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00037014297322457423
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,586 INFO epoch # 459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00032872940801098594
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,611 INFO epoch # 460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00034411367619213706
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:48,611 INFO *** epoch 460, rolling-avg-loss (window=10)= 0.00038088662660697994
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,636 INFO epoch # 461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00035698878309631255
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,660 INFO epoch # 462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00032662313583387004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,685 INFO epoch # 463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00032933385705291585
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,710 INFO epoch # 464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00036204388538862986
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,735 INFO epoch # 465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00035269827321826597
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,760 INFO epoch # 466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00039540893794765
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,785 INFO epoch # 467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000340888307619025
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,811 INFO epoch # 468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003509208765990479
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,836 INFO epoch # 469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003511427682951762
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,861 INFO epoch # 470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0004568346141695656
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:48,861 INFO *** epoch 470, rolling-avg-loss (window=10)= 0.0003622883439220459
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,885 INFO epoch # 471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003675315786040301
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,910 INFO epoch # 472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003543261273080134
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,935 INFO epoch # 473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003503504624404741
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,960 INFO epoch # 474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00035211539534429903
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:48,985 INFO epoch # 475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00033530699511175044
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,010 INFO epoch # 476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00033257725783641945
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,034 INFO epoch # 477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003349367759710731
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,059 INFO epoch # 478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00032908277398746577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,084 INFO epoch # 479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003567725225366303
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,109 INFO epoch # 480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00032152663311535434
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:49,109 INFO *** epoch 480, rolling-avg-loss (window=10)= 0.00034345265222555097
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,133 INFO epoch # 481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003144945732174165
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,159 INFO epoch # 482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00035598038152784284
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,184 INFO epoch # 483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003955319141368818
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,209 INFO epoch # 484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00035480298060974746
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,234 INFO epoch # 485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003384555336651829
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,260 INFO epoch # 486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000401714213694504
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,284 INFO epoch # 487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00035859043964592274
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,309 INFO epoch # 488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00033784533684411144
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,334 INFO epoch # 489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00032068073073787673
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,359 INFO epoch # 490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00038337979708558123
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:49,359 INFO *** epoch 490, rolling-avg-loss (window=10)= 0.0003561475901165068
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,384 INFO epoch # 491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003555815484332925
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,409 INFO epoch # 492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003303405787846714
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,435 INFO epoch # 493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00032382470294578525
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,459 INFO epoch # 494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003032217750842392
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,484 INFO epoch # 495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003066796448365494
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,509 INFO epoch # 496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00041591852641431615
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,534 INFO epoch # 497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0004193237391518778
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,559 INFO epoch # 498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00036573420970853476
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,584 INFO epoch # 499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00032573500789112586
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,609 INFO epoch # 500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003155389425728572
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:49,609 INFO *** epoch 500, rolling-avg-loss (window=10)= 0.00034618986758232494
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,635 INFO epoch # 501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003283098244537541
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,660 INFO epoch # 502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031159154491433583
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,684 INFO epoch # 503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003259547036122967
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,709 INFO epoch # 504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000355549942014477
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,734 INFO epoch # 505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00036219010712557065
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,759 INFO epoch # 506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00032966863750516495
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,784 INFO epoch # 507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00036736476477017277
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,809 INFO epoch # 508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003372378575932089
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,834 INFO epoch # 509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003610460303207219
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,859 INFO epoch # 510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00034900279183602834
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:49,859 INFO *** epoch 510, rolling-avg-loss (window=10)= 0.00034279162041457313
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,885 INFO epoch # 511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031385070974465634
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,910 INFO epoch # 512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00033256443759910326
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,935 INFO epoch # 513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00034922872964671114
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,960 INFO epoch # 514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003348092373016698
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:49,985 INFO epoch # 515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00033269815980929707
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,010 INFO epoch # 516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003289799007006877
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,035 INFO epoch # 517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00037869254310862743
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,060 INFO epoch # 518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003295846756827814
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,085 INFO epoch # 519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031975524780136766
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,109 INFO epoch # 520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003782141920964932
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:50,110 INFO *** epoch 520, rolling-avg-loss (window=10)= 0.0003398377833491395
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,134 INFO epoch # 521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0004021487723093742
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,159 INFO epoch # 522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0004136450900205091
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,183 INFO epoch # 523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00033602704479562817
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,208 INFO epoch # 524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003410290216834255
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,233 INFO epoch # 525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0004110342806598055
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,257 INFO epoch # 526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0004884378784026921
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,282 INFO epoch # 527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031618713251191366
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,307 INFO epoch # 528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00037769595610370743
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,332 INFO epoch # 529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003868342885198217
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,356 INFO epoch # 530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00039544352534903737
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:50,356 INFO *** epoch 530, rolling-avg-loss (window=10)= 0.0003868482990355915
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,381 INFO epoch # 531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00035070059743702586
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,405 INFO epoch # 532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000330250274600985
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,430 INFO epoch # 533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003547125540990237
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,454 INFO epoch # 534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003447702822541032
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,479 INFO epoch # 535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002879062951706146
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,504 INFO epoch # 536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002963356141663098
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,528 INFO epoch # 537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002884497102968453
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,552 INFO epoch # 538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002956651321710524
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,576 INFO epoch # 539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00029964430677864584
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,601 INFO epoch # 540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00027205495678117586
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:50,601 INFO *** epoch 540, rolling-avg-loss (window=10)= 0.00031204897237557816
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,625 INFO epoch # 541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002918437351127068
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,650 INFO epoch # 542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003341749621768031
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,674 INFO epoch # 543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003900588810665795
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,699 INFO epoch # 544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0005120496857671242
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,724 INFO epoch # 545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003561442022146366
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,748 INFO epoch # 546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00033606961846999184
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,772 INFO epoch # 547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003055437498460378
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,796 INFO epoch # 548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003096138685805272
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,820 INFO epoch # 549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00032139198810909875
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,845 INFO epoch # 550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031568278086524515
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:50,845 INFO *** epoch 550, rolling-avg-loss (window=10)= 0.0003472573472208751
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,869 INFO epoch # 551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000281068690014763
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,894 INFO epoch # 552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000317625311026859
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,918 INFO epoch # 553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028423011303857493
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,943 INFO epoch # 554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003081645008933265
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,967 INFO epoch # 555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00029814076515322085
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:50,991 INFO epoch # 556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00029961690836444177
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,015 INFO epoch # 557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028704588339678594
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,039 INFO epoch # 558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002996227974563226
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,064 INFO epoch # 559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002894697063311469
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,088 INFO epoch # 560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003040932429030363
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:51,088 INFO *** epoch 560, rolling-avg-loss (window=10)= 0.00029690779185784775
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,113 INFO epoch # 561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003141402598885179
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,138 INFO epoch # 562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0004312039543492574
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,163 INFO epoch # 563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0005139044760653633
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,187 INFO epoch # 564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003135467329684616
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,211 INFO epoch # 565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002837965009803156
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,235 INFO epoch # 566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00029452797002704756
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,260 INFO epoch # 567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003760359476245867
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,284 INFO epoch # 568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003448847414802003
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,309 INFO epoch # 569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028154096048638166
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,334 INFO epoch # 570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00029268214257172076
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:51,334 INFO *** epoch 570, rolling-avg-loss (window=10)= 0.0003446263686441853
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,359 INFO epoch # 571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00030951214785090997
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,384 INFO epoch # 572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00029824131729583314
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,409 INFO epoch # 573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003139066814128455
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,433 INFO epoch # 574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031970858026397764
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,457 INFO epoch # 575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003146897602164245
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,481 INFO epoch # 576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00030867121381561446
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,505 INFO epoch # 577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003564463108887139
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,530 INFO epoch # 578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00029262888779157947
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,554 INFO epoch # 579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002734490594775707
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,578 INFO epoch # 580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002940527309647223
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:51,578 INFO *** epoch 580, rolling-avg-loss (window=10)= 0.00030813066899781916
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,603 INFO epoch # 581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031149002870733966
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,627 INFO epoch # 582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003167127899814659
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,651 INFO epoch # 583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00029628415200022573
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,676 INFO epoch # 584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002770052849427884
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,700 INFO epoch # 585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031976562354429916
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,724 INFO epoch # 586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028975763234484475
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,749 INFO epoch # 587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003093453913152189
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,773 INFO epoch # 588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002572477026205888
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,799 INFO epoch # 589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028448300270156324
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,823 INFO epoch # 590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0004147433623984398
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:51,823 INFO *** epoch 590, rolling-avg-loss (window=10)= 0.00030768349705567744
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,848 INFO epoch # 591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0004270515923963103
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,872 INFO epoch # 592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00036906938510128384
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,896 INFO epoch # 593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002663272474592304
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,920 INFO epoch # 594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002742797698829236
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,944 INFO epoch # 595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028582903610185895
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,969 INFO epoch # 596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002970434446751824
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:51,993 INFO epoch # 597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00027480547510094766
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,018 INFO epoch # 598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028664702813330223
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,042 INFO epoch # 599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000299871808692842
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,066 INFO epoch # 600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00027550746085580613
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:52,066 INFO *** epoch 600, rolling-avg-loss (window=10)= 0.0003056432248399688
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,091 INFO epoch # 601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003082404455199139
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,115 INFO epoch # 602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026799940587807214
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,140 INFO epoch # 603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026941346072817396
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,164 INFO epoch # 604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002673865741371628
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,188 INFO epoch # 605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003131497192043753
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,213 INFO epoch # 606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00027337285678186163
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,237 INFO epoch # 607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028678694775408076
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,262 INFO epoch # 608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00027338262316334294
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,286 INFO epoch # 609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002647562353104149
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,311 INFO epoch # 610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002733453840164657
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:52,311 INFO *** epoch 610, rolling-avg-loss (window=10)= 0.00027978336524938643
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,335 INFO epoch # 611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002838966022409295
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,359 INFO epoch # 612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00027786792014694583
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,384 INFO epoch # 613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000296418598281889
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,409 INFO epoch # 614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00027627785698314256
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,433 INFO epoch # 615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002664163749841464
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,458 INFO epoch # 616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002560489024290291
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,482 INFO epoch # 617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00027784225892446557
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,507 INFO epoch # 618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026685978730256465
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,531 INFO epoch # 619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00035162106178177055
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,555 INFO epoch # 620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00047455107096538995
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:52,555 INFO *** epoch 620, rolling-avg-loss (window=10)= 0.00030278004340402733
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,579 INFO epoch # 621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002983342976676795
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,604 INFO epoch # 622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003655130262814055
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,628 INFO epoch # 623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00033128230870715925
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,653 INFO epoch # 624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002716150374908466
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,677 INFO epoch # 625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003056708806070674
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,702 INFO epoch # 626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024247752753581153
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,726 INFO epoch # 627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026317433753320074
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,750 INFO epoch # 628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002515999046863726
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,774 INFO epoch # 629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002673844167020434
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,799 INFO epoch # 630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002533299100377917
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:52,799 INFO *** epoch 630, rolling-avg-loss (window=10)= 0.00028503816472493784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,823 INFO epoch # 631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002568881574234183
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,848 INFO epoch # 632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002680948485931367
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,872 INFO epoch # 633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025248967301649827
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,898 INFO epoch # 634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025318393738871237
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,923 INFO epoch # 635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028260325120754715
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,947 INFO epoch # 636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002421717754259589
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,971 INFO epoch # 637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00027394556650506274
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:52,995 INFO epoch # 638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000312828976120727
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,019 INFO epoch # 639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031218664480547886
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,044 INFO epoch # 640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002658418848113797
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:53,044 INFO *** epoch 640, rolling-avg-loss (window=10)= 0.000272023471529792
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,069 INFO epoch # 641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002648286653084142
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,093 INFO epoch # 642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026649117080523865
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,118 INFO epoch # 643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002830061960139574
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,143 INFO epoch # 644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002723341868886564
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,167 INFO epoch # 645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003123817016330577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,191 INFO epoch # 646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002794244815049751
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,215 INFO epoch # 647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002642861704771349
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,239 INFO epoch # 648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002460269897710532
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,264 INFO epoch # 649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023313918597978045
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,289 INFO epoch # 650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024877368343823036
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:53,289 INFO *** epoch 650, rolling-avg-loss (window=10)= 0.00026706924318204984
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,315 INFO epoch # 651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025484851403234643
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,339 INFO epoch # 652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024073699285054317
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,364 INFO epoch # 653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024407332159626094
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,388 INFO epoch # 654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025796339673433977
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,412 INFO epoch # 655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003159134976158384
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,437 INFO epoch # 656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002667761839347804
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,461 INFO epoch # 657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025611373837364226
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,485 INFO epoch # 658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002969930633298645
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,510 INFO epoch # 659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00030471590707747964
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,534 INFO epoch # 660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002972415336444101
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:53,534 INFO *** epoch 660, rolling-avg-loss (window=10)= 0.0002735376149189506
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,559 INFO epoch # 661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002569528455751424
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,584 INFO epoch # 662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002651976938068401
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,608 INFO epoch # 663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024373608403038816
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,633 INFO epoch # 664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026856037789002585
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,657 INFO epoch # 665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002840368834995388
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,681 INFO epoch # 666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002568059021541558
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,706 INFO epoch # 667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024647948634992645
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,730 INFO epoch # 668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022292776407084602
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,755 INFO epoch # 669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025151943145829136
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,779 INFO epoch # 670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023308488891871093
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:53,780 INFO *** epoch 670, rolling-avg-loss (window=10)= 0.0002529301357753866
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,805 INFO epoch # 671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024538535012652574
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,830 INFO epoch # 672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002663988205995338
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,854 INFO epoch # 673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028751396507686877
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,879 INFO epoch # 674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002585418756098079
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,903 INFO epoch # 675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002935246770903177
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,927 INFO epoch # 676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002363307727364372
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,952 INFO epoch # 677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002605189358746429
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:53,976 INFO epoch # 678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002768891273490226
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,000 INFO epoch # 679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026077388554313075
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,025 INFO epoch # 680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003505288721044053
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:54,025 INFO *** epoch 680, rolling-avg-loss (window=10)= 0.0002736406282110693
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,049 INFO epoch # 681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00044272699528846715
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,073 INFO epoch # 682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002725365968672122
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,097 INFO epoch # 683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023722790660940518
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,121 INFO epoch # 684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00033193625972671725
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,145 INFO epoch # 685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002772343573269609
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,170 INFO epoch # 686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026758407670968154
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,194 INFO epoch # 687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002556541694502812
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,219 INFO epoch # 688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026008713189185073
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,243 INFO epoch # 689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024463752174597175
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,267 INFO epoch # 690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023977631894922524
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:54,267 INFO *** epoch 690, rolling-avg-loss (window=10)= 0.0002829401334565773
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,291 INFO epoch # 691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002688100061050136
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,316 INFO epoch # 692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022074807611716096
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,340 INFO epoch # 693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002513550487037719
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,365 INFO epoch # 694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025074174482142553
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,389 INFO epoch # 695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002460338387209049
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,414 INFO epoch # 696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025288207257290196
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,439 INFO epoch # 697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024130337192218576
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,463 INFO epoch # 698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002521495475775737
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,487 INFO epoch # 699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024360392990274704
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,511 INFO epoch # 700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002319727091162349
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:54,512 INFO *** epoch 700, rolling-avg-loss (window=10)= 0.000245960034555992
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,536 INFO epoch # 701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023885516952759644
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,560 INFO epoch # 702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002622702604639926
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,585 INFO epoch # 703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022325439329051733
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,609 INFO epoch # 704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002492628486834292
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,634 INFO epoch # 705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025386251104464463
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,658 INFO epoch # 706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002551150872704966
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,683 INFO epoch # 707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002552580288011086
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,707 INFO epoch # 708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031528632257504796
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,731 INFO epoch # 709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002901949478655297
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,755 INFO epoch # 710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025908625048032263
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:54,755 INFO *** epoch 710, rolling-avg-loss (window=10)= 0.0002602445820002686
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,779 INFO epoch # 711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002159701431310168
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,805 INFO epoch # 712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023093700565368636
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,830 INFO epoch # 713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023963972034835024
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,854 INFO epoch # 714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026779168888424465
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,878 INFO epoch # 715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025053123431462154
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,903 INFO epoch # 716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025177764018735616
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,927 INFO epoch # 717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023727149118712987
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,951 INFO epoch # 718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022584739713238378
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:54,975 INFO epoch # 719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024000132930268592
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,000 INFO epoch # 720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002826541506237845
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:55,000 INFO *** epoch 720, rolling-avg-loss (window=10)= 0.000244242180076526
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,024 INFO epoch # 721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002607145845558989
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,049 INFO epoch # 722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022823364849955396
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,073 INFO epoch # 723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002261255153825914
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,097 INFO epoch # 724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021889161280341796
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,122 INFO epoch # 725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023840659059715108
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,146 INFO epoch # 726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022356859722094669
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,170 INFO epoch # 727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002644675421379361
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,194 INFO epoch # 728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003358936644417554
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,218 INFO epoch # 729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00042085275231329433
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,242 INFO epoch # 730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026216791366096004
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:55,243 INFO *** epoch 730, rolling-avg-loss (window=10)= 0.00026793224216135057
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,267 INFO epoch # 731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002400712605776789
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,292 INFO epoch # 732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002884836515022471
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,317 INFO epoch # 733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002504734115973406
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,341 INFO epoch # 734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002508390405182581
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,366 INFO epoch # 735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022923157484910917
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,390 INFO epoch # 736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021210325314768852
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,414 INFO epoch # 737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023342623330790957
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,438 INFO epoch # 738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026301876107481803
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,462 INFO epoch # 739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002497701809716091
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,486 INFO epoch # 740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002492696925173732
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:55,487 INFO *** epoch 740, rolling-avg-loss (window=10)= 0.0002466687060064032
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,513 INFO epoch # 741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002468216391662281
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,538 INFO epoch # 742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002446160175395562
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,563 INFO epoch # 743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023080090409166587
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,587 INFO epoch # 744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021297992225299822
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,611 INFO epoch # 745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021830225489338773
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,636 INFO epoch # 746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001953742623754806
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,660 INFO epoch # 747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020146868530446227
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,684 INFO epoch # 748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022407451922390464
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,709 INFO epoch # 749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00038705671795469243
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,733 INFO epoch # 750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002457774021422665
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:55,734 INFO *** epoch 750, rolling-avg-loss (window=10)= 0.00024072723249446426
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,758 INFO epoch # 751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022099785689988494
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,783 INFO epoch # 752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002173915920593572
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,807 INFO epoch # 753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021495761700407456
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,832 INFO epoch # 754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024167925664642098
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,856 INFO epoch # 755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002230968333947203
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,880 INFO epoch # 756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021128722687535628
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,905 INFO epoch # 757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002280744715790206
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,929 INFO epoch # 758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025214802974460326
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,954 INFO epoch # 759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022423626342060743
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:55,979 INFO epoch # 760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002689908002366792
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:55,979 INFO *** epoch 760, rolling-avg-loss (window=10)= 0.00023028599478607247
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,004 INFO epoch # 761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002317253257615448
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,028 INFO epoch # 762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019988378198831924
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,052 INFO epoch # 763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002683388944433318
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,076 INFO epoch # 764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023531769318196893
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,100 INFO epoch # 765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022042951252387866
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,124 INFO epoch # 766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021862053426957573
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,149 INFO epoch # 767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002396444742771564
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,174 INFO epoch # 768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002202159014359495
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,198 INFO epoch # 769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023252944765772554
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,223 INFO epoch # 770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022385269301139488
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:56,223 INFO *** epoch 770, rolling-avg-loss (window=10)= 0.00022905582585508455
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,247 INFO epoch # 771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023817430837880238
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,271 INFO epoch # 772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022755466329726914
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,296 INFO epoch # 773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024122479896959703
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,320 INFO epoch # 774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022095982149039628
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,345 INFO epoch # 775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021322668362699915
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,369 INFO epoch # 776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001985726734119453
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,394 INFO epoch # 777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001986717766158108
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,419 INFO epoch # 778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002341471015370189
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,443 INFO epoch # 779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002222343675839511
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,467 INFO epoch # 780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022092447636623547
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:56,467 INFO *** epoch 780, rolling-avg-loss (window=10)= 0.00022156906712780255
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,491 INFO epoch # 781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026035614496322523
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,516 INFO epoch # 782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024094386026263237
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,540 INFO epoch # 783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031971891326065816
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,565 INFO epoch # 784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026406231700093485
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,589 INFO epoch # 785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002005734112344726
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,614 INFO epoch # 786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020949963868588384
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,639 INFO epoch # 787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019680869604599138
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,663 INFO epoch # 788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002136652044555376
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,688 INFO epoch # 789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023115049168609403
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,712 INFO epoch # 790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002041079533228185
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:56,712 INFO *** epoch 790, rolling-avg-loss (window=10)= 0.00023408866309182486
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,736 INFO epoch # 791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023097900668744842
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,760 INFO epoch # 792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000210769478826478
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,785 INFO epoch # 793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021949670986032288
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,809 INFO epoch # 794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020820028180423833
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,834 INFO epoch # 795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002132337228886172
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,858 INFO epoch # 796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00027615704789241136
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,883 INFO epoch # 797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023600374652232858
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,907 INFO epoch # 798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022337822167628474
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,932 INFO epoch # 799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002096171550647341
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,956 INFO epoch # 800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021636651922563033
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:56,956 INFO *** epoch 800, rolling-avg-loss (window=10)= 0.0002244201890448494
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:56,981 INFO epoch # 801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018793023821217503
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,005 INFO epoch # 802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021707739142584614
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,030 INFO epoch # 803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021514629247576522
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,055 INFO epoch # 804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002008813049769742
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,079 INFO epoch # 805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021824892007771268
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,103 INFO epoch # 806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023040696498810576
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,128 INFO epoch # 807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020118290717618947
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,152 INFO epoch # 808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022297673024240794
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,176 INFO epoch # 809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00031164695110419416
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,200 INFO epoch # 810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002454253068435719
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:57,201 INFO *** epoch 810, rolling-avg-loss (window=10)= 0.00022509230075229425
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,225 INFO epoch # 811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020793260341633868
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,252 INFO epoch # 812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000197694106873314
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,276 INFO epoch # 813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023535205491498346
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,302 INFO epoch # 814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024825525804317294
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,326 INFO epoch # 815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018474768626219884
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,350 INFO epoch # 816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025311864521881944
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,374 INFO epoch # 817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024936475301728933
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,398 INFO epoch # 818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002321242026255277
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,422 INFO epoch # 819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028479551156124217
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,446 INFO epoch # 820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019964896523561038
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:57,446 INFO *** epoch 820, rolling-avg-loss (window=10)= 0.0002293033787168497
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,471 INFO epoch # 821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001936609578478965
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,495 INFO epoch # 822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002043965611164822
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,520 INFO epoch # 823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020854880108345242
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,544 INFO epoch # 824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019074265082963393
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,568 INFO epoch # 825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002071992366836639
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,593 INFO epoch # 826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022262760489866196
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,617 INFO epoch # 827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018538544486546016
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,642 INFO epoch # 828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001834987760958029
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,666 INFO epoch # 829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002696333176572807
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,690 INFO epoch # 830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00028478184071900614
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:57,691 INFO *** epoch 830, rolling-avg-loss (window=10)= 0.0002150475191797341
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,715 INFO epoch # 831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024260809573206643
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,740 INFO epoch # 832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024244202791123826
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,764 INFO epoch # 833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020124722243508586
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,788 INFO epoch # 834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023745539647279656
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,813 INFO epoch # 835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000210633631240853
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,837 INFO epoch # 836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021169361207284965
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,862 INFO epoch # 837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019321836180097307
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,886 INFO epoch # 838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018406187621167192
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,910 INFO epoch # 839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019009533264124912
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,935 INFO epoch # 840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020127208199482993
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:57,935 INFO *** epoch 840, rolling-avg-loss (window=10)= 0.00021147276385136138
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,960 INFO epoch # 841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019382407799639623
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:57,984 INFO epoch # 842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021640872114403464
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,008 INFO epoch # 843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021741296268373844
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,032 INFO epoch # 844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023075019919360784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,057 INFO epoch # 845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000238298962585759
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,081 INFO epoch # 846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001967852092548128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,105 INFO epoch # 847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020792313375750382
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,130 INFO epoch # 848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017053244698672643
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,154 INFO epoch # 849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018166877424619088
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,179 INFO epoch # 850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002267138861498097
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:58,179 INFO *** epoch 850, rolling-avg-loss (window=10)= 0.00020803183739985797
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,203 INFO epoch # 851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002004105976993742
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,228 INFO epoch # 852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023734870887892612
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,252 INFO epoch # 853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002728256104091997
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,276 INFO epoch # 854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002891379849643272
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,301 INFO epoch # 855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026573624654702144
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,325 INFO epoch # 856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001990995676806051
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,350 INFO epoch # 857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021940790995245152
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,375 INFO epoch # 858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018056729652471404
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,399 INFO epoch # 859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002092322213229636
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,423 INFO epoch # 860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018174032607021218
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:58,423 INFO *** epoch 860, rolling-avg-loss (window=10)= 0.00022555064700497952
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,448 INFO epoch # 861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022339231793466752
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,472 INFO epoch # 862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00025165459965137416
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,496 INFO epoch # 863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018332245417695958
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,520 INFO epoch # 864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019857677898471593
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,544 INFO epoch # 865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021296395800618484
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,569 INFO epoch # 866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019947619443883013
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,593 INFO epoch # 867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018645756955493198
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,619 INFO epoch # 868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018882687163568335
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,643 INFO epoch # 869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002154015596715908
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,667 INFO epoch # 870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022724947342567248
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:58,667 INFO *** epoch 870, rolling-avg-loss (window=10)= 0.00020873217774806108
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,692 INFO epoch # 871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001949068106341656
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,716 INFO epoch # 872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019298194808925473
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,740 INFO epoch # 873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001785201931170377
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,764 INFO epoch # 874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020024454966005578
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,789 INFO epoch # 875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002709328332457517
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,814 INFO epoch # 876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00029471827485849644
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,839 INFO epoch # 877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000184318460128452
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,863 INFO epoch # 878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001952619579697057
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,887 INFO epoch # 879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021512929902200995
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,911 INFO epoch # 880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019008645358553622
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:58,912 INFO *** epoch 880, rolling-avg-loss (window=10)= 0.00021171007803104657
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,936 INFO epoch # 881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020294080366056733
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,960 INFO epoch # 882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021084057061671047
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:58,984 INFO epoch # 883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023415443251906254
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,008 INFO epoch # 884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002902421174439951
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,034 INFO epoch # 885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023620274441782385
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,059 INFO epoch # 886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023173390366082458
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,083 INFO epoch # 887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002159759083042445
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,108 INFO epoch # 888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002128357868969033
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,132 INFO epoch # 889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001721220040735716
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,156 INFO epoch # 890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019091580020358379
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:59,156 INFO *** epoch 890, rolling-avg-loss (window=10)= 0.0002197964071797287
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,180 INFO epoch # 891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016732452058931813
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,204 INFO epoch # 892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000182406802650803
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,229 INFO epoch # 893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001896900353131059
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,253 INFO epoch # 894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001740160065537566
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,278 INFO epoch # 895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001936055405735715
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,303 INFO epoch # 896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016770437537161342
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,327 INFO epoch # 897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018448376806645683
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,351 INFO epoch # 898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018965623439726187
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,375 INFO epoch # 899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001837206177697226
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,399 INFO epoch # 900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001761856655093652
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:59,400 INFO *** epoch 900, rolling-avg-loss (window=10)= 0.00018087935667949752
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,424 INFO epoch # 901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018895534003604553
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,448 INFO epoch # 902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017289953188992513
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,473 INFO epoch # 903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017191528547755297
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,497 INFO epoch # 904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018299618147921137
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,521 INFO epoch # 905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020862521250819555
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,546 INFO epoch # 906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001702736121842463
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,570 INFO epoch # 907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019800752443188685
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,594 INFO epoch # 908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022025421026228287
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,618 INFO epoch # 909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00024091039460927277
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,643 INFO epoch # 910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023738549236895778
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:59,643 INFO *** epoch 910, rolling-avg-loss (window=10)= 0.0001992222785247577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,668 INFO epoch # 911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022663156551061547
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,693 INFO epoch # 912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002766111231267132
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,717 INFO epoch # 913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020007784644349158
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,742 INFO epoch # 914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018027440597734312
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,766 INFO epoch # 915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018766733114716772
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,790 INFO epoch # 916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002253465991657322
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,815 INFO epoch # 917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017038627254351013
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,839 INFO epoch # 918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018373141142546956
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,864 INFO epoch # 919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017866502150809538
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,888 INFO epoch # 920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019306058402435156
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:57:59,888 INFO *** epoch 920, rolling-avg-loss (window=10)= 0.000202245216087249
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,912 INFO epoch # 921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017340625385031672
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,937 INFO epoch # 922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001889108002615103
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,961 INFO epoch # 923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001640671252971515
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:57:59,985 INFO epoch # 924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019541832762115519
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,010 INFO epoch # 925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018611648204114317
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,034 INFO epoch # 926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017730499394019716
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,058 INFO epoch # 927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018917898444215098
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,082 INFO epoch # 928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016159311462615733
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,107 INFO epoch # 929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018245851811116154
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,131 INFO epoch # 930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019665453362449625
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:58:00,131 INFO *** epoch 930, rolling-avg-loss (window=10)= 0.00018151091338154403
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,156 INFO epoch # 931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002083375584334135
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,181 INFO epoch # 932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002005196217851335
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,205 INFO epoch # 933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018976134310833004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,229 INFO epoch # 934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019150317541516415
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,253 INFO epoch # 935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022292747246410727
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,278 INFO epoch # 936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016831656148497132
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,303 INFO epoch # 937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001848943479672016
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,328 INFO epoch # 938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021464426561124128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,352 INFO epoch # 939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017639165025684633
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,377 INFO epoch # 940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018474691103165242
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:58:00,377 INFO *** epoch 940, rolling-avg-loss (window=10)= 0.00019420429075580614
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,402 INFO epoch # 941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002583809257430403
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,426 INFO epoch # 942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022607232529026078
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,450 INFO epoch # 943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0003681717687413766
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,475 INFO epoch # 944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021178434940338775
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,499 INFO epoch # 945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019161422142133233
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,523 INFO epoch # 946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021977602204970026
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,548 INFO epoch # 947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020047251211963157
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,573 INFO epoch # 948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001600640489414218
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,598 INFO epoch # 949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017265674898681027
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,622 INFO epoch # 950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017149174709629733
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:58:00,622 INFO *** epoch 950, rolling-avg-loss (window=10)= 0.0002180484669793259
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,647 INFO epoch # 951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015854001799198159
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,671 INFO epoch # 952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018680848154417617
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,695 INFO epoch # 953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001838135202092417
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,720 INFO epoch # 954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001646789410187921
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,744 INFO epoch # 955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017025485203703283
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,769 INFO epoch # 956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000173828114270691
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,795 INFO epoch # 957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019138690595355
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,820 INFO epoch # 958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016772558512911928
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,844 INFO epoch # 959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017514423268494284
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,869 INFO epoch # 960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001703061444118248
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:58:00,869 INFO *** epoch 960, rolling-avg-loss (window=10)= 0.00017424867952513522
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,893 INFO epoch # 961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018027357293703972
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,917 INFO epoch # 962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020831324241044058
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,941 INFO epoch # 963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019246172018938523
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,966 INFO epoch # 964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018051096310500725
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:00,990 INFO epoch # 965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021130706682015443
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,015 INFO epoch # 966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017608679479508282
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,039 INFO epoch # 967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022517915704156621
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,064 INFO epoch # 968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001731471692210107
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,088 INFO epoch # 969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018009930306561728
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,112 INFO epoch # 970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001663297309733025
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:58:01,112 INFO *** epoch 970, rolling-avg-loss (window=10)= 0.00018937087205586067
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,136 INFO epoch # 971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019937595868668723
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,160 INFO epoch # 972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020802931959451598
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,185 INFO epoch # 973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017630388913403294
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,209 INFO epoch # 974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015856685695325723
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,233 INFO epoch # 975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019468472714834206
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,257 INFO epoch # 976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019413969732795522
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,282 INFO epoch # 977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015899682460940312
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,306 INFO epoch # 978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001558427955501429
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,330 INFO epoch # 979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016155452135535597
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,354 INFO epoch # 980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015171725829077332
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:58:01,354 INFO *** epoch 980, rolling-avg-loss (window=10)= 0.0001759211848650466
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,379 INFO epoch # 981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017801044327825366
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,403 INFO epoch # 982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001939736998792796
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,427 INFO epoch # 983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001644423152811214
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,452 INFO epoch # 984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018469655026365217
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,477 INFO epoch # 985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017855790429166518
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,501 INFO epoch # 986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018658870635590574
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,525 INFO epoch # 987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022687155171752238
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,549 INFO epoch # 988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00026427306931964267
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,573 INFO epoch # 989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016835691604910608
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,598 INFO epoch # 990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000155157506355863
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:58:01,598 INFO *** epoch 990, rolling-avg-loss (window=10)= 0.0001900928662792012
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,623 INFO epoch # 991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001576881157916432
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,648 INFO epoch # 992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019934310739699868
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,672 INFO epoch # 993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020775113114268606
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,696 INFO epoch # 994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019976757505446585
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,721 INFO epoch # 995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017477365997820016
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,745 INFO epoch # 996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017677277912753198
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,769 INFO epoch # 997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016661215602198354
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,794 INFO epoch # 998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018652137612207298
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,818 INFO epoch # 999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017651079429015226
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:58:01,843 INFO epoch # 1000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018859476665511465
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:58:01,843 INFO *** epoch 1000, rolling-avg-loss (window=10)= 0.00018343354615808495
[experiments_sandbox.py:920 -   <module>()] 2023-04-25 20:58:01,843 INFO training time in seconds = 26
[experiments_sandbox.py:936 -   <module>()] 2023-04-25 20:58:01,946 INFO train-epochs-loss curve df :
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 20:58:01,950 INFO 
    epochs      loss
0       10  0.087493
1       20  0.052605
2       30  0.042902
3       40  0.032529
4       50  0.030490
5       60  0.032120
6       70  0.018406
7       80  0.018577
8       90  0.013330
9      100  0.009770
10     110  0.009459
11     120  0.010425
12     130  0.007892
13     140  0.009842
14     150  0.004783
15     160  0.004824
16     170  0.003458
17     180  0.005039
18     190  0.002933
19     200  0.002605
20     210  0.002149
21     220  0.002004
22     230  0.001665
23     240  0.001639
24     250  0.001661
25     260  0.001371
26     270  0.001176
27     280  0.001015
28     290  0.000938
29     300  0.000966
30     310  0.000785
31     320  0.000844
32     330  0.000785
33     340  0.000637
34     350  0.000586
35     360  0.000568
36     370  0.000522
37     380  0.000525
38     390  0.000495
39     400  0.000484
40     410  0.000420
41     420  0.000415
42     430  0.000414
43     440  0.000379
44     450  0.000397
45     460  0.000381
46     470  0.000362
47     480  0.000343
48     490  0.000356
49     500  0.000346
50     510  0.000343
51     520  0.000340
52     530  0.000387
53     540  0.000312
54     550  0.000347
55     560  0.000297
56     570  0.000345
57     580  0.000308
58     590  0.000308
59     600  0.000306
60     610  0.000280
61     620  0.000303
62     630  0.000285
63     640  0.000272
64     650  0.000267
65     660  0.000274
66     670  0.000253
67     680  0.000274
68     690  0.000283
69     700  0.000246
70     710  0.000260
71     720  0.000244
72     730  0.000268
73     740  0.000247
74     750  0.000241
75     760  0.000230
76     770  0.000229
77     780  0.000222
78     790  0.000234
79     800  0.000224
80     810  0.000225
81     820  0.000229
82     830  0.000215
83     840  0.000211
84     850  0.000208
85     860  0.000226
86     870  0.000209
87     880  0.000212
88     890  0.000220
89     900  0.000181
90     910  0.000199
91     920  0.000202
92     930  0.000182
93     940  0.000194
94     950  0.000218
95     960  0.000174
96     970  0.000189
97     980  0.000176
98     990  0.000190
99    1000  0.000183
[experiments_sandbox.py:939 -   <module>()] 2023-04-25 20:58:01,950 INFO Model parameters after training
[experiments_sandbox.py:940 -   <module>()] 2023-04-25 20:58:01,950 INFO Model = NNmodel
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 20:58:01,951 INFO net.0.weight = Parameter containing:
tensor([[ 5.3926e-10, -1.3570e-09],
        [ 1.2186e+00,  5.4546e-01],
        [ 1.1439e-01,  5.8802e-01],
        [-1.1507e+00, -6.2742e-01],
        [ 1.7506e-41,  7.5124e-42],
        [-1.1777e+00, -4.5935e-01],
        [-1.1191e+00, -5.2045e-01],
        [ 8.1232e-01, -2.7955e-01],
        [-8.2465e-21,  1.7363e-20],
        [-5.0359e-01, -7.6150e-01],
        [-1.0071e+00,  5.7955e-01],
        [-1.2384e+00, -5.4325e-01],
        [ 2.7727e-01, -1.6890e-01],
        [-2.2621e-01,  2.2660e-01],
        [-3.8822e-04,  2.5321e-03],
        [ 1.3479e+00,  5.9677e-01],
        [ 3.9305e-02,  2.7395e-01],
        [-1.0258e+00,  4.8082e-01],
        [ 1.6692e+00, -9.5138e-01],
        [ 2.9024e-06, -1.4676e-05],
        [ 7.2675e-02,  4.0580e-01],
        [-1.3726e+00,  2.1788e+00],
        [ 1.3312e+00,  6.2872e-01],
        [ 1.0190e+00, -4.7298e-01],
        [-8.7947e-01, -3.9137e-01],
        [ 6.1901e-01, -3.0832e-01],
        [-1.4220e-06,  1.8749e-06],
        [ 7.4738e-01, -4.4126e-01],
        [ 1.3236e+00,  5.8950e-01],
        [-8.6103e-01, -1.3202e+00],
        [ 6.8700e-01, -2.2574e+00],
        [-5.2652e-02, -3.1499e-01],
        [-9.7361e-02,  9.2689e-01],
        [ 4.6753e-02,  3.0351e-01],
        [-7.4761e-02, -3.7996e-01],
        [ 1.8610e+00, -2.5023e+00],
        [ 9.0395e-01,  9.1277e-01],
        [ 8.2857e-01, -3.9675e-01],
        [ 1.3148e-03, -1.3430e-03],
        [-1.3175e+00, -5.8412e-01],
        [ 3.3711e-02,  2.6073e-01],
        [ 1.9007e-01,  6.2810e-01],
        [-6.9197e-01,  3.7228e-01],
        [ 2.4994e-02, -6.6678e-01],
        [-7.4919e-02, -5.3963e-01],
        [-9.1566e-01,  2.1349e+00],
        [ 2.3351e+00,  1.1663e+00],
        [ 6.3778e-02,  3.6162e-01],
        [-5.5083e-09,  6.0887e-09],
        [-3.5437e-02,  6.3038e-01]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 20:58:01,951 INFO net.0.bias = Parameter containing:
tensor([ 2.0151e-09, -1.2214e+00,  1.7180e+00, -4.1632e-01,  1.2041e-41,
        -5.9513e-01,  5.5124e-01, -4.1025e-01, -4.6168e-20,  2.7208e+00,
         1.7281e+00,  1.4090e+00,  2.3776e-01, -4.8662e-01, -2.2947e-03,
         2.1374e+00, -2.7436e-02, -1.6016e+00, -2.0601e+00,  2.1986e-05,
         2.6733e-02,  2.4510e-01, -2.7311e+00,  5.9021e-01,  1.4998e-01,
        -4.6589e-01,  1.9055e-08, -1.1965e-01, -2.5264e-01,  6.7399e-01,
        -2.1613e-01,  2.6917e-02,  5.3280e-01, -3.8747e-02,  2.9310e-01,
        -2.6743e-01, -4.9352e-01,  4.1965e-01, -2.1076e-05, -9.3577e-01,
        -2.6736e-02, -1.1410e+00,  4.3699e-01, -1.3989e+00,  5.2686e-02,
         2.3263e-01,  7.6021e-01, -2.8002e-02,  2.9950e-09, -3.0040e-01],
       requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 20:58:01,952 INFO net.3.weight = Parameter containing:
tensor([[-6.6229e-10, -4.1305e-02,  1.0236e+00,  3.2130e-01, -1.3182e-41,
          2.2112e-01,  5.5310e-02,  1.4601e-01,  4.3574e-20, -3.8405e-01,
         -8.6197e-02, -5.8486e-02, -6.5494e-01,  6.7923e-01,  6.1157e-04,
          2.8608e-02,  3.3594e-01, -8.2883e-02,  6.6259e-02, -8.5095e-06,
          5.1273e-01,  1.2417e-01, -2.4142e-02, -1.1583e-01, -3.7915e-02,
         -7.6993e-01, -1.0119e-06,  5.1064e-01,  1.8001e-01,  3.9293e-02,
         -6.6671e-02, -3.3554e-01, -5.5911e-02,  2.3765e-01, -3.6905e-01,
          5.2272e-02,  5.3280e-02,  1.1215e-01,  1.1683e-04, -2.9548e-01,
          3.7435e-01,  2.6559e-01,  1.1262e-01,  2.0826e-01, -4.3384e-01,
         -1.2265e-01,  8.1334e-02,  4.0040e-01,  1.6327e-08, -6.1178e-01],
        [-7.3106e-10,  9.3640e-01, -1.7123e+00, -6.5443e-01, -9.6353e-42,
         -1.1422e+00, -6.4306e-01, -6.1799e-01, -2.6770e-20,  2.1371e+00,
          1.4597e+00, -8.5296e-01, -1.7066e-01,  1.5093e-01,  1.3234e-03,
          1.3880e+00, -3.5887e-01,  1.7556e+00,  1.5987e-01, -6.5861e-06,
         -9.8461e-01,  4.0790e-01,  1.3355e+00, -9.4729e-01, -4.0638e-01,
         -1.3691e+00, -8.8057e-08, -3.5345e-01,  7.6308e-01, -9.2371e-02,
         -1.6850e-01,  4.7809e-01, -4.5439e-01, -3.4956e-01,  6.9530e-01,
          1.9195e-01, -1.8224e-01, -9.9872e-01, -1.9587e-04, -5.8237e-01,
         -3.4933e-01, -2.0079e+00,  2.6372e+00,  2.0744e+00,  1.8413e+00,
         -3.5776e-01, -1.0054e-01, -7.1520e-01, -4.2638e-11, -5.0495e-01]],
       requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 20:58:01,952 INFO net.3.bias = Parameter containing:
tensor([ 0.0835, -0.2988], requires_grad=True)
[experiments_sandbox.py:945 -   <module>()] 2023-04-25 20:58:01,952 INFO Out-of sample batch-test
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,953 INFO test-batch  # 0 => test-loss = 0.00033674584119580686
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,953 INFO test-batch  # 1 => test-loss = 0.0038381563499569893
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,953 INFO test-batch  # 2 => test-loss = 0.0007310207583941519
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,954 INFO test-batch  # 3 => test-loss = 0.00021879663108848035
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,954 INFO test-batch  # 4 => test-loss = 0.00024256075266748667
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,954 INFO test-batch  # 5 => test-loss = 0.00017869278963189572
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,955 INFO test-batch  # 6 => test-loss = 0.00017883264808915555
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,955 INFO test-batch  # 7 => test-loss = 0.0006797821843065321
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,955 INFO test-batch  # 8 => test-loss = 0.00011777934560086578
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,956 INFO test-batch  # 9 => test-loss = 4.604776040650904e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,956 INFO test-batch  # 10 => test-loss = 0.0009650834836065769
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,956 INFO test-batch  # 11 => test-loss = 4.572076795739122e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,957 INFO test-batch  # 12 => test-loss = 0.00018136850849259645
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,957 INFO test-batch  # 13 => test-loss = 0.0001971842721104622
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,957 INFO test-batch  # 14 => test-loss = 0.002285024616867304
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,958 INFO test-batch  # 15 => test-loss = 0.00010734143143054098
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,958 INFO test-batch  # 16 => test-loss = 7.74877262301743e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,958 INFO test-batch  # 17 => test-loss = 3.853969246847555e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,959 INFO test-batch  # 18 => test-loss = 8.177256677299738e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,959 INFO test-batch  # 19 => test-loss = 4.766484926221892e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,959 INFO test-batch  # 20 => test-loss = 0.0002714850998017937
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,960 INFO test-batch  # 21 => test-loss = 3.7482524930965155e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,960 INFO test-batch  # 22 => test-loss = 0.00017739493341650814
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,960 INFO test-batch  # 23 => test-loss = 0.012201010249555111
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,961 INFO test-batch  # 24 => test-loss = 6.560244946740568e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,961 INFO test-batch  # 25 => test-loss = 0.0006850758800283074
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,961 INFO test-batch  # 26 => test-loss = 5.413191320258193e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,962 INFO test-batch  # 27 => test-loss = 0.00019613897893577814
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,962 INFO test-batch  # 28 => test-loss = 0.0001819778117351234
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,963 INFO test-batch  # 29 => test-loss = 7.898572221165523e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,963 INFO test-batch  # 30 => test-loss = 0.0001855793088907376
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:58:01,963 INFO test-batch  # 31 => test-loss = 4.767523569171317e-05
