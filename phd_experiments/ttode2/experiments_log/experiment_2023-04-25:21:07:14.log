[experiments_sandbox.py:737 -   <module>()] 2023-04-25 21:07:14,112 INFO SEED = 42
[experiments_sandbox.py:818 -   <module>()] 2023-04-25 21:07:14,113 INFO model = ***
TTRBF
order = 4
num_rbf_centers= 16
tt_rank = 3
dim = 4
learnable_numel = 184
***

[experiments_sandbox.py:819 -   <module>()] 2023-04-25 21:07:14,113 INFO optimizer  = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.1
    maximize: False
    weight_decay: 0
)
[experiments_sandbox.py:827 -   <module>()] 2023-04-25 21:07:14,113 INFO lr_scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f761c86f8e0>
[experiments_sandbox.py:830 -   <module>()] 2023-04-25 21:07:14,113 INFO Normalize-Data-source-X-train = False
[experiments_sandbox.py:831 -   <module>()] 2023-04-25 21:07:14,113 INFO Normalize-Data-source-Y-train = False
[experiments_sandbox.py:832 -   <module>()] 2023-04-25 21:07:14,113 INFO Normalize-Data-source-X-test = False
[experiments_sandbox.py:833 -   <module>()] 2023-04-25 21:07:14,113 INFO Normalize-Data-source-Y-test = False
[experiments_sandbox.py:862 -   <module>()] 2023-04-25 21:07:14,114 INFO train-dataset = 
***
Lorenz-System
N = 1000rho = 28
sigma = 10
beta = 2.6666666666666665
normalize_X = Falsenormalize_Y = False****

[experiments_sandbox.py:863 -   <module>()] 2023-04-25 21:07:14,114 INFO test-dataset = 
***
Lorenz-System
N = 1000rho = 28
sigma = 10
beta = 2.6666666666666665
normalize_X = Falsenormalize_Y = False****

[experiments_sandbox.py:864 -   <module>()] 2023-04-25 21:07:14,114 INFO train-epochs = 1000
[experiments_sandbox.py:868 -   <module>()] 2023-04-25 21:07:14,114 INFO Input batch normalization = False
[experiments_sandbox.py:869 -   <module>()] 2023-04-25 21:07:14,114 INFO Output Normalization = None
[experiments_sandbox.py:870 -   <module>()] 2023-04-25 21:07:14,115 INFO Gradient-clipping max-norm = 10
[experiments_sandbox.py:872 -   <module>()] 2023-04-25 21:07:14,115 INFO epochs_losses_window = 10
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,196 INFO epoch # 0 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 259.1226143836975
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,256 INFO epoch # 1 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 105.85714745521545
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,318 INFO epoch # 2 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 59.53254836797714
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,384 INFO epoch # 3 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 54.14257174730301
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,451 INFO epoch # 4 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 49.82777953147888
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,514 INFO epoch # 5 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 46.095364063978195
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,580 INFO epoch # 6 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 24.948741883039474
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,644 INFO epoch # 7 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 19.468419656157494
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,707 INFO epoch # 8 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 35.04764626920223
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,768 INFO epoch # 9 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 23.80987250804901
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,831 INFO epoch # 10 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 13.337525010108948
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:14,831 INFO *** epoch 10, rolling-avg-loss (window=10)= 43.206761649250986
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,892 INFO epoch # 11 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 27.57584737241268
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:14,953 INFO epoch # 12 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 21.048652917146683
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,015 INFO epoch # 13 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 26.690175093710423
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,077 INFO epoch # 14 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 19.679856278002262
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,139 INFO epoch # 15 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 10.163154274225235
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,201 INFO epoch # 16 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 17.650952011346817
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,262 INFO epoch # 17 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 14.533016465604305
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,325 INFO epoch # 18 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 16.11392743885517
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,387 INFO epoch # 19 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 13.201236680150032
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,452 INFO epoch # 20 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 18.103027902543545
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:15,452 INFO *** epoch 20, rolling-avg-loss (window=10)= 18.475984643399716
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,513 INFO epoch # 21 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 10.627026997506618
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,575 INFO epoch # 22 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 18.652000576257706
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,638 INFO epoch # 23 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 20.476542696356773
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,700 INFO epoch # 24 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 12.380578465759754
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,762 INFO epoch # 25 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 15.900392070412636
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,824 INFO epoch # 26 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.085 -loss = 14.227246731519699
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,887 INFO epoch # 27 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 10.269870955497026
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:15,949 INFO epoch # 28 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 10.011561945080757
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,011 INFO epoch # 29 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 11.042128071188927
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,074 INFO epoch # 30 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 13.115118712186813
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:16,074 INFO *** epoch 30, rolling-avg-loss (window=10)= 13.670246722176671
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,138 INFO epoch # 31 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 10.6425122320652
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,199 INFO epoch # 32 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 9.426598086953163
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,261 INFO epoch # 33 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 12.357475705444813
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,324 INFO epoch # 34 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 16.882266871631145
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,385 INFO epoch # 35 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 11.35566820204258
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,447 INFO epoch # 36 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 8.353026144206524
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,509 INFO epoch # 37 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 8.999946758151054
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,571 INFO epoch # 38 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 11.135765381157398
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,645 INFO epoch # 39 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 19.818790420889854
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,708 INFO epoch # 40 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 16.550879932940006
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:16,708 INFO *** epoch 40, rolling-avg-loss (window=10)= 12.552292973548173
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,778 INFO epoch # 41 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 11.2494485527277
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,854 INFO epoch # 42 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 10.49284414947033
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:16,933 INFO epoch # 43 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 9.758047483861446
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,012 INFO epoch # 44 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 12.887244053184986
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,091 INFO epoch # 45 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 17.5827443189919
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,175 INFO epoch # 46 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.085 -loss = 9.585689082741737
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,256 INFO epoch # 47 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.085-> 0.07225 -loss = 14.259551294147968
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,335 INFO epoch # 48 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 16.825606875121593
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,413 INFO epoch # 49 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 9.959240198135376
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,494 INFO epoch # 50 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 12.382332168519497
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:17,495 INFO *** epoch 50, rolling-avg-loss (window=10)= 12.498274817690254
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,574 INFO epoch # 51 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 6.235153026878834
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,654 INFO epoch # 52 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 8.074797753244638
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,734 INFO epoch # 53 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 7.562626328319311
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,815 INFO epoch # 54 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 5.07332231476903
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,893 INFO epoch # 55 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 9.952536970376968
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:17,973 INFO epoch # 56 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 8.486289218068123
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,052 INFO epoch # 57 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 13.299011528491974
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,131 INFO epoch # 58 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 10.235012911260128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,212 INFO epoch # 59 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 13.779003202915192
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,292 INFO epoch # 60 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 11.439071498811245
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:18,292 INFO *** epoch 60, rolling-avg-loss (window=10)= 9.413682475313545
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,371 INFO epoch # 61 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 9.050569355487823
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,454 INFO epoch # 62 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 10.38351634517312
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,533 INFO epoch # 63 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 7.183030303567648
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,614 INFO epoch # 64 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.07225 -loss = 7.654849264770746
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,695 INFO epoch # 65 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.07225-> 0.0614125 -loss = 7.460503568872809
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,774 INFO epoch # 66 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 6.196162536740303
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,852 INFO epoch # 67 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 6.83856800571084
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:18,932 INFO epoch # 68 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 8.828647173941135
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,012 INFO epoch # 69 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 7.7620835937559605
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,090 INFO epoch # 70 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 4.674993745982647
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:19,091 INFO *** epoch 70, rolling-avg-loss (window=10)= 7.603292389400304
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,171 INFO epoch # 71 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 5.505203817039728
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,251 INFO epoch # 72 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 5.064488500356674
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,332 INFO epoch # 73 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 7.321488928049803
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,412 INFO epoch # 74 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 6.235946387052536
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,492 INFO epoch # 75 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 6.019737530499697
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,571 INFO epoch # 76 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 6.0663609355688095
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,652 INFO epoch # 77 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 9.765338867902756
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,732 INFO epoch # 78 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 12.058310933411121
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,811 INFO epoch # 79 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 8.829611748456955
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,891 INFO epoch # 80 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.0614125 -loss = 7.347915008664131
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:19,891 INFO *** epoch 80, rolling-avg-loss (window=10)= 7.421440265700221
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:19,970 INFO epoch # 81 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0614125-> 0.05220063 -loss = 15.805421195924282
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,048 INFO epoch # 82 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 5.19707940146327
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,129 INFO epoch # 83 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 5.017599789425731
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,210 INFO epoch # 84 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.3783721923828125
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,289 INFO epoch # 85 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.19480961561203
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,368 INFO epoch # 86 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.78183900564909
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,447 INFO epoch # 87 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 5.257040427997708
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,529 INFO epoch # 88 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 5.9123503640294075
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,611 INFO epoch # 89 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 5.826525401324034
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,692 INFO epoch # 90 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.173362214118242
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:20,692 INFO *** epoch 90, rolling-avg-loss (window=10)= 6.054439960792661
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,771 INFO epoch # 91 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 5.430767640471458
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,851 INFO epoch # 92 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 5.491407630965114
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:20,930 INFO epoch # 93 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.0707194451242685
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,009 INFO epoch # 94 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 8.468219127506018
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,088 INFO epoch # 95 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.77512963488698
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,170 INFO epoch # 96 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 5.53303874656558
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,248 INFO epoch # 97 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 6.726583085954189
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,327 INFO epoch # 98 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.384356092661619
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,406 INFO epoch # 99 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 9.386363163590431
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,487 INFO epoch # 100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 3.880747703835368
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:21,487 INFO *** epoch 100, rolling-avg-loss (window=10)= 5.814733227156102
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,567 INFO epoch # 101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 10.38907328993082
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,649 INFO epoch # 102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.0713653564453125
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,729 INFO epoch # 103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 5.457295369356871
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,809 INFO epoch # 104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.757979512214661
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,888 INFO epoch # 105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.250780094414949
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:21,967 INFO epoch # 106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 3.7017984334379435
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,047 INFO epoch # 107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 5.4570936523377895
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,128 INFO epoch # 108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.9149866327643394
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,207 INFO epoch # 109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.428686257451773
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,287 INFO epoch # 110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.450502809137106
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:22,288 INFO *** epoch 110, rolling-avg-loss (window=10)= 5.187956140749156
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,368 INFO epoch # 111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 4.24615840613842
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,447 INFO epoch # 112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 8.546240363270044
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,530 INFO epoch # 113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 8.151797067373991
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,609 INFO epoch # 114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 6.9338525496423244
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,690 INFO epoch # 115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 6.499378275126219
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,770 INFO epoch # 116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.05220063 -loss = 7.103119656443596
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,849 INFO epoch # 117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.05220063-> 0.04437053 -loss = 8.014445569366217
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:22,928 INFO epoch # 118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 5.505246710032225
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,009 INFO epoch # 119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 5.740715533494949
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,087 INFO epoch # 120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 4.001232769340277
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:23,088 INFO *** epoch 120, rolling-avg-loss (window=10)= 6.474218690022826
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,169 INFO epoch # 121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 2.9684722144156694
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,248 INFO epoch # 122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 5.729915261268616
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,327 INFO epoch # 123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 4.267749037593603
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,405 INFO epoch # 124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 4.31847520545125
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,486 INFO epoch # 125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 2.677573375403881
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,570 INFO epoch # 126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 2.5739680472761393
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,649 INFO epoch # 127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 6.011727321892977
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,727 INFO epoch # 128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 6.468008019030094
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,806 INFO epoch # 129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 7.009444296360016
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,885 INFO epoch # 130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 6.146940488368273
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:23,885 INFO *** epoch 130, rolling-avg-loss (window=10)= 4.8172273267060515
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:23,963 INFO epoch # 131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 3.7596671413630247
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,044 INFO epoch # 132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 2.9469403214752674
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,123 INFO epoch # 133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 4.701228350400925
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,202 INFO epoch # 134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 4.219927748665214
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,281 INFO epoch # 135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 3.6287752836942673
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,359 INFO epoch # 136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.04437053 -loss = 2.659985052421689
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,438 INFO epoch # 137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04437053-> 0.03771495 -loss = 2.970503242686391
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,516 INFO epoch # 138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 4.828168151900172
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,599 INFO epoch # 139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 5.139385737478733
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,681 INFO epoch # 140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 4.958360528573394
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:24,681 INFO *** epoch 140, rolling-avg-loss (window=10)= 3.981294155865908
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,760 INFO epoch # 141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 5.367116883397102
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,839 INFO epoch # 142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 4.770132549107075
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,918 INFO epoch # 143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 2.0559730799868703
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:24,996 INFO epoch # 144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 3.6632541939616203
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,075 INFO epoch # 145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 2.454848125576973
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,155 INFO epoch # 146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 5.213582869619131
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,234 INFO epoch # 147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 2.5546809174120426
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,312 INFO epoch # 148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 1.957890622317791
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,391 INFO epoch # 149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 2.516074288636446
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,471 INFO epoch # 150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 2.959247089922428
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:25,471 INFO *** epoch 150, rolling-avg-loss (window=10)= 3.351280061993748
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,554 INFO epoch # 151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 2.7540126238018274
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,634 INFO epoch # 152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 2.4862974043935537
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,713 INFO epoch # 153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 5.751990200951695
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,792 INFO epoch # 154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 5.169193979352713
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,870 INFO epoch # 155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 2.5934544410556555
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:25,950 INFO epoch # 156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 5.666840266436338
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,029 INFO epoch # 157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 7.33794479817152
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,107 INFO epoch # 158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03771495 -loss = 5.736899368464947
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,186 INFO epoch # 159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03771495-> 0.03205771 -loss = 3.585889001376927
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,265 INFO epoch # 160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03205771-> 0.03205771 -loss = 2.9788001850247383
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:26,265 INFO *** epoch 160, rolling-avg-loss (window=10)= 4.406132226902992
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,343 INFO epoch # 161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03205771-> 0.03205771 -loss = 2.4985840190201998
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,422 INFO epoch # 162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03205771-> 0.03205771 -loss = 2.032275041565299
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,502 INFO epoch # 163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03205771-> 0.03205771 -loss = 2.60801444016397
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,584 INFO epoch # 164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03205771-> 0.03205771 -loss = 2.022901512682438
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,663 INFO epoch # 165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03205771-> 0.03205771 -loss = 2.4638773277401924
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,741 INFO epoch # 166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03205771-> 0.03205771 -loss = 2.0599822625517845
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,820 INFO epoch # 167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03205771-> 0.03205771 -loss = 2.101111715659499
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,900 INFO epoch # 168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03205771-> 0.03205771 -loss = 3.763804607093334
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:26,978 INFO epoch # 169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03205771-> 0.03205771 -loss = 3.8703982550650835
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,057 INFO epoch # 170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03205771-> 0.02724905 -loss = 3.706013012677431
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:27,057 INFO *** epoch 170, rolling-avg-loss (window=10)= 2.712696219421923
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,135 INFO epoch # 171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 2.7064495608210564
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,214 INFO epoch # 172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 2.908005742356181
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,292 INFO epoch # 173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 2.9346305932849646
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,373 INFO epoch # 174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 2.0139103597030044
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,451 INFO epoch # 175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.7518979385495186
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,530 INFO epoch # 176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.789721474982798
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,612 INFO epoch # 177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.6827701069414616
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,690 INFO epoch # 178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.6020336300134659
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,769 INFO epoch # 179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 2.6126080630347133
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,848 INFO epoch # 180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 2.0200205920264125
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:27,848 INFO *** epoch 180, rolling-avg-loss (window=10)= 2.2022048061713577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:27,927 INFO epoch # 181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 2.2434355802834034
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,007 INFO epoch # 182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.99423111602664
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,086 INFO epoch # 183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 2.2404634561389685
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,165 INFO epoch # 184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.7948322892189026
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,244 INFO epoch # 185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.8493157010525465
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,323 INFO epoch # 186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.4880884727463126
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,401 INFO epoch # 187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.663441302254796
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,480 INFO epoch # 188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.374601629562676
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,559 INFO epoch # 189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.5892668515443802
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,641 INFO epoch # 190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.6194398161023855
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:28,641 INFO *** epoch 190, rolling-avg-loss (window=10)= 1.785711621493101
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,721 INFO epoch # 191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.8363752141594887
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,801 INFO epoch # 192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.5026539005339146
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,880 INFO epoch # 193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 2.2870150031521916
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:28,959 INFO epoch # 194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.5653842501342297
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,037 INFO epoch # 195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 3.128025672864169
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,116 INFO epoch # 196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 2.2382256537675858
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,195 INFO epoch # 197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.7708277581259608
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,274 INFO epoch # 198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02724905 -loss = 1.508984662592411
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,352 INFO epoch # 199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02724905-> 0.02316169 -loss = 1.9815168753266335
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,431 INFO epoch # 200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 2.122203436680138
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:29,431 INFO *** epoch 200, rolling-avg-loss (window=10)= 1.9941212427336723
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,510 INFO epoch # 201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 1.2431667260825634
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,589 INFO epoch # 202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 1.0172784551978111
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,671 INFO epoch # 203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 1.1135093346238136
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,752 INFO epoch # 204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 1.3593069724738598
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,830 INFO epoch # 205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 1.5608861418440938
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,909 INFO epoch # 206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 2.3017717245966196
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:29,987 INFO epoch # 207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 2.2218920877203345
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,066 INFO epoch # 208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 1.4143944308161736
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,145 INFO epoch # 209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 1.3544896356761456
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,223 INFO epoch # 210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 1.1767722442746162
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:30,224 INFO *** epoch 210, rolling-avg-loss (window=10)= 1.4763467753306032
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,302 INFO epoch # 211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 1.2358560925349593
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,381 INFO epoch # 212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.02316169 -loss = 1.1403948981314898
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,459 INFO epoch # 213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02316169-> 0.01968744 -loss = 1.3741248929873109
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,538 INFO epoch # 214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.0326565196737647
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,620 INFO epoch # 215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.105745297856629
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,701 INFO epoch # 216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.1337372092530131
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,780 INFO epoch # 217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 0.9366233246400952
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,858 INFO epoch # 218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.220945737324655
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:30,937 INFO epoch # 219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.120049661025405
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,016 INFO epoch # 220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.0305339619517326
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:31,016 INFO *** epoch 220, rolling-avg-loss (window=10)= 1.1330667595379054
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,095 INFO epoch # 221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 0.9795011496171355
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,174 INFO epoch # 222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 0.9721381328999996
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,252 INFO epoch # 223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.0189297790639102
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,333 INFO epoch # 224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.5353151941671968
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,412 INFO epoch # 225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 0.8758635381236672
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,490 INFO epoch # 226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 0.768460220657289
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,569 INFO epoch # 227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.2653178125619888
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,650 INFO epoch # 228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 0.8860965184867382
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,729 INFO epoch # 229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.1035611787810922
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,808 INFO epoch # 230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 0.9900536388158798
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:31,808 INFO *** epoch 230, rolling-avg-loss (window=10)= 1.0395237163174897
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,887 INFO epoch # 231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.963341509923339
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:31,965 INFO epoch # 232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.0428617927245796
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,044 INFO epoch # 233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 1.323658618144691
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,123 INFO epoch # 234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 0.9073006715625525
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,201 INFO epoch # 235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 0.8711329903453588
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,281 INFO epoch # 236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01968744 -loss = 0.8880455670878291
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,361 INFO epoch # 237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01968744-> 0.01673432 -loss = 0.89971271622926
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,440 INFO epoch # 238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 0.7155541637912393
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,520 INFO epoch # 239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 0.757704867515713
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,600 INFO epoch # 240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 0.7563692815601826
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:32,600 INFO *** epoch 240, rolling-avg-loss (window=10)= 1.0125682178884745
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,680 INFO epoch # 241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 0.5711383763700724
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,760 INFO epoch # 242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 0.9740135134197772
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,840 INFO epoch # 243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 1.1151026086881757
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,919 INFO epoch # 244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 1.2166517227888107
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:32,998 INFO epoch # 245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 1.316461008042097
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,077 INFO epoch # 246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 1.1358958566561341
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,155 INFO epoch # 247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 1.0333320070058107
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,234 INFO epoch # 248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 0.7140317540615797
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,313 INFO epoch # 249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 1.0635252119973302
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,391 INFO epoch # 250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 0.7972316262312233
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:33,391 INFO *** epoch 250, rolling-avg-loss (window=10)= 0.9937383685261011
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,470 INFO epoch # 251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01673432 -loss = 0.8385568596422672
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,549 INFO epoch # 252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01673432-> 0.01422418 -loss = 0.7656493184622377
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,627 INFO epoch # 253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.6855249712243676
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,708 INFO epoch # 254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.6493984158150852
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,788 INFO epoch # 255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.5716679245233536
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,866 INFO epoch # 256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.6889037173241377
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:33,946 INFO epoch # 257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.8452853763010353
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,025 INFO epoch # 258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.7420826014131308
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,103 INFO epoch # 259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.9075038181617856
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,183 INFO epoch # 260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.8906565858051181
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:34,183 INFO *** epoch 260, rolling-avg-loss (window=10)= 0.7585229588672519
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,261 INFO epoch # 261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.5367779736407101
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,340 INFO epoch # 262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.6613785955123603
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,419 INFO epoch # 263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.7422925014980137
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,497 INFO epoch # 264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.7198157585225999
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,576 INFO epoch # 265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.7400086047127843
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,656 INFO epoch # 266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.8165759169496596
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,736 INFO epoch # 267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.6180771365761757
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,816 INFO epoch # 268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.7712908568792045
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,895 INFO epoch # 269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 1.2414558725431561
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:34,974 INFO epoch # 270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.9832356860861182
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:34,974 INFO *** epoch 270, rolling-avg-loss (window=10)= 0.7830908902920782
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,053 INFO epoch # 271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01422418 -loss = 0.6459785867482424
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,134 INFO epoch # 272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01422418-> 0.01209055 -loss = 0.5576469185762107
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,213 INFO epoch # 273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.4874297503847629
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,291 INFO epoch # 274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.4795718137174845
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,370 INFO epoch # 275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.6158784800209105
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,450 INFO epoch # 276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.4581533761229366
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,529 INFO epoch # 277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.5672542438842356
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,608 INFO epoch # 278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.4815852763131261
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,690 INFO epoch # 279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.41610279958695173
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,770 INFO epoch # 280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.43750914419069886
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:35,770 INFO *** epoch 280, rolling-avg-loss (window=10)= 0.5147110389545559
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,850 INFO epoch # 281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.4865461168810725
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:35,929 INFO epoch # 282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.4959212471731007
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,008 INFO epoch # 283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.5507730972021818
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,087 INFO epoch # 284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.4748308116104454
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,167 INFO epoch # 285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.4936619237996638
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,245 INFO epoch # 286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.47877955297008157
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,325 INFO epoch # 287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.9374087094329298
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,404 INFO epoch # 288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.6962107212748379
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,482 INFO epoch # 289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01209055 -loss = 0.48968291096389294
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,561 INFO epoch # 290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01209055-> 0.01027697 -loss = 0.5191361573524773
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:36,562 INFO *** epoch 290, rolling-avg-loss (window=10)= 0.5622951248660684
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,646 INFO epoch # 291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.7323341923765838
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,728 INFO epoch # 292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.704921624623239
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,804 INFO epoch # 293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.6772446613758802
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,881 INFO epoch # 294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.5886563882231712
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:36,958 INFO epoch # 295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.8868567212484777
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,034 INFO epoch # 296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.7520195804536343
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,111 INFO epoch # 297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.6205931366421282
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,188 INFO epoch # 298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.7063942262902856
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,265 INFO epoch # 299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.4809430525638163
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,341 INFO epoch # 300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.4075436561834067
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:37,342 INFO *** epoch 300, rolling-avg-loss (window=10)= 0.6557507239980623
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,418 INFO epoch # 301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.40169990505091846
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,495 INFO epoch # 302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.6236643618904054
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,572 INFO epoch # 303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.6424340445082635
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,648 INFO epoch # 304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.7043746639974415
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,726 INFO epoch # 305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.5984752327203751
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,803 INFO epoch # 306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.6056881691329181
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,880 INFO epoch # 307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.4767948095686734
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:37,957 INFO epoch # 308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.46798033406957984
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,033 INFO epoch # 309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.37633967061992735
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,110 INFO epoch # 310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.5428041112609208
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:38,111 INFO *** epoch 310, rolling-avg-loss (window=10)= 0.5440255302819423
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,188 INFO epoch # 311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.5117845279164612
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,266 INFO epoch # 312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.3873420124873519
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,342 INFO epoch # 313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.5676177963614464
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,419 INFO epoch # 314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.4543155888095498
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,496 INFO epoch # 315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.3747195927426219
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,573 INFO epoch # 316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.38500441145151854
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,652 INFO epoch # 317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.4063016199506819
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,730 INFO epoch # 318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.33425481780432165
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,806 INFO epoch # 319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.43115245341323316
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,886 INFO epoch # 320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.4203013810329139
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:38,886 INFO *** epoch 320, rolling-avg-loss (window=10)= 0.42727942019701004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:38,962 INFO epoch # 321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.48295738408342004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,039 INFO epoch # 322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.6172309753019363
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,117 INFO epoch # 323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.42438060604035854
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,194 INFO epoch # 324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.3631055229343474
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,270 INFO epoch # 325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.40529501670971513
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,347 INFO epoch # 326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.7541982363909483
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,425 INFO epoch # 327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.6723399814218283
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,501 INFO epoch # 328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.01027697 -loss = 0.5872993925586343
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,578 INFO epoch # 329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01027697-> 0.00873542 -loss = 0.4187893010675907
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,656 INFO epoch # 330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.38081626733765006
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:39,656 INFO *** epoch 330, rolling-avg-loss (window=10)= 0.5106412683846429
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,733 INFO epoch # 331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.38510191277600825
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,809 INFO epoch # 332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.3965245212893933
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,886 INFO epoch # 333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.43223245604895055
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:39,963 INFO epoch # 334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.3509614319773391
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,039 INFO epoch # 335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.32368054147809744
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,117 INFO epoch # 336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.41086071357131004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,194 INFO epoch # 337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.5496149249374866
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,271 INFO epoch # 338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.39276249846443534
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,347 INFO epoch # 339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.633909760043025
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,424 INFO epoch # 340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.550314056687057
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:40,424 INFO *** epoch 340, rolling-avg-loss (window=10)= 0.44259628172731025
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,501 INFO epoch # 341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.49584999890066683
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,578 INFO epoch # 342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.473410336766392
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,656 INFO epoch # 343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.38927313312888145
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,733 INFO epoch # 344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.3920337138697505
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,812 INFO epoch # 345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00873542 -loss = 0.38648268627002835
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,888 INFO epoch # 346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00873542-> 0.00742511 -loss = 0.44890291756018996
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:40,965 INFO epoch # 347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.396354102762416
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,042 INFO epoch # 348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.3492921548895538
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,118 INFO epoch # 349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.2692283869255334
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,198 INFO epoch # 350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.29486753046512604
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:41,198 INFO *** epoch 350, rolling-avg-loss (window=10)= 0.38956949615385383
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,275 INFO epoch # 351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.30300522362813354
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,352 INFO epoch # 352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.32457202346995473
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,431 INFO epoch # 353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.357299632858485
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,509 INFO epoch # 354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.38708564499393106
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,585 INFO epoch # 355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.35305470367893577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,662 INFO epoch # 356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.3913922442588955
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,739 INFO epoch # 357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.3140582039486617
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,816 INFO epoch # 358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.2906875545158982
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,893 INFO epoch # 359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00742511 -loss = 0.305259718792513
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:41,969 INFO epoch # 360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00742511-> 0.00631134 -loss = 0.32885211892426014
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:41,969 INFO *** epoch 360, rolling-avg-loss (window=10)= 0.33552670690696684
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,046 INFO epoch # 361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00631134 -loss = 0.22498849069233984
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,123 INFO epoch # 362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00631134 -loss = 0.2597563106101006
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,200 INFO epoch # 363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00631134 -loss = 0.26154551515355706
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,277 INFO epoch # 364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00631134 -loss = 0.25030463794246316
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,354 INFO epoch # 365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00631134 -loss = 0.28066738974303007
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,430 INFO epoch # 366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00631134 -loss = 0.23153391003143042
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,507 INFO epoch # 367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00631134 -loss = 0.28319313912652433
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,584 INFO epoch # 368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00631134 -loss = 0.2625337631907314
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,661 INFO epoch # 369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00631134 -loss = 0.22786623472347856
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,737 INFO epoch # 370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00631134 -loss = 0.39375237352214754
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:42,737 INFO *** epoch 370, rolling-avg-loss (window=10)= 0.2676141764735803
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,814 INFO epoch # 371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00631134 -loss = 0.2940621750894934
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,892 INFO epoch # 372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00631134-> 0.00536464 -loss = 0.22653272771276534
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:42,969 INFO epoch # 373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.21417687670327723
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,045 INFO epoch # 374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.23150344751775265
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,123 INFO epoch # 375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.24825063347816467
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,199 INFO epoch # 376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.22343216044828296
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,276 INFO epoch # 377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.22280617011711001
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,353 INFO epoch # 378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.1960537675768137
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,429 INFO epoch # 379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.21534716081805527
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,506 INFO epoch # 380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.20385587762575597
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:43,506 INFO *** epoch 380, rolling-avg-loss (window=10)= 0.22760209970874712
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,583 INFO epoch # 381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.2020030147396028
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,659 INFO epoch # 382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.21115018893033266
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,736 INFO epoch # 383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.22904404532164335
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,812 INFO epoch # 384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.20708483876660466
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,889 INFO epoch # 385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.28396822581999004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:43,965 INFO epoch # 386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.23962126206606627
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,042 INFO epoch # 387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.28129528602585196
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,118 INFO epoch # 388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00536464 -loss = 0.25095866236370057
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,195 INFO epoch # 389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00536464-> 0.00455994 -loss = 0.32755046826787293
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,271 INFO epoch # 390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.24894628336187452
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:44,272 INFO *** epoch 390, rolling-avg-loss (window=10)= 0.24816222756635398
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,348 INFO epoch # 391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.20485203538555652
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,425 INFO epoch # 392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.22432040941203013
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,501 INFO epoch # 393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.20500912598799914
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,579 INFO epoch # 394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.20661262376233935
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,656 INFO epoch # 395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.2162916949018836
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,732 INFO epoch # 396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.23060672264546156
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,811 INFO epoch # 397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.1881692297756672
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,888 INFO epoch # 398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.24464507098309696
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:44,965 INFO epoch # 399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.24535997956991196
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,042 INFO epoch # 400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.22886306839063764
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:45,042 INFO *** epoch 400, rolling-avg-loss (window=10)= 0.2194729960814584
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,126 INFO epoch # 401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.19061450543813407
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,206 INFO epoch # 402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.2107211606344208
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,286 INFO epoch # 403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.18687933729961514
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,369 INFO epoch # 404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.22400802350603044
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,447 INFO epoch # 405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.19998759077861905
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,523 INFO epoch # 406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.24808282288722694
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,600 INFO epoch # 407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.19018281949684024
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,677 INFO epoch # 408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.2763331749010831
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,754 INFO epoch # 409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.2820458400528878
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,831 INFO epoch # 410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.2392008132301271
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:45,832 INFO *** epoch 410, rolling-avg-loss (window=10)= 0.22480560882249848
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,908 INFO epoch # 411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.2502382677048445
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:45,984 INFO epoch # 412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.1890216046012938
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,061 INFO epoch # 413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00455994 -loss = 0.20658089476637542
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,138 INFO epoch # 414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00455994-> 0.00387595 -loss = 0.1929153414675966
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,215 INFO epoch # 415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.21919561398681253
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,292 INFO epoch # 416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.17228812095709145
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,369 INFO epoch # 417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.22677003557328135
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,446 INFO epoch # 418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.1719253957271576
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,523 INFO epoch # 419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.22311157686635852
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,600 INFO epoch # 420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.16542111267335713
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:46,600 INFO *** epoch 420, rolling-avg-loss (window=10)= 0.20174679643241689
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,678 INFO epoch # 421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.18400187394581735
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,755 INFO epoch # 422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.16310486942529678
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,834 INFO epoch # 423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.17555915331467986
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,911 INFO epoch # 424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.17841247317846864
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:46,989 INFO epoch # 425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.24330151022877544
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,067 INFO epoch # 426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.21323553146794438
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,144 INFO epoch # 427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.1966247910168022
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,223 INFO epoch # 428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.18142970255576074
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,300 INFO epoch # 429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.17919883911963552
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,377 INFO epoch # 430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.19303653889801353
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:47,377 INFO *** epoch 430, rolling-avg-loss (window=10)= 0.19079052831511945
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,457 INFO epoch # 431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.17948160716332495
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,534 INFO epoch # 432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00387595 -loss = 0.2350512861739844
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,611 INFO epoch # 433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00387595-> 0.00329456 -loss = 0.19880467734765261
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,690 INFO epoch # 434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.16753554379101843
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,767 INFO epoch # 435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.19327136420179158
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,844 INFO epoch # 436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.18239846010692418
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,921 INFO epoch # 437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.19117179501336068
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:47,998 INFO epoch # 438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.1834953542565927
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,075 INFO epoch # 439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.16833558003418148
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,152 INFO epoch # 440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.15554895112290978
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:48,152 INFO *** epoch 440, rolling-avg-loss (window=10)= 0.18550946192117407
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,229 INFO epoch # 441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.17958826571702957
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,306 INFO epoch # 442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.18829102662857622
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,384 INFO epoch # 443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.19473342306446284
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,462 INFO epoch # 444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.19228771317284554
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,539 INFO epoch # 445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.16431639133952558
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,619 INFO epoch # 446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.18670746474526823
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,695 INFO epoch # 447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.18911925016436726
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,772 INFO epoch # 448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.1760231473017484
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,850 INFO epoch # 449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.16095878125634044
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:48,926 INFO epoch # 450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.1465052824933082
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:48,926 INFO *** epoch 450, rolling-avg-loss (window=10)= 0.17785307458834723
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,007 INFO epoch # 451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.16791908629238605
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,083 INFO epoch # 452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.1949893923010677
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,161 INFO epoch # 453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.17140219651628286
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,237 INFO epoch # 454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.21263615367934108
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,314 INFO epoch # 455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.1591496830806136
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,391 INFO epoch # 456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.16067111294250935
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,467 INFO epoch # 457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.14684963808394969
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,545 INFO epoch # 458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.14513966348022223
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,622 INFO epoch # 459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.155166000360623
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,699 INFO epoch # 460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.17700177995720878
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:49,699 INFO *** epoch 460, rolling-avg-loss (window=10)= 0.16909247066942043
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,776 INFO epoch # 461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.13978189206682146
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,853 INFO epoch # 462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.1693253671983257
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:49,930 INFO epoch # 463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.16803197003901005
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,007 INFO epoch # 464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.17320736532565206
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,083 INFO epoch # 465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.16012133157346398
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,160 INFO epoch # 466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.1441092457389459
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,237 INFO epoch # 467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.1737847023177892
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,314 INFO epoch # 468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.1756533884909004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,390 INFO epoch # 469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.16635024221614003
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,467 INFO epoch # 470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.14856135053560138
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:50,467 INFO *** epoch 470, rolling-avg-loss (window=10)= 0.161892685550265
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,544 INFO epoch # 471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00329456 -loss = 0.182692636619322
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,621 INFO epoch # 472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00329456-> 0.00280038 -loss = 0.16074628103524446
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,697 INFO epoch # 473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.1460916819050908
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,774 INFO epoch # 474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.15187497332226485
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,850 INFO epoch # 475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.15530362608842552
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:50,928 INFO epoch # 476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.14475252071861178
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,004 INFO epoch # 477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.15072665468323976
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,082 INFO epoch # 478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.14821187208872288
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,159 INFO epoch # 479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.15266649168916047
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,236 INFO epoch # 480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.1328042697859928
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:51,236 INFO *** epoch 480, rolling-avg-loss (window=10)= 0.15258710079360754
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,313 INFO epoch # 481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.1428754695225507
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,390 INFO epoch # 482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.14541883138008416
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,467 INFO epoch # 483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.13327834580559283
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,543 INFO epoch # 484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.13587393751367927
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,622 INFO epoch # 485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.15545009961351752
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,698 INFO epoch # 486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.12884861114434898
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,775 INFO epoch # 487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.1398330075899139
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,852 INFO epoch # 488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.13017398386728019
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:51,930 INFO epoch # 489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.14261429908219725
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,006 INFO epoch # 490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.14647774305194616
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:52,006 INFO *** epoch 490, rolling-avg-loss (window=10)= 0.14008443285711109
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,083 INFO epoch # 491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.16051193745806813
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,161 INFO epoch # 492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.157068581902422
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,238 INFO epoch # 493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.1395257378462702
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,315 INFO epoch # 494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.13971176056656986
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,392 INFO epoch # 495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.1398236365057528
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,468 INFO epoch # 496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00280038 -loss = 0.14889566565398127
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,546 INFO epoch # 497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00280038-> 0.00238032 -loss = 0.14634650945663452
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,625 INFO epoch # 498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.13263108604587615
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,702 INFO epoch # 499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.13623908534646034
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,779 INFO epoch # 500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.13919363194145262
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:52,779 INFO *** epoch 500, rolling-avg-loss (window=10)= 0.14399476327234878
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,855 INFO epoch # 501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.15775006054900587
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:52,932 INFO epoch # 502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.13956475001759827
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,009 INFO epoch # 503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.14701975625939667
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,086 INFO epoch # 504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.14051852247212082
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,164 INFO epoch # 505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.1474965176312253
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,241 INFO epoch # 506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.132067687693052
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,318 INFO epoch # 507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.12892820581328124
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,395 INFO epoch # 508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.12715342792216688
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,472 INFO epoch # 509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.12748308409936726
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,549 INFO epoch # 510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.14738033479079604
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:53,549 INFO *** epoch 510, rolling-avg-loss (window=10)= 0.13953623472480103
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,627 INFO epoch # 511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.1678275327431038
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,705 INFO epoch # 512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.14868176134768873
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,782 INFO epoch # 513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.13458348874701187
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,859 INFO epoch # 514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.15078071993775666
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:53,938 INFO epoch # 515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.13516436284407973
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,015 INFO epoch # 516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.1314056843984872
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,092 INFO epoch # 517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.1435968135483563
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,174 INFO epoch # 518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.12065280857495964
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,251 INFO epoch # 519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.1478342764894478
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,328 INFO epoch # 520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.13045930757652968
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:54,328 INFO *** epoch 520, rolling-avg-loss (window=10)= 0.14109867562074213
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,407 INFO epoch # 521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.1221599115524441
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,485 INFO epoch # 522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.15606703830417246
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,561 INFO epoch # 523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.14169170591048896
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,643 INFO epoch # 524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.14487531734630466
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,720 INFO epoch # 525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.1237921251449734
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,797 INFO epoch # 526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.14461755088996142
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,874 INFO epoch # 527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.1605530212400481
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:54,950 INFO epoch # 528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00238032 -loss = 0.14039277320262045
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,027 INFO epoch # 529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00238032-> 0.00202327 -loss = 0.13487847230862826
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,103 INFO epoch # 530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.12157787464093417
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:55,104 INFO *** epoch 530, rolling-avg-loss (window=10)= 0.1390605790540576
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,180 INFO epoch # 531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.11354970990214497
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,257 INFO epoch # 532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.1237146818311885
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,334 INFO epoch # 533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.14036586973816156
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,411 INFO epoch # 534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.12921340647153556
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,487 INFO epoch # 535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.11938501289114356
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,564 INFO epoch # 536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.12873138417489827
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,642 INFO epoch # 537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.14375459495931864
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,719 INFO epoch # 538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.12937927583698183
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,796 INFO epoch # 539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.11896798037923872
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,873 INFO epoch # 540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.12700592062901706
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:55,873 INFO *** epoch 540, rolling-avg-loss (window=10)= 0.12740678368136288
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:55,951 INFO epoch # 541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00202327 -loss = 0.12598752847407013
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,028 INFO epoch # 542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00202327-> 0.00171978 -loss = 0.1322163259028457
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,105 INFO epoch # 543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00171978-> 0.00171978 -loss = 0.11931875004665926
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,182 INFO epoch # 544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00171978-> 0.00171978 -loss = 0.12065184459788725
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,259 INFO epoch # 545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00171978-> 0.00171978 -loss = 0.12926763005089015
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,337 INFO epoch # 546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00171978-> 0.00171978 -loss = 0.1237877740059048
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,414 INFO epoch # 547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00171978-> 0.00171978 -loss = 0.13706333766458556
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,491 INFO epoch # 548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00171978-> 0.00171978 -loss = 0.1321240026736632
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,568 INFO epoch # 549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00171978-> 0.00171978 -loss = 0.1333077207673341
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,646 INFO epoch # 550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00171978-> 0.00171978 -loss = 0.1339562573703006
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:56,646 INFO *** epoch 550, rolling-avg-loss (window=10)= 0.12876811715541409
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,722 INFO epoch # 551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00171978-> 0.00171978 -loss = 0.13085505296476185
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,800 INFO epoch # 552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00171978-> 0.00171978 -loss = 0.11786736641079187
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,876 INFO epoch # 553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00171978-> 0.00146181 -loss = 0.11978874611668289
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:56,953 INFO epoch # 554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.12018375971820205
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,030 INFO epoch # 555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11054645985132083
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,107 INFO epoch # 556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11234300385694951
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,185 INFO epoch # 557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.13080158364027739
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,263 INFO epoch # 558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.10774040583055466
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,340 INFO epoch # 559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11218828428536654
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,417 INFO epoch # 560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11118463007733226
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:57,417 INFO *** epoch 560, rolling-avg-loss (window=10)= 0.11734992927522399
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,494 INFO epoch # 561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11898286524228752
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,571 INFO epoch # 562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.12563837610650808
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,648 INFO epoch # 563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.12930221506394446
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,729 INFO epoch # 564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.1139526607003063
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,806 INFO epoch # 565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.10688595741521567
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,883 INFO epoch # 566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11857089109253138
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:57,960 INFO epoch # 567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11104998178780079
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,036 INFO epoch # 568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.1136200373293832
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,113 INFO epoch # 569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.10840789100620896
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,190 INFO epoch # 570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.12493705609813333
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:58,190 INFO *** epoch 570, rolling-avg-loss (window=10)= 0.11713479318423196
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,267 INFO epoch # 571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11330658191582188
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,348 INFO epoch # 572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.10500162572134286
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,426 INFO epoch # 573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.10997348983073607
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,504 INFO epoch # 574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11025944957509637
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,583 INFO epoch # 575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11493677541147918
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,661 INFO epoch # 576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.1082728689070791
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,738 INFO epoch # 577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.10496317350771278
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,815 INFO epoch # 578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11620365921407938
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,892 INFO epoch # 579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.10743044968694448
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:58,969 INFO epoch # 580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.10610698978416622
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:58,969 INFO *** epoch 580, rolling-avg-loss (window=10)= 0.10964550635544583
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,046 INFO epoch # 581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.1089565617730841
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,123 INFO epoch # 582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.10483730200212449
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,200 INFO epoch # 583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.10916554310824722
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,277 INFO epoch # 584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11380539566744119
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,354 INFO epoch # 585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11417880735825747
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,431 INFO epoch # 586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11733692407142371
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,507 INFO epoch # 587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11072225938551128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,584 INFO epoch # 588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11670676659559831
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,661 INFO epoch # 589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11220013012643903
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,739 INFO epoch # 590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11714278534054756
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:07:59,739 INFO *** epoch 590, rolling-avg-loss (window=10)= 0.11250524754286743
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,817 INFO epoch # 591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.1057158766197972
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,893 INFO epoch # 592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00146181 -loss = 0.11042244045529515
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:07:59,970 INFO epoch # 593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00146181-> 0.00124254 -loss = 0.11299493710976094
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,047 INFO epoch # 594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.11030396725982428
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,124 INFO epoch # 595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.10539353231433779
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,201 INFO epoch # 596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.10325171961449087
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,278 INFO epoch # 597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.1080864048562944
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,355 INFO epoch # 598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.1105880195973441
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,431 INFO epoch # 599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.11231687944382429
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,508 INFO epoch # 600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.1060334665235132
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:00,508 INFO *** epoch 600, rolling-avg-loss (window=10)= 0.10851072437944823
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,585 INFO epoch # 601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.12035353481769562
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,662 INFO epoch # 602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.10521489090751857
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,739 INFO epoch # 603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.10958745935931802
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,816 INFO epoch # 604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.11150380095932633
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,892 INFO epoch # 605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.1164671708829701
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:00,969 INFO epoch # 606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00124254 -loss = 0.10351557930698618
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,045 INFO epoch # 607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00124254-> 0.00105616 -loss = 0.10566882300190628
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,122 INFO epoch # 608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.1120344870723784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,199 INFO epoch # 609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.10630788188427687
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,277 INFO epoch # 610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.10117223009001464
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:01,277 INFO *** epoch 610, rolling-avg-loss (window=10)= 0.1091825858282391
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,354 INFO epoch # 611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.10487577668391168
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,433 INFO epoch # 612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.10005347145488486
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,510 INFO epoch # 613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.09771869005635381
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,588 INFO epoch # 614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.10444172832649201
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,666 INFO epoch # 615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.10861283738631755
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,743 INFO epoch # 616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.10744152462575585
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,822 INFO epoch # 617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.10472457885043696
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,899 INFO epoch # 618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.10886256990488619
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:01,975 INFO epoch # 619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.10722641076426953
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,054 INFO epoch # 620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.1069707433343865
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:02,055 INFO *** epoch 620, rolling-avg-loss (window=10)= 0.10509283313876949
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,133 INFO epoch # 621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.10379641235340387
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,210 INFO epoch # 622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.0995614897692576
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,292 INFO epoch # 623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.00105616 -loss = 0.09941686311503872
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,370 INFO epoch # 624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00105616-> 0.001 -loss = 0.11127800081158057
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,447 INFO epoch # 625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11295145866461098
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,523 INFO epoch # 626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09701306524220854
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,600 INFO epoch # 627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09986412385478616
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,677 INFO epoch # 628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10467236413387582
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,754 INFO epoch # 629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09972958767320961
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,831 INFO epoch # 630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09941656037699431
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:02,831 INFO *** epoch 630, rolling-avg-loss (window=10)= 0.10276999259949662
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,907 INFO epoch # 631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09890450278180651
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:02,984 INFO epoch # 632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09832436789292842
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,061 INFO epoch # 633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10127575474325567
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,137 INFO epoch # 634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10128436970990151
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,215 INFO epoch # 635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09947066358290613
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,294 INFO epoch # 636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09598973335232586
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,370 INFO epoch # 637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10494000453036278
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,449 INFO epoch # 638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10442324297036976
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,526 INFO epoch # 639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09951459988951683
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,603 INFO epoch # 640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10512812109664083
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:03,603 INFO *** epoch 640, rolling-avg-loss (window=10)= 0.10092553605500143
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,681 INFO epoch # 641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.1047574930707924
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,758 INFO epoch # 642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10839451366337016
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,835 INFO epoch # 643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10098352166824043
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,911 INFO epoch # 644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09428073617164046
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:03,988 INFO epoch # 645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0961422361433506
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,065 INFO epoch # 646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09859579510521144
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,142 INFO epoch # 647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10134281415957958
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,219 INFO epoch # 648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.101173319038935
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,296 INFO epoch # 649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10649790469324216
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,373 INFO epoch # 650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11884078604634851
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:04,373 INFO *** epoch 650, rolling-avg-loss (window=10)= 0.10310091197607107
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,450 INFO epoch # 651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10086302121635526
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,526 INFO epoch # 652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10099221009295434
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,603 INFO epoch # 653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11252541758585721
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,680 INFO epoch # 654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09278463647933677
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,757 INFO epoch # 655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09563874977175146
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,834 INFO epoch # 656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10250887018628418
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,911 INFO epoch # 657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09679907938698307
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:04,987 INFO epoch # 658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09797434974461794
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,065 INFO epoch # 659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09446198202203959
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,143 INFO epoch # 660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09806096222018823
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:05,143 INFO *** epoch 660, rolling-avg-loss (window=10)= 0.09926092787063681
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,220 INFO epoch # 661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10935335897374898
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,297 INFO epoch # 662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10835485102143139
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,376 INFO epoch # 663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09801571699790657
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,452 INFO epoch # 664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09364543331321329
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,529 INFO epoch # 665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0998459265101701
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,606 INFO epoch # 666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10188159986864775
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,684 INFO epoch # 667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09809502877760679
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,762 INFO epoch # 668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09335443854797632
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,839 INFO epoch # 669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0943067081971094
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,916 INFO epoch # 670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10377189749851823
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:05,916 INFO *** epoch 670, rolling-avg-loss (window=10)= 0.10006249597063288
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:05,993 INFO epoch # 671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09958331065718085
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,070 INFO epoch # 672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0954945195990149
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,147 INFO epoch # 673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09891416027676314
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,224 INFO epoch # 674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11321606545243412
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,303 INFO epoch # 675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10150600993074477
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,380 INFO epoch # 676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09742017311509699
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,457 INFO epoch # 677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10284524201415479
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,534 INFO epoch # 678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09249708033166826
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,611 INFO epoch # 679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10447285114787519
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,689 INFO epoch # 680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10038284864276648
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:06,689 INFO *** epoch 680, rolling-avg-loss (window=10)= 0.10063322611676995
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,766 INFO epoch # 681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09761757089290768
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,842 INFO epoch # 682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10027379455277696
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,919 INFO epoch # 683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09536477422807366
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:06,996 INFO epoch # 684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10906010505277663
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,073 INFO epoch # 685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09828197723254561
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,151 INFO epoch # 686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09390185261145234
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,227 INFO epoch # 687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.12036049115704373
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,304 INFO epoch # 688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09807147260289639
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,381 INFO epoch # 689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10073567717336118
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,459 INFO epoch # 690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09435788996051997
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:07,459 INFO *** epoch 690, rolling-avg-loss (window=10)= 0.10080256054643541
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,535 INFO epoch # 691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09774621415999718
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,612 INFO epoch # 692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10059908794937655
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,690 INFO epoch # 693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09105935372645035
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,767 INFO epoch # 694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09560450806748122
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,844 INFO epoch # 695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.11010676028672606
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,921 INFO epoch # 696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09086512489011511
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:07,998 INFO epoch # 697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09839552105404437
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,076 INFO epoch # 698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09776685235556215
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,153 INFO epoch # 699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09333243395667523
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,230 INFO epoch # 700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08847593597602099
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:08,230 INFO *** epoch 700, rolling-avg-loss (window=10)= 0.09639517924224492
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,308 INFO epoch # 701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09539923188276589
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,385 INFO epoch # 702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10129992826841772
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,462 INFO epoch # 703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09267626435030252
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,540 INFO epoch # 704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09466380946105346
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,624 INFO epoch # 705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0922544983914122
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,701 INFO epoch # 706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09581960563082248
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,779 INFO epoch # 707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09304110991070047
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,856 INFO epoch # 708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10428716684691608
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:08,932 INFO epoch # 709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10650989692658186
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,011 INFO epoch # 710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09958314697723836
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:09,011 INFO *** epoch 710, rolling-avg-loss (window=10)= 0.0975534658646211
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,089 INFO epoch # 711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10083999414928257
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,167 INFO epoch # 712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09708636946743354
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,244 INFO epoch # 713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09801669465377927
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,322 INFO epoch # 714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10542067047208548
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,399 INFO epoch # 715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09828304167604074
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,477 INFO epoch # 716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10042515123495832
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,554 INFO epoch # 717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09744674433022738
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,631 INFO epoch # 718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09151124360505491
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,710 INFO epoch # 719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0962747271405533
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,787 INFO epoch # 720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09804674232145771
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:09,787 INFO *** epoch 720, rolling-avg-loss (window=10)= 0.09833513790508733
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,864 INFO epoch # 721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09112993814051151
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:09,943 INFO epoch # 722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08970399497775361
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,020 INFO epoch # 723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09299841389292851
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,097 INFO epoch # 724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10179676045663655
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,179 INFO epoch # 725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10057091584894806
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,256 INFO epoch # 726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09405998123111203
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,333 INFO epoch # 727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09974155807867646
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,411 INFO epoch # 728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09539061551913619
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,489 INFO epoch # 729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09071699460037053
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,565 INFO epoch # 730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09447955782525241
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:10,565 INFO *** epoch 730, rolling-avg-loss (window=10)= 0.09505887305713259
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,643 INFO epoch # 731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09342429414391518
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,719 INFO epoch # 732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08971276879310608
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,796 INFO epoch # 733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09701417945325375
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,874 INFO epoch # 734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10776438901666552
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:10,951 INFO epoch # 735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09558111219666898
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,028 INFO epoch # 736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09926440619165078
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,107 INFO epoch # 737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10059947019908577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,186 INFO epoch # 738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09540587867377326
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,263 INFO epoch # 739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09072439558804035
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,341 INFO epoch # 740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09034803207032382
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:11,341 INFO *** epoch 740, rolling-avg-loss (window=10)= 0.09598389263264835
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,418 INFO epoch # 741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09293757053092122
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,496 INFO epoch # 742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0913848583586514
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,574 INFO epoch # 743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0940809577004984
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,653 INFO epoch # 744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09074417466763407
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,730 INFO epoch # 745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08934679953381419
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,809 INFO epoch # 746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08902477362425998
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,885 INFO epoch # 747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0870264723780565
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:11,962 INFO epoch # 748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0904403324238956
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,040 INFO epoch # 749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09992856287863106
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,117 INFO epoch # 750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08741398341953754
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:12,117 INFO *** epoch 750, rolling-avg-loss (window=10)= 0.09123284855159
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,194 INFO epoch # 751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09044220577925444
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,271 INFO epoch # 752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10151616099756211
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,349 INFO epoch # 753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0888087865896523
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,426 INFO epoch # 754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08658254740294069
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,504 INFO epoch # 755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09827167133335024
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,581 INFO epoch # 756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09141078288666904
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,658 INFO epoch # 757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09768791124224663
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,735 INFO epoch # 758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09041900897864252
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,813 INFO epoch # 759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09080000023823231
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,889 INFO epoch # 760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09468427544925362
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:12,890 INFO *** epoch 760, rolling-avg-loss (window=10)= 0.09306233508978039
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:12,967 INFO epoch # 761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09436443133745342
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,045 INFO epoch # 762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0924008886795491
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,121 INFO epoch # 763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08829877327661961
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,200 INFO epoch # 764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09566680551506579
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,277 INFO epoch # 765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0960343664046377
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,354 INFO epoch # 766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09376201662234962
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,431 INFO epoch # 767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08591413439717144
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,508 INFO epoch # 768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09001868776977062
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,585 INFO epoch # 769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09472980909049511
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,661 INFO epoch # 770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09999989793868735
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:13,662 INFO *** epoch 770, rolling-avg-loss (window=10)= 0.09311898110317998
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,738 INFO epoch # 771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08854239512584172
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,815 INFO epoch # 772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0878001453820616
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,892 INFO epoch # 773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0933779162587598
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:13,969 INFO epoch # 774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10084583354182541
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,046 INFO epoch # 775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08852252212818712
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,125 INFO epoch # 776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08971672743791714
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,210 INFO epoch # 777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08951651799725369
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,287 INFO epoch # 778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09353126835776493
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,364 INFO epoch # 779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09067013103049248
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,441 INFO epoch # 780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08536385314073414
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:14,441 INFO *** epoch 780, rolling-avg-loss (window=10)= 0.09078873104008381
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,517 INFO epoch # 781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08889450435526669
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,594 INFO epoch # 782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08796575985616073
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,672 INFO epoch # 783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09367048053536564
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,749 INFO epoch # 784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09382380603346974
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,826 INFO epoch # 785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09211829851847142
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,903 INFO epoch # 786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08658529678359628
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:14,979 INFO epoch # 787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08942481718258932
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,056 INFO epoch # 788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09211460780352354
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,134 INFO epoch # 789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09284297173144296
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,211 INFO epoch # 790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09546406148001552
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:15,212 INFO *** epoch 790, rolling-avg-loss (window=10)= 0.09129046042799019
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,288 INFO epoch # 791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09108947025379166
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,366 INFO epoch # 792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0918391706654802
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,443 INFO epoch # 793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09114089165814221
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,520 INFO epoch # 794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0876150542171672
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,597 INFO epoch # 795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08718666579807177
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,675 INFO epoch # 796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09577460144646466
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,752 INFO epoch # 797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09964137943461537
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,829 INFO epoch # 798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0912486556917429
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,905 INFO epoch # 799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08853536425158381
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:15,982 INFO epoch # 800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08438821631716564
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:15,983 INFO *** epoch 800, rolling-avg-loss (window=10)= 0.09084594697342255
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,060 INFO epoch # 801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08741086639929563
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,137 INFO epoch # 802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08696655952371657
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,214 INFO epoch # 803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0933064998826012
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,292 INFO epoch # 804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08748977578943595
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,369 INFO epoch # 805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0892985439277254
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,446 INFO epoch # 806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09617998136673123
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,523 INFO epoch # 807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09401571389753371
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,600 INFO epoch # 808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08831522392574698
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,689 INFO epoch # 809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0899738835869357
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,772 INFO epoch # 810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0909740703064017
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:16,772 INFO *** epoch 810, rolling-avg-loss (window=10)= 0.09039311186061241
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,849 INFO epoch # 811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08725521503947675
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:16,927 INFO epoch # 812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09252781374379992
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,004 INFO epoch # 813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08762940007727593
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,081 INFO epoch # 814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08661727234721184
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,158 INFO epoch # 815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0906169592635706
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,236 INFO epoch # 816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09564088901970536
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,312 INFO epoch # 817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09342138993088156
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,390 INFO epoch # 818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08867683098651469
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,467 INFO epoch # 819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08533961832290515
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,544 INFO epoch # 820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08900687546702102
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:17,544 INFO *** epoch 820, rolling-avg-loss (window=10)= 0.08967322641983629
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,621 INFO epoch # 821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09231920796446502
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,699 INFO epoch # 822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0914897482143715
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,776 INFO epoch # 823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09230446559377015
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,852 INFO epoch # 824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09631625341717154
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:17,929 INFO epoch # 825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08523043594323099
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,005 INFO epoch # 826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0871326039195992
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,082 INFO epoch # 827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08808023837627843
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,159 INFO epoch # 828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09649077424546704
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,236 INFO epoch # 829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09084618533961475
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,313 INFO epoch # 830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08793800184503198
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:18,313 INFO *** epoch 830, rolling-avg-loss (window=10)= 0.09081479148590006
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,389 INFO epoch # 831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09245074546197429
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,466 INFO epoch # 832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09085498779313639
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,543 INFO epoch # 833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08528777334140614
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,620 INFO epoch # 834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09150413336465135
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,697 INFO epoch # 835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09041178738698363
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,774 INFO epoch # 836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08800596941728145
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,851 INFO epoch # 837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08271966513711959
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:18,927 INFO epoch # 838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08753705001436174
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,004 INFO epoch # 839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08450853812973946
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,081 INFO epoch # 840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0821587287937291
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:19,081 INFO *** epoch 840, rolling-avg-loss (window=10)= 0.08754393788403832
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,158 INFO epoch # 841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0848449447657913
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,235 INFO epoch # 842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09906573256012052
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,313 INFO epoch # 843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08291696530068293
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,389 INFO epoch # 844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0836376472725533
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,466 INFO epoch # 845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08496772439684719
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,543 INFO epoch # 846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0929998637875542
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,620 INFO epoch # 847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08680057537276298
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,697 INFO epoch # 848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08871881326194853
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,774 INFO epoch # 849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09062905982136726
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,851 INFO epoch # 850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08831242250744253
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:19,851 INFO *** epoch 850, rolling-avg-loss (window=10)= 0.08828937490470708
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:19,928 INFO epoch # 851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0865460773347877
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,005 INFO epoch # 852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08625698531977832
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,082 INFO epoch # 853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08192144689382985
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,159 INFO epoch # 854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09933164448011667
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,235 INFO epoch # 855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09419115877244622
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,312 INFO epoch # 856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08740273059811443
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,389 INFO epoch # 857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0848297665361315
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,466 INFO epoch # 858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08618519885931164
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,543 INFO epoch # 859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08430636476259679
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,621 INFO epoch # 860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08476826659170911
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:20,621 INFO *** epoch 860, rolling-avg-loss (window=10)= 0.08757396401488222
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,698 INFO epoch # 861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08496690314495936
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,775 INFO epoch # 862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09120736236218363
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,852 INFO epoch # 863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.093278500251472
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:20,929 INFO epoch # 864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08435913291759789
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,006 INFO epoch # 865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08078542642761022
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,083 INFO epoch # 866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08191128534963354
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,160 INFO epoch # 867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09586067235795781
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,237 INFO epoch # 868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10128329531289637
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,314 INFO epoch # 869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08150945336092263
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,391 INFO epoch # 870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0856523546972312
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:21,391 INFO *** epoch 870, rolling-avg-loss (window=10)= 0.08808143861824647
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,468 INFO epoch # 871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09015038586221635
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,545 INFO epoch # 872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08877510472666472
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,622 INFO epoch # 873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0861017222632654
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,699 INFO epoch # 874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09001920325681567
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,776 INFO epoch # 875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.10440866544377059
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,855 INFO epoch # 876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08129246055614203
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:21,931 INFO epoch # 877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08610086876433343
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,008 INFO epoch # 878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09636289323680103
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,085 INFO epoch # 879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08709830971201882
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,162 INFO epoch # 880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09153117856476456
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:22,162 INFO *** epoch 880, rolling-avg-loss (window=10)= 0.09018407923867926
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,239 INFO epoch # 881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08509311435045674
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,316 INFO epoch # 882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08797161735128611
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,393 INFO epoch # 883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08949962118640542
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,472 INFO epoch # 884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0899551427573897
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,549 INFO epoch # 885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08246782748028636
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,629 INFO epoch # 886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08338227879721671
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,708 INFO epoch # 887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08144172595348209
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,786 INFO epoch # 888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0817516716197133
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,863 INFO epoch # 889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08753940870519727
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:22,943 INFO epoch # 890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08814639726188034
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:22,943 INFO *** epoch 890, rolling-avg-loss (window=10)= 0.0857248805463314
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,020 INFO epoch # 891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09022513614036143
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,097 INFO epoch # 892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08569316321518272
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,179 INFO epoch # 893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08494141552364454
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,256 INFO epoch # 894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07863801828352734
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,333 INFO epoch # 895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08627039636485279
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,411 INFO epoch # 896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0841667465865612
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,487 INFO epoch # 897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08288399229058996
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,564 INFO epoch # 898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09331745735835284
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,646 INFO epoch # 899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08299923688173294
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,723 INFO epoch # 900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08475456101587042
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:23,723 INFO *** epoch 900, rolling-avg-loss (window=10)= 0.08538901236606762
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,800 INFO epoch # 901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08432013698620722
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,880 INFO epoch # 902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08036602224456146
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:23,957 INFO epoch # 903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07956045860191807
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,034 INFO epoch # 904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09228040627203882
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,113 INFO epoch # 905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08734213130082935
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,193 INFO epoch # 906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08828007045667619
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,270 INFO epoch # 907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08393113600322977
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,350 INFO epoch # 908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08284374547656626
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,427 INFO epoch # 909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07841567724244669
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,504 INFO epoch # 910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08483720471849665
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:24,504 INFO *** epoch 910, rolling-avg-loss (window=10)= 0.08421769893029704
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,585 INFO epoch # 911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07771021057851613
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,665 INFO epoch # 912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08407414588145912
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,741 INFO epoch # 913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08369558985577896
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,821 INFO epoch # 914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08124032500199974
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,898 INFO epoch # 915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0801696278504096
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:24,975 INFO epoch # 916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08892967528663576
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,054 INFO epoch # 917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0823910286417231
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,134 INFO epoch # 918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08118111081421375
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,211 INFO epoch # 919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08081185794435441
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,291 INFO epoch # 920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0809560707421042
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:25,291 INFO *** epoch 920, rolling-avg-loss (window=10)= 0.08211596425971948
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,368 INFO epoch # 921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08528975595254451
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,445 INFO epoch # 922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08965172700118273
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,525 INFO epoch # 923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09319280460476875
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,602 INFO epoch # 924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07957909459946677
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,679 INFO epoch # 925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08121474814834073
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,756 INFO epoch # 926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08515738751157187
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,833 INFO epoch # 927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08699349174275994
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,910 INFO epoch # 928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08069955528480932
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:25,987 INFO epoch # 929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0808137638377957
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,063 INFO epoch # 930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08378916524816304
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:26,064 INFO *** epoch 930, rolling-avg-loss (window=10)= 0.08463814939314034
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,141 INFO epoch # 931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08485049539012834
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,218 INFO epoch # 932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0825548927532509
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,294 INFO epoch # 933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07876377945649438
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,372 INFO epoch # 934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08221231092466041
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,448 INFO epoch # 935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08021862909663469
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,525 INFO epoch # 936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0843558877822943
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,602 INFO epoch # 937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08761372062144801
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,680 INFO epoch # 938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08522273431299254
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,757 INFO epoch # 939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08352397003909573
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,833 INFO epoch # 940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08154311927501112
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:26,834 INFO *** epoch 940, rolling-avg-loss (window=10)= 0.08308595396520105
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,910 INFO epoch # 941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08684492646716535
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:26,987 INFO epoch # 942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07897984102601185
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,064 INFO epoch # 943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0774300952325575
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,140 INFO epoch # 944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08117407246027142
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,217 INFO epoch # 945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08998710790183395
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,293 INFO epoch # 946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09228836034890264
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,370 INFO epoch # 947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.09529299312271178
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,447 INFO epoch # 948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08618189254775643
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,524 INFO epoch # 949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07462477224180475
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,601 INFO epoch # 950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08517672511516139
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:27,601 INFO *** epoch 950, rolling-avg-loss (window=10)= 0.08479807864641771
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,678 INFO epoch # 951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07704780669882894
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,755 INFO epoch # 952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07975189812714234
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,831 INFO epoch # 953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08324189949780703
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,910 INFO epoch # 954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07979863253422081
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:27,986 INFO epoch # 955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08277054189238697
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,065 INFO epoch # 956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08286076819058508
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,143 INFO epoch # 957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08041467174189165
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,220 INFO epoch # 958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07866595726227388
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,298 INFO epoch # 959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0808910554042086
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,376 INFO epoch # 960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07969608629355207
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:28,376 INFO *** epoch 960, rolling-avg-loss (window=10)= 0.08051393176428974
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,453 INFO epoch # 961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0806763848522678
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,532 INFO epoch # 962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08300450129900128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,609 INFO epoch # 963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0858817637199536
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,686 INFO epoch # 964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08006711542839184
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,762 INFO epoch # 965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07860513238119893
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,844 INFO epoch # 966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07722199312411249
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,921 INFO epoch # 967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08035975182428956
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:28,997 INFO epoch # 968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08294638281222433
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,074 INFO epoch # 969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07868843030882999
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,151 INFO epoch # 970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07735040388070047
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:29,151 INFO *** epoch 970, rolling-avg-loss (window=10)= 0.08048018596309703
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,228 INFO epoch # 971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07888646307401359
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,305 INFO epoch # 972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07810005894862115
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,382 INFO epoch # 973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07925276597961783
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,460 INFO epoch # 974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0833116882131435
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,536 INFO epoch # 975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08006177429342642
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,613 INFO epoch # 976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07728259253781289
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,691 INFO epoch # 977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08016829489497468
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,768 INFO epoch # 978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07905502885114402
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,845 INFO epoch # 979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07974904187722132
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,922 INFO epoch # 980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08315407729241997
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:29,922 INFO *** epoch 980, rolling-avg-loss (window=10)= 0.07990217859623953
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:29,999 INFO epoch # 981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07677987599163316
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,076 INFO epoch # 982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0800670805037953
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,157 INFO epoch # 983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0870516806608066
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,234 INFO epoch # 984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08266681566601619
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,311 INFO epoch # 985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07825733808567747
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,390 INFO epoch # 986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07765391521388665
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,468 INFO epoch # 987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07671073114033788
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,544 INFO epoch # 988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08081118908012286
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,623 INFO epoch # 989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0850903763785027
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,700 INFO epoch # 990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08032177021959797
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:30,700 INFO *** epoch 990, rolling-avg-loss (window=10)= 0.08054107729403767
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,776 INFO epoch # 991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08084603847237304
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,856 INFO epoch # 992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0845080295111984
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:30,933 INFO epoch # 993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07871370628708974
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:31,010 INFO epoch # 994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07823775272117928
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:31,089 INFO epoch # 995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08403687545796856
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:31,168 INFO epoch # 996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0868149691959843
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:31,245 INFO epoch # 997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08371966704726219
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:31,324 INFO epoch # 998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0786362198414281
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:31,401 INFO epoch # 999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.08040277223335579
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:08:31,477 INFO epoch # 1000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.07967298734001815
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:08:31,477 INFO *** epoch 1000, rolling-avg-loss (window=10)= 0.08155890181078576
[experiments_sandbox.py:920 -   <module>()] 2023-04-25 21:08:31,478 INFO training time in seconds = 77
[experiments_sandbox.py:936 -   <module>()] 2023-04-25 21:08:31,619 INFO train-epochs-loss curve df :
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 21:08:31,626 INFO 
    epochs       loss
0       10  43.206762
1       20  18.475985
2       30  13.670247
3       40  12.552293
4       50  12.498275
5       60   9.413682
6       70   7.603292
7       80   7.421440
8       90   6.054440
9      100   5.814733
10     110   5.187956
11     120   6.474219
12     130   4.817227
13     140   3.981294
14     150   3.351280
15     160   4.406132
16     170   2.712696
17     180   2.202205
18     190   1.785712
19     200   1.994121
20     210   1.476347
21     220   1.133067
22     230   1.039524
23     240   1.012568
24     250   0.993738
25     260   0.758523
26     270   0.783091
27     280   0.514711
28     290   0.562295
29     300   0.655751
30     310   0.544026
31     320   0.427279
32     330   0.510641
33     340   0.442596
34     350   0.389569
35     360   0.335527
36     370   0.267614
37     380   0.227602
38     390   0.248162
39     400   0.219473
40     410   0.224806
41     420   0.201747
42     430   0.190791
43     440   0.185509
44     450   0.177853
45     460   0.169092
46     470   0.161893
47     480   0.152587
48     490   0.140084
49     500   0.143995
50     510   0.139536
51     520   0.141099
52     530   0.139061
53     540   0.127407
54     550   0.128768
55     560   0.117350
56     570   0.117135
57     580   0.109646
58     590   0.112505
59     600   0.108511
60     610   0.109183
61     620   0.105093
62     630   0.102770
63     640   0.100926
64     650   0.103101
65     660   0.099261
66     670   0.100062
67     680   0.100633
68     690   0.100803
69     700   0.096395
70     710   0.097553
71     720   0.098335
72     730   0.095059
73     740   0.095984
74     750   0.091233
75     760   0.093062
76     770   0.093119
77     780   0.090789
78     790   0.091290
79     800   0.090846
80     810   0.090393
81     820   0.089673
82     830   0.090815
83     840   0.087544
84     850   0.088289
85     860   0.087574
86     870   0.088081
87     880   0.090184
88     890   0.085725
89     900   0.085389
90     910   0.084218
91     920   0.082116
92     930   0.084638
93     940   0.083086
94     950   0.084798
95     960   0.080514
96     970   0.080480
97     980   0.079902
98     990   0.080541
99    1000   0.081559
[experiments_sandbox.py:939 -   <module>()] 2023-04-25 21:08:31,626 INFO Model parameters after training
[experiments_sandbox.py:940 -   <module>()] 2023-04-25 21:08:31,626 INFO Model = TTRBF
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 21:08:31,627 INFO G0 = Parameter containing:
tensor([[ 0.0104, -0.0353, -0.2175],
        [-1.2750, -0.5809,  0.9267],
        [-1.4598, -0.0273,  1.0369],
        [-0.7787,  0.2420,  0.4045]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 21:08:31,627 INFO G1 = Parameter containing:
tensor([[[-4.8030e-02,  2.9421e-02,  2.0549e+00],
         [-9.4330e+00,  3.7695e+00,  6.9242e+00],
         [-1.0464e-01,  2.6575e-01,  2.4304e+00],
         [ 3.9465e+00, -4.1821e+00,  2.5038e+00]],

        [[-1.4830e-01,  1.2922e-02,  2.8508e-01],
         [ 3.2671e+00, -1.3700e+00, -5.1893e+00],
         [ 1.2416e-01, -8.7597e-02,  2.9588e-01],
         [-1.9049e+00,  1.4313e+00, -6.7736e-01]],

        [[-1.4299e-01, -4.5188e-02, -1.8035e+00],
         [ 8.6255e+00, -3.7653e+00, -6.2833e+00],
         [ 4.8181e-01,  4.7557e-03, -1.8401e+00],
         [-4.0944e+00,  3.9673e+00, -1.7270e+00]]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 21:08:31,628 INFO G2 = Parameter containing:
tensor([[[ 0.6308, -2.3066, -0.2005],
         [ 1.3933, -1.7674, -0.8819],
         [-0.2421, -3.9583, -0.3149],
         [ 0.3979, -1.3495, -1.5617]],

        [[-0.2279, -0.3912, -0.1650],
         [-0.9340,  0.8254, -0.1162],
         [-0.3056,  1.0479,  1.3678],
         [-0.7917,  1.5618, -0.2059]],

        [[ 3.3019,  0.1845, -3.0078],
         [-1.8975, -1.8723,  1.8487],
         [ 2.0572,  1.9214, -2.6009],
         [-3.0660, -0.7277,  3.3869]]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 21:08:31,628 INFO G3 = Parameter containing:
tensor([[[-0.2225, -2.2911,  1.2810],
         [ 0.4301,  5.0983, -0.3560],
         [-0.1158, -3.0345,  0.9673],
         [ 3.3258, -3.4198,  5.1274]],

        [[-2.3948, -0.7630,  0.2971],
         [ 2.5046,  1.9930, -0.0869],
         [-2.0232, -2.3674, -0.0836],
         [-2.1087, -1.9949, -1.4600]],

        [[-1.8362,  2.7808,  1.1874],
         [ 3.0896, -2.4329, -0.4198],
         [-1.9151,  3.4532,  1.0989],
         [ 0.4997,  0.6397,  3.9355]]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 21:08:31,629 INFO rbf_module.centres = Parameter containing:
tensor([[ 3.1809,  2.6452,  1.8128],
        [-2.0545, -0.1699, -2.5407],
        [-1.4074, -0.7357, -2.0701],
        [ 1.9050,  1.6632, -4.3642],
        [-1.7156,  0.2050, -2.9875],
        [ 0.0238,  6.5452,  1.7360],
        [-1.6524,  0.7541, -3.0336],
        [ 0.7018, -6.1763,  1.2498],
        [ 4.5551, -0.4933,  4.9611],
        [ 4.8558, -0.6194, -4.4611],
        [-5.2688, -0.5691, -4.8278],
        [-4.9476, -0.6305,  5.0904],
        [-0.9269,  0.9982, -0.2452],
        [-0.1354,  0.2172,  2.1108],
        [-1.9866, -0.5892, -2.7191],
        [ 1.8505,  3.2318, -5.1139]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 21:08:31,629 INFO rbf_module.log_sigmas = Parameter containing:
tensor([-1.2447,  7.5480,  7.9609,  0.1251,  7.2223,  1.4125,  7.0521,  1.4043,
         1.5169,  1.5647,  1.5731,  1.5311,  7.6664,  2.0554,  6.8507,  0.9751],
       requires_grad=True)
[experiments_sandbox.py:945 -   <module>()] 2023-04-25 21:08:31,629 INFO Out-of sample batch-test
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,631 INFO test-batch  # 0 => test-loss = 0.19882243871688843
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,632 INFO test-batch  # 1 => test-loss = 0.07578108459711075
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,633 INFO test-batch  # 2 => test-loss = 0.08513340353965759
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,634 INFO test-batch  # 3 => test-loss = 0.06948388367891312
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,635 INFO test-batch  # 4 => test-loss = 0.1017744317650795
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,637 INFO test-batch  # 5 => test-loss = 0.17339497804641724
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,638 INFO test-batch  # 6 => test-loss = 0.07608605921268463
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,639 INFO test-batch  # 7 => test-loss = 0.6459485292434692
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,640 INFO test-batch  # 8 => test-loss = 0.2545962333679199
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,641 INFO test-batch  # 9 => test-loss = 0.048446595668792725
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,642 INFO test-batch  # 10 => test-loss = 0.060871634632349014
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,644 INFO test-batch  # 11 => test-loss = 0.47556567192077637
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,645 INFO test-batch  # 12 => test-loss = 0.05555475130677223
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,646 INFO test-batch  # 13 => test-loss = 0.6383844614028931
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,647 INFO test-batch  # 14 => test-loss = 0.2559170722961426
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,648 INFO test-batch  # 15 => test-loss = 0.13986541330814362
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,649 INFO test-batch  # 16 => test-loss = 0.2706414461135864
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,650 INFO test-batch  # 17 => test-loss = 0.05648614093661308
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,652 INFO test-batch  # 18 => test-loss = 0.05619947984814644
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,653 INFO test-batch  # 19 => test-loss = 0.04396918788552284
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,654 INFO test-batch  # 20 => test-loss = 0.05058973655104637
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,655 INFO test-batch  # 21 => test-loss = 0.03836413845419884
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,657 INFO test-batch  # 22 => test-loss = 0.06280927360057831
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,658 INFO test-batch  # 23 => test-loss = 0.6783679127693176
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,659 INFO test-batch  # 24 => test-loss = 0.04577541723847389
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,660 INFO test-batch  # 25 => test-loss = 0.1650676131248474
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,662 INFO test-batch  # 26 => test-loss = 0.08499204367399216
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,663 INFO test-batch  # 27 => test-loss = 0.02896159701049328
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,664 INFO test-batch  # 28 => test-loss = 0.05042276903986931
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,665 INFO test-batch  # 29 => test-loss = 0.12163088470697403
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,666 INFO test-batch  # 30 => test-loss = 0.10763289779424667
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:08:31,667 INFO test-batch  # 31 => test-loss = 0.031034281477332115
