[experiments_sandbox.py:737 -   <module>()] 2023-04-25 20:52:07,836 INFO SEED = 42
[experiments_sandbox.py:818 -   <module>()] 2023-04-25 20:52:07,837 INFO model = ***
TTRBF
order = 4
num_rbf_centers= 16
tt_rank = 3
dim = 4
learnable_numel = 156
***

[experiments_sandbox.py:819 -   <module>()] 2023-04-25 20:52:07,837 INFO optimizer  = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.1
    maximize: False
    weight_decay: 0
)
[experiments_sandbox.py:827 -   <module>()] 2023-04-25 20:52:07,837 INFO lr_scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7febdc373700>
[experiments_sandbox.py:830 -   <module>()] 2023-04-25 20:52:07,837 INFO Normalize-Data-source-X-train = False
[experiments_sandbox.py:831 -   <module>()] 2023-04-25 20:52:07,837 INFO Normalize-Data-source-Y-train = False
[experiments_sandbox.py:832 -   <module>()] 2023-04-25 20:52:07,837 INFO Normalize-Data-source-X-test = False
[experiments_sandbox.py:833 -   <module>()] 2023-04-25 20:52:07,837 INFO Normalize-Data-source-Y-test = False
[experiments_sandbox.py:862 -   <module>()] 2023-04-25 20:52:07,839 INFO train-dataset = ***
VDP Dataset
N=1000
mio = 0.5
x_gen_norm_mean = 0
x_gen_norm_std = 1
normalize_X = False
normalize_Y = False
train-or-test = train
***
[experiments_sandbox.py:863 -   <module>()] 2023-04-25 20:52:07,839 INFO test-dataset = ***
VDP Dataset
N=1000
mio = 0.5
x_gen_norm_mean = 0
x_gen_norm_std = 1
normalize_X = False
normalize_Y = False
train-or-test = test
***
[experiments_sandbox.py:864 -   <module>()] 2023-04-25 20:52:07,839 INFO train-epochs = 1000
[experiments_sandbox.py:868 -   <module>()] 2023-04-25 20:52:07,839 INFO Input batch normalization = False
[experiments_sandbox.py:869 -   <module>()] 2023-04-25 20:52:07,839 INFO Output Normalization = None
[experiments_sandbox.py:870 -   <module>()] 2023-04-25 20:52:07,839 INFO Gradient-clipping max-norm = 10
[experiments_sandbox.py:872 -   <module>()] 2023-04-25 20:52:07,839 INFO epochs_losses_window = 10
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:09,461 INFO epoch # 0 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.7845972343347967
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:09,520 INFO epoch # 1 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.32918681227602065
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:09,580 INFO epoch # 2 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.11462254083016887
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:09,643 INFO epoch # 3 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.057748988387174904
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:09,703 INFO epoch # 4 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.053336434153607115
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:09,763 INFO epoch # 5 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.04979234840720892
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:09,822 INFO epoch # 6 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.03734869114123285
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:09,886 INFO epoch # 7 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.0468926663743332
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:09,946 INFO epoch # 8 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.08903449692297727
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,006 INFO epoch # 9 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.046887561518815346
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,066 INFO epoch # 10 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.03821030430844985
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:10,066 INFO *** epoch 10, rolling-avg-loss (window=10)= 0.0863060844319989
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,133 INFO epoch # 11 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.03785893990425393
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,193 INFO epoch # 12 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.05026848876150325
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,253 INFO epoch # 13 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.0487767455924768
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,313 INFO epoch # 14 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.040818065899657086
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,376 INFO epoch # 15 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.04159871871524956
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,436 INFO epoch # 16 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.032240837696008384
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,497 INFO epoch # 17 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.029088777140714228
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,556 INFO epoch # 18 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.02485276691731997
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,621 INFO epoch # 19 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.031878169771516696
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,682 INFO epoch # 20 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.03767667425563559
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:10,683 INFO *** epoch 20, rolling-avg-loss (window=10)= 0.03750581846543355
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,744 INFO epoch # 21 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.057678935409057885
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,804 INFO epoch # 22 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.025040188804268837
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,868 INFO epoch # 23 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.06844799852115102
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,930 INFO epoch # 24 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.045858932280680165
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:10,991 INFO epoch # 25 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.03516502189449966
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,051 INFO epoch # 26 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.02296111396572087
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,115 INFO epoch # 27 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.018130770040443167
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,176 INFO epoch # 28 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.017688639054540545
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,237 INFO epoch # 29 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.029513705158024095
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,296 INFO epoch # 30 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.028148390498245135
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:11,296 INFO *** epoch 30, rolling-avg-loss (window=10)= 0.034863369562663137
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,360 INFO epoch # 31 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.08969443396199495
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,421 INFO epoch # 32 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.15506368305068463
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,481 INFO epoch # 33 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.08517168927937746
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,540 INFO epoch # 34 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.05704141352180159
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,604 INFO epoch # 35 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.022247867345868144
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,664 INFO epoch # 36 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.026137540451600216
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,725 INFO epoch # 37 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.01723238431441132
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,785 INFO epoch # 38 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.018755987854092382
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,847 INFO epoch # 39 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.030253901124524418
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,907 INFO epoch # 40 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.020069133839569986
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:11,908 INFO *** epoch 40, rolling-avg-loss (window=10)= 0.05216680347439251
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:11,968 INFO epoch # 41 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.00912754646196845
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,028 INFO epoch # 42 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.016251617911620997
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,092 INFO epoch # 43 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.023060661304043606
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,152 INFO epoch # 44 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.02260520079289563
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,213 INFO epoch # 45 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.02729540236759931
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,274 INFO epoch # 46 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.015260497115377802
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,337 INFO epoch # 47 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.009343315032310784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,397 INFO epoch # 48 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.017149879611679353
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,457 INFO epoch # 49 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.01275272794009652
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,516 INFO epoch # 50 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.014293472733697854
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:12,516 INFO *** epoch 50, rolling-avg-loss (window=10)= 0.01671403212712903
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,580 INFO epoch # 51 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 0.011186587769770995
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,641 INFO epoch # 52 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.08 -loss = 0.03039901527517941
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,702 INFO epoch # 53 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.009859169693299918
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,761 INFO epoch # 54 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.008392667743464699
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,825 INFO epoch # 55 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.00894366927241208
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,885 INFO epoch # 56 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.008627282313682372
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:12,945 INFO epoch # 57 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.007118783272744622
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,005 INFO epoch # 58 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.02950241769576678
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,067 INFO epoch # 59 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.020167199945717584
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,130 INFO epoch # 60 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.009975774606573395
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:13,130 INFO *** epoch 60, rolling-avg-loss (window=10)= 0.014417256758861186
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,190 INFO epoch # 61 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.006667872155958321
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,249 INFO epoch # 62 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.009522424639726523
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,313 INFO epoch # 63 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.0238708748947829
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,373 INFO epoch # 64 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.009970549937861506
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,433 INFO epoch # 65 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.010099403676576912
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,492 INFO epoch # 66 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.010651454344042577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,555 INFO epoch # 67 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.007186971095507033
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,617 INFO epoch # 68 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.006347679231112124
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,677 INFO epoch # 69 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.022045084959245287
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,739 INFO epoch # 70 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.020699804241303355
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:13,739 INFO *** epoch 70, rolling-avg-loss (window=10)= 0.012706211917611654
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,803 INFO epoch # 71 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.007095737655617995
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,864 INFO epoch # 72 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.011067892461142037
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,925 INFO epoch # 73 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.011537658832821762
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:13,984 INFO epoch # 74 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.005573123824433424
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,048 INFO epoch # 75 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.008663927226734813
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,111 INFO epoch # 76 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.029089054893120192
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,172 INFO epoch # 77 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.014630911071435548
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,232 INFO epoch # 78 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.015967980652931146
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,296 INFO epoch # 79 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.0125454964581877
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,356 INFO epoch # 80 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.01120122572319815
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:14,356 INFO *** epoch 80, rolling-avg-loss (window=10)= 0.012737300879962277
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,417 INFO epoch # 81 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.01019435244234046
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,476 INFO epoch # 82 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.00487488640283118
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,540 INFO epoch # 83 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.004629090357411769
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,604 INFO epoch # 84 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.005384656215028372
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,664 INFO epoch # 85 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.006371087911247741
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,725 INFO epoch # 86 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.0059483706045284634
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,788 INFO epoch # 87 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.020849938635365106
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,848 INFO epoch # 88 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.010543960474024061
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,908 INFO epoch # 89 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.010168696520850062
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:14,967 INFO epoch # 90 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.009215197700541466
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:14,967 INFO *** epoch 90, rolling-avg-loss (window=10)= 0.008818023726416869
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,031 INFO epoch # 91 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.01592799992795335
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,093 INFO epoch # 92 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.010865928081329912
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,154 INFO epoch # 93 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 0.013955719317891635
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,214 INFO epoch # 94 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.064 -loss = 0.011564441658265423
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,277 INFO epoch # 95 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.004183571305475198
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,337 INFO epoch # 96 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.0031219901402437245
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,398 INFO epoch # 97 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.0029733679875789676
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,457 INFO epoch # 98 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.005034979456468136
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,520 INFO epoch # 99 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.022664971518679522
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,580 INFO epoch # 100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.022045285746571608
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:15,581 INFO *** epoch 100, rolling-avg-loss (window=10)= 0.011233825514045748
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,642 INFO epoch # 101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.018345116011914797
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,702 INFO epoch # 102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.00810534544143593
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,766 INFO epoch # 103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.004687119442678522
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,827 INFO epoch # 104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.008979162346804515
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,887 INFO epoch # 105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.008279225578007754
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:15,947 INFO epoch # 106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.004968935238139238
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,011 INFO epoch # 107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 0.003951864253394888
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,071 INFO epoch # 108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.0512 -loss = 0.004336530490036239
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,132 INFO epoch # 109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.002429473228403367
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,192 INFO epoch # 110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0033291348827333422
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:16,192 INFO *** epoch 110, rolling-avg-loss (window=10)= 0.006741190691354859
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,258 INFO epoch # 111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0022232814890230657
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,319 INFO epoch # 112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.003683292594359955
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,379 INFO epoch # 113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0023726603903924115
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,440 INFO epoch # 114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0029898609482188476
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,504 INFO epoch # 115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0032012816627684515
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,564 INFO epoch # 116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.003521433402056573
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,634 INFO epoch # 117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.005462220142362639
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,694 INFO epoch # 118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0043219412727921735
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,758 INFO epoch # 119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0031146675173658878
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,819 INFO epoch # 120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.004166573202383006
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:16,820 INFO *** epoch 120, rolling-avg-loss (window=10)= 0.003505721262172301
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,879 INFO epoch # 121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.002712764662646805
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:16,939 INFO epoch # 122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0017222539163412875
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,003 INFO epoch # 123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0021158760919206543
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,064 INFO epoch # 124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0034521674388088286
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,125 INFO epoch # 125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0035729863957385533
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,184 INFO epoch # 126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.008257414312538458
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,251 INFO epoch # 127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0022827016036899295
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,311 INFO epoch # 128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.002097951482028293
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,371 INFO epoch # 129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.0015392026471090503
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,431 INFO epoch # 130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.001883410865048063
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:17,431 INFO *** epoch 130, rolling-avg-loss (window=10)= 0.002963672941586992
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,496 INFO epoch # 131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.004166152104517096
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,557 INFO epoch # 132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.005071434537967434
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,618 INFO epoch # 133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.004569912711303914
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,678 INFO epoch # 134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.004886129905571579
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,743 INFO epoch # 135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.011224253532418516
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,805 INFO epoch # 136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.004243063805915881
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,864 INFO epoch # 137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.004972134443960385
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,924 INFO epoch # 138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.028888505956274457
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:17,989 INFO epoch # 139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.008266271892352961
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,049 INFO epoch # 140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.04096 -loss = 0.008360380554222502
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:18,049 INFO *** epoch 140, rolling-avg-loss (window=10)= 0.008464823944450472
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,109 INFO epoch # 141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.009568271569150966
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,169 INFO epoch # 142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.005653387492202455
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,235 INFO epoch # 143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.003780664794248878
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,296 INFO epoch # 144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.004346563557191985
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,355 INFO epoch # 145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.004929072620143415
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,415 INFO epoch # 146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.0034883224125223933
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,480 INFO epoch # 147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.0038887124064785894
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,540 INFO epoch # 148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.0027825248398585245
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,601 INFO epoch # 149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.0033657921667327173
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,661 INFO epoch # 150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.002721776345424587
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:18,661 INFO *** epoch 150, rolling-avg-loss (window=10)= 0.004452508820395451
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,728 INFO epoch # 151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.032768 -loss = 0.0023163299601947074
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,788 INFO epoch # 152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.0025502591670374386
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,848 INFO epoch # 153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.0034859914412663784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,908 INFO epoch # 154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.0017862557288026437
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:18,973 INFO epoch # 155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.0015543801127932966
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,033 INFO epoch # 156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.0015425900828631711
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,093 INFO epoch # 157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.001513965811682283
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,152 INFO epoch # 158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.0019525258485373342
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,217 INFO epoch # 159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.0016421777954747085
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,278 INFO epoch # 160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.002186072453696397
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:19,279 INFO *** epoch 160, rolling-avg-loss (window=10)= 0.002053054840234836
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,339 INFO epoch # 161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.0032233599849860184
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,399 INFO epoch # 162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.007057994116621558
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,464 INFO epoch # 163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.002681722044144408
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,524 INFO epoch # 164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.002464478551701177
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,586 INFO epoch # 165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.004331059892138001
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,646 INFO epoch # 166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.002759580214842572
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,709 INFO epoch # 167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.0018414091746308259
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,770 INFO epoch # 168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.0262144 -loss = 0.0033902501872944413
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,830 INFO epoch # 169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.003673806437291205
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,890 INFO epoch # 170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0013983497956360225
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:19,890 INFO *** epoch 170, rolling-avg-loss (window=10)= 0.0032822010399286227
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:19,954 INFO epoch # 171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.001491350982178119
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,016 INFO epoch # 172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0010691672814573394
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,077 INFO epoch # 173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0013711618648812873
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,137 INFO epoch # 174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0019691241559485206
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,201 INFO epoch # 175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0018290043817614787
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,262 INFO epoch # 176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0019023441118406481
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,323 INFO epoch # 177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.001168989937468723
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,383 INFO epoch # 178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0010715232174334233
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,446 INFO epoch # 179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0017374761246173875
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,506 INFO epoch # 180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0011631054621830117
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:20,507 INFO *** epoch 180, rolling-avg-loss (window=10)= 0.0014773247519769938
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,568 INFO epoch # 181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0014929291646694764
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,628 INFO epoch # 182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.0020032767788507044
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,693 INFO epoch # 183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.02097152 -loss = 0.0014934630253264913
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,754 INFO epoch # 184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.00108596696099994
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,814 INFO epoch # 185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.00110155982656579
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,874 INFO epoch # 186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0010853018411580706
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:20,939 INFO epoch # 187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0009009023115140735
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,001 INFO epoch # 188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0009020247903208656
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,061 INFO epoch # 189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0010289173787896289
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,122 INFO epoch # 190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0010787643341245712
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:21,122 INFO *** epoch 190, rolling-avg-loss (window=10)= 0.0012173106412319613
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,188 INFO epoch # 191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0008863430011842865
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,250 INFO epoch # 192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0009414667647433816
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,311 INFO epoch # 193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0017992065750149777
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,371 INFO epoch # 194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0020221411614329554
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,435 INFO epoch # 195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0011785499218603945
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,496 INFO epoch # 196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0012105618297937326
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,557 INFO epoch # 197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0012136952891523833
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,618 INFO epoch # 198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0014366135646923794
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,683 INFO epoch # 199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.003111671470833244
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,745 INFO epoch # 200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0015777999087731587
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:21,745 INFO *** epoch 200, rolling-avg-loss (window=10)= 0.0015378049487480895
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,807 INFO epoch # 201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.001327077735822968
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,867 INFO epoch # 202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.01677722 -loss = 0.0012086917631677352
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,932 INFO epoch # 203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.001138304209234775
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:21,993 INFO epoch # 204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0010740081520452804
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,055 INFO epoch # 205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0006045252553121827
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,116 INFO epoch # 206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0008564904278500762
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,180 INFO epoch # 207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0012230368110976997
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,242 INFO epoch # 208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0010355964786867844
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,303 INFO epoch # 209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0010935101954601123
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,365 INFO epoch # 210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.001247743808562518
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:22,365 INFO *** epoch 210, rolling-avg-loss (window=10)= 0.0010808984837240132
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,429 INFO epoch # 211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0007281787525243999
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,491 INFO epoch # 212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0009522653263047687
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,552 INFO epoch # 213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0010362209286540747
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,615 INFO epoch # 214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0006068369839340448
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,682 INFO epoch # 215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.0007344088935496984
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,747 INFO epoch # 216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01342177 -loss = 0.0016846033231558977
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,810 INFO epoch # 217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0011810819432866992
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,872 INFO epoch # 218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0009090067896977416
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:22,937 INFO epoch # 219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0007740249166090507
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,000 INFO epoch # 220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0006007372148815193
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:23,000 INFO *** epoch 220, rolling-avg-loss (window=10)= 0.0009207365072597895
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,062 INFO epoch # 221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0007098248470356339
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,125 INFO epoch # 222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0008735502610761614
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,191 INFO epoch # 223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0007300984034372959
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,254 INFO epoch # 224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0008715661101632577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,317 INFO epoch # 225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0005235056100900692
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,379 INFO epoch # 226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0006106223950155254
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,445 INFO epoch # 227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0008421705222190212
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,509 INFO epoch # 228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0014908138164173579
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,571 INFO epoch # 229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0011118131383227592
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,634 INFO epoch # 230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.000986566762549046
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:23,634 INFO *** epoch 230, rolling-avg-loss (window=10)= 0.0008750531866326128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,700 INFO epoch # 231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0007025158065516734
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,764 INFO epoch # 232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0006392308323484031
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,826 INFO epoch # 233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.000740221236810612
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,888 INFO epoch # 234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0009286811491620028
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:23,954 INFO epoch # 235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0012791316103175632
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,017 INFO epoch # 236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01073742 -loss = 0.0010855332834580622
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,081 INFO epoch # 237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0007244961661854177
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,143 INFO epoch # 238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0006921802469150862
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,209 INFO epoch # 239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0007937575346659287
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,271 INFO epoch # 240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0009094960951188114
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:24,272 INFO *** epoch 240, rolling-avg-loss (window=10)= 0.000849524396153356
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,334 INFO epoch # 241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0007575074223495903
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,397 INFO epoch # 242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0006425113911063818
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,462 INFO epoch # 243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.00045097263500792906
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,525 INFO epoch # 244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0006376481148890889
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,589 INFO epoch # 245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0009029466091305949
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,652 INFO epoch # 246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.000523646493547858
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,718 INFO epoch # 247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.00047074475605768384
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,781 INFO epoch # 248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0005236138754298736
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,844 INFO epoch # 249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0007975353428264498
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,906 INFO epoch # 250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0008437333385700185
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:24,906 INFO *** epoch 250, rolling-avg-loss (window=10)= 0.0006550859978915469
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:24,972 INFO epoch # 251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0010209473211943987
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,036 INFO epoch # 252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0006892537530802656
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,098 INFO epoch # 253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0007012555715846247
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,161 INFO epoch # 254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.00858993 -loss = 0.0006129380617494462
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,227 INFO epoch # 255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0005718975210129429
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,290 INFO epoch # 256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0005428403769656143
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,353 INFO epoch # 257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0006650355549027154
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,415 INFO epoch # 258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0006614654521399643
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,480 INFO epoch # 259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0005656775547322468
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,543 INFO epoch # 260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0006718402250953659
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:25,543 INFO *** epoch 260, rolling-avg-loss (window=10)= 0.0006703151392457585
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,607 INFO epoch # 261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0005900737687625224
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,670 INFO epoch # 262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0005810632087559497
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,736 INFO epoch # 263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.000522912956057553
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,797 INFO epoch # 264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0004354296529527346
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,856 INFO epoch # 265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0005348807148948254
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,917 INFO epoch # 266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0008105593615255202
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:25,984 INFO epoch # 267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0006867203837828129
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,048 INFO epoch # 268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0005536453995773627
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,111 INFO epoch # 269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.000514119262334134
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,173 INFO epoch # 270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0005698736140402616
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:26,174 INFO *** epoch 270, rolling-avg-loss (window=10)= 0.0005799278322683677
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,240 INFO epoch # 271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0005145725981492433
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,303 INFO epoch # 272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0006957894452170876
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,366 INFO epoch # 273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0018154032989059488
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,429 INFO epoch # 274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0006319293456726882
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,494 INFO epoch # 275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00687195 -loss = 0.0004956643142577377
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,557 INFO epoch # 276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0006834409323346335
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,620 INFO epoch # 277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0005635432903545734
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,683 INFO epoch # 278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0004115805754736357
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,750 INFO epoch # 279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0004337076663887274
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,813 INFO epoch # 280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0006330596515908837
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:26,813 INFO *** epoch 280, rolling-avg-loss (window=10)= 0.000687869111834516
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,876 INFO epoch # 281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0006404439441212162
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:26,938 INFO epoch # 282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0004896804102827446
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,005 INFO epoch # 283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.000384775197289855
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,069 INFO epoch # 284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0004086357766936999
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,133 INFO epoch # 285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0006928511534169957
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,195 INFO epoch # 286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.00044776906270271866
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,261 INFO epoch # 287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0004351128482085187
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,325 INFO epoch # 288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.00040213167176261777
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,388 INFO epoch # 289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.000494865810196643
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,450 INFO epoch # 290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.00036397247799868637
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:27,450 INFO *** epoch 290, rolling-avg-loss (window=10)= 0.0004760238352673696
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,517 INFO epoch # 291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0004849030278819555
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,582 INFO epoch # 292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0005141234887560131
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,644 INFO epoch # 293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0004168284951902024
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,707 INFO epoch # 294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0005339272674973472
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,773 INFO epoch # 295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0004906729946014821
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,836 INFO epoch # 296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.00042280849083908834
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,898 INFO epoch # 297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0004369481025605637
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:27,960 INFO epoch # 298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0003792859406530624
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,026 INFO epoch # 299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.00036589483033822034
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,091 INFO epoch # 300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.00044337275039652013
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:28,091 INFO *** epoch 300, rolling-avg-loss (window=10)= 0.00044887653887144553
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,154 INFO epoch # 301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00549756 -loss = 0.0004933565282954078
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,216 INFO epoch # 302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0004271034110843175
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,283 INFO epoch # 303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0004973346685801516
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,347 INFO epoch # 304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0004693867585956468
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,410 INFO epoch # 305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0005335077951258427
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,472 INFO epoch # 306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0005536384333026945
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,538 INFO epoch # 307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0003568469953734166
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,603 INFO epoch # 308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00033477879105703323
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,668 INFO epoch # 309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.000559579065793514
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,731 INFO epoch # 310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00041046030833058467
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:28,731 INFO *** epoch 310, rolling-avg-loss (window=10)= 0.00046359927555386096
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,801 INFO epoch # 311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00033509906438666803
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,866 INFO epoch # 312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00035655529018185916
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,929 INFO epoch # 313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0004105026691831881
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:28,991 INFO epoch # 314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00036181806626700563
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,060 INFO epoch # 315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0003980261681135744
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,125 INFO epoch # 316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00046044048121984815
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,189 INFO epoch # 317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0005406537427461444
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,252 INFO epoch # 318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00037023877257524873
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,318 INFO epoch # 319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00032974853195355536
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,382 INFO epoch # 320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0003620905849857081
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:29,382 INFO *** epoch 320, rolling-avg-loss (window=10)= 0.00039251733716128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,445 INFO epoch # 321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00044168369413455366
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,507 INFO epoch # 322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00042116888039345213
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,577 INFO epoch # 323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00040843472925189417
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,642 INFO epoch # 324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0004184067749974929
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,705 INFO epoch # 325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00032511630888620857
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,767 INFO epoch # 326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0003200661958544515
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,835 INFO epoch # 327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0007310893747671798
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,898 INFO epoch # 328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0006700950589220156
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:29,961 INFO epoch # 329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0005474641247928957
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,023 INFO epoch # 330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0003446767063906009
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:30,023 INFO *** epoch 330, rolling-avg-loss (window=10)= 0.0004628201848390745
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,091 INFO epoch # 331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0007156359529290057
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,156 INFO epoch # 332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.000724477943549573
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,220 INFO epoch # 333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0005154155892341805
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,281 INFO epoch # 334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.000656094735404622
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,347 INFO epoch # 335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00040348942866330617
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,411 INFO epoch # 336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.00038273988525361347
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,474 INFO epoch # 337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00439805 -loss = 0.0003712591399107623
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,536 INFO epoch # 338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.00030996525151749665
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,602 INFO epoch # 339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0003435880737470143
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,665 INFO epoch # 340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0003317879078394981
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:30,666 INFO *** epoch 340, rolling-avg-loss (window=10)= 0.0004754453908049072
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,729 INFO epoch # 341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0002850329626653547
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,792 INFO epoch # 342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0003366537746387621
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,858 INFO epoch # 343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.00031637901474823593
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,922 INFO epoch # 344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0003238067117763421
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:30,985 INFO epoch # 345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.00027744378394345404
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,048 INFO epoch # 346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.00040045337686933635
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,114 INFO epoch # 347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.00039514121203865216
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,178 INFO epoch # 348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.00027943134136876324
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,241 INFO epoch # 349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.000326362623809473
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,304 INFO epoch # 350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.00033621938177930133
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:31,304 INFO *** epoch 350, rolling-avg-loss (window=10)= 0.0003276924183637675
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,371 INFO epoch # 351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.00032834788635227596
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,435 INFO epoch # 352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.00034310644855395367
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,498 INFO epoch # 353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0006769382439415494
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,560 INFO epoch # 354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.00035549014069147233
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,628 INFO epoch # 355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.00035623268081508286
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,691 INFO epoch # 356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00351844 -loss = 0.00041472452664947923
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,754 INFO epoch # 357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.00029360942244238686
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,817 INFO epoch # 358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0003665162703327951
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,885 INFO epoch # 359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0002880577417272434
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:31,948 INFO epoch # 360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0003088848384322773
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:31,949 INFO *** epoch 360, rolling-avg-loss (window=10)= 0.0003731908199938516
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,012 INFO epoch # 361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0002661416615410417
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,074 INFO epoch # 362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.00025275271195823734
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,142 INFO epoch # 363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0003163133974339871
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,206 INFO epoch # 364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.00038395287538151024
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,269 INFO epoch # 365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0003187663583048561
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,332 INFO epoch # 366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.00029720615225414804
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,397 INFO epoch # 367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.00030257896605689893
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,461 INFO epoch # 368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.00036985004180678516
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,523 INFO epoch # 369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.00026713486454355007
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,588 INFO epoch # 370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.00033289754628640367
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:32,588 INFO *** epoch 370, rolling-avg-loss (window=10)= 0.0003107594575567418
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,654 INFO epoch # 371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.00031183777036858373
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,718 INFO epoch # 372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.00028743662119268265
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,780 INFO epoch # 373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00281475 -loss = 0.00028420372791515547
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,843 INFO epoch # 374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0002822978649419383
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,909 INFO epoch # 375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.000297858391832051
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:32,971 INFO epoch # 376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.00028028178712702356
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,034 INFO epoch # 377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0002449128738817308
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,098 INFO epoch # 378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.000259630365007979
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,164 INFO epoch # 379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0003139395403195522
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,227 INFO epoch # 380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0002674452562132501
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:33,227 INFO *** epoch 380, rolling-avg-loss (window=10)= 0.00028298441987999466
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,290 INFO epoch # 381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.00024224357480306935
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,352 INFO epoch # 382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.00028502142822617316
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,419 INFO epoch # 383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.00028147201078354556
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,482 INFO epoch # 384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.00032855595691216877
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,546 INFO epoch # 385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.00031337527047980984
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,610 INFO epoch # 386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.000349319935821768
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,676 INFO epoch # 387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0003014704107044963
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,740 INFO epoch # 388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0002563185589679051
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,803 INFO epoch # 389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.00025988602396864735
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,866 INFO epoch # 390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0002585144011391094
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:33,867 INFO *** epoch 390, rolling-avg-loss (window=10)= 0.0002876177571806693
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,933 INFO epoch # 391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0002594334345076277
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:33,997 INFO epoch # 392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.0022518 -loss = 0.00026900432590082346
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,060 INFO epoch # 393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002427343208637467
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,124 INFO epoch # 394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00026105801089215674
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,190 INFO epoch # 395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0003067907532567915
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,252 INFO epoch # 396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002621256612655998
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,315 INFO epoch # 397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00027736076958717604
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,377 INFO epoch # 398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002343615794870857
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,439 INFO epoch # 399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002750286691934889
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,500 INFO epoch # 400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002953401262857369
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:34,500 INFO *** epoch 400, rolling-avg-loss (window=10)= 0.00026832376512402333
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,561 INFO epoch # 401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0003610973831200681
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,624 INFO epoch # 402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002351307724666185
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,686 INFO epoch # 403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002895279719723476
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,748 INFO epoch # 404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00022930231284590263
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,809 INFO epoch # 405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00024227640926710592
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,870 INFO epoch # 406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00024707404008950107
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,931 INFO epoch # 407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00026966281961904315
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:34,993 INFO epoch # 408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00024101503959172987
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,055 INFO epoch # 409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002738550861067779
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,117 INFO epoch # 410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002595653761545691
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:35,117 INFO *** epoch 410, rolling-avg-loss (window=10)= 0.0002648507211233664
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,178 INFO epoch # 411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00022848246339890466
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,239 INFO epoch # 412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00025552969418640714
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,302 INFO epoch # 413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00023364302614936605
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,363 INFO epoch # 414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002868733163268189
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,425 INFO epoch # 415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00028695193395833485
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,487 INFO epoch # 416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002786351662962261
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,551 INFO epoch # 417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.000245685427898934
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,614 INFO epoch # 418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00034160991526732687
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,676 INFO epoch # 419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00024213992264776607
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,737 INFO epoch # 420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.00025012109836097807
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:35,738 INFO *** epoch 420, rolling-avg-loss (window=10)= 0.0002649671964491063
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,799 INFO epoch # 421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.0002356620409500465
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,860 INFO epoch # 422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.00180144 -loss = 0.0003199308898729214
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,924 INFO epoch # 423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.00022087281467975117
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:35,986 INFO epoch # 424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.00019182258586170065
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,048 INFO epoch # 425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0002118576346674672
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,111 INFO epoch # 426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0002232037679732457
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,173 INFO epoch # 427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0002047133704081716
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,234 INFO epoch # 428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0002331636503640766
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,296 INFO epoch # 429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.00020359495943012007
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,358 INFO epoch # 430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.00019761068324442022
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:36,358 INFO *** epoch 430, rolling-avg-loss (window=10)= 0.00022424323974519212
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,420 INFO epoch # 431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0002188952651067666
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,483 INFO epoch # 432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.0001988473871961105
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,545 INFO epoch # 433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.00038011281731087365
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,607 INFO epoch # 434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.00021924402960848965
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,670 INFO epoch # 435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00144115 -loss = 0.00025149244459043985
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,732 INFO epoch # 436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00022852797235373146
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,794 INFO epoch # 437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0002153934940452018
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,856 INFO epoch # 438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00022056076670651237
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,919 INFO epoch # 439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0002374474536281923
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:36,981 INFO epoch # 440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.000267091226987759
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:36,982 INFO *** epoch 440, rolling-avg-loss (window=10)= 0.00024376128575340772
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,043 INFO epoch # 441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00019083942606812343
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,105 INFO epoch # 442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00019032437921850942
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,167 INFO epoch # 443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00018281878055859124
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,229 INFO epoch # 444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00023351481922873063
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,291 INFO epoch # 445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0002522833869988972
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,353 INFO epoch # 446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0001976841331270407
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,415 INFO epoch # 447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00020541177468658134
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,477 INFO epoch # 448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00022293694212294213
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,540 INFO epoch # 449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0002465436707552726
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,601 INFO epoch # 450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00021883001977585081
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:37,602 INFO *** epoch 450, rolling-avg-loss (window=10)= 0.00021411873325405395
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,665 INFO epoch # 451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00019192436673165503
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,727 INFO epoch # 452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.00021615245282191609
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,789 INFO epoch # 453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.0002203503047439881
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,852 INFO epoch # 454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00115292 -loss = 0.00022393852049162888
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,915 INFO epoch # 455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0001934003325914091
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:37,978 INFO epoch # 456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00018756184874746396
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,040 INFO epoch # 457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00020473716540436726
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,102 INFO epoch # 458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00019125885830817424
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,166 INFO epoch # 459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0001835839107116044
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,228 INFO epoch # 460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00020487705842242576
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:38,228 INFO *** epoch 460, rolling-avg-loss (window=10)= 0.0002017784818974633
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,290 INFO epoch # 461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0001767303449469182
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,353 INFO epoch # 462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00018758900080229068
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,416 INFO epoch # 463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00019233686691677576
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,479 INFO epoch # 464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0002078064646866551
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,542 INFO epoch # 465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0001887746052489092
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,612 INFO epoch # 466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00020221596616920579
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,677 INFO epoch # 467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.0001873973677675167
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,739 INFO epoch # 468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00020664278133608605
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,804 INFO epoch # 469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00019537885248155362
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,866 INFO epoch # 470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00019541302697234642
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:38,866 INFO *** epoch 470, rolling-avg-loss (window=10)= 0.00019402852773282576
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,930 INFO epoch # 471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.00019490370175390126
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:38,993 INFO epoch # 472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.001 -loss = 0.00018251708422667434
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,062 INFO epoch # 473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002254945388813212
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,126 INFO epoch # 474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001893556197956059
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,190 INFO epoch # 475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018465316964011436
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,253 INFO epoch # 476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020477355815273768
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,316 INFO epoch # 477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017999211127062154
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,378 INFO epoch # 478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017995661380609818
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,442 INFO epoch # 479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017374312790252588
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,504 INFO epoch # 480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001841399261479637
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:39,504 INFO *** epoch 480, rolling-avg-loss (window=10)= 0.0001899529451577564
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,567 INFO epoch # 481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018050883556952613
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,630 INFO epoch # 482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018994715549069952
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,694 INFO epoch # 483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017457714830015902
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,758 INFO epoch # 484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021264139877530397
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,820 INFO epoch # 485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019057988947679405
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,883 INFO epoch # 486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017544002650993207
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:39,951 INFO epoch # 487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017951364975488104
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,013 INFO epoch # 488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020724077523937012
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,076 INFO epoch # 489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019804210160145885
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,142 INFO epoch # 490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020037351441715145
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:40,142 INFO *** epoch 490, rolling-avg-loss (window=10)= 0.0001908864495135276
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,207 INFO epoch # 491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020632549296806246
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,270 INFO epoch # 492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020386984817832854
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,332 INFO epoch # 493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002043414669969934
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,394 INFO epoch # 494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001933790051680262
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,459 INFO epoch # 495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019766602554227575
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,522 INFO epoch # 496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018758392570816795
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,586 INFO epoch # 497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002045405035460135
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,648 INFO epoch # 498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018958245772182636
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,715 INFO epoch # 499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019515021676852484
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,778 INFO epoch # 500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001807370134656594
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:40,778 INFO *** epoch 500, rolling-avg-loss (window=10)= 0.00019631759560638783
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,841 INFO epoch # 501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001840168187072777
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,906 INFO epoch # 502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018798025917021732
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:40,969 INFO epoch # 503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017495335043804516
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,032 INFO epoch # 504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018588938667107868
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,095 INFO epoch # 505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001759061688062502
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,157 INFO epoch # 506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019423953472141875
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,220 INFO epoch # 507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00022422123970500252
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,282 INFO epoch # 508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001814681298810683
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,344 INFO epoch # 509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018299051134818
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,407 INFO epoch # 510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001853592945053606
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:41,407 INFO *** epoch 510, rolling-avg-loss (window=10)= 0.00018770246939538992
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,469 INFO epoch # 511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017560243350089877
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,532 INFO epoch # 512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018375543595539057
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,595 INFO epoch # 513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018883232303323894
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,658 INFO epoch # 514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001791764017298192
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,721 INFO epoch # 515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017385738931352535
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,783 INFO epoch # 516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017792781136449776
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,846 INFO epoch # 517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018039056840279954
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,909 INFO epoch # 518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018450051720719784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:41,975 INFO epoch # 519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019798823620931216
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,037 INFO epoch # 520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018953669859911315
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:42,037 INFO *** epoch 520, rolling-avg-loss (window=10)= 0.00018315678153157933
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,100 INFO epoch # 521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017323561439752666
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,163 INFO epoch # 522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001853298634841849
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,225 INFO epoch # 523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019141703330660675
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,288 INFO epoch # 524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017748778077475436
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,350 INFO epoch # 525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018525510722611216
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,413 INFO epoch # 526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019304865759295353
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,476 INFO epoch # 527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001854338584053039
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,539 INFO epoch # 528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001792326299892011
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,601 INFO epoch # 529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019064055231865495
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,664 INFO epoch # 530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020276071370517457
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:42,664 INFO *** epoch 530, rolling-avg-loss (window=10)= 0.0001863841811200473
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,727 INFO epoch # 531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001850721062055527
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,790 INFO epoch # 532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001923014689282354
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,854 INFO epoch # 533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018623483197188762
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,917 INFO epoch # 534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018719251374932355
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:42,984 INFO epoch # 535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018937471236313286
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,046 INFO epoch # 536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018499986583719874
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,110 INFO epoch # 537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018169169720749778
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,172 INFO epoch # 538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018786857901886833
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,236 INFO epoch # 539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020057098731740552
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,299 INFO epoch # 540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019570911501887167
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:43,299 INFO *** epoch 540, rolling-avg-loss (window=10)= 0.0001891015877617974
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,363 INFO epoch # 541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001625041685429096
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,426 INFO epoch # 542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020967078648936877
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,489 INFO epoch # 543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002588973140973394
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,552 INFO epoch # 544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018104207975966347
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,615 INFO epoch # 545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017640568148635793
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,678 INFO epoch # 546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018282121237689353
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,742 INFO epoch # 547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017847506637735933
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,805 INFO epoch # 548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020758860978276061
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,868 INFO epoch # 549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018747493095361278
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,931 INFO epoch # 550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017046780294549535
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:43,931 INFO *** epoch 550, rolling-avg-loss (window=10)= 0.0001915347652811761
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:43,999 INFO epoch # 551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016932885887399607
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,062 INFO epoch # 552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002094243769761306
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,126 INFO epoch # 553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001844475403913748
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,189 INFO epoch # 554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015698867366609193
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,253 INFO epoch # 555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017491348148723773
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,317 INFO epoch # 556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017426969196776554
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,380 INFO epoch # 557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017631653031457972
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,444 INFO epoch # 558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001623260124006265
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,507 INFO epoch # 559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001653725752248647
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,570 INFO epoch # 560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019872125130859786
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:44,570 INFO *** epoch 560, rolling-avg-loss (window=10)= 0.00017721089926112655
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,634 INFO epoch # 561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001713955739433004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,697 INFO epoch # 562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018065359813590476
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,761 INFO epoch # 563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001868701390321803
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,825 INFO epoch # 564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018639232189343602
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,888 INFO epoch # 565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001916208261718566
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:44,953 INFO epoch # 566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00021657440026956465
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,019 INFO epoch # 567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002576955184849794
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,083 INFO epoch # 568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001740379259445035
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,146 INFO epoch # 569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001642895850864079
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,210 INFO epoch # 570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001760635607297445
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:45,210 INFO *** epoch 570, rolling-avg-loss (window=10)= 0.0001905593449691878
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,274 INFO epoch # 571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001800583631847985
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,338 INFO epoch # 572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001687090818904835
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,402 INFO epoch # 573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023497651113757456
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,465 INFO epoch # 574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002806034736977381
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,529 INFO epoch # 575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017355427939946821
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,593 INFO epoch # 576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001834342169786396
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,657 INFO epoch # 577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017516487412194692
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,720 INFO epoch # 578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016617872643109877
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,784 INFO epoch # 579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017774661307612405
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,847 INFO epoch # 580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016385531580453971
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:45,848 INFO *** epoch 580, rolling-avg-loss (window=10)= 0.0001904281455722412
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,911 INFO epoch # 581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017412209922440525
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:45,976 INFO epoch # 582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002036761818544619
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,039 INFO epoch # 583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00023608569324551354
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,104 INFO epoch # 584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016996883834963228
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,167 INFO epoch # 585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017545992943723832
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,231 INFO epoch # 586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002127437114722852
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,294 INFO epoch # 587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016030466224492557
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,358 INFO epoch # 588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015950802230690897
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,422 INFO epoch # 589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015204406452085095
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,485 INFO epoch # 590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015635770046174002
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:46,485 INFO *** epoch 590, rolling-avg-loss (window=10)= 0.0001800270903117962
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,548 INFO epoch # 591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017666488940903946
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,613 INFO epoch # 592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001788374285069949
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,677 INFO epoch # 593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001660036786006458
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,741 INFO epoch # 594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001640484177869439
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,805 INFO epoch # 595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015453942012300104
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,869 INFO epoch # 596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015748158602946205
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,932 INFO epoch # 597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001745703309552482
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:46,996 INFO epoch # 598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016548271526062308
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,060 INFO epoch # 599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016629648644084227
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,126 INFO epoch # 600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019289821318579925
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:47,126 INFO *** epoch 600, rolling-avg-loss (window=10)= 0.00016968231662986
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,197 INFO epoch # 601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000188253870646804
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,261 INFO epoch # 602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001943377419593162
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,325 INFO epoch # 603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000193450766460046
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,389 INFO epoch # 604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016451505382519827
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,452 INFO epoch # 605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016143091988851666
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,516 INFO epoch # 606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019872298082646012
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,580 INFO epoch # 607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016653709315050946
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,644 INFO epoch # 608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016078554835985415
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,707 INFO epoch # 609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017687899367047066
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,771 INFO epoch # 610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002068776248620452
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:47,772 INFO *** epoch 610, rolling-avg-loss (window=10)= 0.00018117905936492206
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,836 INFO epoch # 611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001793059049077783
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,900 INFO epoch # 612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016253381488695595
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:47,964 INFO epoch # 613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017332411266579584
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,029 INFO epoch # 614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018540322412263777
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,093 INFO epoch # 615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001731321175384437
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,157 INFO epoch # 616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017193733515341592
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,221 INFO epoch # 617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017475532888511225
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,284 INFO epoch # 618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016756046409227565
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,348 INFO epoch # 619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001785055142136116
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,413 INFO epoch # 620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018461086415300088
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:48,413 INFO *** epoch 620, rolling-avg-loss (window=10)= 0.00017510686806190278
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,477 INFO epoch # 621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019846488066832535
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,540 INFO epoch # 622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016822488908019295
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,605 INFO epoch # 623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018255403415423643
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,669 INFO epoch # 624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019618650799202442
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,733 INFO epoch # 625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000153106887978538
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,797 INFO epoch # 626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016982553154321067
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,861 INFO epoch # 627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002056836046904209
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,925 INFO epoch # 628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00020278272040741285
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:48,989 INFO epoch # 629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017961062781068904
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,054 INFO epoch # 630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016660759501974098
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:49,054 INFO *** epoch 630, rolling-avg-loss (window=10)= 0.00018230472793447916
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,119 INFO epoch # 631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016109459181734564
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,183 INFO epoch # 632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015281022376711917
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,247 INFO epoch # 633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015297768970867764
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,311 INFO epoch # 634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014967330213266905
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,374 INFO epoch # 635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015381407615677745
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,438 INFO epoch # 636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016207351688990457
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,502 INFO epoch # 637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015107430039051906
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,566 INFO epoch # 638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015742488790237985
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,631 INFO epoch # 639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015208234481178806
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,695 INFO epoch # 640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001943925857403883
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:49,695 INFO *** epoch 640, rolling-avg-loss (window=10)= 0.00015874175193175688
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,759 INFO epoch # 641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015576609945355813
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,824 INFO epoch # 642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014581538650304537
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,888 INFO epoch # 643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015706845147178683
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:49,952 INFO epoch # 644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016472907191200648
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,016 INFO epoch # 645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018214825763607223
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,081 INFO epoch # 646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017358773402520455
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,146 INFO epoch # 647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014487996941170422
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,210 INFO epoch # 648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015013418521903077
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,273 INFO epoch # 649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014774904923342547
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,337 INFO epoch # 650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018255163149660802
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:50,338 INFO *** epoch 650, rolling-avg-loss (window=10)= 0.0001604429836362442
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,401 INFO epoch # 651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0002062909240976296
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,465 INFO epoch # 652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001541227381949284
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,529 INFO epoch # 653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016052798406462898
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,595 INFO epoch # 654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016598973667214523
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,664 INFO epoch # 655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016593774250850402
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,730 INFO epoch # 656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015951927014157263
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,793 INFO epoch # 657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015617453118466074
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,858 INFO epoch # 658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001604839118272139
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,922 INFO epoch # 659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015468358697035
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:50,986 INFO epoch # 660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016252923751380877
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:50,986 INFO *** epoch 660, rolling-avg-loss (window=10)= 0.00016462596631754421
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,051 INFO epoch # 661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001580601333444065
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,117 INFO epoch # 662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015125030756735214
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,181 INFO epoch # 663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015271671509253792
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,245 INFO epoch # 664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015211348522825574
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,309 INFO epoch # 665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001661578403400199
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,373 INFO epoch # 666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015541145012321067
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,436 INFO epoch # 667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016130537721892324
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,501 INFO epoch # 668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001458490345385144
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,565 INFO epoch # 669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014588743044896546
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,630 INFO epoch # 670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001567881244000091
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:51,630 INFO *** epoch 670, rolling-avg-loss (window=10)= 0.0001545539898302195
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,694 INFO epoch # 671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017462165499182447
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,758 INFO epoch # 672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014215129931471893
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,822 INFO epoch # 673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017746170374266512
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,887 INFO epoch # 674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016582063904024835
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:51,951 INFO epoch # 675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014545287592682143
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,015 INFO epoch # 676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018395582196717442
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,080 INFO epoch # 677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019514306052315078
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,145 INFO epoch # 678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014432615569148766
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,210 INFO epoch # 679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014824248637523851
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,274 INFO epoch # 680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001564681886065955
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:52,274 INFO *** epoch 680, rolling-avg-loss (window=10)= 0.0001633643886179925
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,338 INFO epoch # 681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018036982419289416
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,402 INFO epoch # 682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014440677227867127
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,466 INFO epoch # 683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017326354839042324
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,530 INFO epoch # 684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013948644391348353
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,595 INFO epoch # 685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001385792626251714
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,660 INFO epoch # 686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014249745163397165
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,725 INFO epoch # 687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015213660526569583
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,789 INFO epoch # 688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001576799660369943
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,853 INFO epoch # 689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017531027947370603
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,919 INFO epoch # 690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016814219179650536
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:52,919 INFO *** epoch 690, rolling-avg-loss (window=10)= 0.00015718723456075167
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:52,983 INFO epoch # 691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015281325136129453
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,047 INFO epoch # 692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001567087799685396
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,111 INFO epoch # 693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016924444310006947
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,176 INFO epoch # 694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014160329862988874
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,240 INFO epoch # 695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014983154244418984
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,304 INFO epoch # 696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001650961224868297
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,368 INFO epoch # 697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015461341763511882
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,432 INFO epoch # 698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015580823901473195
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,497 INFO epoch # 699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015892202031864144
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,560 INFO epoch # 700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015691084479385609
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:53,561 INFO *** epoch 700, rolling-avg-loss (window=10)= 0.00015615519597531602
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,625 INFO epoch # 701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016021530097987124
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,689 INFO epoch # 702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001902370327115932
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,754 INFO epoch # 703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016402116648350784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,818 INFO epoch # 704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00017439148393805226
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,882 INFO epoch # 705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001722722153090217
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:53,947 INFO epoch # 706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016971577593949405
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,011 INFO epoch # 707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015646534916413657
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,076 INFO epoch # 708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016019578004033974
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,142 INFO epoch # 709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013952980953035876
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,208 INFO epoch # 710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014734771593793994
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:54,208 INFO *** epoch 710, rolling-avg-loss (window=10)= 0.00016343916300343153
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,273 INFO epoch # 711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015327899694739244
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,337 INFO epoch # 712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001533546242740158
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,402 INFO epoch # 713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014235749222279992
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,468 INFO epoch # 714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013028923035562912
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,533 INFO epoch # 715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014347015940074925
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,597 INFO epoch # 716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000126975297575882
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,662 INFO epoch # 717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014141210738216614
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,728 INFO epoch # 718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014517183433326863
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,793 INFO epoch # 719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014048310458747437
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,857 INFO epoch # 720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014160322507450473
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:54,857 INFO *** epoch 720, rolling-avg-loss (window=10)= 0.00014183960721538824
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,921 INFO epoch # 721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001506740730974343
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:54,988 INFO epoch # 722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013605475214717444
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,052 INFO epoch # 723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014937303546957992
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,117 INFO epoch # 724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018093158882948046
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,181 INFO epoch # 725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014898757979153743
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,248 INFO epoch # 726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014660903116237023
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,313 INFO epoch # 727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001403527842285257
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,377 INFO epoch # 728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001369840240386111
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,442 INFO epoch # 729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013701387467790482
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,508 INFO epoch # 730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015876033194217598
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:55,508 INFO *** epoch 730, rolling-avg-loss (window=10)= 0.00014857410753847943
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,574 INFO epoch # 731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014104142007909104
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,639 INFO epoch # 732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001485657659259232
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,703 INFO epoch # 733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001409773334444253
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,768 INFO epoch # 734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001329000759255905
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,833 INFO epoch # 735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014176300470580827
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,897 INFO epoch # 736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013834997264439153
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:55,962 INFO epoch # 737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013549696529935318
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,027 INFO epoch # 738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013669712393493683
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,092 INFO epoch # 739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014575765425206555
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,157 INFO epoch # 740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015535913064468332
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:56,158 INFO *** epoch 740, rolling-avg-loss (window=10)= 0.00014169084468562687
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,223 INFO epoch # 741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013323939526799222
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,289 INFO epoch # 742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014019762352290854
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,354 INFO epoch # 743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014051323603325727
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,418 INFO epoch # 744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014635951231412037
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,483 INFO epoch # 745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013699403041300684
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,549 INFO epoch # 746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001168980123225083
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,615 INFO epoch # 747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012857296121637773
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,680 INFO epoch # 748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013911371297581354
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,745 INFO epoch # 749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001415163842466427
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,811 INFO epoch # 750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001234862870660436
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:56,811 INFO *** epoch 750, rolling-avg-loss (window=10)= 0.0001346891155378671
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,877 INFO epoch # 751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012783209365352377
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:56,941 INFO epoch # 752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012488433588941916
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,006 INFO epoch # 753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015669762615289073
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,072 INFO epoch # 754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014846494309495029
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,144 INFO epoch # 755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001504980585878002
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,208 INFO epoch # 756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013859502234936372
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,273 INFO epoch # 757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012799091655324446
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,338 INFO epoch # 758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001253377898819963
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,403 INFO epoch # 759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013225031045749347
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,468 INFO epoch # 760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001218056830794012
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:57,468 INFO *** epoch 760, rolling-avg-loss (window=10)= 0.00013543567797000833
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,534 INFO epoch # 761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012868227076978656
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,600 INFO epoch # 762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000136460900421298
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,665 INFO epoch # 763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012410476153945638
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,729 INFO epoch # 764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013610172288736067
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,795 INFO epoch # 765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012255281933448714
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,861 INFO epoch # 766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001241044677726677
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,926 INFO epoch # 767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011446329853015413
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:57,991 INFO epoch # 768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001350959898900328
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,057 INFO epoch # 769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012690606752130407
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,123 INFO epoch # 770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001320691493447157
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:58,123 INFO *** epoch 770, rolling-avg-loss (window=10)= 0.0001280541448011263
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,187 INFO epoch # 771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001209551315923818
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,252 INFO epoch # 772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012943676540544402
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,319 INFO epoch # 773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001474450302794139
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,384 INFO epoch # 774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013174609671295912
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,449 INFO epoch # 775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012375527444419276
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,514 INFO epoch # 776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012967764439508755
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,580 INFO epoch # 777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011038598506729613
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,647 INFO epoch # 778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013078322479032067
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,711 INFO epoch # 779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001421173996050129
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,776 INFO epoch # 780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012204854442643409
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:58,776 INFO *** epoch 780, rolling-avg-loss (window=10)= 0.0001288351096718543
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,842 INFO epoch # 781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011802171934505168
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,908 INFO epoch # 782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013747189888135836
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:58,973 INFO epoch # 783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001515625027650458
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,039 INFO epoch # 784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001293696694801838
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,105 INFO epoch # 785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014966153673867666
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,170 INFO epoch # 786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001284015788769466
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,235 INFO epoch # 787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014226160305952362
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,300 INFO epoch # 788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012127833622344042
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,365 INFO epoch # 789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014511407232475904
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,430 INFO epoch # 790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014783464450829342
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:52:59,430 INFO *** epoch 790, rolling-avg-loss (window=10)= 0.00013709775622032793
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,495 INFO epoch # 791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014228984036890324
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,561 INFO epoch # 792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013209249624424046
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,627 INFO epoch # 793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013046911976744013
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,692 INFO epoch # 794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014151683717500418
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,757 INFO epoch # 795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014644839086486172
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,822 INFO epoch # 796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011618878897934337
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,887 INFO epoch # 797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012204321183162392
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:52:59,952 INFO epoch # 798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012489113601077406
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,017 INFO epoch # 799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001368346148638011
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,083 INFO epoch # 800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014826953270130616
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:00,083 INFO *** epoch 800, rolling-avg-loss (window=10)= 0.00013410439688072985
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,152 INFO epoch # 801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001385355948286815
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,217 INFO epoch # 802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013137019942632833
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,282 INFO epoch # 803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011992293741514004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,347 INFO epoch # 804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011401740914607217
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,412 INFO epoch # 805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001246145380946473
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,476 INFO epoch # 806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001245500413915579
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,541 INFO epoch # 807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013055959755092772
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,607 INFO epoch # 808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001355920223886642
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,672 INFO epoch # 809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012840150452575472
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,737 INFO epoch # 810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001287405476659842
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:00,737 INFO *** epoch 810, rolling-avg-loss (window=10)= 0.0001276304392433758
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,806 INFO epoch # 811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001361161463364624
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,871 INFO epoch # 812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010908063063652662
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:00,936 INFO epoch # 813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013329087744295975
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,001 INFO epoch # 814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001520537014130241
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,068 INFO epoch # 815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001182347567691977
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,135 INFO epoch # 816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013098205272399355
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,200 INFO epoch # 817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011506372010217092
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,265 INFO epoch # 818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011691656322909694
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,332 INFO epoch # 819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001299097064020316
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,397 INFO epoch # 820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012463446182664484
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:01,397 INFO *** epoch 820, rolling-avg-loss (window=10)= 0.00012662826168821084
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,462 INFO epoch # 821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010996978284083525
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,530 INFO epoch # 822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013117542232521373
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,596 INFO epoch # 823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012025795615500101
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,661 INFO epoch # 824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012472985349631927
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,727 INFO epoch # 825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011759035368186233
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,793 INFO epoch # 826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001314123321662919
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,858 INFO epoch # 827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013622517587918992
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,923 INFO epoch # 828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00016482062733302882
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:01,988 INFO epoch # 829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014549282826692433
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,053 INFO epoch # 830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012667497321672272
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:02,053 INFO *** epoch 830, rolling-avg-loss (window=10)= 0.00013083493053613892
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,120 INFO epoch # 831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012978743154690164
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,185 INFO epoch # 832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011539210765931784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,250 INFO epoch # 833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012112253625673475
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,318 INFO epoch # 834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011929811910249555
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,383 INFO epoch # 835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011876532119003969
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,448 INFO epoch # 836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012476127699301287
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,513 INFO epoch # 837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001246488313881855
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,580 INFO epoch # 838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012970520828048393
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,647 INFO epoch # 839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.000132221291210044
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,712 INFO epoch # 840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012444214667084452
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:02,712 INFO *** epoch 840, rolling-avg-loss (window=10)= 0.00012401442702980603
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,777 INFO epoch # 841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001255911376460972
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,845 INFO epoch # 842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011501035203309584
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,910 INFO epoch # 843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012562959341266833
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:02,975 INFO epoch # 844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013921137121997162
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,040 INFO epoch # 845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015192083776582876
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,108 INFO epoch # 846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001283678755044093
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,174 INFO epoch # 847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011442930940575025
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,239 INFO epoch # 848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012164528698122012
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,303 INFO epoch # 849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001315759495810198
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,371 INFO epoch # 850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012194666055620473
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:03,371 INFO *** epoch 850, rolling-avg-loss (window=10)= 0.0001275328374106266
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,437 INFO epoch # 851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001321873728556966
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,501 INFO epoch # 852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011586063322965856
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,567 INFO epoch # 853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010662158706509217
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,635 INFO epoch # 854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010759399822291016
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,701 INFO epoch # 855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011827094942873373
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,765 INFO epoch # 856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011540138461896277
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,830 INFO epoch # 857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011378238741599489
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,898 INFO epoch # 858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001085491526282567
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:03,964 INFO epoch # 859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013417938816928654
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,029 INFO epoch # 860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001259800508250919
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:04,029 INFO *** epoch 860, rolling-avg-loss (window=10)= 0.0001178426904459684
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,094 INFO epoch # 861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001454255960311457
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,163 INFO epoch # 862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013453167025545554
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,229 INFO epoch # 863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014371650661360036
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,294 INFO epoch # 864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011739430533452833
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,359 INFO epoch # 865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014604061232148524
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,427 INFO epoch # 866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001366550726515925
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,492 INFO epoch # 867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011542494041805185
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,557 INFO epoch # 868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011041187076443748
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,623 INFO epoch # 869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014493220601252688
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,690 INFO epoch # 870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012942556691086793
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:04,690 INFO *** epoch 870, rolling-avg-loss (window=10)= 0.00013239583473136917
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,756 INFO epoch # 871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012883064653124165
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,822 INFO epoch # 872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011461593851436191
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,887 INFO epoch # 873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012534840175248974
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:04,952 INFO epoch # 874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012323075122822047
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,017 INFO epoch # 875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012117139328893245
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,083 INFO epoch # 876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010738474782101548
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,148 INFO epoch # 877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011156838263559621
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,215 INFO epoch # 878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010291172861798259
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,280 INFO epoch # 879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011263343958489713
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,345 INFO epoch # 880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011254803052906937
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:05,346 INFO *** epoch 880, rolling-avg-loss (window=10)= 0.0001160243460503807
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,411 INFO epoch # 881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011709937723480834
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,479 INFO epoch # 882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013478230607688602
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,545 INFO epoch # 883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011814892383199549
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,613 INFO epoch # 884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012883336023605807
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,678 INFO epoch # 885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012287862614357437
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,746 INFO epoch # 886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012275954100005038
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,811 INFO epoch # 887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010127623801281516
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,877 INFO epoch # 888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010874812778638443
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:05,944 INFO epoch # 889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011208203125079308
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,010 INFO epoch # 890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012379967870401742
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:06,010 INFO *** epoch 890, rolling-avg-loss (window=10)= 0.00011904082102773828
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,075 INFO epoch # 891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001430934509016879
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,148 INFO epoch # 892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010209648326053866
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,215 INFO epoch # 893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010751156668220574
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,280 INFO epoch # 894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011995706159950714
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,345 INFO epoch # 895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012407963652094622
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,414 INFO epoch # 896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011136666239508486
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,479 INFO epoch # 897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011702744620833982
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,545 INFO epoch # 898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011440130685969052
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,613 INFO epoch # 899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001184923386290393
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,679 INFO epoch # 900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014698366294396692
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:06,679 INFO *** epoch 900, rolling-avg-loss (window=10)= 0.00012050096160010071
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,744 INFO epoch # 901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012526457396688784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,811 INFO epoch # 902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011187676852841832
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,876 INFO epoch # 903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011613102108753992
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:06,942 INFO epoch # 904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001507049754536638
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,008 INFO epoch # 905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001102869128999373
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,073 INFO epoch # 906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014665550691006501
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,139 INFO epoch # 907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001274796233019515
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,204 INFO epoch # 908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012121710562951193
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,270 INFO epoch # 909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001456011569871407
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,336 INFO epoch # 910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014506223021726328
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:07,336 INFO *** epoch 910, rolling-avg-loss (window=10)= 0.00013002798749823797
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,401 INFO epoch # 911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013024052373111772
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,467 INFO epoch # 912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011857336363618742
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,533 INFO epoch # 913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001223177217752891
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,600 INFO epoch # 914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001110492439124755
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,665 INFO epoch # 915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013004444076614163
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,730 INFO epoch # 916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012171238040536991
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,796 INFO epoch # 917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012378177336813678
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,862 INFO epoch # 918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010044209727766429
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,927 INFO epoch # 919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001120646031722572
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:07,993 INFO epoch # 920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011458121957730327
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:07,993 INFO *** epoch 920, rolling-avg-loss (window=10)= 0.00011848073676219428
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,059 INFO epoch # 921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010250264875821813
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,124 INFO epoch # 922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001255075388826299
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,190 INFO epoch # 923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012953231623669126
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,256 INFO epoch # 924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001080167120335318
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,322 INFO epoch # 925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012505869278811588
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,388 INFO epoch # 926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013533881065086462
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,454 INFO epoch # 927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011845883830119419
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,520 INFO epoch # 928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001106456472825812
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,586 INFO epoch # 929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011725751562607911
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,652 INFO epoch # 930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012429441164840682
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:08,652 INFO *** epoch 930, rolling-avg-loss (window=10)= 0.00011966131322083129
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,718 INFO epoch # 931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012642905085158418
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,784 INFO epoch # 932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011109046073443096
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,849 INFO epoch # 933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011520589850988472
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,915 INFO epoch # 934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011696674528138828
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:08,981 INFO epoch # 935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010929629547717923
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,047 INFO epoch # 936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010840907657438947
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,115 INFO epoch # 937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011132914517020254
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,180 INFO epoch # 938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001122971837617115
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,247 INFO epoch # 939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011085910438168867
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,313 INFO epoch # 940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010216707329391284
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:09,313 INFO *** epoch 940, rolling-avg-loss (window=10)= 0.00011240500340363724
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,379 INFO epoch # 941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001130680807364115
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,444 INFO epoch # 942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010257754530584862
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,511 INFO epoch # 943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011467180343061045
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,577 INFO epoch # 944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012040477753316736
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,644 INFO epoch # 945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011886504137237353
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,710 INFO epoch # 946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011423012267641752
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,776 INFO epoch # 947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014476672322416562
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,842 INFO epoch # 948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00019041001348796271
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,908 INFO epoch # 949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011027819277842354
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:09,975 INFO epoch # 950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010376296233971516
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:09,975 INFO *** epoch 950, rolling-avg-loss (window=10)= 0.0001233035262885096
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,041 INFO epoch # 951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011449843088939815
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,107 INFO epoch # 952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010391645685103867
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,175 INFO epoch # 953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013364876042487595
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,241 INFO epoch # 954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001113941812604935
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,308 INFO epoch # 955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012507232065672724
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,373 INFO epoch # 956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010745162558123411
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,440 INFO epoch # 957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 9.896651749841112e-05
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,507 INFO epoch # 958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011761654502606689
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,572 INFO epoch # 959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001049616809609688
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,640 INFO epoch # 960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012910250939057732
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:10,640 INFO *** epoch 960, rolling-avg-loss (window=10)= 0.00011466290285397918
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,706 INFO epoch # 961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012039350212944555
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,772 INFO epoch # 962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010732277138458812
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,839 INFO epoch # 963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011905133851541905
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,905 INFO epoch # 964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011824269347471272
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:10,971 INFO epoch # 965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00015451305330316245
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,036 INFO epoch # 966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010229796077965148
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,104 INFO epoch # 967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010796923299949412
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,170 INFO epoch # 968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010126524057341157
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,236 INFO epoch # 969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 9.719846713096558e-05
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,303 INFO epoch # 970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001003305575864033
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:11,303 INFO *** epoch 970, rolling-avg-loss (window=10)= 0.00011285848178772539
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,369 INFO epoch # 971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010183790408291316
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,435 INFO epoch # 972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 9.644314201295856e-05
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,501 INFO epoch # 973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001099621227353964
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,568 INFO epoch # 974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012036837119921984
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,634 INFO epoch # 975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001163303589919451
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,700 INFO epoch # 976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013720877547029886
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,766 INFO epoch # 977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012887852949461376
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,832 INFO epoch # 978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011026057066487738
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,897 INFO epoch # 979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 9.98160580820695e-05
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:11,963 INFO epoch # 980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011975647930739797
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:11,963 INFO *** epoch 980, rolling-avg-loss (window=10)= 0.00011408623120416906
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,029 INFO epoch # 981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012487549838624545
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,094 INFO epoch # 982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001045908110199889
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,160 INFO epoch # 983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011130549552262892
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,227 INFO epoch # 984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001058494201515714
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,293 INFO epoch # 985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010899024744048802
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,358 INFO epoch # 986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010602099666812137
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,424 INFO epoch # 987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011861038490224018
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,490 INFO epoch # 988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00014017984528891247
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,556 INFO epoch # 989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00013453183430556237
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,623 INFO epoch # 990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00018371673036199354
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:12,623 INFO *** epoch 990, rolling-avg-loss (window=10)= 0.00012386712640477526
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,689 INFO epoch # 991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011226368906136486
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,755 INFO epoch # 992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011979809096374083
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,821 INFO epoch # 993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001222752609919553
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,887 INFO epoch # 994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011141412824144936
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:12,953 INFO epoch # 995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010912799916695803
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:13,019 INFO epoch # 996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00010791757074457564
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:13,085 INFO epoch # 997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 8.749455867018696e-05
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:13,151 INFO epoch # 998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00011462915915672056
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:13,217 INFO epoch # 999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00012499514798491873
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 20:53:13,284 INFO epoch # 1000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0001078620891803439
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 20:53:13,284 INFO *** epoch 1000, rolling-avg-loss (window=10)= 0.00011177776941622142
[experiments_sandbox.py:920 -   <module>()] 2023-04-25 20:53:13,284 INFO training time in seconds = 65
[experiments_sandbox.py:936 -   <module>()] 2023-04-25 20:53:13,402 INFO train-epochs-loss curve df :
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 20:53:13,407 INFO 
    epochs      loss
0       10  0.086306
1       20  0.037506
2       30  0.034863
3       40  0.052167
4       50  0.016714
5       60  0.014417
6       70  0.012706
7       80  0.012737
8       90  0.008818
9      100  0.011234
10     110  0.006741
11     120  0.003506
12     130  0.002964
13     140  0.008465
14     150  0.004453
15     160  0.002053
16     170  0.003282
17     180  0.001477
18     190  0.001217
19     200  0.001538
20     210  0.001081
21     220  0.000921
22     230  0.000875
23     240  0.000850
24     250  0.000655
25     260  0.000670
26     270  0.000580
27     280  0.000688
28     290  0.000476
29     300  0.000449
30     310  0.000464
31     320  0.000393
32     330  0.000463
33     340  0.000475
34     350  0.000328
35     360  0.000373
36     370  0.000311
37     380  0.000283
38     390  0.000288
39     400  0.000268
40     410  0.000265
41     420  0.000265
42     430  0.000224
43     440  0.000244
44     450  0.000214
45     460  0.000202
46     470  0.000194
47     480  0.000190
48     490  0.000191
49     500  0.000196
50     510  0.000188
51     520  0.000183
52     530  0.000186
53     540  0.000189
54     550  0.000192
55     560  0.000177
56     570  0.000191
57     580  0.000190
58     590  0.000180
59     600  0.000170
60     610  0.000181
61     620  0.000175
62     630  0.000182
63     640  0.000159
64     650  0.000160
65     660  0.000165
66     670  0.000155
67     680  0.000163
68     690  0.000157
69     700  0.000156
70     710  0.000163
71     720  0.000142
72     730  0.000149
73     740  0.000142
74     750  0.000135
75     760  0.000135
76     770  0.000128
77     780  0.000129
78     790  0.000137
79     800  0.000134
80     810  0.000128
81     820  0.000127
82     830  0.000131
83     840  0.000124
84     850  0.000128
85     860  0.000118
86     870  0.000132
87     880  0.000116
88     890  0.000119
89     900  0.000121
90     910  0.000130
91     920  0.000118
92     930  0.000120
93     940  0.000112
94     950  0.000123
95     960  0.000115
96     970  0.000113
97     980  0.000114
98     990  0.000124
99    1000  0.000112
[experiments_sandbox.py:939 -   <module>()] 2023-04-25 20:53:13,407 INFO Model parameters after training
[experiments_sandbox.py:940 -   <module>()] 2023-04-25 20:53:13,407 INFO Model = TTRBF
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 20:53:13,408 INFO G0 = Parameter containing:
tensor([[ 1.3235,  0.5467, -0.3515],
        [-0.9671, -0.1440,  0.6008],
        [-1.2948, -0.4500, -0.6238],
        [-0.2456,  1.2751, -0.0251]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 20:53:13,408 INFO G1 = Parameter containing:
tensor([[[ 1.5530, -0.3468, -1.1448],
         [-1.9703, -0.2712,  1.1599],
         [ 1.7553,  3.0021, -2.3509],
         [-0.2052, -1.3707,  0.6526]],

        [[ 1.8458, -0.1037, -1.7267],
         [ 1.7640,  2.2165, -1.6127],
         [-0.8352, -1.0922,  0.3036],
         [-1.7623,  0.1389,  0.2199]],

        [[ 0.5142,  0.2158, -0.4010],
         [ 0.4626, -1.4000,  0.2828],
         [ 0.5068, -0.7978, -0.0402],
         [-0.0279,  0.3735,  0.3162]]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 20:53:13,409 INFO G2 = Parameter containing:
tensor([[[ -0.3306,  -1.2867,  -0.1973],
         [  1.7336,  -0.8038,  -0.0555],
         [-11.2801,   1.9490,   0.4135],
         [ -0.9164,   2.6337,  -0.6302]],

        [[ -0.1613,  -0.4221,  -0.0328],
         [  1.1152,   1.2738,   0.2199],
         [ -6.0512,   0.7103,   0.8114],
         [  0.0114,  -0.6883,   1.0699]],

        [[  0.7120,   1.0974,  -0.1900],
         [ -0.6576,  -0.2765,  -1.0305],
         [  7.0169,  -0.2879,  -1.1032],
         [  0.2441,  -0.7268,  -0.3429]]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 20:53:13,409 INFO G3 = Parameter containing:
tensor([[[-0.1804,  3.8354],
         [-0.2963,  3.4162],
         [ 0.0535, -0.4257],
         [ 0.2637, -3.3910]],

        [[-0.5158,  1.5273],
         [ 0.4951, -1.2374],
         [-2.3659, -0.2936],
         [ 0.5180,  0.0593]],

        [[ 0.7063,  0.5073],
         [ 0.8727, -0.4586],
         [-0.6132,  0.4727],
         [-0.0833, -0.3581]]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 20:53:13,410 INFO rbf_module.centres = Parameter containing:
tensor([[ 3.4596,  5.5571],
        [ 0.9785, -4.4159],
        [ 4.2280, -3.7434],
        [-3.5086, -5.2683],
        [-2.2809,  5.2300],
        [ 3.1372, -3.4497],
        [-5.5662, -2.4976],
        [ 0.3273,  3.8934],
        [ 5.2546,  3.4647],
        [ 2.6943, -1.9124],
        [-3.4105,  3.7827],
        [-1.4219,  3.6747],
        [ 4.8042,  2.2057],
        [ 3.9706,  0.5671],
        [-3.0196, -3.4863],
        [-4.5084, -0.9289]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 20:53:13,410 INFO rbf_module.log_sigmas = Parameter containing:
tensor([ 1.5314,  0.1054,  0.8901,  1.6513, -0.8395,  1.1226,  1.2718,  5.1800,
        -0.7301,  5.0688,  0.7453,  1.2087,  0.8701,  1.9095,  3.1705,  1.8564],
       requires_grad=True)
[experiments_sandbox.py:945 -   <module>()] 2023-04-25 20:53:13,410 INFO Out-of sample batch-test
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,411 INFO test-batch  # 0 => test-loss = 0.0001044516175170429
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,412 INFO test-batch  # 1 => test-loss = 5.947483441559598e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,413 INFO test-batch  # 2 => test-loss = 8.547717152396217e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,414 INFO test-batch  # 3 => test-loss = 8.263925701612607e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,415 INFO test-batch  # 4 => test-loss = 3.789483162108809e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,416 INFO test-batch  # 5 => test-loss = 0.00187418214045465
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,417 INFO test-batch  # 6 => test-loss = 3.667869896162301e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,418 INFO test-batch  # 7 => test-loss = 0.00015059480210766196
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,419 INFO test-batch  # 8 => test-loss = 0.00010916816972894594
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,420 INFO test-batch  # 9 => test-loss = 3.40982478519436e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,421 INFO test-batch  # 10 => test-loss = 0.0005270199617370963
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,422 INFO test-batch  # 11 => test-loss = 3.405355892027728e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,423 INFO test-batch  # 12 => test-loss = 0.0027206894010305405
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,424 INFO test-batch  # 13 => test-loss = 5.181316373636946e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,425 INFO test-batch  # 14 => test-loss = 7.441903289873153e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,426 INFO test-batch  # 15 => test-loss = 7.080340583343059e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,427 INFO test-batch  # 16 => test-loss = 0.0001843636855483055
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,428 INFO test-batch  # 17 => test-loss = 0.0001132241595769301
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,429 INFO test-batch  # 18 => test-loss = 0.00021815448417328298
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,430 INFO test-batch  # 19 => test-loss = 6.090145689086057e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,431 INFO test-batch  # 20 => test-loss = 4.828438613913022e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,432 INFO test-batch  # 21 => test-loss = 5.6588298321003094e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,432 INFO test-batch  # 22 => test-loss = 0.00012091475946363062
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,433 INFO test-batch  # 23 => test-loss = 5.7653414842206985e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,434 INFO test-batch  # 24 => test-loss = 7.988979632500559e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,435 INFO test-batch  # 25 => test-loss = 3.7231933674775064e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,436 INFO test-batch  # 26 => test-loss = 0.000515948748216033
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,437 INFO test-batch  # 27 => test-loss = 0.0001203252759296447
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,438 INFO test-batch  # 28 => test-loss = 0.00012870473437942564
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,439 INFO test-batch  # 29 => test-loss = 5.5968113883864135e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,440 INFO test-batch  # 30 => test-loss = 7.253522926475853e-05
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 20:53:13,441 INFO test-batch  # 31 => test-loss = 0.00013113333261571825
