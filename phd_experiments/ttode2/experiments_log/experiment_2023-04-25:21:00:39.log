[experiments_sandbox.py:737 -   <module>()] 2023-04-25 21:00:39,793 INFO SEED = 42
[experiments_sandbox.py:818 -   <module>()] 2023-04-25 21:00:39,794 INFO model = 
***
NN-Model 
Sequential(
  (0): Linear(in_features=3, out_features=50, bias=True)
  (1): Identity()
  (2): Tanh()
  (3): Linear(in_features=50, out_features=3, bias=True)
)
numel_learnable = 353
***
[experiments_sandbox.py:819 -   <module>()] 2023-04-25 21:00:39,794 INFO optimizer  = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.1
    maximize: False
    weight_decay: 0
)
[experiments_sandbox.py:827 -   <module>()] 2023-04-25 21:00:39,794 INFO lr_scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c33753910>
[experiments_sandbox.py:830 -   <module>()] 2023-04-25 21:00:39,794 INFO Normalize-Data-source-X-train = False
[experiments_sandbox.py:831 -   <module>()] 2023-04-25 21:00:39,794 INFO Normalize-Data-source-Y-train = False
[experiments_sandbox.py:832 -   <module>()] 2023-04-25 21:00:39,794 INFO Normalize-Data-source-X-test = False
[experiments_sandbox.py:833 -   <module>()] 2023-04-25 21:00:39,794 INFO Normalize-Data-source-Y-test = False
[experiments_sandbox.py:862 -   <module>()] 2023-04-25 21:00:39,795 INFO train-dataset = 
***
Lorenz-System
N = 1000rho = 28
sigma = 10
beta = 2.6666666666666665
normalize_X = Falsenormalize_Y = False****

[experiments_sandbox.py:863 -   <module>()] 2023-04-25 21:00:39,795 INFO test-dataset = 
***
Lorenz-System
N = 1000rho = 28
sigma = 10
beta = 2.6666666666666665
normalize_X = Falsenormalize_Y = False****

[experiments_sandbox.py:864 -   <module>()] 2023-04-25 21:00:39,795 INFO train-epochs = 1000
[experiments_sandbox.py:868 -   <module>()] 2023-04-25 21:00:39,795 INFO Input batch normalization = False
[experiments_sandbox.py:869 -   <module>()] 2023-04-25 21:00:39,795 INFO Output Normalization = None
[experiments_sandbox.py:870 -   <module>()] 2023-04-25 21:00:39,795 INFO Gradient-clipping max-norm = 10
[experiments_sandbox.py:872 -   <module>()] 2023-04-25 21:00:39,795 INFO epochs_losses_window = 10
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,390 INFO epoch # 0 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 249.20212650299072
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,414 INFO epoch # 1 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 99.87543323636055
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,438 INFO epoch # 2 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 43.438524320721626
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,462 INFO epoch # 3 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 24.999413944780827
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,487 INFO epoch # 4 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 16.618100244551897
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,511 INFO epoch # 5 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 14.411621361970901
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,535 INFO epoch # 6 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 12.63926201313734
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,560 INFO epoch # 7 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 11.260852798819542
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,584 INFO epoch # 8 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 9.761124223470688
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,609 INFO epoch # 9 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 9.859423760324717
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,634 INFO epoch # 10 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 9.654288463294506
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:41,634 INFO *** epoch 10, rolling-avg-loss (window=10)= 25.25180443674326
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,658 INFO epoch # 11 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 7.486879710108042
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,683 INFO epoch # 12 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 8.213646523654461
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,708 INFO epoch # 13 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 8.312385357916355
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,732 INFO epoch # 14 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 8.02011039853096
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,756 INFO epoch # 15 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 7.382176019251347
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,780 INFO epoch # 16 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 6.906770408153534
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,805 INFO epoch # 17 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 7.493847846984863
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,830 INFO epoch # 18 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 6.521056428551674
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,854 INFO epoch # 19 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 7.3552459962666035
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,879 INFO epoch # 20 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 6.947631131857634
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:41,879 INFO *** epoch 20, rolling-avg-loss (window=10)= 7.463974982127548
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,903 INFO epoch # 21 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 6.176738161593676
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,927 INFO epoch # 22 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 6.147063363343477
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,951 INFO epoch # 23 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 5.96543861925602
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,975 INFO epoch # 24 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 5.744127452373505
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:41,999 INFO epoch # 25 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 7.602701924741268
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,025 INFO epoch # 26 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 5.732389897108078
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,050 INFO epoch # 27 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 6.122531846165657
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,074 INFO epoch # 28 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 6.2204024493694305
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,098 INFO epoch # 29 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 6.974179428070784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,123 INFO epoch # 30 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 5.828883573412895
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:42,123 INFO *** epoch 30, rolling-avg-loss (window=10)= 6.251445671543479
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,147 INFO epoch # 31 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 8.967976495623589
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,171 INFO epoch # 32 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 6.438401143997908
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,195 INFO epoch # 33 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 7.0700722225010395
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,218 INFO epoch # 34 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 6.016730755567551
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,243 INFO epoch # 35 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.032334756106138
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,267 INFO epoch # 36 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 5.380745276808739
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,291 INFO epoch # 37 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 5.890805650502443
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,315 INFO epoch # 38 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 7.210522945970297
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,339 INFO epoch # 39 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 5.513554885983467
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,363 INFO epoch # 40 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 5.477572966367006
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:42,363 INFO *** epoch 40, rolling-avg-loss (window=10)= 6.199871709942817
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,387 INFO epoch # 41 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.191258527338505
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,411 INFO epoch # 42 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.065489124506712
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,435 INFO epoch # 43 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 3.944678943604231
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,459 INFO epoch # 44 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 3.773790583014488
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,483 INFO epoch # 45 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.975552298128605
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,507 INFO epoch # 46 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 5.1177216321229935
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,532 INFO epoch # 47 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 7.4099943824112415
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,559 INFO epoch # 48 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 5.920207645744085
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,583 INFO epoch # 49 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.77308976277709
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,607 INFO epoch # 50 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 5.598258316516876
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:42,607 INFO *** epoch 50, rolling-avg-loss (window=10)= 4.977004121616483
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,631 INFO epoch # 51 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.034623119980097
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,655 INFO epoch # 52 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 3.617391474545002
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,679 INFO epoch # 53 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.532926823943853
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,703 INFO epoch # 54 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.531892169266939
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,727 INFO epoch # 55 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.913834359496832
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,752 INFO epoch # 56 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.389839921146631
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,776 INFO epoch # 57 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 6.779690742492676
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,800 INFO epoch # 58 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.719011578708887
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,824 INFO epoch # 59 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.858245365321636
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,848 INFO epoch # 60 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 4.04723366163671
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:42,848 INFO *** epoch 60, rolling-avg-loss (window=10)= 4.6424689216539266
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,872 INFO epoch # 61 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 3.800260502845049
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,896 INFO epoch # 62 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.1 -loss = 8.051092378795147
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,920 INFO epoch # 63 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1-> 0.08 -loss = 5.602026324719191
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,944 INFO epoch # 64 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 3.507005024701357
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,968 INFO epoch # 65 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 3.251682488247752
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:42,992 INFO epoch # 66 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 3.1321561094373465
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,016 INFO epoch # 67 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 2.6978906635195017
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,040 INFO epoch # 68 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 3.110290691256523
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,064 INFO epoch # 69 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 3.113054934889078
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,088 INFO epoch # 70 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 2.936189329251647
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:43,088 INFO *** epoch 70, rolling-avg-loss (window=10)= 3.9201648447662594
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,112 INFO epoch # 71 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 2.550781788304448
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,136 INFO epoch # 72 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 3.265040198341012
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,160 INFO epoch # 73 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 3.2048079520463943
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,184 INFO epoch # 74 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 5.461065182462335
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,209 INFO epoch # 75 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 2.9949704091995955
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,232 INFO epoch # 76 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 3.9622198566794395
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,256 INFO epoch # 77 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 3.066992498934269
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,280 INFO epoch # 78 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 3.5151675939559937
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,304 INFO epoch # 79 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 4.938985411077738
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,328 INFO epoch # 80 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 5.5969486609101295
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:43,328 INFO *** epoch 80, rolling-avg-loss (window=10)= 3.8556979551911352
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,352 INFO epoch # 81 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.08 -loss = 3.495013400912285
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,376 INFO epoch # 82 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08-> 0.064 -loss = 2.8575588166713715
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,400 INFO epoch # 83 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 2.9904669634997845
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,424 INFO epoch # 84 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 1.7430445263162255
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,448 INFO epoch # 85 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 1.4992480492219329
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,473 INFO epoch # 86 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 1.8844823008403182
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,496 INFO epoch # 87 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 2.2295355908572674
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,521 INFO epoch # 88 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 2.1556341573596
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,545 INFO epoch # 89 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 1.941599844954908
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,569 INFO epoch # 90 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 1.6222062464803457
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:43,569 INFO *** epoch 90, rolling-avg-loss (window=10)= 2.2418789897114038
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,593 INFO epoch # 91 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 1.6306315287947655
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,617 INFO epoch # 92 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 1.7419972009956837
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,641 INFO epoch # 93 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 2.7087277360260487
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,665 INFO epoch # 94 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 2.4761999752372503
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,689 INFO epoch # 95 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.064 -loss = 2.2903778525069356
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,713 INFO epoch # 96 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.064-> 0.0512 -loss = 1.9678942784667015
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,737 INFO epoch # 97 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 1.3409195197746158
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,761 INFO epoch # 98 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 1.030197178479284
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,786 INFO epoch # 99 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.7726919227279723
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,810 INFO epoch # 100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.7033044937998056
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:43,810 INFO *** epoch 100, rolling-avg-loss (window=10)= 1.6662941686809063
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,835 INFO epoch # 101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.7090411433018744
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,859 INFO epoch # 102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.6902472977526486
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,882 INFO epoch # 103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.7872311063110828
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,906 INFO epoch # 104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.5768974423408508
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,930 INFO epoch # 105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.8226889735087752
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,954 INFO epoch # 106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.9902905570343137
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:43,978 INFO epoch # 107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 1.1190943783149123
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,002 INFO epoch # 108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.7898955326527357
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,028 INFO epoch # 109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.657085195183754
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,052 INFO epoch # 110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.9722180007956922
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:44,052 INFO *** epoch 110, rolling-avg-loss (window=10)= 0.811468962719664
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,076 INFO epoch # 111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.7838073018938303
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,100 INFO epoch # 112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.729574419092387
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,124 INFO epoch # 113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.7187744975090027
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,148 INFO epoch # 114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.0512 -loss = 0.7261802311986685
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,172 INFO epoch # 115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0512-> 0.04096 -loss = 0.931047469843179
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,196 INFO epoch # 116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.743303488008678
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,220 INFO epoch # 117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.4373926371335983
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,244 INFO epoch # 118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.4849066259339452
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,269 INFO epoch # 119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.5228498098440468
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,293 INFO epoch # 120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.36101980321109295
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:44,293 INFO *** epoch 120, rolling-avg-loss (window=10)= 0.6438856283668428
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,318 INFO epoch # 121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.3714335130061954
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,342 INFO epoch # 122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.3716575223952532
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,366 INFO epoch # 123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.44192064786329865
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,390 INFO epoch # 124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.4777347557246685
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,414 INFO epoch # 125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.38061250234022737
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,438 INFO epoch # 126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.38674755254760385
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,462 INFO epoch # 127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.37960692355409265
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,486 INFO epoch # 128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.34647137159481645
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,510 INFO epoch # 129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.328556812601164
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,534 INFO epoch # 130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.48849912034347653
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:44,534 INFO *** epoch 130, rolling-avg-loss (window=10)= 0.39732407219707966
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,559 INFO epoch # 131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.45759819727391005
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,583 INFO epoch # 132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.5468712942674756
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,606 INFO epoch # 133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 1.0779683776199818
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,631 INFO epoch # 134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.887461224803701
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,655 INFO epoch # 135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.6192636857740581
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,679 INFO epoch # 136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.5007612938061357
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,703 INFO epoch # 137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 1.0593526945449412
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,728 INFO epoch # 138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.9123169491067529
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,753 INFO epoch # 139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.04096 -loss = 0.5611262680031359
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,777 INFO epoch # 140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04096-> 0.032768 -loss = 0.5140677546150982
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:44,777 INFO *** epoch 140, rolling-avg-loss (window=10)= 0.713678773981519
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,801 INFO epoch # 141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.6287551242858171
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,825 INFO epoch # 142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.4278435609303415
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,849 INFO epoch # 143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.2662669289857149
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,873 INFO epoch # 144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.2621529104653746
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,897 INFO epoch # 145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.24494232842698693
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,921 INFO epoch # 146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.4985063539352268
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,945 INFO epoch # 147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.4584693885408342
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,970 INFO epoch # 148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.3612372362986207
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:44,993 INFO epoch # 149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.26721245469525456
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,017 INFO epoch # 150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.24979603779502213
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:45,017 INFO *** epoch 150, rolling-avg-loss (window=10)= 0.36651823243591936
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,041 INFO epoch # 151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.19275144394487143
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,065 INFO epoch # 152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.26414338196627796
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,089 INFO epoch # 153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.293276057112962
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,113 INFO epoch # 154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.2816465904470533
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,138 INFO epoch # 155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.26740213413722813
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,162 INFO epoch # 156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.2864026299212128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,186 INFO epoch # 157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.298402666579932
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,210 INFO epoch # 158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.37682018615305424
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,233 INFO epoch # 159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.41725402721203864
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,257 INFO epoch # 160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.3331340029835701
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:45,257 INFO *** epoch 160, rolling-avg-loss (window=10)= 0.30112331204582005
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,282 INFO epoch # 161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.032768 -loss = 0.3127147445920855
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,306 INFO epoch # 162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.032768-> 0.0262144 -loss = 0.697572368895635
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,330 INFO epoch # 163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.4616987658664584
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,354 INFO epoch # 164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.28429551515728235
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,378 INFO epoch # 165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.3011069283820689
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,402 INFO epoch # 166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.27647236129269004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,426 INFO epoch # 167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.23676473181694746
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,449 INFO epoch # 168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.20172862603794783
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,473 INFO epoch # 169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.1625259816646576
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,497 INFO epoch # 170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.1552548799663782
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:45,497 INFO *** epoch 170, rolling-avg-loss (window=10)= 0.30901349036721515
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,522 INFO epoch # 171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.1522288389969617
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,547 INFO epoch # 172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.2516788716893643
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,571 INFO epoch # 173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.24148176447488368
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,595 INFO epoch # 174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.18215916026383638
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,619 INFO epoch # 175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.2860402532387525
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,643 INFO epoch # 176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.3015723268035799
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,667 INFO epoch # 177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.33544041123241186
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,691 INFO epoch # 178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.2281875058542937
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,715 INFO epoch # 179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.2670321052428335
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,739 INFO epoch # 180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.1776146871270612
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:45,739 INFO *** epoch 180, rolling-avg-loss (window=10)= 0.24234359249239787
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,764 INFO epoch # 181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.0262144 -loss = 0.1974355266429484
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,788 INFO epoch # 182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0262144-> 0.02097152 -loss = 0.1971277624834329
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,812 INFO epoch # 183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.29905740381218493
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,836 INFO epoch # 184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.27212176634930074
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,860 INFO epoch # 185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.34094003634527326
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,884 INFO epoch # 186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.1705548872705549
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,908 INFO epoch # 187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.15632507449481636
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,932 INFO epoch # 188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.12140526995062828
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,957 INFO epoch # 189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.09331022057449445
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:45,981 INFO epoch # 190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.0913913631811738
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:45,981 INFO *** epoch 190, rolling-avg-loss (window=10)= 0.1939669311104808
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,005 INFO epoch # 191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.14097389089874923
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,031 INFO epoch # 192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.1599971199175343
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,055 INFO epoch # 193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.21047422499395907
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,079 INFO epoch # 194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.18377630598843098
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,103 INFO epoch # 195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.18710579467006028
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,127 INFO epoch # 196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.15736850118264556
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,151 INFO epoch # 197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.11799742979928851
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,175 INFO epoch # 198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.10542291356250644
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,199 INFO epoch # 199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.10003128601238132
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,223 INFO epoch # 200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.09080366045236588
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:46,223 INFO *** epoch 200, rolling-avg-loss (window=10)= 0.14539511274779215
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,247 INFO epoch # 201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.11481321236351505
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,271 INFO epoch # 202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.11785809090360999
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,295 INFO epoch # 203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.16329013532958925
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,319 INFO epoch # 204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.18563329125754535
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,343 INFO epoch # 205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.14784797432366759
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,367 INFO epoch # 206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.1851190240122378
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,391 INFO epoch # 207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.3657361927907914
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,415 INFO epoch # 208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.5601991433650255
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,440 INFO epoch # 209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.39981384272687137
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,464 INFO epoch # 210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.02097152 -loss = 0.31435892870649695
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:46,464 INFO *** epoch 210, rolling-avg-loss (window=10)= 0.255466983577935
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,488 INFO epoch # 211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02097152-> 0.01677722 -loss = 0.24723968608304858
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,512 INFO epoch # 212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.20836677867919207
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,536 INFO epoch # 213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.11190751404501498
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,560 INFO epoch # 214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.08858443069038913
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,587 INFO epoch # 215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.07305112550966442
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,612 INFO epoch # 216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.08872808038722724
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,637 INFO epoch # 217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.07925601175520569
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,661 INFO epoch # 218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.07460116618312895
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,685 INFO epoch # 219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.06516813539201394
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,709 INFO epoch # 220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.08261122845578939
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:46,709 INFO *** epoch 220, rolling-avg-loss (window=10)= 0.11195141571806744
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,733 INFO epoch # 221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.08824761933647096
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,757 INFO epoch # 222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.12101852090563625
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,781 INFO epoch # 223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.12631366110872477
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,805 INFO epoch # 224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.1049984156852588
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,829 INFO epoch # 225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.11519220215268433
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,854 INFO epoch # 226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.13617825449910015
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,878 INFO epoch # 227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.17779406404588372
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,902 INFO epoch # 228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.18551768991164863
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,927 INFO epoch # 229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01677722 -loss = 0.14835581160150468
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,951 INFO epoch # 230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01677722-> 0.01342177 -loss = 0.07834260165691376
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:46,951 INFO *** epoch 230, rolling-avg-loss (window=10)= 0.1281958840903826
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,975 INFO epoch # 231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.06310765747912228
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:46,999 INFO epoch # 232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.06295822630636394
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,025 INFO epoch # 233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.06199339707382023
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,049 INFO epoch # 234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.07353974226862192
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,075 INFO epoch # 235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.09485994174610823
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,100 INFO epoch # 236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.06901368970284238
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,124 INFO epoch # 237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.0671077425358817
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,149 INFO epoch # 238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.08723325573373586
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,173 INFO epoch # 239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.07330569706391543
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,197 INFO epoch # 240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.06479580793529749
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:47,197 INFO *** epoch 240, rolling-avg-loss (window=10)= 0.07179151578457095
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,221 INFO epoch # 241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.07556360290618613
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,245 INFO epoch # 242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.07421395997516811
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,270 INFO epoch # 243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01342177 -loss = 0.08297993277665228
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,294 INFO epoch # 244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01342177-> 0.01073742 -loss = 0.06864129332825541
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,318 INFO epoch # 245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.06304705061484128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,343 INFO epoch # 246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.06215306115336716
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,367 INFO epoch # 247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.07035804988117889
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,391 INFO epoch # 248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.07051690225489438
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,415 INFO epoch # 249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.06245542882243171
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,439 INFO epoch # 250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.056541247002314776
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:47,439 INFO *** epoch 250, rolling-avg-loss (window=10)= 0.06864705287152902
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,463 INFO epoch # 251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.048632826074026525
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,487 INFO epoch # 252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.056039656861685216
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,512 INFO epoch # 253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.054999325424432755
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,537 INFO epoch # 254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.05503404408227652
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,561 INFO epoch # 255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.04770454403478652
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,586 INFO epoch # 256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.05430182797135785
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,610 INFO epoch # 257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.062391281535383314
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,634 INFO epoch # 258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.06911188317462802
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,658 INFO epoch # 259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.07696199975907803
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,683 INFO epoch # 260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.11534912430215627
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:47,683 INFO *** epoch 260, rolling-avg-loss (window=10)= 0.0640526513219811
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,707 INFO epoch # 261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.10080154764000326
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,732 INFO epoch # 262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.08684433629969135
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,756 INFO epoch # 263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.10453957167919725
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,780 INFO epoch # 264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.12625956436386332
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,804 INFO epoch # 265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.01073742 -loss = 0.0810254670213908
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,828 INFO epoch # 266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01073742-> 0.00858993 -loss = 0.06227088154992089
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,852 INFO epoch # 267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.05326294945552945
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,876 INFO epoch # 268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.04951974540017545
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,900 INFO epoch # 269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0583248853799887
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,925 INFO epoch # 270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.07133762794546783
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:47,925 INFO *** epoch 270, rolling-avg-loss (window=10)= 0.07941865767352282
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,949 INFO epoch # 271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.061760280979797244
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,974 INFO epoch # 272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.04752012301469222
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:47,998 INFO epoch # 273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.04961701657157391
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,023 INFO epoch # 274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.05382862064288929
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,048 INFO epoch # 275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.051913160539697856
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,072 INFO epoch # 276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.04903552506584674
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,096 INFO epoch # 277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.059110230300575495
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,120 INFO epoch # 278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.05346307659056038
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,144 INFO epoch # 279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.049161896749865264
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,169 INFO epoch # 280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.04995906591648236
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:48,169 INFO *** epoch 280, rolling-avg-loss (window=10)= 0.05253689963719808
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,193 INFO epoch # 281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.05600747390417382
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,217 INFO epoch # 282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00858993 -loss = 0.0649722043890506
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,242 INFO epoch # 283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00858993-> 0.00687195 -loss = 0.07170352787943557
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,266 INFO epoch # 284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.08483947248896584
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,290 INFO epoch # 285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0497273369692266
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,314 INFO epoch # 286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.04018164880108088
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,338 INFO epoch # 287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.048862035269849
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,363 INFO epoch # 288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.05153883819002658
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,387 INFO epoch # 289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.04300175089156255
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,412 INFO epoch # 290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.037749222945421934
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:48,412 INFO *** epoch 290, rolling-avg-loss (window=10)= 0.054858351172879335
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,436 INFO epoch # 291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.04802703036693856
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,460 INFO epoch # 292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0468321802909486
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,484 INFO epoch # 293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.0400941816624254
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,508 INFO epoch # 294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.04786768648773432
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,533 INFO epoch # 295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.04645362281007692
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,557 INFO epoch # 296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.04422017326578498
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,582 INFO epoch # 297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.04335814790101722
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,607 INFO epoch # 298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.04140856157755479
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,631 INFO epoch # 299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.04202739000902511
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,655 INFO epoch # 300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00687195 -loss = 0.047814090037718415
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:48,655 INFO *** epoch 300, rolling-avg-loss (window=10)= 0.04481030644092243
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,679 INFO epoch # 301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00687195-> 0.00549756 -loss = 0.057286411174573004
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,704 INFO epoch # 302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.043770606192993
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,728 INFO epoch # 303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03867589094443247
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,752 INFO epoch # 304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03894225921249017
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,776 INFO epoch # 305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03936290362617001
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,800 INFO epoch # 306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03791228315094486
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,825 INFO epoch # 307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0404410160263069
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,849 INFO epoch # 308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03909101843601093
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,873 INFO epoch # 309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0380418318673037
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,897 INFO epoch # 310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03623138036346063
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:48,897 INFO *** epoch 310, rolling-avg-loss (window=10)= 0.040975560099468566
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,922 INFO epoch # 311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.040666551678441465
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,946 INFO epoch # 312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.044070700474549085
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,970 INFO epoch # 313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.04184650076786056
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:48,994 INFO epoch # 314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.040572961210273206
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,019 INFO epoch # 315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.04189021373167634
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,043 INFO epoch # 316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.04280485992785543
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,068 INFO epoch # 317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.04036418843315914
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,092 INFO epoch # 318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03574055165518075
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,117 INFO epoch # 319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.034969885426107794
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,141 INFO epoch # 320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03982178180012852
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:49,141 INFO *** epoch 320, rolling-avg-loss (window=10)= 0.04027481951052323
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,166 INFO epoch # 321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.039206924266181886
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,190 INFO epoch # 322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03634599328506738
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,215 INFO epoch # 323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.038304379559122026
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,240 INFO epoch # 324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03807870630407706
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,264 INFO epoch # 325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03391324682161212
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,289 INFO epoch # 326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.035677183128427714
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,313 INFO epoch # 327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.035979692242108285
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,337 INFO epoch # 328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.042821720940992236
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,361 INFO epoch # 329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0407125279889442
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,385 INFO epoch # 330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.0424871533177793
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:49,385 INFO *** epoch 330, rolling-avg-loss (window=10)= 0.038352752785431224
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,410 INFO epoch # 331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.04145804332802072
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,434 INFO epoch # 332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.04063665069406852
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,458 INFO epoch # 333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.04509464802686125
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,482 INFO epoch # 334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.043808819726109505
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,506 INFO epoch # 335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03747626565746032
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,531 INFO epoch # 336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03344800241757184
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,555 INFO epoch # 337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.038968684573774226
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,579 INFO epoch # 338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.039592121000168845
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,603 INFO epoch # 339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03889931086450815
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,628 INFO epoch # 340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.038501248171087354
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:49,628 INFO *** epoch 340, rolling-avg-loss (window=10)= 0.03978837944596307
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,653 INFO epoch # 341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.041350137966219336
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,677 INFO epoch # 342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.04422771203098819
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,702 INFO epoch # 343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.04158157191704959
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,727 INFO epoch # 344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.03723182197427377
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,753 INFO epoch # 345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.04476277623325586
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,777 INFO epoch # 346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00549756 -loss = 0.040459326293785125
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,802 INFO epoch # 347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00549756-> 0.00439805 -loss = 0.05696651275502518
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,826 INFO epoch # 348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.04579275631112978
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,850 INFO epoch # 349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.038176836795173585
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,874 INFO epoch # 350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.03529708040878177
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:49,874 INFO *** epoch 350, rolling-avg-loss (window=10)= 0.042584653268568216
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,898 INFO epoch # 351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.03340995547478087
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,922 INFO epoch # 352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.030648519430542365
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,947 INFO epoch # 353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.03085342509439215
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,971 INFO epoch # 354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.0356628040317446
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:49,995 INFO epoch # 355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.036200228496454656
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,019 INFO epoch # 356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.033532847883179784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,044 INFO epoch # 357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.033192425093147904
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,068 INFO epoch # 358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.03419195377500728
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,092 INFO epoch # 359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.03303390892688185
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,116 INFO epoch # 360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.032401599193690345
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:50,116 INFO *** epoch 360, rolling-avg-loss (window=10)= 0.03331276673998218
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,141 INFO epoch # 361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.03436589671764523
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,165 INFO epoch # 362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00439805 -loss = 0.039183064945973456
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,189 INFO epoch # 363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00439805-> 0.00351844 -loss = 0.042963567248079926
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,213 INFO epoch # 364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.03001302998745814
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,237 INFO epoch # 365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.02710483004921116
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,262 INFO epoch # 366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.02836382042733021
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,286 INFO epoch # 367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.02770875472924672
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,311 INFO epoch # 368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.02990894587128423
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,335 INFO epoch # 369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.032577077799942344
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,360 INFO epoch # 370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.028374512738082558
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:50,360 INFO *** epoch 370, rolling-avg-loss (window=10)= 0.032056350051425396
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,385 INFO epoch # 371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0301222674024757
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,409 INFO epoch # 372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.031413561024237424
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,433 INFO epoch # 373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.03271120577119291
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,457 INFO epoch # 374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.027782608871348202
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,481 INFO epoch # 375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.027625795773928985
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,505 INFO epoch # 376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.026538620062638074
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,530 INFO epoch # 377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.026829951733816415
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,555 INFO epoch # 378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.027459499484393746
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,580 INFO epoch # 379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.026040078460937366
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,605 INFO epoch # 380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.025851443264400586
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:50,605 INFO *** epoch 380, rolling-avg-loss (window=10)= 0.02823750318493694
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,630 INFO epoch # 381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.02492610231274739
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,654 INFO epoch # 382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0274943477998022
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,678 INFO epoch # 383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.0274350762629183
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,702 INFO epoch # 384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.02612626607879065
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,726 INFO epoch # 385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.02768728247610852
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,750 INFO epoch # 386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.027054555335780606
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,776 INFO epoch # 387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.030984073411673307
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,800 INFO epoch # 388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.030956422619055957
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,825 INFO epoch # 389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.030479337845463306
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,849 INFO epoch # 390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.03070053076953627
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:50,850 INFO *** epoch 390, rolling-avg-loss (window=10)= 0.02838439949118765
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,874 INFO epoch # 391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00351844 -loss = 0.028134018095443025
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,898 INFO epoch # 392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00351844-> 0.00281475 -loss = 0.03301752803963609
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,922 INFO epoch # 393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02543508290546015
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,946 INFO epoch # 394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.024510589806595817
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,970 INFO epoch # 395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02479368116473779
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:50,995 INFO epoch # 396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02620429202215746
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,020 INFO epoch # 397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.026156595355132595
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,045 INFO epoch # 398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.026356747461250052
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,070 INFO epoch # 399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02465162391308695
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,094 INFO epoch # 400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02420471585355699
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:51,094 INFO *** epoch 400, rolling-avg-loss (window=10)= 0.026346487461705693
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,118 INFO epoch # 401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.025340758584206924
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,142 INFO epoch # 402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02782085008220747
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,166 INFO epoch # 403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.028009066154481843
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,191 INFO epoch # 404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.030342668906087056
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,215 INFO epoch # 405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02574597709462978
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,240 INFO epoch # 406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.024729149998165667
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,264 INFO epoch # 407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02619032165966928
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,289 INFO epoch # 408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.025161860830849037
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,313 INFO epoch # 409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.023514174536103383
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,337 INFO epoch # 410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.027245608187513426
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:51,337 INFO *** epoch 410, rolling-avg-loss (window=10)= 0.026410043603391387
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,361 INFO epoch # 411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.027343191526597366
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,385 INFO epoch # 412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.028805748093873262
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,409 INFO epoch # 413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.028677930211415514
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,433 INFO epoch # 414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.03230000103940256
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,458 INFO epoch # 415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.025022454268764704
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,483 INFO epoch # 416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.024904696008889005
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,507 INFO epoch # 417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.0234956064959988
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,532 INFO epoch # 418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02642068383283913
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,556 INFO epoch # 419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02324807390687056
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,580 INFO epoch # 420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.025004382070619613
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:51,581 INFO *** epoch 420, rolling-avg-loss (window=10)= 0.02652227674552705
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,605 INFO epoch # 421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.025848993856925517
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,629 INFO epoch # 422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.024861314421286806
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,654 INFO epoch # 423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02428749541286379
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,678 INFO epoch # 424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02374773981864564
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,703 INFO epoch # 425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.023065444198437035
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,727 INFO epoch # 426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.023822171322535723
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,752 INFO epoch # 427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.024593790527433157
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,776 INFO epoch # 428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02398788943537511
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,800 INFO epoch # 429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.024443267233436927
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,824 INFO epoch # 430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.027306736155878752
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:51,824 INFO *** epoch 430, rolling-avg-loss (window=10)= 0.024596484238281847
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,849 INFO epoch # 431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.03045745281269774
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,874 INFO epoch # 432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.025797552487347275
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,898 INFO epoch # 433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.024936628848081455
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,922 INFO epoch # 434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.027244594413787127
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,946 INFO epoch # 435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.00281475 -loss = 0.02968372355098836
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,970 INFO epoch # 436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00281475-> 0.0022518 -loss = 0.02640358393546194
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:51,994 INFO epoch # 437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.02424145463737659
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,018 INFO epoch # 438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.023796777677489445
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,042 INFO epoch # 439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.02421322575537488
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,066 INFO epoch # 440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.02251449559116736
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:52,066 INFO *** epoch 440, rolling-avg-loss (window=10)= 0.025928948970977216
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,091 INFO epoch # 441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.021769656625110656
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,115 INFO epoch # 442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.021241117210593075
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,139 INFO epoch # 443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.02372794324764982
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,163 INFO epoch # 444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.024030913482420146
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,187 INFO epoch # 445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.02342799774487503
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,211 INFO epoch # 446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.022498074307804927
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,235 INFO epoch # 447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.02139122676453553
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,259 INFO epoch # 448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.022409499855712056
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,283 INFO epoch # 449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.022036416921764612
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,308 INFO epoch # 450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.02172606100793928
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:52,308 INFO *** epoch 450, rolling-avg-loss (window=10)= 0.022425890716840514
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,332 INFO epoch # 451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.02279061244917102
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,356 INFO epoch # 452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.0022518 -loss = 0.021729443164076656
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,380 INFO epoch # 453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0022518-> 0.00180144 -loss = 0.022665825090371072
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,404 INFO epoch # 454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.022043630509870127
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,428 INFO epoch # 455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.020329737337306142
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,452 INFO epoch # 456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.019829888711683452
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,476 INFO epoch # 457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.02061142906313762
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,500 INFO epoch # 458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.021502482995856553
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,525 INFO epoch # 459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.021417916985228658
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,549 INFO epoch # 460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.02141310993465595
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:52,549 INFO *** epoch 460, rolling-avg-loss (window=10)= 0.021433407624135724
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,573 INFO epoch # 461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.02096229206654243
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,597 INFO epoch # 462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.021475494315382093
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,622 INFO epoch # 463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.021968393644783646
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,646 INFO epoch # 464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.020628678001230583
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,670 INFO epoch # 465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.021271311037708074
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,695 INFO epoch # 466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00180144 -loss = 0.020066946977749467
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,719 INFO epoch # 467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00180144-> 0.00144115 -loss = 0.020253401336958632
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,743 INFO epoch # 468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.019790644175373018
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,767 INFO epoch # 469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.019007498834980652
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,792 INFO epoch # 470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.019601091946242377
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:52,792 INFO *** epoch 470, rolling-avg-loss (window=10)= 0.020502575233695097
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,816 INFO epoch # 471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.019325914792716503
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,840 INFO epoch # 472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.01904753057169728
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,864 INFO epoch # 473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.020247702923370525
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,888 INFO epoch # 474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.02194144483655691
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,911 INFO epoch # 475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.021180416690185666
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,935 INFO epoch # 476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.02056831677327864
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,960 INFO epoch # 477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.01906886805954855
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:52,984 INFO epoch # 478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.02025198654155247
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,008 INFO epoch # 479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00144115 -loss = 0.020643614378059283
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,032 INFO epoch # 480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00144115-> 0.00115292 -loss = 0.019813478807918727
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:53,033 INFO *** epoch 480, rolling-avg-loss (window=10)= 0.020208927437488457
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,057 INFO epoch # 481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01974125739070587
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,081 INFO epoch # 482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.019362949708011
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,105 INFO epoch # 483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01911568973446265
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,129 INFO epoch # 484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018648587574716657
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,153 INFO epoch # 485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018774875352391973
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,177 INFO epoch # 486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018618830479681492
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,201 INFO epoch # 487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.019045729626668617
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,226 INFO epoch # 488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01913150559994392
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,250 INFO epoch # 489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018664822418941185
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,274 INFO epoch # 490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01866599582717754
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:53,275 INFO *** epoch 490, rolling-avg-loss (window=10)= 0.01897702437127009
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,299 INFO epoch # 491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.019422208657488227
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,323 INFO epoch # 492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01846288552042097
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,346 INFO epoch # 493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018890015926444903
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,371 INFO epoch # 494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.020033252134453505
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,395 INFO epoch # 495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.019827230513328686
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,419 INFO epoch # 496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018662615708308294
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,443 INFO epoch # 497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018357664986979216
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,467 INFO epoch # 498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018743801658274606
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,491 INFO epoch # 499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01848208243609406
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,514 INFO epoch # 500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018710156058659777
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:53,515 INFO *** epoch 500, rolling-avg-loss (window=10)= 0.018959191360045223
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,539 INFO epoch # 501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.017982017059694044
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,563 INFO epoch # 502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018414895952446386
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,587 INFO epoch # 503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018495937722036615
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,611 INFO epoch # 504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01813285006210208
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,635 INFO epoch # 505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01784370189125184
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,659 INFO epoch # 506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01934196028742008
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,683 INFO epoch # 507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018747761379927397
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,707 INFO epoch # 508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018396447383565828
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,731 INFO epoch # 509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01823899248847738
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,755 INFO epoch # 510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.018276293820235878
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:53,755 INFO *** epoch 510, rolling-avg-loss (window=10)= 0.018387085804715753
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,779 INFO epoch # 511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01898757545859553
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,803 INFO epoch # 512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.019109779794234782
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,827 INFO epoch # 513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.019069898960879073
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,851 INFO epoch # 514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.01873740172595717
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,876 INFO epoch # 515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.00115292 -loss = 0.019123069563647732
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,900 INFO epoch # 516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00115292-> 0.001 -loss = 0.01852192505612038
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,924 INFO epoch # 517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017888269023387693
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,948 INFO epoch # 518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017511499900138006
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,971 INFO epoch # 519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017617682518903166
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:53,995 INFO epoch # 520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017999173694988713
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:53,995 INFO *** epoch 520, rolling-avg-loss (window=10)= 0.018456627569685226
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,020 INFO epoch # 521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018036609777482226
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,044 INFO epoch # 522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01816562918247655
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,068 INFO epoch # 523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018189446127507836
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,092 INFO epoch # 524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017951274072402157
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,116 INFO epoch # 525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01742911944165826
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,141 INFO epoch # 526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017593596974620596
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,164 INFO epoch # 527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018356079439399764
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,188 INFO epoch # 528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018589695275295526
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,212 INFO epoch # 529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018110239718225785
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,236 INFO epoch # 530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017833087884355336
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:54,236 INFO *** epoch 530, rolling-avg-loss (window=10)= 0.018025477789342402
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,260 INFO epoch # 531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01758563076145947
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,284 INFO epoch # 532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018875985755585134
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,308 INFO epoch # 533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019140945485560223
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,332 INFO epoch # 534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01786472983076237
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,356 INFO epoch # 535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01773520593997091
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,380 INFO epoch # 536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01859305618563667
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,404 INFO epoch # 537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019065637578023598
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,428 INFO epoch # 538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017693460773443803
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,452 INFO epoch # 539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01909262192202732
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,476 INFO epoch # 540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018087310541886836
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:54,476 INFO *** epoch 540, rolling-avg-loss (window=10)= 0.018373458477435635
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,501 INFO epoch # 541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01757113973144442
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,526 INFO epoch # 542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01766875307657756
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,550 INFO epoch # 543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017250962831894867
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,574 INFO epoch # 544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018108148040482774
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,598 INFO epoch # 545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01735060941427946
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,622 INFO epoch # 546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017915328149683774
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,646 INFO epoch # 547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017511275713331997
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,670 INFO epoch # 548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017773411062080413
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,695 INFO epoch # 549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01733859023079276
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,719 INFO epoch # 550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017651997128268704
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:54,719 INFO *** epoch 550, rolling-avg-loss (window=10)= 0.017614021537883672
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,743 INFO epoch # 551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01788383792154491
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,767 INFO epoch # 552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017592678224900737
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,791 INFO epoch # 553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018204142281319946
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,815 INFO epoch # 554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017407064413418993
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,839 INFO epoch # 555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01790308317868039
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,863 INFO epoch # 556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017484105366747826
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,887 INFO epoch # 557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018632003891980276
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,911 INFO epoch # 558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01896057405974716
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,935 INFO epoch # 559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017264035006519407
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,959 INFO epoch # 560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018392824014881626
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:54,960 INFO *** epoch 560, rolling-avg-loss (window=10)= 0.017972434835974126
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:54,984 INFO epoch # 561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0178773567895405
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,008 INFO epoch # 562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017636732023674995
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,032 INFO epoch # 563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01759352145018056
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,056 INFO epoch # 564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017994450754486024
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,080 INFO epoch # 565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017615370481507853
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,104 INFO epoch # 566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018631968938279897
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,128 INFO epoch # 567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017612013965845108
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,152 INFO epoch # 568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01730701459746342
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,176 INFO epoch # 569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018234651011880487
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,200 INFO epoch # 570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017284766829106957
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:55,200 INFO *** epoch 570, rolling-avg-loss (window=10)= 0.01777878468419658
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,225 INFO epoch # 571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017049733956810087
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,249 INFO epoch # 572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01844416864332743
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,272 INFO epoch # 573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01763276371639222
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,296 INFO epoch # 574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017212093371199444
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,320 INFO epoch # 575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01683968075667508
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,345 INFO epoch # 576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017476368317147717
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,369 INFO epoch # 577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01753457385348156
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,393 INFO epoch # 578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017097275238484144
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,417 INFO epoch # 579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018184292915975675
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,441 INFO epoch # 580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017736498644808307
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:55,441 INFO *** epoch 580, rolling-avg-loss (window=10)= 0.017520744941430168
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,465 INFO epoch # 581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01709888005279936
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,489 INFO epoch # 582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017215343221323565
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,513 INFO epoch # 583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016984486574074253
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,538 INFO epoch # 584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01690580279682763
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,562 INFO epoch # 585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01682085735956207
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,587 INFO epoch # 586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0170462239329936
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,611 INFO epoch # 587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017359260353259742
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,635 INFO epoch # 588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017476987501140684
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,659 INFO epoch # 589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0171160307363607
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,683 INFO epoch # 590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017379428289132193
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:55,683 INFO *** epoch 590, rolling-avg-loss (window=10)= 0.01714033008174738
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,707 INFO epoch # 591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017003940272843465
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,731 INFO epoch # 592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016951398050878197
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,755 INFO epoch # 593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01724167409702204
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,779 INFO epoch # 594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01758245675591752
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,803 INFO epoch # 595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016886838449863717
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,827 INFO epoch # 596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017569399322383106
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,852 INFO epoch # 597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017392201523762196
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,875 INFO epoch # 598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017309402901446447
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,899 INFO epoch # 599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017043494881363586
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,923 INFO epoch # 600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01685774710495025
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:55,923 INFO *** epoch 600, rolling-avg-loss (window=10)= 0.017183855336043052
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,947 INFO epoch # 601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016450955139589496
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,971 INFO epoch # 602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016533608722966164
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:55,995 INFO epoch # 603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01823881157906726
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,020 INFO epoch # 604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018647828401299194
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,044 INFO epoch # 605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017969703301787376
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,068 INFO epoch # 606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018238583637867123
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,092 INFO epoch # 607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017215560175827704
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,116 INFO epoch # 608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016530908149434254
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,140 INFO epoch # 609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017217199550941586
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,164 INFO epoch # 610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01731505201314576
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:56,164 INFO *** epoch 610, rolling-avg-loss (window=10)= 0.017435821067192592
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,188 INFO epoch # 611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016715726611437276
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,212 INFO epoch # 612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016843329853145406
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,236 INFO epoch # 613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017330104223219678
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,260 INFO epoch # 614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017711786786094308
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,285 INFO epoch # 615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01701735207461752
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,310 INFO epoch # 616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017619634891161695
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,334 INFO epoch # 617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01696203788742423
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,358 INFO epoch # 618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017266022798139602
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,382 INFO epoch # 619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01668729462835472
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,406 INFO epoch # 620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01626738882623613
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:56,406 INFO *** epoch 620, rolling-avg-loss (window=10)= 0.017042067857983057
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,430 INFO epoch # 621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016622778959572315
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,454 INFO epoch # 622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016820237098727375
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,478 INFO epoch # 623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01647247547225561
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,502 INFO epoch # 624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01661875782883726
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,527 INFO epoch # 625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01675632050319109
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,551 INFO epoch # 626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016703186949598603
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,575 INFO epoch # 627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016928299228311516
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,599 INFO epoch # 628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01689381606411189
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,623 INFO epoch # 629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016417473801993765
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,646 INFO epoch # 630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016157417703652754
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:56,647 INFO *** epoch 630, rolling-avg-loss (window=10)= 0.01663907636102522
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,671 INFO epoch # 631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01640699277049862
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,695 INFO epoch # 632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01628229347988963
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,719 INFO epoch # 633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016614736785413697
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,743 INFO epoch # 634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01716829149518162
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,767 INFO epoch # 635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017107641615439206
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,791 INFO epoch # 636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016792098147561774
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,815 INFO epoch # 637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01779054317739792
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,839 INFO epoch # 638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018007191218202934
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,863 INFO epoch # 639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017262969893636182
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,887 INFO epoch # 640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016624063748167828
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:56,887 INFO *** epoch 640, rolling-avg-loss (window=10)= 0.01700568223313894
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,912 INFO epoch # 641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016763494379119948
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,936 INFO epoch # 642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017007393806125037
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,960 INFO epoch # 643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01644197790301405
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:56,984 INFO epoch # 644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016366644034860656
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,008 INFO epoch # 645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01653017057105899
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,032 INFO epoch # 646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01711719497689046
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,056 INFO epoch # 647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016985149748506956
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,080 INFO epoch # 648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016754837255575694
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,104 INFO epoch # 649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016669980162987486
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,128 INFO epoch # 650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01683629915351048
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:57,129 INFO *** epoch 650, rolling-avg-loss (window=10)= 0.016747314199164975
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,153 INFO epoch # 651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017111314955400303
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,177 INFO epoch # 652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016869753831997514
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,201 INFO epoch # 653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01656282163457945
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,225 INFO epoch # 654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01639682360109873
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,248 INFO epoch # 655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017238881875528023
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,273 INFO epoch # 656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016464690124848858
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,297 INFO epoch # 657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016103607893455774
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,321 INFO epoch # 658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016449053335236385
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,345 INFO epoch # 659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016357081301975995
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,369 INFO epoch # 660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01632738785701804
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:57,370 INFO *** epoch 660, rolling-avg-loss (window=10)= 0.016588141641113906
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,393 INFO epoch # 661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016780334059149027
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,417 INFO epoch # 662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017889526701765135
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,441 INFO epoch # 663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017304985434748232
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,465 INFO epoch # 664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016934933519223705
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,489 INFO epoch # 665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016654371516779065
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,513 INFO epoch # 666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016267386730760336
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,538 INFO epoch # 667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015898260520771146
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,562 INFO epoch # 668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01648056789417751
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,586 INFO epoch # 669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015698961447924376
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,610 INFO epoch # 670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01615681394468993
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:57,610 INFO *** epoch 670, rolling-avg-loss (window=10)= 0.016606614176998845
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,634 INFO epoch # 671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016353487051674165
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,658 INFO epoch # 672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016860035713762045
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,682 INFO epoch # 673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016061553789768368
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,706 INFO epoch # 674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01567641041765455
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,730 INFO epoch # 675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0161085916479351
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,754 INFO epoch # 676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015981333257514052
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,779 INFO epoch # 677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016614031497738324
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,803 INFO epoch # 678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016660106179188006
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,827 INFO epoch # 679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015353194161434658
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,850 INFO epoch # 680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016669858028762974
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:57,850 INFO *** epoch 680, rolling-avg-loss (window=10)= 0.016233860174543223
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,874 INFO epoch # 681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016042441624449566
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,898 INFO epoch # 682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015476071028388105
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,922 INFO epoch # 683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015527676267083734
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,946 INFO epoch # 684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015521054825512692
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,970 INFO epoch # 685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016386131290346384
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:57,995 INFO epoch # 686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01632253360003233
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,019 INFO epoch # 687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016279255505651236
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,043 INFO epoch # 688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015765434378408827
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,067 INFO epoch # 689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015394457121146843
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,091 INFO epoch # 690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015862569853197783
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:58,091 INFO *** epoch 690, rolling-avg-loss (window=10)= 0.01585776254942175
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,115 INFO epoch # 691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01612663823470939
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,139 INFO epoch # 692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015694109781179577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,164 INFO epoch # 693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015713849978055805
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,188 INFO epoch # 694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01580566496704705
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,212 INFO epoch # 695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016334881744114682
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,236 INFO epoch # 696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016039475201978348
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,260 INFO epoch # 697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016912131424760446
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,284 INFO epoch # 698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01557414187118411
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,308 INFO epoch # 699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015860561514273286
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,332 INFO epoch # 700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015725920238764957
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:58,332 INFO *** epoch 700, rolling-avg-loss (window=10)= 0.015978737495606764
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,356 INFO epoch # 701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015627199260052294
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,380 INFO epoch # 702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016015930668800138
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,404 INFO epoch # 703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015655756025807932
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,427 INFO epoch # 704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015806767623871565
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,451 INFO epoch # 705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01630382039002143
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,476 INFO epoch # 706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016362168389605358
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,500 INFO epoch # 707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01673096200102009
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,524 INFO epoch # 708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01563795394031331
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,548 INFO epoch # 709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015411135856993496
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,572 INFO epoch # 710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01584857162379194
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:58,572 INFO *** epoch 710, rolling-avg-loss (window=10)= 0.015940026578027754
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,596 INFO epoch # 711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015351547626778483
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,621 INFO epoch # 712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015787661817739718
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,645 INFO epoch # 713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01624265988357365
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,669 INFO epoch # 714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015451289160409942
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,693 INFO epoch # 715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015422657554154284
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,717 INFO epoch # 716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015865422785282135
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,741 INFO epoch # 717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015974067457136698
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,765 INFO epoch # 718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01570381020428613
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,789 INFO epoch # 719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015418929397128522
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,813 INFO epoch # 720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015513132995693013
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:58,813 INFO *** epoch 720, rolling-avg-loss (window=10)= 0.015673117888218256
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,837 INFO epoch # 721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015775947249494493
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,861 INFO epoch # 722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01612889215175528
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,885 INFO epoch # 723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01621204923139885
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,909 INFO epoch # 724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015688592102378607
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,933 INFO epoch # 725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01580058378749527
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,957 INFO epoch # 726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015437216687132604
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:58,981 INFO epoch # 727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015420765470480546
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,005 INFO epoch # 728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015715648711193353
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,029 INFO epoch # 729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01568353046604898
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,054 INFO epoch # 730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016355723346350715
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:59,054 INFO *** epoch 730, rolling-avg-loss (window=10)= 0.01582189492037287
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,079 INFO epoch # 731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015497963118832558
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,103 INFO epoch # 732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015775577048771083
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,127 INFO epoch # 733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015894352516625077
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,151 INFO epoch # 734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016026491517550312
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,175 INFO epoch # 735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015319006910431199
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,199 INFO epoch # 736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015497799700824544
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,223 INFO epoch # 737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01549542011343874
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,247 INFO epoch # 738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01539585817954503
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,271 INFO epoch # 739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016010280436603352
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,296 INFO epoch # 740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01566772001388017
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:59,296 INFO *** epoch 740, rolling-avg-loss (window=10)= 0.015658046955650207
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,320 INFO epoch # 741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01659072027541697
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,344 INFO epoch # 742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015141950323595665
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,368 INFO epoch # 743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015127210048376583
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,392 INFO epoch # 744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01573334587737918
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,416 INFO epoch # 745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015343611434218474
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,440 INFO epoch # 746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015532503064605407
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,464 INFO epoch # 747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015124819226912223
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,488 INFO epoch # 748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015321074635721743
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,512 INFO epoch # 749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015607399371219799
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,537 INFO epoch # 750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015117756673134863
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:59,537 INFO *** epoch 750, rolling-avg-loss (window=10)= 0.015464039093058091
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,561 INFO epoch # 751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015093126450665295
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,585 INFO epoch # 752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015212675876682624
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,609 INFO epoch # 753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015627236920408905
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,632 INFO epoch # 754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014920965797500685
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,657 INFO epoch # 755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01517437158327084
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,681 INFO epoch # 756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014878086891258135
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,705 INFO epoch # 757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014984570952947251
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,729 INFO epoch # 758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015343530714744702
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,753 INFO epoch # 759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015415024186950177
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,778 INFO epoch # 760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015760430076625198
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:00:59,778 INFO *** epoch 760, rolling-avg-loss (window=10)= 0.015241001945105382
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,802 INFO epoch # 761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015639066754374653
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,826 INFO epoch # 762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015543530287686735
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,849 INFO epoch # 763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0155468240409391
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,873 INFO epoch # 764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015074738243129104
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,898 INFO epoch # 765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015251340475515462
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,922 INFO epoch # 766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014981181884650141
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,946 INFO epoch # 767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014981971879024059
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,970 INFO epoch # 768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015116674650926143
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:00:59,994 INFO epoch # 769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015322735736845061
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,018 INFO epoch # 770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015862228523474187
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:00,018 INFO *** epoch 770, rolling-avg-loss (window=10)= 0.015332029247656465
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,043 INFO epoch # 771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015535873506451026
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,067 INFO epoch # 772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015215770399663597
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,091 INFO epoch # 773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01508982443192508
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,115 INFO epoch # 774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015743097377708182
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,139 INFO epoch # 775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015527031908277422
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,163 INFO epoch # 776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01537116797408089
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,187 INFO epoch # 777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014992895303294063
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,211 INFO epoch # 778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015238355234032497
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,235 INFO epoch # 779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01571170482202433
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,259 INFO epoch # 780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014468741923337802
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:00,259 INFO *** epoch 780, rolling-avg-loss (window=10)= 0.015289446288079489
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,283 INFO epoch # 781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015218423563055694
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,307 INFO epoch # 782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015662093850551173
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,331 INFO epoch # 783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015344873900176026
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,355 INFO epoch # 784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016553564317291602
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,380 INFO epoch # 785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01585598723613657
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,404 INFO epoch # 786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01516695530153811
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,428 INFO epoch # 787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01447702819132246
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,452 INFO epoch # 788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014781538222450763
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,476 INFO epoch # 789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01471394048712682
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,500 INFO epoch # 790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015259592633810826
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:00,500 INFO *** epoch 790, rolling-avg-loss (window=10)= 0.015303399770346005
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,525 INFO epoch # 791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014946629860787652
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,549 INFO epoch # 792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014561275049345568
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,574 INFO epoch # 793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015273586293915287
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,599 INFO epoch # 794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015350275367381983
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,624 INFO epoch # 795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01530507902498357
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,649 INFO epoch # 796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014640085719292983
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,673 INFO epoch # 797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015331373346270993
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,698 INFO epoch # 798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01474043310736306
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,722 INFO epoch # 799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014404313929844648
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,746 INFO epoch # 800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014877972673275508
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:00,746 INFO *** epoch 800, rolling-avg-loss (window=10)= 0.014943102437246124
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,771 INFO epoch # 801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01524128916207701
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,795 INFO epoch # 802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014912005426594988
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,819 INFO epoch # 803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015006486792117357
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,843 INFO epoch # 804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014737199555383995
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,867 INFO epoch # 805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016043591356719844
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,891 INFO epoch # 806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015399248790345155
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,915 INFO epoch # 807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015999287425074726
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,939 INFO epoch # 808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017213088751304895
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,963 INFO epoch # 809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015558723855065182
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:00,987 INFO epoch # 810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014270483021391556
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:00,987 INFO *** epoch 810, rolling-avg-loss (window=10)= 0.01543814041360747
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,011 INFO epoch # 811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01484659151174128
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,036 INFO epoch # 812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015421488700667396
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,060 INFO epoch # 813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016300952454912476
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,084 INFO epoch # 814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015169791411608458
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,108 INFO epoch # 815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014697513484861702
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,132 INFO epoch # 816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01432169257896021
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,155 INFO epoch # 817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014593041691114195
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,179 INFO epoch # 818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014699218969326466
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,203 INFO epoch # 819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014918662578566
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,227 INFO epoch # 820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014032568433322012
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:01,228 INFO *** epoch 820, rolling-avg-loss (window=10)= 0.01490015218150802
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,252 INFO epoch # 821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014700626066769473
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,276 INFO epoch # 822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015045355175971054
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,300 INFO epoch # 823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014394186437129974
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,323 INFO epoch # 824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014804505568463355
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,347 INFO epoch # 825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01421080538420938
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,371 INFO epoch # 826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014214201451977715
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,395 INFO epoch # 827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014796339673921466
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,422 INFO epoch # 828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014065570139791816
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,446 INFO epoch # 829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014396916507394053
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,470 INFO epoch # 830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014543921424774453
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:01,470 INFO *** epoch 830, rolling-avg-loss (window=10)= 0.014517242783040275
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,494 INFO epoch # 831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01455027042538859
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,518 INFO epoch # 832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014685570815345272
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,542 INFO epoch # 833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015009290189482272
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,565 INFO epoch # 834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014858732814900577
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,589 INFO epoch # 835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015326625638408586
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,613 INFO epoch # 836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014599623827962205
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,637 INFO epoch # 837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014659614142146893
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,662 INFO epoch # 838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014737528646946885
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,686 INFO epoch # 839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014193500697729178
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,710 INFO epoch # 840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014423686850932427
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:01,710 INFO *** epoch 840, rolling-avg-loss (window=10)= 0.014704444404924289
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,734 INFO epoch # 841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014589749422157183
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,757 INFO epoch # 842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014756673539523035
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,781 INFO epoch # 843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015126214930205606
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,806 INFO epoch # 844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014739145728526637
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,831 INFO epoch # 845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014413967102882452
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,856 INFO epoch # 846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014370872886502184
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,881 INFO epoch # 847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014121099084150046
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,906 INFO epoch # 848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014453024574322626
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,930 INFO epoch # 849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01411744422512129
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,955 INFO epoch # 850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014495719500700943
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:01,955 INFO *** epoch 850, rolling-avg-loss (window=10)= 0.0145183910994092
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:01,979 INFO epoch # 851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014290876759332605
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,003 INFO epoch # 852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014402595843421295
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,028 INFO epoch # 853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014144731729174964
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,053 INFO epoch # 854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01398736254486721
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,078 INFO epoch # 855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014117115511908196
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,103 INFO epoch # 856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014225387232727371
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,129 INFO epoch # 857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014384451467776671
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,154 INFO epoch # 858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015296838697395287
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,178 INFO epoch # 859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014414490782655776
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,202 INFO epoch # 860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01410741763538681
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:02,202 INFO *** epoch 860, rolling-avg-loss (window=10)= 0.014337126820464619
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,226 INFO epoch # 861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014642362788436003
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,250 INFO epoch # 862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014566956291673705
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,275 INFO epoch # 863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015215706487651914
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,299 INFO epoch # 864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015266195288859308
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,324 INFO epoch # 865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01435970673628617
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,348 INFO epoch # 866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014105797439697199
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,373 INFO epoch # 867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013882400264265016
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,397 INFO epoch # 868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014352494414197281
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,421 INFO epoch # 869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01471542994841002
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,445 INFO epoch # 870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014160247315885499
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:02,445 INFO *** epoch 870, rolling-avg-loss (window=10)= 0.014526729697536211
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,469 INFO epoch # 871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013714023545617238
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,494 INFO epoch # 872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013328104323591106
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,519 INFO epoch # 873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014085122282267548
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,543 INFO epoch # 874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013771351310424507
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,568 INFO epoch # 875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01392117072828114
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,592 INFO epoch # 876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0158099996187957
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,616 INFO epoch # 877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014535233291098848
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,640 INFO epoch # 878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014299342699814588
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,664 INFO epoch # 879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014607166813220829
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,689 INFO epoch # 880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01503001588571351
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:02,689 INFO *** epoch 880, rolling-avg-loss (window=10)= 0.014310153049882502
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,714 INFO epoch # 881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014674171368824318
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,739 INFO epoch # 882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014022666393429972
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,763 INFO epoch # 883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014098084400757216
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,788 INFO epoch # 884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013965954014565796
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,812 INFO epoch # 885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013759189387201332
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,836 INFO epoch # 886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014559170449501835
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,860 INFO epoch # 887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013847033202182502
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,883 INFO epoch # 888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013954895868664607
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,908 INFO epoch # 889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013776784835499711
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,932 INFO epoch # 890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01390525211172644
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:02,933 INFO *** epoch 890, rolling-avg-loss (window=10)= 0.014056320203235374
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,957 INFO epoch # 891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013808107032673433
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:02,982 INFO epoch # 892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013765342766419053
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,007 INFO epoch # 893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014159019890939817
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,031 INFO epoch # 894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01402293486171402
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,055 INFO epoch # 895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01378946693148464
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,079 INFO epoch # 896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013470203266479075
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,103 INFO epoch # 897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014311416583950631
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,127 INFO epoch # 898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013630664558149874
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,152 INFO epoch # 899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01402074626821559
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,177 INFO epoch # 900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013975576788652688
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:03,178 INFO *** epoch 900, rolling-avg-loss (window=10)= 0.013895347894867882
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,203 INFO epoch # 901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014426197827560827
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,228 INFO epoch # 902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014907282835338265
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,251 INFO epoch # 903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013495181745383888
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,275 INFO epoch # 904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014051442412892357
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,299 INFO epoch # 905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013730276419664733
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,323 INFO epoch # 906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013626636151457205
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,348 INFO epoch # 907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01364426568034105
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,373 INFO epoch # 908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014094751968514174
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,397 INFO epoch # 909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013541731997975148
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,422 INFO epoch # 910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01357406827446539
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:03,422 INFO *** epoch 910, rolling-avg-loss (window=10)= 0.013909183531359304
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,447 INFO epoch # 911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01349251504871063
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,471 INFO epoch # 912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013571529867476784
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,495 INFO epoch # 913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013415727531537414
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,519 INFO epoch # 914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013558133170590736
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,543 INFO epoch # 915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014368680669576861
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,568 INFO epoch # 916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014541739146807231
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,593 INFO epoch # 917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014085313770920038
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,617 INFO epoch # 918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01412773213814944
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,642 INFO epoch # 919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013908818102208897
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,666 INFO epoch # 920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014195917538017966
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:03,666 INFO *** epoch 920, rolling-avg-loss (window=10)= 0.0139266106983996
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,691 INFO epoch # 921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013509594777133316
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,715 INFO epoch # 922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015499298431677744
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,738 INFO epoch # 923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015181351132923737
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,762 INFO epoch # 924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01409438598784618
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,787 INFO epoch # 925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013736948996665888
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,811 INFO epoch # 926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014970645977882668
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,835 INFO epoch # 927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014186857122695073
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,860 INFO epoch # 928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014157061610603705
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,884 INFO epoch # 929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014346329844556749
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,908 INFO epoch # 930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014750175789231434
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:03,908 INFO *** epoch 930, rolling-avg-loss (window=10)= 0.01444326496712165
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,932 INFO epoch # 931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014400618965737522
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,956 INFO epoch # 932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01376497712044511
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:03,980 INFO epoch # 933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01372165244538337
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,004 INFO epoch # 934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013034276402322575
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,028 INFO epoch # 935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01365044291014783
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,053 INFO epoch # 936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013988412218168378
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,077 INFO epoch # 937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013802986388327554
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,101 INFO epoch # 938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013236545724794269
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,125 INFO epoch # 939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012965406640432775
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,149 INFO epoch # 940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013136584879248403
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:04,149 INFO *** epoch 940, rolling-avg-loss (window=10)= 0.013570190369500778
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,173 INFO epoch # 941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013513784913811833
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,197 INFO epoch # 942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013478417880833149
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,221 INFO epoch # 943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014072246704017743
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,245 INFO epoch # 944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013915609961259179
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,270 INFO epoch # 945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013684614415979013
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,294 INFO epoch # 946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01423512514156755
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,318 INFO epoch # 947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014844026474747807
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,342 INFO epoch # 948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015595806355122477
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,366 INFO epoch # 949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014318168367026374
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,390 INFO epoch # 950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013198726242990233
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:04,390 INFO *** epoch 950, rolling-avg-loss (window=10)= 0.014085652645735535
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,414 INFO epoch # 951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01332106871996075
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,438 INFO epoch # 952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01318356151750777
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,462 INFO epoch # 953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013421530951745808
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,486 INFO epoch # 954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014571737527148798
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,511 INFO epoch # 955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01348334364593029
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,535 INFO epoch # 956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013397965652984567
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,559 INFO epoch # 957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013370933098485693
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,583 INFO epoch # 958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01321086022653617
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,607 INFO epoch # 959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013503919515642338
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,631 INFO epoch # 960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013498067710315809
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:04,631 INFO *** epoch 960, rolling-avg-loss (window=10)= 0.013496298856625798
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,655 INFO epoch # 961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013349242814001627
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,679 INFO epoch # 962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013789175456622615
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,703 INFO epoch # 963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013432485662633553
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,728 INFO epoch # 964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012970544892596081
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,752 INFO epoch # 965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013264807013911195
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,776 INFO epoch # 966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01330567421973683
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,800 INFO epoch # 967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014014185697305948
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,824 INFO epoch # 968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013651464600116014
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,848 INFO epoch # 969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01270547010062728
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,872 INFO epoch # 970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012986929141334258
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:04,872 INFO *** epoch 970, rolling-avg-loss (window=10)= 0.01334699795988854
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,896 INFO epoch # 971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014026654738700017
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,920 INFO epoch # 972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013580675862613134
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,945 INFO epoch # 973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013081566576147452
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,969 INFO epoch # 974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012711753079202026
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:04,993 INFO epoch # 975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013806339309667237
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,017 INFO epoch # 976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014696296668262221
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,041 INFO epoch # 977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01378380844835192
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,065 INFO epoch # 978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01311080131563358
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,089 INFO epoch # 979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013261478132335469
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,113 INFO epoch # 980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012994134405744262
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:05,113 INFO *** epoch 980, rolling-avg-loss (window=10)= 0.013505350853665731
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,137 INFO epoch # 981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012676780272158794
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,162 INFO epoch # 982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013350706503842957
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,186 INFO epoch # 983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013384672682150267
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,210 INFO epoch # 984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013750462181633338
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,235 INFO epoch # 985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0133759681484662
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,259 INFO epoch # 986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013627852196805179
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,282 INFO epoch # 987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014068963006138802
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,306 INFO epoch # 988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013025998181547038
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,330 INFO epoch # 989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012864247080869973
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,354 INFO epoch # 990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01350464829010889
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:05,355 INFO *** epoch 990, rolling-avg-loss (window=10)= 0.013363029854372144
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,379 INFO epoch # 991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013094634865410626
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,403 INFO epoch # 992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012751778282108717
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,427 INFO epoch # 993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013250871284981258
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,451 INFO epoch # 994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013577280173194595
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,475 INFO epoch # 995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014018086934811436
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,499 INFO epoch # 996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014313700594357215
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,523 INFO epoch # 997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013556598787545227
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,547 INFO epoch # 998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013894230331061408
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,571 INFO epoch # 999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01324295115773566
[experiments_sandbox.py:900 -   <module>()] 2023-04-25 21:01:05,596 INFO epoch # 1000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012787358413334005
[experiments_sandbox.py:912 -   <module>()] 2023-04-25 21:01:05,596 INFO *** epoch 1000, rolling-avg-loss (window=10)= 0.013448749082454015
[experiments_sandbox.py:920 -   <module>()] 2023-04-25 21:01:05,596 INFO training time in seconds = 25
[experiments_sandbox.py:936 -   <module>()] 2023-04-25 21:01:05,699 INFO train-epochs-loss curve df :
[experiments_sandbox.py:937 -   <module>()] 2023-04-25 21:01:05,703 INFO 
    epochs       loss
0       10  25.251804
1       20   7.463975
2       30   6.251446
3       40   6.199872
4       50   4.977004
5       60   4.642469
6       70   3.920165
7       80   3.855698
8       90   2.241879
9      100   1.666294
10     110   0.811469
11     120   0.643886
12     130   0.397324
13     140   0.713679
14     150   0.366518
15     160   0.301123
16     170   0.309013
17     180   0.242344
18     190   0.193967
19     200   0.145395
20     210   0.255467
21     220   0.111951
22     230   0.128196
23     240   0.071792
24     250   0.068647
25     260   0.064053
26     270   0.079419
27     280   0.052537
28     290   0.054858
29     300   0.044810
30     310   0.040976
31     320   0.040275
32     330   0.038353
33     340   0.039788
34     350   0.042585
35     360   0.033313
36     370   0.032056
37     380   0.028238
38     390   0.028384
39     400   0.026346
40     410   0.026410
41     420   0.026522
42     430   0.024596
43     440   0.025929
44     450   0.022426
45     460   0.021433
46     470   0.020503
47     480   0.020209
48     490   0.018977
49     500   0.018959
50     510   0.018387
51     520   0.018457
52     530   0.018025
53     540   0.018373
54     550   0.017614
55     560   0.017972
56     570   0.017779
57     580   0.017521
58     590   0.017140
59     600   0.017184
60     610   0.017436
61     620   0.017042
62     630   0.016639
63     640   0.017006
64     650   0.016747
65     660   0.016588
66     670   0.016607
67     680   0.016234
68     690   0.015858
69     700   0.015979
70     710   0.015940
71     720   0.015673
72     730   0.015822
73     740   0.015658
74     750   0.015464
75     760   0.015241
76     770   0.015332
77     780   0.015289
78     790   0.015303
79     800   0.014943
80     810   0.015438
81     820   0.014900
82     830   0.014517
83     840   0.014704
84     850   0.014518
85     860   0.014337
86     870   0.014527
87     880   0.014310
88     890   0.014056
89     900   0.013895
90     910   0.013909
91     920   0.013927
92     930   0.014443
93     940   0.013570
94     950   0.014086
95     960   0.013496
96     970   0.013347
97     980   0.013505
98     990   0.013363
99    1000   0.013449
[experiments_sandbox.py:939 -   <module>()] 2023-04-25 21:01:05,703 INFO Model parameters after training
[experiments_sandbox.py:940 -   <module>()] 2023-04-25 21:01:05,703 INFO Model = NNmodel
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 21:01:05,704 INFO net.0.weight = Parameter containing:
tensor([[ 1.5892e+00,  1.0302e-02, -1.8247e+00],
        [ 6.7178e-01, -1.9982e-01,  6.8558e-01],
        [ 1.5741e+00,  4.4505e-02,  1.7681e+00],
        [ 2.0692e+00,  1.3559e-02, -2.2773e+00],
        [-2.0463e+00,  1.9580e-01,  2.4417e+00],
        [ 2.0778e+00, -6.4701e-02,  2.2831e+00],
        [ 2.2088e+00,  1.7873e-01,  2.4407e+00],
        [-4.4728e-01,  2.1453e-01, -3.9603e-01],
        [ 2.8046e-01, -4.0704e-01, -1.7704e-01],
        [-1.7395e+00,  4.0720e-01, -2.0483e+00],
        [ 1.5553e+00,  1.2547e-01,  1.8804e+00],
        [ 2.1947e+00, -2.4876e-01,  2.5281e+00],
        [-1.7377e+00,  4.4438e-01, -2.0977e+00],
        [-6.6339e-01,  2.7474e-01, -2.0786e+00],
        [ 2.4094e+00, -2.3937e-03, -1.1214e+00],
        [-1.9552e+00, -4.2792e-01,  2.6546e+00],
        [-1.8091e+00, -1.2746e-01, -1.9875e+00],
        [ 8.6356e-02, -5.5588e-01,  2.8744e+00],
        [ 2.6161e+00,  1.8508e-02, -2.8396e+00],
        [ 1.9557e+00,  1.1036e-02, -2.4343e+00],
        [-2.3934e+00, -9.8263e-03,  2.8268e+00],
        [ 1.1850e+00,  8.6805e-02, -1.1569e+00],
        [ 2.0012e+00,  1.3198e-01, -2.5470e+00],
        [ 1.7646e+00, -8.3839e-01, -1.9692e+00],
        [-2.7234e+00,  2.9399e-02,  1.1658e+00],
        [ 2.1859e+00,  5.2975e-02, -2.3744e+00],
        [ 2.2028e+00, -3.9527e-01,  2.0977e+00],
        [ 1.4996e+00, -1.6806e-01,  2.1169e+00],
        [ 7.3001e-02,  4.3671e-01,  1.2281e+00],
        [ 1.9460e+00,  1.7773e-01,  2.2215e+00],
        [-1.9136e+00, -1.4575e-02,  1.3883e+00],
        [ 2.0468e+00,  8.7316e-02, -2.5349e+00],
        [ 2.4617e+00,  2.9713e-02,  2.6676e+00],
        [-1.3289e+00,  2.0118e-01, -1.6107e+00],
        [-1.3288e+00,  2.3725e-01, -1.4694e+00],
        [ 4.2032e-01, -5.7946e-01, -2.6509e-01],
        [-2.1386e+00, -3.6050e-01,  2.1567e+00],
        [-1.2705e+00, -4.4905e-02, -1.5051e+00],
        [-2.1756e+00,  3.0210e-01,  2.3097e+00],
        [-2.1991e+00, -1.2064e-02,  2.5624e+00],
        [ 2.3233e+00,  1.8607e-02, -2.1825e+00],
        [ 2.1594e+00,  2.2012e-01,  2.9672e+00],
        [-3.1105e-01, -3.8751e-01, -5.5577e-03],
        [-2.0780e+00, -2.6720e-01, -2.5602e+00],
        [-2.0115e+00, -5.4734e-02,  2.3043e+00],
        [-1.4401e+00, -9.2390e-03,  1.5948e+00],
        [-3.5533e-01,  5.3141e-01,  2.0694e-01],
        [ 2.3409e+00, -2.8925e-03, -2.6527e+00],
        [ 2.1761e+00,  5.3218e-02,  2.4128e+00],
        [-2.4200e+00, -4.7307e-02, -2.6406e+00]], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 21:01:05,705 INFO net.0.bias = Parameter containing:
tensor([ -1.4867,   0.5790,  -1.3218,   4.0298,   8.0767,   8.4385,  -8.4728,
          0.9205,   0.0673,  -8.9897,   3.8425,   7.2528,  -8.1940,   7.7090,
          9.7085,   9.1936,   6.3309,  -8.9650,   6.8909,  -6.1566,   5.7375,
          8.0900,  -7.7407,   8.7405, -10.9225,   9.6697,  -8.0313,   9.5940,
         -7.7459,   9.1303,  -8.2377,   7.6495,  -7.2156,  -9.2193,  -3.1298,
          1.8041,   8.7806,  -1.7323,   9.1213,   3.7602,   7.3146,   9.8093,
          0.9724,  -7.0583,  -9.2233,  -1.5897,   1.5744,  -7.0404,  -3.5543,
          5.5664], requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 21:01:05,706 INFO net.3.weight = Parameter containing:
tensor([[-2.8072e-01, -5.0563e+00, -5.0773e-01,  6.6410e-02,  2.0972e-01,
         -1.1316e+00, -4.2337e-01,  6.2021e+00, -1.6923e+01,  2.6339e-01,
          1.1421e+00, -2.5069e+00,  3.4161e+00, -3.0465e-01,  6.4184e-01,
         -1.3562e-01,  1.0017e-01, -1.0557e+00, -1.3002e-01,  5.2436e-01,
          2.9454e-01, -1.1480e+00, -3.0841e-01, -1.7192e+00,  1.2978e-01,
          7.4197e-01, -7.2938e-01,  1.0561e-01, -9.8876e-01,  3.5027e-01,
          4.9682e-01,  1.8018e-02, -4.3731e-01, -2.8510e-01,  2.9324e+00,
         -1.0709e+01, -9.8152e-01, -3.5985e-01,  8.2570e-01,  3.3597e-02,
         -1.0474e-02,  1.0353e+00, -1.0344e+00, -1.4818e+00, -7.8044e-01,
          2.4658e-01,  1.2413e+01, -3.9677e-01, -3.2626e-01,  3.8869e-01],
        [-6.3646e+00, -3.6974e+00,  5.9494e+00,  8.8329e+00,  8.3275e+00,
         -1.1089e+01,  8.9784e+00, -1.2509e+00,  1.0420e+00,  1.1003e+01,
         -1.0578e+01, -8.6752e+00,  9.3538e+00, -5.6443e+00,  1.2440e+01,
          8.7795e+00, -8.3470e+00,  8.8940e+00,  9.5033e+00, -5.8608e+00,
          9.1324e+00,  7.9100e+00, -8.2671e+00,  7.1340e+00, -1.1616e+01,
          1.0517e+01,  6.3646e+00, -9.1095e+00,  8.1214e+00, -1.0171e+01,
         -9.2830e+00,  9.5062e+00,  9.5241e+00,  7.9708e+00,  6.6747e+00,
          1.4709e+00,  7.4894e+00,  8.5917e+00,  8.0198e+00,  7.2406e+00,
          9.4908e+00, -8.6085e+00, -2.8216e-01,  7.2621e+00, -9.8965e+00,
         -7.9805e+00,  2.0205e-01, -8.1120e+00,  6.8614e+00, -8.5993e+00],
        [-1.5233e-01, -1.4982e+00, -9.8913e-02, -8.4778e-02,  4.2187e-01,
         -6.0338e-01,  9.1005e-01,  5.6136e+00,  4.1359e+00, -1.0486e-01,
         -3.4925e-01, -5.5415e-03, -6.7487e-01,  2.9212e-01,  1.3721e-01,
         -8.0043e-02, -5.3253e-01, -7.3292e-01, -8.9349e-02,  2.0341e-01,
          5.5789e-02,  7.9435e-01,  2.3965e-01,  7.9954e-01,  3.7165e-01,
         -6.2923e-01, -1.9805e+00, -5.0824e-01, -1.3656e+00, -1.0773e+00,
          5.6913e-01, -3.3313e-02, -1.9659e-01,  8.8158e-01,  2.4375e-01,
          4.7939e+00, -8.9317e-02,  3.4369e-01, -1.4771e-01, -1.4625e-02,
         -1.6304e-01, -9.4233e-02, -6.5243e+00,  6.3519e-01,  1.4674e-01,
          2.4987e-01, -3.9438e-01, -1.8992e-01,  6.0889e-02, -5.5245e-02]],
       requires_grad=True)
[experiments_sandbox.py:942 -   <module>()] 2023-04-25 21:01:05,706 INFO net.3.bias = Parameter containing:
tensor([ 0.6299, -4.0439, -1.4585], requires_grad=True)
[experiments_sandbox.py:945 -   <module>()] 2023-04-25 21:01:05,706 INFO Out-of sample batch-test
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,706 INFO test-batch  # 0 => test-loss = 48.83851623535156
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,707 INFO test-batch  # 1 => test-loss = 0.03639598190784454
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,707 INFO test-batch  # 2 => test-loss = 0.07227254658937454
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,707 INFO test-batch  # 3 => test-loss = 0.1356656849384308
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,708 INFO test-batch  # 4 => test-loss = 0.02236608974635601
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,708 INFO test-batch  # 5 => test-loss = 0.014373785816133022
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,708 INFO test-batch  # 6 => test-loss = 0.03359712287783623
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,709 INFO test-batch  # 7 => test-loss = 0.05466802045702934
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,709 INFO test-batch  # 8 => test-loss = 0.03178147226572037
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,709 INFO test-batch  # 9 => test-loss = 0.025394177064299583
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,710 INFO test-batch  # 10 => test-loss = 0.566400945186615
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,710 INFO test-batch  # 11 => test-loss = 0.19068282842636108
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,710 INFO test-batch  # 12 => test-loss = 0.18457911908626556
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,711 INFO test-batch  # 13 => test-loss = 0.017451878637075424
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,711 INFO test-batch  # 14 => test-loss = 0.08717884868383408
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,711 INFO test-batch  # 15 => test-loss = 0.02653087116777897
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,712 INFO test-batch  # 16 => test-loss = 0.0532732792198658
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,712 INFO test-batch  # 17 => test-loss = 0.033997587859630585
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,712 INFO test-batch  # 18 => test-loss = 0.020518161356449127
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,713 INFO test-batch  # 19 => test-loss = 0.028699710965156555
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,713 INFO test-batch  # 20 => test-loss = 0.020267998799681664
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,713 INFO test-batch  # 21 => test-loss = 0.21768403053283691
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,714 INFO test-batch  # 22 => test-loss = 0.018868226557970047
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,714 INFO test-batch  # 23 => test-loss = 0.011864406056702137
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,714 INFO test-batch  # 24 => test-loss = 0.01953550986945629
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,715 INFO test-batch  # 25 => test-loss = 0.06574979424476624
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,715 INFO test-batch  # 26 => test-loss = 0.02746865712106228
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,715 INFO test-batch  # 27 => test-loss = 0.026715286076068878
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,716 INFO test-batch  # 28 => test-loss = 0.008635801263153553
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,716 INFO test-batch  # 29 => test-loss = 0.08261672407388687
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,716 INFO test-batch  # 30 => test-loss = 0.053417813032865524
[experiments_sandbox.py:950 -   <module>()] 2023-04-25 21:01:05,717 INFO test-batch  # 31 => test-loss = 0.016757629811763763
