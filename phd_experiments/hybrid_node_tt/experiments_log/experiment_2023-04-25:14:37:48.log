[experiments_sandbox.py:736 -   <module>()] 2023-04-25 14:37:48,038 INFO SEED = 42
[experiments_sandbox.py:817 -   <module>()] 2023-04-25 14:37:48,039 INFO model = 
***
NN-Model 
Sequential(
  (0): Linear(in_features=3, out_features=50, bias=True)
  (1): Identity()
  (2): Tanh()
  (3): Linear(in_features=50, out_features=3, bias=True)
)
numel_learnable = 353
***
[experiments_sandbox.py:818 -   <module>()] 2023-04-25 14:37:48,039 INFO optimizer  = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.2
    maximize: False
    weight_decay: 0
)
[experiments_sandbox.py:826 -   <module>()] 2023-04-25 14:37:48,039 INFO lr_scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f29fcad44f0>
[experiments_sandbox.py:829 -   <module>()] 2023-04-25 14:37:48,040 INFO Normalize-Data-source-X-train = False
[experiments_sandbox.py:830 -   <module>()] 2023-04-25 14:37:48,040 INFO Normalize-Data-source-Y-train = False
[experiments_sandbox.py:831 -   <module>()] 2023-04-25 14:37:48,040 INFO Normalize-Data-source-X-test = False
[experiments_sandbox.py:832 -   <module>()] 2023-04-25 14:37:48,040 INFO Normalize-Data-source-Y-test = False
[experiments_sandbox.py:861 -   <module>()] 2023-04-25 14:37:48,042 INFO train-dataset = 
***
Lorenz-System
N = 1000rho = 28
sigma = 10
beta = 2.6666666666666665
normalize_X = Falsenormalize_Y = False****

[experiments_sandbox.py:862 -   <module>()] 2023-04-25 14:37:48,042 INFO test-dataset = 
***
Lorenz-System
N = 1000rho = 28
sigma = 10
beta = 2.6666666666666665
normalize_X = Falsenormalize_Y = False****

[experiments_sandbox.py:863 -   <module>()] 2023-04-25 14:37:48,042 INFO train-epochs = 10000
[experiments_sandbox.py:867 -   <module>()] 2023-04-25 14:37:48,042 INFO Input batch normalization = False
[experiments_sandbox.py:868 -   <module>()] 2023-04-25 14:37:48,042 INFO Output Normalization = None
[experiments_sandbox.py:869 -   <module>()] 2023-04-25 14:37:48,042 INFO Gradient-clipping max-norm = 10
[experiments_sandbox.py:871 -   <module>()] 2023-04-25 14:37:48,042 INFO epochs_losses_window = 10
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,661 INFO epoch # 0 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 199.81889653205872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,685 INFO epoch # 1 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 54.65110655128956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,708 INFO epoch # 2 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 30.959193974733353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,731 INFO epoch # 3 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 20.749977707862854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,754 INFO epoch # 4 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 20.598104164004326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,778 INFO epoch # 5 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 20.639302104711533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,801 INFO epoch # 6 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 20.388713978230953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,824 INFO epoch # 7 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 16.270186260342598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,848 INFO epoch # 8 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 16.451615907251835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,871 INFO epoch # 9 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 16.28344750776887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,895 INFO epoch # 10 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 14.715382158756256
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:49,895 INFO *** epoch 10, rolling-avg-loss (window=10)= 23.170703031495215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,919 INFO epoch # 11 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 16.72354907542467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,942 INFO epoch # 12 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 13.097344227135181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,966 INFO epoch # 13 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 17.92801247164607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:49,989 INFO epoch # 14 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 17.154661029577255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,012 INFO epoch # 15 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 18.095617040991783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,036 INFO epoch # 16 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 20.396206498146057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,059 INFO epoch # 17 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 19.42458852007985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,082 INFO epoch # 18 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 14.830054845660925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,106 INFO epoch # 19 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 15.302748590707779
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,129 INFO epoch # 20 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 13.35671554505825
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:50,129 INFO *** epoch 20, rolling-avg-loss (window=10)= 16.630949784442784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,153 INFO epoch # 21 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 16.624506264925003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,176 INFO epoch # 22 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.2 -loss = 13.965942353010178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,199 INFO epoch # 23 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.2-> 0.16 -loss = 13.971434101462364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,222 INFO epoch # 24 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 10.497338242828846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,246 INFO epoch # 25 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 10.818011045455933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,269 INFO epoch # 26 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 10.96745116263628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,293 INFO epoch # 27 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 8.252608437091112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,316 INFO epoch # 28 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 12.886233247816563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,339 INFO epoch # 29 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 9.751194544136524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,363 INFO epoch # 30 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 8.917024821043015
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:50,364 INFO *** epoch 30, rolling-avg-loss (window=10)= 11.665174422040582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,387 INFO epoch # 31 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 8.106176372617483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,410 INFO epoch # 32 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 14.050414212048054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,434 INFO epoch # 33 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 13.757787141948938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,457 INFO epoch # 34 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 14.758420415222645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,480 INFO epoch # 35 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 11.369987688958645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,503 INFO epoch # 36 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 10.022981010377407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,526 INFO epoch # 37 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 9.392575547099113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,550 INFO epoch # 38 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 10.455891393125057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,573 INFO epoch # 39 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 11.91808469593525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,596 INFO epoch # 40 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 11.849685907363892
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:50,596 INFO *** epoch 40, rolling-avg-loss (window=10)= 11.568200438469649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,619 INFO epoch # 41 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.16 -loss = 8.789632432162762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,643 INFO epoch # 42 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.16-> 0.128 -loss = 9.155404575169086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,667 INFO epoch # 43 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 9.191933743655682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,690 INFO epoch # 44 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 6.422435902059078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,713 INFO epoch # 45 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 6.684412211179733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,736 INFO epoch # 46 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 6.074817702174187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,760 INFO epoch # 47 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 8.557924143970013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,783 INFO epoch # 48 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 9.711485050618649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,806 INFO epoch # 49 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 9.202553398907185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,830 INFO epoch # 50 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 7.842012524604797
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:50,830 INFO *** epoch 50, rolling-avg-loss (window=10)= 8.163261168450116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,853 INFO epoch # 51 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 5.795530948787928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,876 INFO epoch # 52 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 4.8393399938941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,900 INFO epoch # 53 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 5.94204743206501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,923 INFO epoch # 54 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 5.830911640077829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,946 INFO epoch # 55 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 4.911972537636757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,970 INFO epoch # 56 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 5.791942059993744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:50,993 INFO epoch # 57 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 5.0328903421759605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,016 INFO epoch # 58 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 5.31267923861742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,039 INFO epoch # 59 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 7.131312381476164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,062 INFO epoch # 60 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 5.577980075031519
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:51,062 INFO *** epoch 60, rolling-avg-loss (window=10)= 5.616660664975643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,085 INFO epoch # 61 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 5.80212464928627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,109 INFO epoch # 62 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.128 -loss = 5.7182451374828815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,132 INFO epoch # 63 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.128-> 0.1024 -loss = 5.750639449805021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,155 INFO epoch # 64 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 7.28577795624733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,178 INFO epoch # 65 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 5.16421851888299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,202 INFO epoch # 66 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 4.446690060198307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,225 INFO epoch # 67 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.4272992871701717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,248 INFO epoch # 68 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 4.978780206292868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,271 INFO epoch # 69 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.8718617744743824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,294 INFO epoch # 70 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 4.009266015142202
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:51,294 INFO *** epoch 70, rolling-avg-loss (window=10)= 5.045490305498243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,317 INFO epoch # 71 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.5551399551331997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,340 INFO epoch # 72 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 4.133357238024473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,364 INFO epoch # 73 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.690827939659357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,388 INFO epoch # 74 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 5.296560753136873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,412 INFO epoch # 75 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.7674760930240154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,436 INFO epoch # 76 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.2744100987911224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,459 INFO epoch # 77 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.2228175066411495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,482 INFO epoch # 78 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 2.905129386112094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,505 INFO epoch # 79 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.0575748421251774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,528 INFO epoch # 80 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.6728619299829006
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:51,528 INFO *** epoch 80, rolling-avg-loss (window=10)= 3.657615574263036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,551 INFO epoch # 81 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.1924965418875217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,575 INFO epoch # 82 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 2.6234820168465376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,598 INFO epoch # 83 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 2.4714699052274227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,621 INFO epoch # 84 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.0738042630255222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,644 INFO epoch # 85 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 2.9543264620006084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,668 INFO epoch # 86 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.0576291270554066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,691 INFO epoch # 87 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.569116970524192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,714 INFO epoch # 88 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.3573723696172237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,737 INFO epoch # 89 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 3.9449685253202915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,760 INFO epoch # 90 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 2.842211976647377
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:51,760 INFO *** epoch 90, rolling-avg-loss (window=10)= 3.1086878158152103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,783 INFO epoch # 91 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 2.780240811407566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,807 INFO epoch # 92 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 4.256818536669016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,830 INFO epoch # 93 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.1024 -loss = 4.369760148227215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,854 INFO epoch # 94 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.1024-> 0.08192 -loss = 3.5334038734436035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,877 INFO epoch # 95 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 3.4326574988663197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,900 INFO epoch # 96 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.2537752501666546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,923 INFO epoch # 97 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.2195348292589188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,946 INFO epoch # 98 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.39041399769485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,969 INFO epoch # 99 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 4.542582638561726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:51,992 INFO epoch # 100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.6208330262452364
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:51,993 INFO *** epoch 100, rolling-avg-loss (window=10)= 3.2400020610541107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,016 INFO epoch # 101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 3.1627861130982637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,040 INFO epoch # 102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.0923057179898024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,063 INFO epoch # 103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.2233591601252556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,086 INFO epoch # 104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.1173238195478916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,109 INFO epoch # 105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.9162457417696714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,132 INFO epoch # 106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.3918225038796663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,155 INFO epoch # 107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.777922946959734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,178 INFO epoch # 108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.2437998317182064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,201 INFO epoch # 109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.6084429640322924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,225 INFO epoch # 110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.5194275602698326
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:52,225 INFO *** epoch 110, rolling-avg-loss (window=10)= 2.2053436359390615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,248 INFO epoch # 111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.509805865585804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,271 INFO epoch # 112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.653889385983348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,294 INFO epoch # 113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.814700512215495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,317 INFO epoch # 114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.9943178500980139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,340 INFO epoch # 115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.04121414758265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,364 INFO epoch # 116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.1176873426884413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,387 INFO epoch # 117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.7500656563788652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,411 INFO epoch # 118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.7084228247404099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,434 INFO epoch # 119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.7384229060262442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,457 INFO epoch # 120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.4975970471277833
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:52,457 INFO *** epoch 120, rolling-avg-loss (window=10)= 1.7826123538427054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,480 INFO epoch # 121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.4727857876569033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,503 INFO epoch # 122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.1239842688664794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,526 INFO epoch # 123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.6487667579203844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,549 INFO epoch # 124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.7074806243181229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,573 INFO epoch # 125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.713130233809352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,597 INFO epoch # 126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.7883339170366526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,620 INFO epoch # 127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.5518071930855513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,643 INFO epoch # 128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.0133631555363536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,666 INFO epoch # 129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.9789274036884308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,689 INFO epoch # 130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.4127821810543537
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:52,689 INFO *** epoch 130, rolling-avg-loss (window=10)= 1.8411361522972585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,712 INFO epoch # 131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.4799211751669645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,736 INFO epoch # 132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.80382842104882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,759 INFO epoch # 133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 1.5460758320987225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,781 INFO epoch # 134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.0940261855721474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,805 INFO epoch # 135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.313157580792904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,828 INFO epoch # 136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.131637141108513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,851 INFO epoch # 137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.502627659589052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,875 INFO epoch # 138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 4.937029777094722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,898 INFO epoch # 139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.690839088521898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,921 INFO epoch # 140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.08192 -loss = 2.279622670263052
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:52,921 INFO *** epoch 140, rolling-avg-loss (window=10)= 2.3778765531256796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,944 INFO epoch # 141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.08192-> 0.065536 -loss = 1.7740999087691307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,967 INFO epoch # 142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.9393322672694921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:52,990 INFO epoch # 143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.705415066331625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,013 INFO epoch # 144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.1193654723465443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,037 INFO epoch # 145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.2582479994744062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,060 INFO epoch # 146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.8375055696815252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,083 INFO epoch # 147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.2639834638684988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,107 INFO epoch # 148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.378021027892828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,130 INFO epoch # 149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.212673069909215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,153 INFO epoch # 150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 0.8510966068133712
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:53,153 INFO *** epoch 150, rolling-avg-loss (window=10)= 1.4339740452356637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,176 INFO epoch # 151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 0.8510234570130706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,199 INFO epoch # 152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.078695916570723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,223 INFO epoch # 153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.1358091840520501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,246 INFO epoch # 154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.3649780694395304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,269 INFO epoch # 155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.945046292617917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,292 INFO epoch # 156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.1894685188308358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,315 INFO epoch # 157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 0.9086409201845527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,339 INFO epoch # 158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.0933447098359466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,362 INFO epoch # 159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.155699446797371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,385 INFO epoch # 160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.065536 -loss = 1.0419956911355257
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:53,385 INFO *** epoch 160, rolling-avg-loss (window=10)= 1.1764702206477522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,408 INFO epoch # 161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.065536-> 0.0524288 -loss = 1.2769576404243708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,432 INFO epoch # 162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 1.0102554764598608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,455 INFO epoch # 163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.7542103384621441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,478 INFO epoch # 164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.7953738197684288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,501 INFO epoch # 165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.7992799174971879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,525 INFO epoch # 166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.6768757104873657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,548 INFO epoch # 167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.6571475882083178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,571 INFO epoch # 168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.6167671561706811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,594 INFO epoch # 169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.5108468597754836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,617 INFO epoch # 170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.4372216477058828
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:53,618 INFO *** epoch 170, rolling-avg-loss (window=10)= 0.7534936154959724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,641 INFO epoch # 171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.47021557623520494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,664 INFO epoch # 172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.5662047881633043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,688 INFO epoch # 173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.8213374838232994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,711 INFO epoch # 174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.7486507007852197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,734 INFO epoch # 175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.6558385025709867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,757 INFO epoch # 176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.7119789635762572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,780 INFO epoch # 177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 1.095235712826252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,803 INFO epoch # 178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.7422717399895191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,826 INFO epoch # 179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 1.070786857046187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,849 INFO epoch # 180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.0524288 -loss = 0.8161740647628903
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:53,849 INFO *** epoch 180, rolling-avg-loss (window=10)= 0.769869438977912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,873 INFO epoch # 181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0524288-> 0.04194304 -loss = 0.9179572109133005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,897 INFO epoch # 182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.6391079500317574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,920 INFO epoch # 183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.5497606620192528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,943 INFO epoch # 184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.46369833080098033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,966 INFO epoch # 185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.39369957707822323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:53,989 INFO epoch # 186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.3263455340638757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,012 INFO epoch # 187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.42266437062062323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,035 INFO epoch # 188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.46570409648120403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,058 INFO epoch # 189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.5167349982075393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,081 INFO epoch # 190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.48185197031125426
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:54,081 INFO *** epoch 190, rolling-avg-loss (window=10)= 0.5177524700528011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,105 INFO epoch # 191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.4175810036249459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,128 INFO epoch # 192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.34981033205986023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,151 INFO epoch # 193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.4304408780299127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,174 INFO epoch # 194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.44818569626659155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,197 INFO epoch # 195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.4577324572019279
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,220 INFO epoch # 196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.04194304 -loss = 0.47517806850373745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,243 INFO epoch # 197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.04194304-> 0.03355443 -loss = 0.4033252210356295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,266 INFO epoch # 198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.3338523400016129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,289 INFO epoch # 199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.34291830426082015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,313 INFO epoch # 200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.37452830374240875
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:54,313 INFO *** epoch 200, rolling-avg-loss (window=10)= 0.4033552604727447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,336 INFO epoch # 201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.37414614856243134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,360 INFO epoch # 202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.2795678593683988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,383 INFO epoch # 203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.28427498415112495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,406 INFO epoch # 204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.2463114489801228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,429 INFO epoch # 205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.30037158401682973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,452 INFO epoch # 206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.26018929202109575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,475 INFO epoch # 207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.25472550466656685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,498 INFO epoch # 208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.25801347102969885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,521 INFO epoch # 209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.3198421630077064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,545 INFO epoch # 210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.31113277305848897
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:54,545 INFO *** epoch 210, rolling-avg-loss (window=10)= 0.28885752288624644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,568 INFO epoch # 211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.26082942681387067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,591 INFO epoch # 212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.22731031710281968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,614 INFO epoch # 213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.34185465751215816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,637 INFO epoch # 214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.24476756807416677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,661 INFO epoch # 215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.2611108742421493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,684 INFO epoch # 216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.30675941659137607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,707 INFO epoch # 217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.3045165347866714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,730 INFO epoch # 218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.3706094496883452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,754 INFO epoch # 219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.502213828265667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,777 INFO epoch # 220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.404887689743191
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:54,777 INFO *** epoch 220, rolling-avg-loss (window=10)= 0.3224859762820415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,801 INFO epoch # 221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.4899407774209976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,824 INFO epoch # 222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.03355443 -loss = 0.45839009806513786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,847 INFO epoch # 223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.03355443-> 0.02684355 -loss = 0.45530800661072135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,871 INFO epoch # 224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.42744737956672907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,894 INFO epoch # 225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.31853403663262725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,918 INFO epoch # 226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.24934855569154024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,941 INFO epoch # 227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.22152565326541662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,964 INFO epoch # 228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.22818889771588147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:54,987 INFO epoch # 229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.24000746477395296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,010 INFO epoch # 230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.23285079351626337
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:55,010 INFO *** epoch 230, rolling-avg-loss (window=10)= 0.33215416632592676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,033 INFO epoch # 231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.236042614094913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,056 INFO epoch # 232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.22919836966320872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,079 INFO epoch # 233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.21162391593679786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,102 INFO epoch # 234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.23392967879772186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,126 INFO epoch # 235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.241034742211923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,149 INFO epoch # 236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.2156654172576964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,173 INFO epoch # 237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.23268531868234277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,196 INFO epoch # 238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.20359016815200448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,219 INFO epoch # 239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.2182316742837429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,242 INFO epoch # 240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.20947177498601377
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:55,242 INFO *** epoch 240, rolling-avg-loss (window=10)= 0.2231473674066365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,265 INFO epoch # 241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.21559072216041386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,288 INFO epoch # 242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.24308804469183087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,311 INFO epoch # 243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.17946718249004334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,334 INFO epoch # 244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.20017437334172428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,358 INFO epoch # 245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.20135580119676888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,383 INFO epoch # 246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.27113644522614777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,406 INFO epoch # 247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.2917166971601546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,429 INFO epoch # 248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.2630784334614873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,452 INFO epoch # 249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.239747412269935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,475 INFO epoch # 250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.27309981524012983
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:55,475 INFO *** epoch 250, rolling-avg-loss (window=10)= 0.23784549272386357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,498 INFO epoch # 251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.2301500455942005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,521 INFO epoch # 252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.2094321979675442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,545 INFO epoch # 253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02684355 -loss = 0.2089114875998348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,568 INFO epoch # 254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02684355-> 0.02147484 -loss = 0.299089151667431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,591 INFO epoch # 255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.19792360230349004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,614 INFO epoch # 256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.17081010341644287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,637 INFO epoch # 257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.15280290180817246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,661 INFO epoch # 258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.13882330898195505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,684 INFO epoch # 259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.13860804738942534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,707 INFO epoch # 260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.16438498417846859
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:55,707 INFO *** epoch 260, rolling-avg-loss (window=10)= 0.1910935830906965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,730 INFO epoch # 261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.1551529709249735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,753 INFO epoch # 262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.16584289423190057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,777 INFO epoch # 263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.1982069411315024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,800 INFO epoch # 264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.19075851188972592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,823 INFO epoch # 265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.20225316588766873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,846 INFO epoch # 266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.16598888742737472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,870 INFO epoch # 267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.16946686571463943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,893 INFO epoch # 268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.2036200521979481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,916 INFO epoch # 269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.02147484 -loss = 0.1727292991708964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,940 INFO epoch # 270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.02147484-> 0.01717987 -loss = 0.18321560812182724
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:55,940 INFO *** epoch 270, rolling-avg-loss (window=10)= 0.1807235196698457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,963 INFO epoch # 271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.19003317435272038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:55,986 INFO epoch # 272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.14906849572435021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,010 INFO epoch # 273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.12894377019256353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,033 INFO epoch # 274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.12404467188753188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,056 INFO epoch # 275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.1319241444580257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,079 INFO epoch # 276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.1444974667392671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,102 INFO epoch # 277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.12735183106269687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,125 INFO epoch # 278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.12195854203309864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,148 INFO epoch # 279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.1622823083307594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,171 INFO epoch # 280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.15010905195958912
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:56,171 INFO *** epoch 280, rolling-avg-loss (window=10)= 0.14302134567406027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,195 INFO epoch # 281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.1385543195065111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,218 INFO epoch # 282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.13458676426671445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,241 INFO epoch # 283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.14974445768166333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,264 INFO epoch # 284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.18509839405305684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,287 INFO epoch # 285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.14110325346700847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,310 INFO epoch # 286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.12645068718120456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,333 INFO epoch # 287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.1801349234301597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,356 INFO epoch # 288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.01717987 -loss = 0.207033772720024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,380 INFO epoch # 289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01717987-> 0.0137439 -loss = 0.16905142273753881
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,403 INFO epoch # 290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.12472230510320514
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:56,403 INFO *** epoch 290, rolling-avg-loss (window=10)= 0.15564803001470864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,427 INFO epoch # 291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.1072276767808944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,450 INFO epoch # 292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.14309629681520164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,473 INFO epoch # 293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.10981455398723483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,496 INFO epoch # 294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.10577488876879215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,519 INFO epoch # 295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.10317735373973846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,542 INFO epoch # 296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.11776921467389911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,566 INFO epoch # 297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.105454012285918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,589 INFO epoch # 298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.10411679930984974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,612 INFO epoch # 299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.10547430778387934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,636 INFO epoch # 300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.10629328456707299
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:56,636 INFO *** epoch 300, rolling-avg-loss (window=10)= 0.11081983887124806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,659 INFO epoch # 301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.2646601952146739
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,682 INFO epoch # 302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.20481686713173985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,705 INFO epoch # 303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.17315796972252429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,728 INFO epoch # 304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.12817459634970874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,752 INFO epoch # 305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.0137439 -loss = 0.12274417548906058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,775 INFO epoch # 306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0137439-> 0.01099512 -loss = 0.11208586290013045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,798 INFO epoch # 307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.10943356156349182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,821 INFO epoch # 308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.09928439627401531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,844 INFO epoch # 309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.09238117444328964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,868 INFO epoch # 310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.08665774774271995
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:56,868 INFO *** epoch 310, rolling-avg-loss (window=10)= 0.13933965468313544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,891 INFO epoch # 311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.0890309801325202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,914 INFO epoch # 312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.08863167290110141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,937 INFO epoch # 313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.10008549713529646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,960 INFO epoch # 314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.0911302505992353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:56,983 INFO epoch # 315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.09140090609434992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,007 INFO epoch # 316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.09942562261130661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,033 INFO epoch # 317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.10687253158539534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,056 INFO epoch # 318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.1013673321576789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,080 INFO epoch # 319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.11194876814261079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,103 INFO epoch # 320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.01099512 -loss = 0.11119845614302903
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:57,103 INFO *** epoch 320, rolling-avg-loss (window=10)= 0.0991092017502524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,126 INFO epoch # 321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.01099512-> 0.00879609 -loss = 0.1011222138768062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,148 INFO epoch # 322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.09330208785831928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,172 INFO epoch # 323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08878020604606718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,195 INFO epoch # 324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08597520040348172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,218 INFO epoch # 325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.07989048585295677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,241 INFO epoch # 326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.07786153466440737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,264 INFO epoch # 327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.0805254535516724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,287 INFO epoch # 328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08175565884448588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,310 INFO epoch # 329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08085128176026046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,333 INFO epoch # 330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.0811266996897757
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:57,333 INFO *** epoch 330, rolling-avg-loss (window=10)= 0.0851190822548233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,357 INFO epoch # 331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.07742739224340767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,380 INFO epoch # 332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08822528796736151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,403 INFO epoch # 333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08700935775414109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,426 INFO epoch # 334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08263090113177896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,449 INFO epoch # 335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.086186129366979
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,473 INFO epoch # 336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.0866756736068055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,496 INFO epoch # 337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08533647062722594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,519 INFO epoch # 338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.07582293031737208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,542 INFO epoch # 339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.07206741708796471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,565 INFO epoch # 340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08209297526627779
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:57,565 INFO *** epoch 340, rolling-avg-loss (window=10)= 0.08234745353693143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,588 INFO epoch # 341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.07072850392432883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,611 INFO epoch # 342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.07574196008499712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,635 INFO epoch # 343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.07781666878145188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,659 INFO epoch # 344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08022820705082268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,682 INFO epoch # 345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.09393351967446506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,705 INFO epoch # 346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.07794736267533153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,728 INFO epoch # 347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.09146786062046885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,751 INFO epoch # 348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08235778333619237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,774 INFO epoch # 349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08134970196988434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,797 INFO epoch # 350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.08262210129760206
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:57,797 INFO *** epoch 350, rolling-avg-loss (window=10)= 0.08141936694155447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,821 INFO epoch # 351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00879609 -loss = 0.07903335493756458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,845 INFO epoch # 352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00879609-> 0.00703687 -loss = 0.07594121887814254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,870 INFO epoch # 353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07129682245431468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,894 INFO epoch # 354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07346790086012334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,917 INFO epoch # 355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07034541910979897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,940 INFO epoch # 356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07464064506348222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,963 INFO epoch # 357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07147472514770925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:57,986 INFO epoch # 358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.06672405690187588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,009 INFO epoch # 359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.06751855509355664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,032 INFO epoch # 360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.06684654159471393
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:58,032 INFO *** epoch 360, rolling-avg-loss (window=10)= 0.0717289240041282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,056 INFO epoch # 361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07620013842824847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,079 INFO epoch # 362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07485009741503745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,102 INFO epoch # 363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07793004403356463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,125 INFO epoch # 364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.06538267468567938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,148 INFO epoch # 365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.0732906695920974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,171 INFO epoch # 366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07810631766915321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,194 INFO epoch # 367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.06873635505326092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,217 INFO epoch # 368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07086991902906448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,240 INFO epoch # 369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07956021290738136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,264 INFO epoch # 370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.07950148405507207
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:58,264 INFO *** epoch 370, rolling-avg-loss (window=10)= 0.07444279128685594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,287 INFO epoch # 371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.08244695491157472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,310 INFO epoch # 372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.09643254592083395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,333 INFO epoch # 373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.10756648587994277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,356 INFO epoch # 374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.00703687 -loss = 0.08320551132783294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,379 INFO epoch # 375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00703687-> 0.0056295 -loss = 0.07773418119177222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,402 INFO epoch # 376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06894408515654504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,425 INFO epoch # 377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.0637061134329997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,449 INFO epoch # 378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06157874932978302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,472 INFO epoch # 379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.060509489849209785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,496 INFO epoch # 380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.0676601575105451
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:58,496 INFO *** epoch 380, rolling-avg-loss (window=10)= 0.07697842745110392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,519 INFO epoch # 381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06342411751393229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,542 INFO epoch # 382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.062100283335894346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,569 INFO epoch # 383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.062358309456612915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,598 INFO epoch # 384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06118847627658397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,627 INFO epoch # 385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.0603078156709671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,658 INFO epoch # 386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06330067291855812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,685 INFO epoch # 387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06543600047007203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,709 INFO epoch # 388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06276725919451565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,732 INFO epoch # 389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06664813833776861
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,755 INFO epoch # 390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.05874549882719293
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:58,755 INFO *** epoch 390, rolling-avg-loss (window=10)= 0.0626276572002098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,778 INFO epoch # 391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06473127764184028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,801 INFO epoch # 392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06384998088469729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,824 INFO epoch # 393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06493383017368615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,847 INFO epoch # 394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06136386375874281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,871 INFO epoch # 395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06142464285949245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,895 INFO epoch # 396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06351207161787897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,919 INFO epoch # 397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.05958686425583437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,943 INFO epoch # 398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06069602956995368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,966 INFO epoch # 399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06219912972301245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:58,990 INFO epoch # 400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0056295 -loss = 0.06300894566811621
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:58,990 INFO *** epoch 400, rolling-avg-loss (window=10)= 0.06253066361532547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,014 INFO epoch # 401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0056295-> 0.0045036 -loss = 0.06381295074243098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,038 INFO epoch # 402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.06242597312666476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,061 INFO epoch # 403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.05807717749848962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,084 INFO epoch # 404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.06875587615650147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,107 INFO epoch # 405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.05631517758592963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,130 INFO epoch # 406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.05615483596920967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,153 INFO epoch # 407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.05987662903498858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,176 INFO epoch # 408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.053983481833711267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,199 INFO epoch # 409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.05618808593135327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,222 INFO epoch # 410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.06028249394148588
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:59,222 INFO *** epoch 410, rolling-avg-loss (window=10)= 0.05958726818207651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,246 INFO epoch # 411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.05902123067062348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,269 INFO epoch # 412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.057259777560830116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,292 INFO epoch # 413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.05619420169387013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,315 INFO epoch # 414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.058261021971702576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,339 INFO epoch # 415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.05404238510527648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,362 INFO epoch # 416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.05572421883698553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,386 INFO epoch # 417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.05489484843565151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,409 INFO epoch # 418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.0045036 -loss = 0.05985421780496836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,432 INFO epoch # 419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0045036-> 0.00360288 -loss = 0.055984621634706855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,455 INFO epoch # 420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.05562528769951314
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:59,455 INFO *** epoch 420, rolling-avg-loss (window=10)= 0.056686181141412814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,479 INFO epoch # 421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.05556611355859786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,502 INFO epoch # 422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.056300423108041286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,525 INFO epoch # 423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.049842541688121855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,549 INFO epoch # 424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.04985041049076244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,572 INFO epoch # 425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.05040980636840686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,596 INFO epoch # 426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.05294234643224627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,619 INFO epoch # 427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.05152317404281348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,642 INFO epoch # 428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.050262692966498435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,665 INFO epoch # 429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.05325106857344508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,688 INFO epoch # 430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.05256764841033146
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:59,688 INFO *** epoch 430, rolling-avg-loss (window=10)= 0.0522516225639265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,712 INFO epoch # 431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.05545165407238528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,735 INFO epoch # 432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.053479033755138516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,758 INFO epoch # 433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.00360288 -loss = 0.0545327341533266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,782 INFO epoch # 434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00360288-> 0.0028823 -loss = 0.05596597777912393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,805 INFO epoch # 435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05333496752427891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,828 INFO epoch # 436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04906357708387077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,851 INFO epoch # 437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05003527330700308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,875 INFO epoch # 438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05282316077500582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,898 INFO epoch # 439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05304765503387898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,921 INFO epoch # 440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05104418640257791
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:37:59,921 INFO *** epoch 440, rolling-avg-loss (window=10)= 0.05287782198865898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,945 INFO epoch # 441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04926418012473732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,968 INFO epoch # 442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.050560387840960175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:37:59,991 INFO epoch # 443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.050217563461046666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,015 INFO epoch # 444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.049267853260971606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,038 INFO epoch # 445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04836327163502574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,061 INFO epoch # 446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04848478833446279
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,084 INFO epoch # 447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.051270683703478426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,107 INFO epoch # 448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04949825897347182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,130 INFO epoch # 449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04967705259332433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,153 INFO epoch # 450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04892010206822306
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:00,153 INFO *** epoch 450, rolling-avg-loss (window=10)= 0.0495524141995702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,177 INFO epoch # 451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04741301405010745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,200 INFO epoch # 452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.047365737380459905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,223 INFO epoch # 453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.049127761041745543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,246 INFO epoch # 454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05002800398506224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,269 INFO epoch # 455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04822809842880815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,293 INFO epoch # 456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04842587636085227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,316 INFO epoch # 457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04792039142921567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,339 INFO epoch # 458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04776602832134813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,362 INFO epoch # 459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05090299918083474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,387 INFO epoch # 460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05209957016631961
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:00,387 INFO *** epoch 460, rolling-avg-loss (window=10)= 0.048927748034475374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,410 INFO epoch # 461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04785450559575111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,433 INFO epoch # 462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04728261544369161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,457 INFO epoch # 463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04751086176838726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,480 INFO epoch # 464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.048068509553559124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,503 INFO epoch # 465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04850157047621906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,527 INFO epoch # 466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.0476608996395953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,550 INFO epoch # 467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05068435409339145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,573 INFO epoch # 468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04950750258285552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,597 INFO epoch # 469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.047583808947820216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,620 INFO epoch # 470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.048895552405156195
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:00,620 INFO *** epoch 470, rolling-avg-loss (window=10)= 0.048355018050642684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,643 INFO epoch # 471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05010251695057377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,667 INFO epoch # 472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.047100554627832025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,690 INFO epoch # 473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05081819253973663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,713 INFO epoch # 474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04816775320796296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,736 INFO epoch # 475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.05010796809801832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,759 INFO epoch # 476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04731582727981731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,783 INFO epoch # 477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.050425140478182584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,806 INFO epoch # 478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.048638344276696444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,829 INFO epoch # 479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04760063620051369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,852 INFO epoch # 480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04787993733771145
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:00,852 INFO *** epoch 480, rolling-avg-loss (window=10)= 0.04881568709970452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,877 INFO epoch # 481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04915918753249571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,900 INFO epoch # 482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.0028823 -loss = 0.04881574952742085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,923 INFO epoch # 483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.0028823-> 0.00230584 -loss = 0.047718373127281666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,947 INFO epoch # 484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.046989210008177906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,971 INFO epoch # 485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.045832223957404494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:00,995 INFO epoch # 486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.044615937513299286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,019 INFO epoch # 487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04528481786837801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,042 INFO epoch # 488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04439180105691776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,066 INFO epoch # 489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04408867278834805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,089 INFO epoch # 490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04599967825924978
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:01,089 INFO *** epoch 490, rolling-avg-loss (window=10)= 0.04628956516389735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,112 INFO epoch # 491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.044414237316232175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,135 INFO epoch # 492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04510265635326505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,158 INFO epoch # 493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04604867298621684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,181 INFO epoch # 494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.050163139938376844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,205 INFO epoch # 495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.045564884319901466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,228 INFO epoch # 496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.044308762764558196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,251 INFO epoch # 497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.044265473377890885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,274 INFO epoch # 498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.044293277722317725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,297 INFO epoch # 499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04362162674078718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,321 INFO epoch # 500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04432331619318575
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:01,321 INFO *** epoch 500, rolling-avg-loss (window=10)= 0.04521060477127321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,344 INFO epoch # 501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04307936364784837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,368 INFO epoch # 502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04412671487079933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,392 INFO epoch # 503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.045824832923244685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,415 INFO epoch # 504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04299012350384146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,439 INFO epoch # 505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04580648918636143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,462 INFO epoch # 506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.044434936600737274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,485 INFO epoch # 507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04577424266608432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,508 INFO epoch # 508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.045891253277659416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,531 INFO epoch # 509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04367137560620904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,555 INFO epoch # 510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04473315965151414
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:01,555 INFO *** epoch 510, rolling-avg-loss (window=10)= 0.044633249193429946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,579 INFO epoch # 511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04466748749837279
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,602 INFO epoch # 512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.043480748194269836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,626 INFO epoch # 513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04393975285347551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,649 INFO epoch # 514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00230584 -loss = 0.04456108593149111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,672 INFO epoch # 515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00230584-> 0.00184467 -loss = 0.04334550234489143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,696 INFO epoch # 516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04245140007697046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,719 INFO epoch # 517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.043006165826227516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,742 INFO epoch # 518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.0423641818924807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,765 INFO epoch # 519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04101041005924344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,788 INFO epoch # 520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04166938445996493
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:01,788 INFO *** epoch 520, rolling-avg-loss (window=10)= 0.043049611913738774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,812 INFO epoch # 521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04266029625432566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,835 INFO epoch # 522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04031760763609782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,859 INFO epoch # 523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.040903197310399264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,883 INFO epoch # 524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04134626733139157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,906 INFO epoch # 525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04024961922550574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,929 INFO epoch # 526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04134998063091189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,955 INFO epoch # 527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.042447204468771815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:01,979 INFO epoch # 528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.042463383346330374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,003 INFO epoch # 529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.040836996893631294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,027 INFO epoch # 530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04116745770443231
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:02,027 INFO *** epoch 530, rolling-avg-loss (window=10)= 0.04137420108017977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,051 INFO epoch # 531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04140507150441408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,075 INFO epoch # 532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04027933918405324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,099 INFO epoch # 533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04118996026227251
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,122 INFO epoch # 534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04121824202593416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,145 INFO epoch # 535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00184467 -loss = 0.04194570617983118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,169 INFO epoch # 536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00184467-> 0.00147574 -loss = 0.040856936771888286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,192 INFO epoch # 537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.04022440541302785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,215 INFO epoch # 538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.03948180441511795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,239 INFO epoch # 539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.0419412637129426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,262 INFO epoch # 540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.041906454716809094
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:02,263 INFO *** epoch 540, rolling-avg-loss (window=10)= 0.041044918418629096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,286 INFO epoch # 541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.03943127189995721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,309 INFO epoch # 542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.03939294518204406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,332 INFO epoch # 543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.039079791924450547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,356 INFO epoch # 544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.04049701808253303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,380 INFO epoch # 545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.04083400091622025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,403 INFO epoch # 546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.04025799536611885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,426 INFO epoch # 547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.04039101768285036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,450 INFO epoch # 548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.04130226606503129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,473 INFO epoch # 549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.03909617126919329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,497 INFO epoch # 550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.039845584193244576
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:02,497 INFO *** epoch 550, rolling-avg-loss (window=10)= 0.04001280625816435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,521 INFO epoch # 551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.04025397991063073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,544 INFO epoch # 552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.03975458920467645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,568 INFO epoch # 553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00147574 -loss = 0.04004959115991369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,592 INFO epoch # 554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00147574-> 0.00118059 -loss = 0.039980730798561126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,616 INFO epoch # 555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03903415478998795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,639 INFO epoch # 556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.038364453066606075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,663 INFO epoch # 557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03957804630044848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,686 INFO epoch # 558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03959303296869621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,709 INFO epoch # 559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.038608954462688416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,733 INFO epoch # 560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.038625917746685445
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:02,733 INFO *** epoch 560, rolling-avg-loss (window=10)= 0.03938434504088946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,757 INFO epoch # 561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03804992651566863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,780 INFO epoch # 562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03916618856601417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,803 INFO epoch # 563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03888516593724489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,826 INFO epoch # 564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.04047002922743559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,850 INFO epoch # 565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03898015106096864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,875 INFO epoch # 566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.038024836394470185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,898 INFO epoch # 567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03824581467779353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,922 INFO epoch # 568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03809301747241989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,945 INFO epoch # 569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03839816205436364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,968 INFO epoch # 570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03820472286315635
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:02,968 INFO *** epoch 570, rolling-avg-loss (window=10)= 0.03865180147695355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:02,992 INFO epoch # 571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.04059504019096494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,015 INFO epoch # 572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03915862715803087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,038 INFO epoch # 573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03942882112460211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,061 INFO epoch # 574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.038239849149249494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,085 INFO epoch # 575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.03809880628250539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,108 INFO epoch # 576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.00118059 -loss = 0.039072639716323465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,132 INFO epoch # 577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.00118059-> 0.001 -loss = 0.038396491785533726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,155 INFO epoch # 578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03779828822007403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,178 INFO epoch # 579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03778163273818791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,201 INFO epoch # 580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03766401659231633
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:03,202 INFO *** epoch 580, rolling-avg-loss (window=10)= 0.03862342129577882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,225 INFO epoch # 581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03748240025015548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,248 INFO epoch # 582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03840639372356236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,271 INFO epoch # 583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037989329372067004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,294 INFO epoch # 584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03682730998843908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,318 INFO epoch # 585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037299247400369495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,341 INFO epoch # 586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03676032944349572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,365 INFO epoch # 587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03727345302468166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,389 INFO epoch # 588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03895655431551859
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,412 INFO epoch # 589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03751592669868842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,435 INFO epoch # 590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0371966288657859
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:03,435 INFO *** epoch 590, rolling-avg-loss (window=10)= 0.03757075730827637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,459 INFO epoch # 591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036827503878157586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,482 INFO epoch # 592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03794035367900506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,505 INFO epoch # 593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03791512950556353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,529 INFO epoch # 594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03699330106610432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,552 INFO epoch # 595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03690900397486985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,575 INFO epoch # 596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03787685377756134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,599 INFO epoch # 597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03759207946131937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,622 INFO epoch # 598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03789458025130443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,645 INFO epoch # 599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03683120006462559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,668 INFO epoch # 600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037090551981236786
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:03,668 INFO *** epoch 600, rolling-avg-loss (window=10)= 0.03738705576397479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,693 INFO epoch # 601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03681797665194608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,716 INFO epoch # 602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03686871286481619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,740 INFO epoch # 603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037080008594784886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,763 INFO epoch # 604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03857734869234264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,786 INFO epoch # 605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0380487113725394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,810 INFO epoch # 606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037453374883625656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,833 INFO epoch # 607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03697296808240935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,856 INFO epoch # 608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037368519289884716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,880 INFO epoch # 609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036595866607967764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,903 INFO epoch # 610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036182698619086295
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:03,903 INFO *** epoch 610, rolling-avg-loss (window=10)= 0.037196618565940295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,927 INFO epoch # 611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036397531104739755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,950 INFO epoch # 612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03740776132326573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,973 INFO epoch # 613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.038496279274113476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:03,997 INFO epoch # 614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.038070949900429696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,020 INFO epoch # 615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037430494907312095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,043 INFO epoch # 616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03768905019387603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,066 INFO epoch # 617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03641192056238651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,089 INFO epoch # 618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036250431439839303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,112 INFO epoch # 619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036687976971734315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,136 INFO epoch # 620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037347722391132265
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:04,136 INFO *** epoch 620, rolling-avg-loss (window=10)= 0.03721901180688292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,160 INFO epoch # 621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03664508898509666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,183 INFO epoch # 622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03602442849660292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,207 INFO epoch # 623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036569489806424826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,231 INFO epoch # 624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.038075310876592994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,254 INFO epoch # 625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03823740500956774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,278 INFO epoch # 626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035903685668017715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,301 INFO epoch # 627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0360270802048035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,324 INFO epoch # 628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03666888200677931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,347 INFO epoch # 629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03622952534351498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,372 INFO epoch # 630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03640833304962143
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:04,372 INFO *** epoch 630, rolling-avg-loss (window=10)= 0.03667892294470221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,396 INFO epoch # 631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036969195352867246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,420 INFO epoch # 632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03597885527415201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,443 INFO epoch # 633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036042421823367476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,466 INFO epoch # 634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03719236236065626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,489 INFO epoch # 635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036067173699848354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,512 INFO epoch # 636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03650394076248631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,535 INFO epoch # 637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03775675146607682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,558 INFO epoch # 638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03624629840487614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,582 INFO epoch # 639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03775274893268943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,605 INFO epoch # 640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03740566986380145
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:04,605 INFO *** epoch 640, rolling-avg-loss (window=10)= 0.03679154179408215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,629 INFO epoch # 641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036749261955264956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,652 INFO epoch # 642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036081453843507916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,675 INFO epoch # 643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037416243634652346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,699 INFO epoch # 644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03842149686533958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,722 INFO epoch # 645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.037280649819877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,745 INFO epoch # 646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03637520287884399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,768 INFO epoch # 647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036101535079069436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,791 INFO epoch # 648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03573844814673066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,815 INFO epoch # 649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0357645753538236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,838 INFO epoch # 650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03604361682664603
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:04,838 INFO *** epoch 650, rolling-avg-loss (window=10)= 0.036597248440375554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,861 INFO epoch # 651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036002338340040296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,885 INFO epoch # 652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03617592799128033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,908 INFO epoch # 653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03719830635236576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,931 INFO epoch # 654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0362377276760526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,954 INFO epoch # 655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03644479502690956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:04,978 INFO epoch # 656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03582404798362404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,002 INFO epoch # 657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03574589599156752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,025 INFO epoch # 658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03636923700105399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,049 INFO epoch # 659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03553463995922357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,074 INFO epoch # 660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034964580059750006
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:05,074 INFO *** epoch 660, rolling-avg-loss (window=10)= 0.036049749638186766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,097 INFO epoch # 661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035325892677064985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,120 INFO epoch # 662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036283597466535866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,144 INFO epoch # 663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036504332965705544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,167 INFO epoch # 664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036386887077242136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,190 INFO epoch # 665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03794505732366815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,214 INFO epoch # 666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03646879841107875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,237 INFO epoch # 667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035999136918690056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,260 INFO epoch # 668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03639957250561565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,284 INFO epoch # 669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03520518000004813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,307 INFO epoch # 670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03506140236277133
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:05,307 INFO *** epoch 670, rolling-avg-loss (window=10)= 0.03615798577084206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,331 INFO epoch # 671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0355647059623152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,354 INFO epoch # 672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0358778047375381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,378 INFO epoch # 673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03674066253006458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,402 INFO epoch # 674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035535502538550645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,425 INFO epoch # 675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03536054020514712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,449 INFO epoch # 676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035000127856619656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,472 INFO epoch # 677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035940597008448094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,496 INFO epoch # 678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.036343078943900764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,519 INFO epoch # 679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035199257894419134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,542 INFO epoch # 680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035834940092172474
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:05,543 INFO *** epoch 680, rolling-avg-loss (window=10)= 0.035739721776917574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,566 INFO epoch # 681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03515392041299492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,589 INFO epoch # 682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03522385630640201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,612 INFO epoch # 683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0354567805188708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,636 INFO epoch # 684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03522680082824081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,660 INFO epoch # 685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035135111305862665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,684 INFO epoch # 686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03613257053075358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,707 INFO epoch # 687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034222046757349744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,730 INFO epoch # 688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03443898819386959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,753 INFO epoch # 689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03499838744755834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,776 INFO epoch # 690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03558025066740811
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:05,776 INFO *** epoch 690, rolling-avg-loss (window=10)= 0.035156871296931055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,800 INFO epoch # 691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0362534633022733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,824 INFO epoch # 692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035649870114866644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,847 INFO epoch # 693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03514650248689577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,871 INFO epoch # 694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03463710646610707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,895 INFO epoch # 695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034237952437251806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,919 INFO epoch # 696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03473469277378172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,942 INFO epoch # 697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034571794618386775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,965 INFO epoch # 698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03472115640761331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:05,988 INFO epoch # 699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03531333478167653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,011 INFO epoch # 700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03567102088709362
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:06,011 INFO *** epoch 700, rolling-avg-loss (window=10)= 0.03509368942759465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,035 INFO epoch # 701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035136278602294624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,059 INFO epoch # 702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03517641534563154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,082 INFO epoch # 703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035316044581122696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,105 INFO epoch # 704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034387326042633504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,128 INFO epoch # 705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03381950632319786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,152 INFO epoch # 706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03435834389529191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,175 INFO epoch # 707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034537314902991056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,198 INFO epoch # 708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03410316730150953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,221 INFO epoch # 709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034544912923593074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,245 INFO epoch # 710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03501850395696238
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:06,245 INFO *** epoch 710, rolling-avg-loss (window=10)= 0.034639781387522814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,268 INFO epoch # 711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03530599898658693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,292 INFO epoch # 712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03565189536311664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,317 INFO epoch # 713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03412525393650867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,340 INFO epoch # 714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03350428256089799
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,364 INFO epoch # 715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035794634255580604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,387 INFO epoch # 716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03528307791566476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,410 INFO epoch # 717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03356931050075218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,434 INFO epoch # 718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03413428633939475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,457 INFO epoch # 719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033889965212438256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,480 INFO epoch # 720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035247447551228106
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:06,480 INFO *** epoch 720, rolling-avg-loss (window=10)= 0.03465061526221689
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,504 INFO epoch # 721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03548028413206339
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,527 INFO epoch # 722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03526491858065128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,550 INFO epoch # 723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03473092440981418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,574 INFO epoch # 724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035120205546263605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,597 INFO epoch # 725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03408206955646165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,621 INFO epoch # 726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03376524627674371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,644 INFO epoch # 727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03378214719123207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,668 INFO epoch # 728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03367035818519071
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,691 INFO epoch # 729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034066864755004644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,715 INFO epoch # 730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035143535875249654
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:06,715 INFO *** epoch 730, rolling-avg-loss (window=10)= 0.03451065545086749
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,739 INFO epoch # 731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03516506950836629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,762 INFO epoch # 732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03522616933332756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,786 INFO epoch # 733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035499899415299296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,809 INFO epoch # 734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03663109848275781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,833 INFO epoch # 735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033788001688662916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,856 INFO epoch # 736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033922888280358166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,881 INFO epoch # 737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034051158640068024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,904 INFO epoch # 738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03381809592247009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,928 INFO epoch # 739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03536537173204124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,951 INFO epoch # 740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033218348340597004
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:06,951 INFO *** epoch 740, rolling-avg-loss (window=10)= 0.03466861013439484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,974 INFO epoch # 741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034072375507093966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:06,997 INFO epoch # 742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034283613378647715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,021 INFO epoch # 743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03375118161784485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,044 INFO epoch # 744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0345994321978651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,067 INFO epoch # 745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033658330561593175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,090 INFO epoch # 746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03355548216495663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,114 INFO epoch # 747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03342243761289865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,137 INFO epoch # 748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03412674032733776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,160 INFO epoch # 749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03340400184970349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,183 INFO epoch # 750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03350762400077656
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:07,184 INFO *** epoch 750, rolling-avg-loss (window=10)= 0.033838121921871786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,207 INFO epoch # 751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03385384602006525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,230 INFO epoch # 752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03318924456834793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,253 INFO epoch # 753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035607161349616945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,276 INFO epoch # 754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033535233698785305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,300 INFO epoch # 755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03443955641705543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,323 INFO epoch # 756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03331692671054043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,346 INFO epoch # 757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03268218369339593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,370 INFO epoch # 758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03298906749114394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,394 INFO epoch # 759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03338582967990078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,417 INFO epoch # 760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03367707331199199
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:07,417 INFO *** epoch 760, rolling-avg-loss (window=10)= 0.033667612294084395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,440 INFO epoch # 761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.034082294383551925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,463 INFO epoch # 762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03320610773516819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,487 INFO epoch # 763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03365235548699275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,510 INFO epoch # 764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0335123126860708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,533 INFO epoch # 765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033451410941779613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,557 INFO epoch # 766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03328572696773335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,580 INFO epoch # 767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033486429252661765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,603 INFO epoch # 768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0336313939478714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,627 INFO epoch # 769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033247938787098974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,650 INFO epoch # 770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03320599836297333
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:07,650 INFO *** epoch 770, rolling-avg-loss (window=10)= 0.03347619685519021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,674 INFO epoch # 771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.035002852440811694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,697 INFO epoch # 772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03284774368512444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,720 INFO epoch # 773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03396778949536383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,743 INFO epoch # 774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03328410227550194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,767 INFO epoch # 775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033163332351250574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,791 INFO epoch # 776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03343087964458391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,814 INFO epoch # 777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0338699045823887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,837 INFO epoch # 778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033657814434263855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,860 INFO epoch # 779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0341205129516311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,884 INFO epoch # 780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033347481570672244
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:07,884 INFO *** epoch 780, rolling-avg-loss (window=10)= 0.033669241343159226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,908 INFO epoch # 781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033622543211095035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,931 INFO epoch # 782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03470718412427232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,954 INFO epoch # 783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03373627352993935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:07,978 INFO epoch # 784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03431496454868466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,001 INFO epoch # 785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033465338055975735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,024 INFO epoch # 786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033710364485159516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,048 INFO epoch # 787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032673168869223446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,071 INFO epoch # 788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03236552648013458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,094 INFO epoch # 789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0322421895689331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,117 INFO epoch # 790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0341581788379699
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:08,118 INFO *** epoch 790, rolling-avg-loss (window=10)= 0.03349957317113876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,141 INFO epoch # 791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03292874403996393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,164 INFO epoch # 792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033485027961432934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,188 INFO epoch # 793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032637069874908775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,211 INFO epoch # 794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03248055555741303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,234 INFO epoch # 795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03327682625968009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,258 INFO epoch # 796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032372712972573936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,281 INFO epoch # 797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033038119319826365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,304 INFO epoch # 798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0327168878284283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,327 INFO epoch # 799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032621623016893864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,350 INFO epoch # 800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03199125899118371
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:08,351 INFO *** epoch 800, rolling-avg-loss (window=10)= 0.03275488258223049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,374 INFO epoch # 801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03244752009049989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,398 INFO epoch # 802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032271875883452594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,421 INFO epoch # 803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033105409413110465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,444 INFO epoch # 804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032141533214598894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,468 INFO epoch # 805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03405557927908376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,491 INFO epoch # 806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03296703199157491
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,514 INFO epoch # 807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03256070922361687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,537 INFO epoch # 808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032324526633601636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,560 INFO epoch # 809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03316172549966723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,584 INFO epoch # 810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03236598236253485
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:08,584 INFO *** epoch 810, rolling-avg-loss (window=10)= 0.03274018935917411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,607 INFO epoch # 811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03244638128671795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,631 INFO epoch # 812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03290882444707677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,654 INFO epoch # 813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03314135508844629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,677 INFO epoch # 814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032417005277238786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,701 INFO epoch # 815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032465325319208205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,724 INFO epoch # 816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03282228816533461
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,747 INFO epoch # 817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.033950925047975034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,771 INFO epoch # 818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03263126240926795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,794 INFO epoch # 819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03190240869298577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,817 INFO epoch # 820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031706564070191234
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:08,817 INFO *** epoch 820, rolling-avg-loss (window=10)= 0.03263923398044426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,841 INFO epoch # 821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03172756399726495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,864 INFO epoch # 822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03229558130260557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,887 INFO epoch # 823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032884874555747956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,911 INFO epoch # 824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03367133502615616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,934 INFO epoch # 825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03169431441347115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,957 INFO epoch # 826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0330553826643154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:08,980 INFO epoch # 827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032812433142680675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,004 INFO epoch # 828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03157023215317167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,027 INFO epoch # 829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03271864802809432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,051 INFO epoch # 830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03236630919855088
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:09,051 INFO *** epoch 830, rolling-avg-loss (window=10)= 0.03247966744820587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,074 INFO epoch # 831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03286592138465494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,097 INFO epoch # 832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03250555007252842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,120 INFO epoch # 833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031968458264600486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,143 INFO epoch # 834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032462190836668015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,166 INFO epoch # 835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0328509307873901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,189 INFO epoch # 836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032614005147479475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,213 INFO epoch # 837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0326945714186877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,236 INFO epoch # 838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032251426426228136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,260 INFO epoch # 839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032041811966337264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,283 INFO epoch # 840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032026566099375486
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:09,283 INFO *** epoch 840, rolling-avg-loss (window=10)= 0.032428143240395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,306 INFO epoch # 841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03189450578065589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,329 INFO epoch # 842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03151236969279125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,353 INFO epoch # 843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03176895447541028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,376 INFO epoch # 844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031647815863834694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,400 INFO epoch # 845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031459673191420734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,424 INFO epoch # 846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03152812988264486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,447 INFO epoch # 847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03199473017593846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,470 INFO epoch # 848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031839998497162014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,493 INFO epoch # 849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03151925964630209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,516 INFO epoch # 850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03098575447802432
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:09,516 INFO *** epoch 850, rolling-avg-loss (window=10)= 0.03161511916841846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,540 INFO epoch # 851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03241702844388783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,562 INFO epoch # 852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03165030537638813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,585 INFO epoch # 853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032452340179588646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,609 INFO epoch # 854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03132459442713298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,632 INFO epoch # 855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03126913507003337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,655 INFO epoch # 856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030971793428761885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,679 INFO epoch # 857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03111321505275555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,702 INFO epoch # 858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0325268592860084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,725 INFO epoch # 859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03125787427416071
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,748 INFO epoch # 860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031501813675276935
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:09,748 INFO *** epoch 860, rolling-avg-loss (window=10)= 0.03164849592139944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,771 INFO epoch # 861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03108967887237668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,794 INFO epoch # 862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03172160842223093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,818 INFO epoch # 863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031541119737084955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,841 INFO epoch # 864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03170326561667025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,865 INFO epoch # 865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03097962256288156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,890 INFO epoch # 866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030748581368243322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,913 INFO epoch # 867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031975768419215456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,936 INFO epoch # 868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.032127733749803156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,959 INFO epoch # 869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031953198777046055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:09,982 INFO epoch # 870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030785618466325104
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:09,982 INFO *** epoch 870, rolling-avg-loss (window=10)= 0.031462619599187745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,006 INFO epoch # 871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031306331511586905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,029 INFO epoch # 872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030434419808443636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,052 INFO epoch # 873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031095163198187947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,075 INFO epoch # 874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031395814672578126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,099 INFO epoch # 875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03131972980918363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,122 INFO epoch # 876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03293402469716966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,145 INFO epoch # 877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031465439358726144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,168 INFO epoch # 878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03089185681892559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,191 INFO epoch # 879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03145378080080263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,214 INFO epoch # 880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030651469016447663
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:10,214 INFO *** epoch 880, rolling-avg-loss (window=10)= 0.03129480296920519
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,238 INFO epoch # 881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030712606181623414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,261 INFO epoch # 882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03156688349554315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,285 INFO epoch # 883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030602508690208197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,308 INFO epoch # 884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030839863582514226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,331 INFO epoch # 885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031130086426855996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,355 INFO epoch # 886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031137888377998024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,378 INFO epoch # 887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030426936806179583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,401 INFO epoch # 888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030096502479864284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,424 INFO epoch # 889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03017433438799344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,448 INFO epoch # 890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030800710432231426
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:10,448 INFO *** epoch 890, rolling-avg-loss (window=10)= 0.030748832086101175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,471 INFO epoch # 891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031123348278924823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,495 INFO epoch # 892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.031641202163882554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,518 INFO epoch # 893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03166856151074171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,541 INFO epoch # 894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029916463536210358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,564 INFO epoch # 895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030985790072008967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,588 INFO epoch # 896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030969032202847302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,611 INFO epoch # 897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03049471351550892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,636 INFO epoch # 898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030337513046106324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,662 INFO epoch # 899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030364857229869813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,688 INFO epoch # 900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030430960294324905
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:10,688 INFO *** epoch 900, rolling-avg-loss (window=10)= 0.03079324418504257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,714 INFO epoch # 901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03150332934455946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,738 INFO epoch # 902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030829686322249472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,762 INFO epoch # 903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030334448732901365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,785 INFO epoch # 904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03133906997391023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,809 INFO epoch # 905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030536996928276494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,832 INFO epoch # 906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029970747127663344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,857 INFO epoch # 907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03001134644728154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,884 INFO epoch # 908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03122697281651199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,908 INFO epoch # 909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03014683397486806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,932 INFO epoch # 910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03011305129621178
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:10,932 INFO *** epoch 910, rolling-avg-loss (window=10)= 0.030601248296443374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,955 INFO epoch # 911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03032435884233564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:10,979 INFO epoch # 912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030077221570536494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,002 INFO epoch # 913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03100066346814856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,025 INFO epoch # 914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030338781944010407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,048 INFO epoch # 915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02975465392228216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,071 INFO epoch # 916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02936992095783353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,095 INFO epoch # 917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03015983011573553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,118 INFO epoch # 918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030759084271267056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,141 INFO epoch # 919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030401447031181306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,165 INFO epoch # 920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03092583059333265
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:11,165 INFO *** epoch 920, rolling-avg-loss (window=10)= 0.030311179271666332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,188 INFO epoch # 921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03027248688158579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,211 INFO epoch # 922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030229583266191185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,234 INFO epoch # 923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030427162651903927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,258 INFO epoch # 924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030225210561184213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,282 INFO epoch # 925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029637671657837927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,306 INFO epoch # 926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03010866421391256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,329 INFO epoch # 927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029671128373593092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,353 INFO epoch # 928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030709383019711822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,377 INFO epoch # 929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029920192784629762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,401 INFO epoch # 930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030491013429127634
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:11,401 INFO *** epoch 930, rolling-avg-loss (window=10)= 0.03016924968396779
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,424 INFO epoch # 931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029959033883642405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,447 INFO epoch # 932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02966955112060532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,471 INFO epoch # 933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029551291547250003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,494 INFO epoch # 934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029482199141057208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,517 INFO epoch # 935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03043131291633472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,542 INFO epoch # 936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029789457126753405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,565 INFO epoch # 937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03056594345252961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,589 INFO epoch # 938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02961452805902809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,612 INFO epoch # 939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02960009893286042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,636 INFO epoch # 940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02977064941660501
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:11,636 INFO *** epoch 940, rolling-avg-loss (window=10)= 0.029843406559666618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,659 INFO epoch # 941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03012268472230062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,682 INFO epoch # 942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030101122392807156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,705 INFO epoch # 943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02982672824873589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,728 INFO epoch # 944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030705201905220747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,751 INFO epoch # 945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029343433474423364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,775 INFO epoch # 946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029107070702593774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,798 INFO epoch # 947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029489261854905635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,821 INFO epoch # 948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02952886640559882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,845 INFO epoch # 949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02869421456125565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,868 INFO epoch # 950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028892855974845588
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:11,868 INFO *** epoch 950, rolling-avg-loss (window=10)= 0.029581144024268723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,892 INFO epoch # 951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029056167870294303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,915 INFO epoch # 952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029173618968343362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,939 INFO epoch # 953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029630000703036785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,967 INFO epoch # 954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030988672864623368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:11,996 INFO epoch # 955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029445580788888037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,023 INFO epoch # 956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030539139814209193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,048 INFO epoch # 957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02968991018133238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,071 INFO epoch # 958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029670221003470942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,094 INFO epoch # 959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028403895325027406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,117 INFO epoch # 960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028869376837974414
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:12,117 INFO *** epoch 960, rolling-avg-loss (window=10)= 0.02954665843572002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,140 INFO epoch # 961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028671528096310794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,164 INFO epoch # 962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029352224461035803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,187 INFO epoch # 963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028810922987759113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,211 INFO epoch # 964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028820895415265113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,234 INFO epoch # 965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02866752105182968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,257 INFO epoch # 966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029600844223750755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,281 INFO epoch # 967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029808002640493214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,306 INFO epoch # 968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028556997887790203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,330 INFO epoch # 969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029949260380817577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,355 INFO epoch # 970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029131541290553287
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:12,355 INFO *** epoch 970, rolling-avg-loss (window=10)= 0.029136973843560555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,382 INFO epoch # 971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.030879269557772204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,406 INFO epoch # 972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029350190656259656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,431 INFO epoch # 973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02880743137211539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,455 INFO epoch # 974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029068747942801565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,479 INFO epoch # 975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029191765177529305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,503 INFO epoch # 976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029619642184115946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,527 INFO epoch # 977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03028967702994123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,551 INFO epoch # 978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028677546593826264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,576 INFO epoch # 979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029320967878447846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,611 INFO epoch # 980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03002316056517884
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:12,612 INFO *** epoch 980, rolling-avg-loss (window=10)= 0.029522839895798826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,653 INFO epoch # 981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028745068062562495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,691 INFO epoch # 982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028410739760147408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,717 INFO epoch # 983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02829236027901061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,741 INFO epoch # 984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0296552361396607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,766 INFO epoch # 985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028322470898274332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,792 INFO epoch # 986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028573272167705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,817 INFO epoch # 987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02924371199333109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,842 INFO epoch # 988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029011094971792772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,867 INFO epoch # 989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02871998562477529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,892 INFO epoch # 990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028709607082419097
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:12,893 INFO *** epoch 990, rolling-avg-loss (window=10)= 0.02876835469796788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,918 INFO epoch # 991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02827817772049457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,943 INFO epoch # 992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0289075548062101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,969 INFO epoch # 993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028751895937602967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:12,994 INFO epoch # 994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029327009775443003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,020 INFO epoch # 995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02937049197498709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,045 INFO epoch # 996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029410970222670585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,071 INFO epoch # 997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02936470406712033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,096 INFO epoch # 998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029550562147051096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,121 INFO epoch # 999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027835993590997532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,147 INFO epoch # 1000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028807726106606424
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:13,147 INFO *** epoch 1000, rolling-avg-loss (window=10)= 0.02896050863491837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,173 INFO epoch # 1001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027940914733335376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,198 INFO epoch # 1002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028096800058847293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,224 INFO epoch # 1003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027965037181274965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,251 INFO epoch # 1004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02822702148114331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,276 INFO epoch # 1005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028148209472419694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,302 INFO epoch # 1006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028284263069508597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,328 INFO epoch # 1007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028814805700676516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,353 INFO epoch # 1008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029557544243289158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,378 INFO epoch # 1009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.029170166002586484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,404 INFO epoch # 1010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02817228637286462
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:13,404 INFO *** epoch 1010, rolling-avg-loss (window=10)= 0.0284377048315946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,429 INFO epoch # 1011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028672968968749046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,455 INFO epoch # 1012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028235084959305823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,480 INFO epoch # 1013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028596432355698198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,505 INFO epoch # 1014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028145795746240765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,530 INFO epoch # 1015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02847240649862215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,555 INFO epoch # 1016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027951415453571826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,579 INFO epoch # 1017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02807376059354283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,604 INFO epoch # 1018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027754955139243975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,630 INFO epoch # 1019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0281121758162044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,656 INFO epoch # 1020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028898580640088767
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:13,657 INFO *** epoch 1020, rolling-avg-loss (window=10)= 0.028291357617126776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,683 INFO epoch # 1021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02800869633210823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,708 INFO epoch # 1022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027693831332726404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,734 INFO epoch # 1023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028174111736007035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,759 INFO epoch # 1024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02884643731522374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,785 INFO epoch # 1025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028469732322264463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,810 INFO epoch # 1026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02712273591896519
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,837 INFO epoch # 1027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02856066927779466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,863 INFO epoch # 1028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0279902316397056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,889 INFO epoch # 1029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02854463562835008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,915 INFO epoch # 1030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028605473431525752
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:13,916 INFO *** epoch 1030, rolling-avg-loss (window=10)= 0.028201655493467113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,941 INFO epoch # 1031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028264939785003662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,967 INFO epoch # 1032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0279924989445135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:13,992 INFO epoch # 1033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02901484165340662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,018 INFO epoch # 1034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028230756404809654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,043 INFO epoch # 1035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027940067084273323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,069 INFO epoch # 1036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028005016094539315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,093 INFO epoch # 1037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02775704927626066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,119 INFO epoch # 1038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027622560795862228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,145 INFO epoch # 1039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027740391436964273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,171 INFO epoch # 1040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027578645123867318
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:14,171 INFO *** epoch 1040, rolling-avg-loss (window=10)= 0.028014676659950055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,196 INFO epoch # 1041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027777182709542103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,223 INFO epoch # 1042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0279261608957313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,248 INFO epoch # 1043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028105681529268622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,275 INFO epoch # 1044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02852357312804088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,301 INFO epoch # 1045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02722897904459387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,327 INFO epoch # 1046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027699049096554518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,354 INFO epoch # 1047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0278718578920234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,379 INFO epoch # 1048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027455181669211015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,405 INFO epoch # 1049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02801020091283135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,435 INFO epoch # 1050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027353192039299756
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:14,435 INFO *** epoch 1050, rolling-avg-loss (window=10)= 0.027795105891709682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,463 INFO epoch # 1051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027438317658379674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,493 INFO epoch # 1052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02747012718464248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,522 INFO epoch # 1053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02701795901521109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,549 INFO epoch # 1054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028340684773866087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,576 INFO epoch # 1055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027940654050325975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,600 INFO epoch # 1056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02823862267541699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,625 INFO epoch # 1057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02695883682463318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,650 INFO epoch # 1058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028169661964057013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,675 INFO epoch # 1059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028670801228145137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,701 INFO epoch # 1060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028276129480218515
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:14,701 INFO *** epoch 1060, rolling-avg-loss (window=10)= 0.027852179485489615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,726 INFO epoch # 1061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02774535250500776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,751 INFO epoch # 1062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027514000568771735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,776 INFO epoch # 1063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02715359372086823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,800 INFO epoch # 1064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027669098664773628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,825 INFO epoch # 1065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027721038582967594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,850 INFO epoch # 1066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027524685545358807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,876 INFO epoch # 1067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028664321260293946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,901 INFO epoch # 1068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028832847136072814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,926 INFO epoch # 1069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027933987585129216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,951 INFO epoch # 1070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02725798520259559
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:14,951 INFO *** epoch 1070, rolling-avg-loss (window=10)= 0.027801691077183933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:14,976 INFO epoch # 1071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027766045066528022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,001 INFO epoch # 1072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02937601925805211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,026 INFO epoch # 1073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027494440379086882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,050 INFO epoch # 1074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027882425725692883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,074 INFO epoch # 1075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028922371624503285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,099 INFO epoch # 1076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028244701534276828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,124 INFO epoch # 1077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02780277928104624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,148 INFO epoch # 1078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02864449779735878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,173 INFO epoch # 1079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02769577334402129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,197 INFO epoch # 1080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027652727934764698
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:15,198 INFO *** epoch 1080, rolling-avg-loss (window=10)= 0.0281481781945331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,222 INFO epoch # 1081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028252441436052322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,247 INFO epoch # 1082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028740737558109686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,271 INFO epoch # 1083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027378920145565644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,296 INFO epoch # 1084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02761118079070002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,321 INFO epoch # 1085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027926986600505188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,346 INFO epoch # 1086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028579488542163745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,371 INFO epoch # 1087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02736250014277175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,396 INFO epoch # 1088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0287285256490577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,421 INFO epoch # 1089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02739741501864046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,446 INFO epoch # 1090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026887115061981604
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:15,446 INFO *** epoch 1090, rolling-avg-loss (window=10)= 0.02788653109455481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,471 INFO epoch # 1091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027340527303749695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,496 INFO epoch # 1092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026788702845806256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,522 INFO epoch # 1093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027269520884146914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,547 INFO epoch # 1094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027212700195377693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,572 INFO epoch # 1095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026945774821797386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,596 INFO epoch # 1096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02655683527700603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,621 INFO epoch # 1097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027500074851559475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,647 INFO epoch # 1098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027110739989439026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,682 INFO epoch # 1099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027766902465373278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,711 INFO epoch # 1100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026969425787683576
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:15,711 INFO *** epoch 1100, rolling-avg-loss (window=10)= 0.027146120442193934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,740 INFO epoch # 1101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.03125088819069788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,771 INFO epoch # 1102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027565309428609908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,802 INFO epoch # 1103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027346909191692248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,831 INFO epoch # 1104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02697852798155509
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,860 INFO epoch # 1105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027038944012019783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,888 INFO epoch # 1106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026397912675747648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,918 INFO epoch # 1107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027014828025130555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,948 INFO epoch # 1108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026744963775854558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:15,978 INFO epoch # 1109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026827423076611012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,007 INFO epoch # 1110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02713414133177139
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:16,007 INFO *** epoch 1110, rolling-avg-loss (window=10)= 0.027429984768969008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,036 INFO epoch # 1111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02688049033167772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,064 INFO epoch # 1112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028197117382660508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,093 INFO epoch # 1113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026796621474204585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,121 INFO epoch # 1114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02678344282321632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,148 INFO epoch # 1115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027062512352131307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,177 INFO epoch # 1116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026615007867803797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,205 INFO epoch # 1117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02659627204411663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,233 INFO epoch # 1118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02684031456010416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,258 INFO epoch # 1119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027171673078555614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,283 INFO epoch # 1120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02761662646662444
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:16,283 INFO *** epoch 1120, rolling-avg-loss (window=10)= 0.02705600783810951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,307 INFO epoch # 1121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027146753505803645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,332 INFO epoch # 1122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026740151370177045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,356 INFO epoch # 1123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026367441256297752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,380 INFO epoch # 1124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02660580724477768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,405 INFO epoch # 1125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026245100365485996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,429 INFO epoch # 1126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02610539863235317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,454 INFO epoch # 1127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026089638646226376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,478 INFO epoch # 1128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0274145090370439
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,502 INFO epoch # 1129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026583462895359844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,525 INFO epoch # 1130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027090732997749
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:16,526 INFO *** epoch 1130, rolling-avg-loss (window=10)= 0.02663889959512744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,549 INFO epoch # 1131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026810939802089706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,574 INFO epoch # 1132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02677006716839969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,598 INFO epoch # 1133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027123534062411636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,634 INFO epoch # 1134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026658996881451458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,663 INFO epoch # 1135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026288359687896445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,691 INFO epoch # 1136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025789041828829795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,719 INFO epoch # 1137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026181156310485676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,744 INFO epoch # 1138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02625825413269922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,768 INFO epoch # 1139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025989002810092643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,793 INFO epoch # 1140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025657744088675827
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:16,794 INFO *** epoch 1140, rolling-avg-loss (window=10)= 0.02635270967730321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,819 INFO epoch # 1141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02665966917993501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,845 INFO epoch # 1142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02666574649629183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,870 INFO epoch # 1143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027310217614285648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,894 INFO epoch # 1144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027327313146088272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,918 INFO epoch # 1145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0269088834465947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,943 INFO epoch # 1146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026725263945991173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,966 INFO epoch # 1147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02789451414719224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:16,990 INFO epoch # 1148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02727285010041669
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,016 INFO epoch # 1149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026528200309257954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,043 INFO epoch # 1150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025990069756517187
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:17,043 INFO *** epoch 1150, rolling-avg-loss (window=10)= 0.02692827281425707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,069 INFO epoch # 1151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025730103807291016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,095 INFO epoch # 1152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025685838132631034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,119 INFO epoch # 1153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025274863917729817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,143 INFO epoch # 1154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02622992728720419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,167 INFO epoch # 1155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025517585483612493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,190 INFO epoch # 1156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025752284185728058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,215 INFO epoch # 1157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02560817202902399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,241 INFO epoch # 1158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02674878595280461
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,268 INFO epoch # 1159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02589669422013685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,294 INFO epoch # 1160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027043967274948955
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:17,294 INFO *** epoch 1160, rolling-avg-loss (window=10)= 0.0259488222291111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,320 INFO epoch # 1161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027292235841741785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,344 INFO epoch # 1162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026113287516636774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,371 INFO epoch # 1163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02630702147143893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,394 INFO epoch # 1164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02590051360311918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,418 INFO epoch # 1165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02610595218720846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,442 INFO epoch # 1166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02680499348207377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,470 INFO epoch # 1167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0277951899333857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,497 INFO epoch # 1168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02580823708558455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,525 INFO epoch # 1169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026626780338119715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,550 INFO epoch # 1170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02534354169620201
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:17,550 INFO *** epoch 1170, rolling-avg-loss (window=10)= 0.02640977531555109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,575 INFO epoch # 1171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02577822480816394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,599 INFO epoch # 1172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025514884677249938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,623 INFO epoch # 1173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026213167293462902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,647 INFO epoch # 1174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027406257460825145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,671 INFO epoch # 1175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026219416729873046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,696 INFO epoch # 1176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026408805599203333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,720 INFO epoch # 1177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025958440062822774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,745 INFO epoch # 1178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025997066230047494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,770 INFO epoch # 1179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025356771191582084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,794 INFO epoch # 1180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02669797439011745
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:17,795 INFO *** epoch 1180, rolling-avg-loss (window=10)= 0.02615510084433481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,819 INFO epoch # 1181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026200502878054976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,844 INFO epoch # 1182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026230503572151065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,869 INFO epoch # 1183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02604323421837762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,893 INFO epoch # 1184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02506490988889709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,917 INFO epoch # 1185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02597296742897015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,942 INFO epoch # 1186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024935105844633654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,967 INFO epoch # 1187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025941026979126036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:17,990 INFO epoch # 1188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02767638594377786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,014 INFO epoch # 1189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02766432549105957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,038 INFO epoch # 1190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025908267503837124
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:18,038 INFO *** epoch 1190, rolling-avg-loss (window=10)= 0.026163722974888514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,062 INFO epoch # 1191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02573095980915241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,086 INFO epoch # 1192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02565710514318198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,110 INFO epoch # 1193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026545379136223346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,134 INFO epoch # 1194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025799962633755058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,158 INFO epoch # 1195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025455485156271607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,182 INFO epoch # 1196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025652775017078966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,211 INFO epoch # 1197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026337531016906723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,236 INFO epoch # 1198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025988003762904555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,261 INFO epoch # 1199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025096363649936393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,286 INFO epoch # 1200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025218180380761623
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:18,286 INFO *** epoch 1200, rolling-avg-loss (window=10)= 0.025748174570617267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,310 INFO epoch # 1201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025290455494541675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,345 INFO epoch # 1202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026574917807010934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,376 INFO epoch # 1203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02619399520335719
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,406 INFO epoch # 1204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02625048134359531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,438 INFO epoch # 1205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0258396886929404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,468 INFO epoch # 1206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0251368586323224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,498 INFO epoch # 1207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026307878200896084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,526 INFO epoch # 1208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026182044093729928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,555 INFO epoch # 1209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02589663804974407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,584 INFO epoch # 1210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025334290083264932
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:18,584 INFO *** epoch 1210, rolling-avg-loss (window=10)= 0.02590072476014029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,613 INFO epoch # 1211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024903571989852935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,643 INFO epoch # 1212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024735076003707945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,671 INFO epoch # 1213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024778255698038265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,700 INFO epoch # 1214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025104728672886267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,729 INFO epoch # 1215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024416710250079632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,758 INFO epoch # 1216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025407170673133805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,787 INFO epoch # 1217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025274397106841207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,816 INFO epoch # 1218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02493068386684172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,843 INFO epoch # 1219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025463912897976115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,872 INFO epoch # 1220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024603998463135213
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:18,873 INFO *** epoch 1220, rolling-avg-loss (window=10)= 0.02496185056224931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,899 INFO epoch # 1221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02549595828168094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,923 INFO epoch # 1222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02535591668856796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,948 INFO epoch # 1223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025730440771440044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,973 INFO epoch # 1224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025002477486850694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:18,998 INFO epoch # 1225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024953394138719887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,023 INFO epoch # 1226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024968642828753218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,048 INFO epoch # 1227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025467716535786167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,074 INFO epoch # 1228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025551148282829672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,099 INFO epoch # 1229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02509121401817538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,124 INFO epoch # 1230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02595003615715541
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:19,124 INFO *** epoch 1230, rolling-avg-loss (window=10)= 0.025356694518995936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,150 INFO epoch # 1231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.028253858326934278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,177 INFO epoch # 1232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027529525075806305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,202 INFO epoch # 1233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02544735581614077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,228 INFO epoch # 1234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02523119462421164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,254 INFO epoch # 1235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024869069835403934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,279 INFO epoch # 1236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02519837612635456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,303 INFO epoch # 1237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025751792592927814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,331 INFO epoch # 1238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025196362636052072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,356 INFO epoch # 1239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025311722129117697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,388 INFO epoch # 1240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025797625945415348
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:19,388 INFO *** epoch 1240, rolling-avg-loss (window=10)= 0.02585868831083644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,420 INFO epoch # 1241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025809600163483992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,450 INFO epoch # 1242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025132227427093312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,480 INFO epoch # 1243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02520839878707193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,509 INFO epoch # 1244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024991887621581554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,538 INFO epoch # 1245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024347825528820977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,568 INFO epoch # 1246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026712035876698792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,598 INFO epoch # 1247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025901588174747303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,628 INFO epoch # 1248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025095604883972555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,657 INFO epoch # 1249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02486984580173157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,686 INFO epoch # 1250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02456962675205432
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:19,686 INFO *** epoch 1250, rolling-avg-loss (window=10)= 0.025263864101725632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,714 INFO epoch # 1251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025737479620147496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,743 INFO epoch # 1252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025372179021360353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,772 INFO epoch # 1253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024298769480083138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,801 INFO epoch # 1254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024607195373391733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,830 INFO epoch # 1255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025085276050958782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,859 INFO epoch # 1256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02498751721577719
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,888 INFO epoch # 1257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02460908782086335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,917 INFO epoch # 1258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024429829587461427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,947 INFO epoch # 1259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024985159805510193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:19,976 INFO epoch # 1260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.027087785594630986
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:19,976 INFO *** epoch 1260, rolling-avg-loss (window=10)= 0.025120027957018464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,004 INFO epoch # 1261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02619238497572951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,035 INFO epoch # 1262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024620765703730285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,064 INFO epoch # 1263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025172180321533233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,093 INFO epoch # 1264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025521892646793276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,121 INFO epoch # 1265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02455311219091527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,149 INFO epoch # 1266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026006954605691135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,178 INFO epoch # 1267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02457005815813318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,208 INFO epoch # 1268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02472206504899077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,238 INFO epoch # 1269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024721764377318323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,268 INFO epoch # 1270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024870457651559263
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:20,269 INFO *** epoch 1270, rolling-avg-loss (window=10)= 0.025095163568039425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,297 INFO epoch # 1271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024799939332297072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,326 INFO epoch # 1272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024621876800665632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,354 INFO epoch # 1273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024956440494861454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,384 INFO epoch # 1274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024734574311878532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,414 INFO epoch # 1275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024589008564362302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,445 INFO epoch # 1276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024716219515539706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,474 INFO epoch # 1277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02518142349435948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,502 INFO epoch # 1278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02462713365093805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,528 INFO epoch # 1279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024805213906802237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,555 INFO epoch # 1280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02417022717418149
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:20,555 INFO *** epoch 1280, rolling-avg-loss (window=10)= 0.024720205724588596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,580 INFO epoch # 1281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024152056372258812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,606 INFO epoch # 1282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025425129890209064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,631 INFO epoch # 1283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023946332366904244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,661 INFO epoch # 1284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02477542796987109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,690 INFO epoch # 1285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025228860293282196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,719 INFO epoch # 1286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024489602277753875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,748 INFO epoch # 1287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024593729409389198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,777 INFO epoch # 1288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024820884194923565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,806 INFO epoch # 1289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025408396642887965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,837 INFO epoch # 1290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02482478244928643
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:20,837 INFO *** epoch 1290, rolling-avg-loss (window=10)= 0.024766520186676643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,868 INFO epoch # 1291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02434565668227151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,900 INFO epoch # 1292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024826143955579028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,936 INFO epoch # 1293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024815063748974353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:20,973 INFO epoch # 1294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0244675766734872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,006 INFO epoch # 1295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024205539433751255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,035 INFO epoch # 1296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025014145736349747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,066 INFO epoch # 1297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024001723795663565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,095 INFO epoch # 1298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02413913613418117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,124 INFO epoch # 1299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024037839990342036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,152 INFO epoch # 1300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024126479227561504
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:21,152 INFO *** epoch 1300, rolling-avg-loss (window=10)= 0.024397930537816137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,181 INFO epoch # 1301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02456720612826757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,211 INFO epoch # 1302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025133504095720127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,239 INFO epoch # 1303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023802555340807885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,268 INFO epoch # 1304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02446053526364267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,297 INFO epoch # 1305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024564660474425182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,326 INFO epoch # 1306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024321960081579164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,355 INFO epoch # 1307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0234674975508824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,383 INFO epoch # 1308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024628119223052636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,411 INFO epoch # 1309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024494124460034072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,439 INFO epoch # 1310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024018181866267696
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:21,439 INFO *** epoch 1310, rolling-avg-loss (window=10)= 0.02434583444846794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,467 INFO epoch # 1311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02492504907422699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,496 INFO epoch # 1312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024104364129016176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,524 INFO epoch # 1313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02419122910941951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,551 INFO epoch # 1314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024748732947045937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,579 INFO epoch # 1315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023989378241822124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,606 INFO epoch # 1316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023779518145602196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,636 INFO epoch # 1317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024050130014074966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,664 INFO epoch # 1318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02366160397650674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,693 INFO epoch # 1319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024281010730192065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,728 INFO epoch # 1320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024103221599943936
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:21,728 INFO *** epoch 1320, rolling-avg-loss (window=10)= 0.024183423796785064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,757 INFO epoch # 1321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02429396694060415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,781 INFO epoch # 1322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.025350812211399898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,805 INFO epoch # 1323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024634840607177466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,830 INFO epoch # 1324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02473328323685564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,854 INFO epoch # 1325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024456651270156726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,879 INFO epoch # 1326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023633854230865836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,903 INFO epoch # 1327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023765792226186022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,930 INFO epoch # 1328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02386832988122478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,954 INFO epoch # 1329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02373001052183099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:21,979 INFO epoch # 1330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023528628516942263
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:21,979 INFO *** epoch 1330, rolling-avg-loss (window=10)= 0.024199616964324377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,003 INFO epoch # 1331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024484662630129606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,028 INFO epoch # 1332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023756362032145262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,052 INFO epoch # 1333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023670083755860105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,076 INFO epoch # 1334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024416862928774208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,100 INFO epoch # 1335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024790445080725476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,125 INFO epoch # 1336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02314529227442108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,149 INFO epoch # 1337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023921510466607288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,173 INFO epoch # 1338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02355852912296541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,197 INFO epoch # 1339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023614263045601547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,221 INFO epoch # 1340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02397402995848097
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:22,221 INFO *** epoch 1340, rolling-avg-loss (window=10)= 0.023933204129571096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,245 INFO epoch # 1341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024566674168454483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,269 INFO epoch # 1342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024851828056853265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,294 INFO epoch # 1343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024036189453909174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,319 INFO epoch # 1344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02432630377006717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,343 INFO epoch # 1345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024723788257688284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,369 INFO epoch # 1346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024105170916300267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,394 INFO epoch # 1347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023926051129819825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,419 INFO epoch # 1348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02349638083251193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,443 INFO epoch # 1349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02342646094621159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,467 INFO epoch # 1350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022967662836890668
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:22,467 INFO *** epoch 1350, rolling-avg-loss (window=10)= 0.024042651036870664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,491 INFO epoch # 1351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02381027798401192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,514 INFO epoch # 1352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02345163782592863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,538 INFO epoch # 1353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023532472347142175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,563 INFO epoch # 1354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02330380681087263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,587 INFO epoch # 1355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023251909820828587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,612 INFO epoch # 1356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02425802921061404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,636 INFO epoch # 1357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023555302555905655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,661 INFO epoch # 1358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024367508216528222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,687 INFO epoch # 1359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023659835977014154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,712 INFO epoch # 1360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02418099602800794
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:22,713 INFO *** epoch 1360, rolling-avg-loss (window=10)= 0.023737177677685396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,737 INFO epoch # 1361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02299959771335125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,761 INFO epoch # 1362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023537878674687818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,786 INFO epoch # 1363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024044247751589864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,811 INFO epoch # 1364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02432502547162585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,836 INFO epoch # 1365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.026232982869260013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,861 INFO epoch # 1366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024712806509342045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,886 INFO epoch # 1367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023311380035011098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,910 INFO epoch # 1368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024311178334755823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,935 INFO epoch # 1369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02356065431376919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,959 INFO epoch # 1370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023758484283462167
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:22,959 INFO *** epoch 1370, rolling-avg-loss (window=10)= 0.02407942359568551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:22,984 INFO epoch # 1371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02359519962919876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,009 INFO epoch # 1372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023010786302620545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,034 INFO epoch # 1373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02329968148842454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,060 INFO epoch # 1374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023405379964970052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,085 INFO epoch # 1375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02330798184266314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,110 INFO epoch # 1376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023440714488970116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,134 INFO epoch # 1377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023266598203917965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,157 INFO epoch # 1378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023256083339219913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,181 INFO epoch # 1379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023198714043246582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,205 INFO epoch # 1380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023429395077982917
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:23,205 INFO *** epoch 1380, rolling-avg-loss (window=10)= 0.023321053438121454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,229 INFO epoch # 1381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023127589840441942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,254 INFO epoch # 1382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02375729227787815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,279 INFO epoch # 1383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023195552465040237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,306 INFO epoch # 1384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02325047634076327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,332 INFO epoch # 1385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02275993736111559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,357 INFO epoch # 1386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022348393773427233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,382 INFO epoch # 1387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022741588996723294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,406 INFO epoch # 1388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023032180935842916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,431 INFO epoch # 1389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022710998659022152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,455 INFO epoch # 1390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02299605417647399
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:23,456 INFO *** epoch 1390, rolling-avg-loss (window=10)= 0.022992006482672876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,480 INFO epoch # 1391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02279946973430924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,507 INFO epoch # 1392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02384054457070306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,532 INFO epoch # 1393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023320227133808658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,556 INFO epoch # 1394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02254802393144928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,582 INFO epoch # 1395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02261646560509689
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,606 INFO epoch # 1396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023446133360266685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,631 INFO epoch # 1397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024469220312312245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,657 INFO epoch # 1398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02334608501405455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,682 INFO epoch # 1399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022584595091757365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,707 INFO epoch # 1400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022207792120752856
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:23,707 INFO *** epoch 1400, rolling-avg-loss (window=10)= 0.023117855687451082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,733 INFO epoch # 1401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022572517103981227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,758 INFO epoch # 1402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022586375649552792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,784 INFO epoch # 1403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022668990277452394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,809 INFO epoch # 1404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022779877937864512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,836 INFO epoch # 1405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022525971027789637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,862 INFO epoch # 1406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023058449529344216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,887 INFO epoch # 1407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023179902840638533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,911 INFO epoch # 1408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022644023469183594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,936 INFO epoch # 1409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022748403454897925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,960 INFO epoch # 1410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022898609866388142
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:23,961 INFO *** epoch 1410, rolling-avg-loss (window=10)= 0.022766312115709297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:23,986 INFO epoch # 1411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022461917193140835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,010 INFO epoch # 1412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022988732729572803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,035 INFO epoch # 1413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02248778946523089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,060 INFO epoch # 1414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02246043217019178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,086 INFO epoch # 1415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023402557446388528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,110 INFO epoch # 1416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022959347057621926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,135 INFO epoch # 1417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02385498004150577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,159 INFO epoch # 1418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02319075117702596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,183 INFO epoch # 1419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023150709981564432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,207 INFO epoch # 1420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02341267600422725
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:24,207 INFO *** epoch 1420, rolling-avg-loss (window=10)= 0.023036989326647016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,231 INFO epoch # 1421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021869645541300997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,256 INFO epoch # 1422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022058243936044164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,280 INFO epoch # 1423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023133221839088947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,306 INFO epoch # 1424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023686604195972905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,330 INFO epoch # 1425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02272984301089309
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,355 INFO epoch # 1426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023032415483612567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,385 INFO epoch # 1427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023088209331035614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,411 INFO epoch # 1428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024367467151023448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,435 INFO epoch # 1429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023283203219762072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,460 INFO epoch # 1430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022331949148792773
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:24,460 INFO *** epoch 1430, rolling-avg-loss (window=10)= 0.022958080285752656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,485 INFO epoch # 1431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022437841835198924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,510 INFO epoch # 1432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022900264739291742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,535 INFO epoch # 1433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024505066277924925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,559 INFO epoch # 1434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022629362501902506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,584 INFO epoch # 1435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02232915541389957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,609 INFO epoch # 1436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022323129611322656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,635 INFO epoch # 1437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022787435940699652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,660 INFO epoch # 1438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02249903985648416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,685 INFO epoch # 1439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02228195089264773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,710 INFO epoch # 1440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022715951927239075
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:24,711 INFO *** epoch 1440, rolling-avg-loss (window=10)= 0.022740919899661094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,735 INFO epoch # 1441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02311903826193884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,760 INFO epoch # 1442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023049081646604463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,784 INFO epoch # 1443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022866806772071868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,809 INFO epoch # 1444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022395971667720005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,835 INFO epoch # 1445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022848944849101827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,860 INFO epoch # 1446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021943124709650874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,885 INFO epoch # 1447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02228087795083411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,911 INFO epoch # 1448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021639631217112765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,938 INFO epoch # 1449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021778427413664758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,966 INFO epoch # 1450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02231254754588008
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:24,967 INFO *** epoch 1450, rolling-avg-loss (window=10)= 0.02242344520345796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:24,992 INFO epoch # 1451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02181807259330526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,017 INFO epoch # 1452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023222402960527688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,041 INFO epoch # 1453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023442793783033267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,065 INFO epoch # 1454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021980877761961892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,089 INFO epoch # 1455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022384954994777218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,115 INFO epoch # 1456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021806345670484006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,140 INFO epoch # 1457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021782939074910246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,165 INFO epoch # 1458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02199924990418367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,190 INFO epoch # 1459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022856265568407252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,214 INFO epoch # 1460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022830058209365234
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:25,215 INFO *** epoch 1460, rolling-avg-loss (window=10)= 0.022412396052095572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,239 INFO epoch # 1461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022248258464969695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,263 INFO epoch # 1462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021944495267234743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,287 INFO epoch # 1463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022637458838289604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,314 INFO epoch # 1464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022565595048945397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,338 INFO epoch # 1465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02179835509741679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,363 INFO epoch # 1466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022585665166843683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,388 INFO epoch # 1467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022605484205996618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,413 INFO epoch # 1468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021305371046764776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,437 INFO epoch # 1469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0216693481488619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,462 INFO epoch # 1470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022145548107801005
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:25,462 INFO *** epoch 1470, rolling-avg-loss (window=10)= 0.02215055793931242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,486 INFO epoch # 1471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021559753047768027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,511 INFO epoch # 1472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022885431710164994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,536 INFO epoch # 1473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022495310113299638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,561 INFO epoch # 1474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022101052338257432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,586 INFO epoch # 1475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023477006034227088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,611 INFO epoch # 1476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02275736490264535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,636 INFO epoch # 1477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02173860909533687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,662 INFO epoch # 1478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022946617187699303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,687 INFO epoch # 1479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02260964221204631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,711 INFO epoch # 1480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02181739333900623
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:25,712 INFO *** epoch 1480, rolling-avg-loss (window=10)= 0.022438817998045126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,735 INFO epoch # 1481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02174786600517109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,760 INFO epoch # 1482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022059068258386105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,785 INFO epoch # 1483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021966771746519953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,810 INFO epoch # 1484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02138051384827122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,835 INFO epoch # 1485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02175715976045467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,860 INFO epoch # 1486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022718551044818014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,885 INFO epoch # 1487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024087683763355017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,910 INFO epoch # 1488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.024285264080390334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,935 INFO epoch # 1489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023117506061680615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,959 INFO epoch # 1490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022598167008254677
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:25,960 INFO *** epoch 1490, rolling-avg-loss (window=10)= 0.02257185515773017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:25,984 INFO epoch # 1491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02162417594809085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,010 INFO epoch # 1492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02228557667694986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,043 INFO epoch # 1493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021967805630993098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,071 INFO epoch # 1494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022959682595683262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,098 INFO epoch # 1495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021891808632062748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,125 INFO epoch # 1496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022429253469454125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,151 INFO epoch # 1497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022774071810999885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,175 INFO epoch # 1498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021967422479065135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,200 INFO epoch # 1499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021301016851793975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,224 INFO epoch # 1500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02247716739657335
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:26,224 INFO *** epoch 1500, rolling-avg-loss (window=10)= 0.02216779814916663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,249 INFO epoch # 1501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022743046167306602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,273 INFO epoch # 1502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022391955746570602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,298 INFO epoch # 1503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021485259610926732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,322 INFO epoch # 1504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02169223828241229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,347 INFO epoch # 1505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02160346889286302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,373 INFO epoch # 1506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021647892746841535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,400 INFO epoch # 1507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0216861974040512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,425 INFO epoch # 1508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021461272408487275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,449 INFO epoch # 1509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02279211359564215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,473 INFO epoch # 1510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022671959799481556
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:26,473 INFO *** epoch 1510, rolling-avg-loss (window=10)= 0.022017540465458296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,497 INFO epoch # 1511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021587908966466784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,522 INFO epoch # 1512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02195105596911162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,546 INFO epoch # 1513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021903251064941287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,571 INFO epoch # 1514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021352995099732652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,596 INFO epoch # 1515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021023983659688383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,621 INFO epoch # 1516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02137542559648864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,645 INFO epoch # 1517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021656249649822712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,681 INFO epoch # 1518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021374284871853888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,721 INFO epoch # 1519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02135030087083578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,761 INFO epoch # 1520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021553227794356644
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:26,762 INFO *** epoch 1520, rolling-avg-loss (window=10)= 0.02151286835432984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,805 INFO epoch # 1521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02134409756399691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,848 INFO epoch # 1522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02116446671425365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,892 INFO epoch # 1523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020964174371329136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,938 INFO epoch # 1524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021238947490928695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:26,983 INFO epoch # 1525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021692106471164152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,015 INFO epoch # 1526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02164846585947089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,041 INFO epoch # 1527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02124171334435232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,065 INFO epoch # 1528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02214643851039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,089 INFO epoch # 1529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022193518409039825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,113 INFO epoch # 1530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021141449542483315
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:27,113 INFO *** epoch 1530, rolling-avg-loss (window=10)= 0.021477537827740888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,137 INFO epoch # 1531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02105880921590142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,161 INFO epoch # 1532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021763699158327654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,185 INFO epoch # 1533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022604662022786215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,208 INFO epoch # 1534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02186484311823733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,232 INFO epoch # 1535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021604420355288312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,257 INFO epoch # 1536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021585933165624738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,282 INFO epoch # 1537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022011775203282014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,307 INFO epoch # 1538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020830813533393666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,332 INFO epoch # 1539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02127334216493182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,356 INFO epoch # 1540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020550920817186125
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:27,356 INFO *** epoch 1540, rolling-avg-loss (window=10)= 0.02151492187549593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,381 INFO epoch # 1541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021787279867567122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,406 INFO epoch # 1542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021864148322492838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,430 INFO epoch # 1543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021300579464877956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,455 INFO epoch # 1544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02125023354892619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,479 INFO epoch # 1545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021436988608911633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,503 INFO epoch # 1546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02110575174447149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,528 INFO epoch # 1547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020878529699984938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,554 INFO epoch # 1548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021793822845211253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,578 INFO epoch # 1549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021202479460043833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,603 INFO epoch # 1550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021379714482463896
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:27,603 INFO *** epoch 1550, rolling-avg-loss (window=10)= 0.021399952804495116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,627 INFO epoch # 1551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021663832245394588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,652 INFO epoch # 1552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020970786310499534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,676 INFO epoch # 1553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02073559010750614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,701 INFO epoch # 1554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020942151109920815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,725 INFO epoch # 1555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021060746890725568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,749 INFO epoch # 1556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02151525192311965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,774 INFO epoch # 1557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021459918760228902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,798 INFO epoch # 1558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021339846920454875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,823 INFO epoch # 1559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021376779535785317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,847 INFO epoch # 1560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02175032297964208
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:27,847 INFO *** epoch 1560, rolling-avg-loss (window=10)= 0.021281522678327745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,871 INFO epoch # 1561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.023969435918843374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,895 INFO epoch # 1562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022955416061449796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,926 INFO epoch # 1563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021046488371212035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,952 INFO epoch # 1564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02072119087097235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:27,977 INFO epoch # 1565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02115565980784595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,002 INFO epoch # 1566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02286636660574004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,027 INFO epoch # 1567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022446705668698996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,052 INFO epoch # 1568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02107388572767377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,077 INFO epoch # 1569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02106341443140991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,101 INFO epoch # 1570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020728189061628655
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:28,101 INFO *** epoch 1570, rolling-avg-loss (window=10)= 0.021802675252547487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,124 INFO epoch # 1571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020774117845576257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,149 INFO epoch # 1572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020970439196389634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,177 INFO epoch # 1573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020445000904146582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,202 INFO epoch # 1574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020666831405833364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,227 INFO epoch # 1575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020979359454941005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,252 INFO epoch # 1576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02141630760161206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,276 INFO epoch # 1577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02044473434216343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,300 INFO epoch # 1578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02048334458959289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,324 INFO epoch # 1579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02139197415090166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,348 INFO epoch # 1580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021664843894541264
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:28,348 INFO *** epoch 1580, rolling-avg-loss (window=10)= 0.020923695338569814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,373 INFO epoch # 1581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020295692229410633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,399 INFO epoch # 1582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021351090545067564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,423 INFO epoch # 1583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02114220551447943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,447 INFO epoch # 1584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020952058606781065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,472 INFO epoch # 1585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02057411297573708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,495 INFO epoch # 1586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021070211892947555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,519 INFO epoch # 1587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020623934513423592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,543 INFO epoch # 1588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020480694976868108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,567 INFO epoch # 1589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021717077237553895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,592 INFO epoch # 1590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020686295203631744
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:28,592 INFO *** epoch 1590, rolling-avg-loss (window=10)= 0.020889337369590068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,617 INFO epoch # 1591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021144869475392625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,642 INFO epoch # 1592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02127670604386367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,667 INFO epoch # 1593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02113768382696435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,692 INFO epoch # 1594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020470800867769867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,716 INFO epoch # 1595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020954326435457915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,740 INFO epoch # 1596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022142333415104076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,764 INFO epoch # 1597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022386074444511905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,788 INFO epoch # 1598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02072495481115766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,813 INFO epoch # 1599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020575930015183985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,837 INFO epoch # 1600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02062255539931357
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:28,837 INFO *** epoch 1600, rolling-avg-loss (window=10)= 0.021143623473471963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,862 INFO epoch # 1601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020765602181199938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,886 INFO epoch # 1602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021578991960268468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,912 INFO epoch # 1603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02086394265643321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,936 INFO epoch # 1604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020785711792996153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,961 INFO epoch # 1605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02036625987966545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:28,985 INFO epoch # 1606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020226457898388617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,009 INFO epoch # 1607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02022118808235973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,034 INFO epoch # 1608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020379371795570478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,059 INFO epoch # 1609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020366817188914865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,090 INFO epoch # 1610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020948061312083155
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:29,090 INFO *** epoch 1610, rolling-avg-loss (window=10)= 0.020650240474788008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,119 INFO epoch # 1611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020460163766983896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,147 INFO epoch # 1612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02157310748589225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,171 INFO epoch # 1613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02050197435892187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,195 INFO epoch # 1614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022292049194220454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,219 INFO epoch # 1615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021316591242793947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,245 INFO epoch # 1616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020654325431678444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,269 INFO epoch # 1617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02047554639284499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,294 INFO epoch # 1618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021872797136893496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,318 INFO epoch # 1619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02006364183034748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,343 INFO epoch # 1620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020254226779798046
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:29,343 INFO *** epoch 1620, rolling-avg-loss (window=10)= 0.020946442362037486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,366 INFO epoch # 1621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020647723373258486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,390 INFO epoch # 1622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020429646130651236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,414 INFO epoch # 1623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01973572259885259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,438 INFO epoch # 1624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020128937350818887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,463 INFO epoch # 1625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020163480832707137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,487 INFO epoch # 1626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020887034479528666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,512 INFO epoch # 1627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019991302746348083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,537 INFO epoch # 1628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020345063385320827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,561 INFO epoch # 1629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02023980548256077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,585 INFO epoch # 1630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0206737780245021
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:29,586 INFO *** epoch 1630, rolling-avg-loss (window=10)= 0.020324249440454877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,610 INFO epoch # 1631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020298528135754168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,634 INFO epoch # 1632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021284557369654067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,658 INFO epoch # 1633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020076159096788615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,683 INFO epoch # 1634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020759161503519863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,709 INFO epoch # 1635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02046025509480387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,741 INFO epoch # 1636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02065572011633776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,770 INFO epoch # 1637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0205586897500325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,797 INFO epoch # 1638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021061960578663275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,821 INFO epoch # 1639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0219196270336397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,845 INFO epoch # 1640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020611727348295972
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:29,845 INFO *** epoch 1640, rolling-avg-loss (window=10)= 0.020768638602748978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,869 INFO epoch # 1641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020166392991086468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,893 INFO epoch # 1642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01957789860898629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,918 INFO epoch # 1643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019781537557719275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,942 INFO epoch # 1644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019918520614737645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,967 INFO epoch # 1645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020975921535864472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:29,991 INFO epoch # 1646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020763526117661968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,015 INFO epoch # 1647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02089074146351777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,040 INFO epoch # 1648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021381582191679627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,064 INFO epoch # 1649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020975779771106318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,089 INFO epoch # 1650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020555248338496312
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:30,089 INFO *** epoch 1650, rolling-avg-loss (window=10)= 0.020498714919085614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,113 INFO epoch # 1651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020516623713774607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,138 INFO epoch # 1652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019858576590195298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,162 INFO epoch # 1653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020160685933660716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,187 INFO epoch # 1654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020245492894900963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,211 INFO epoch # 1655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02030796479084529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,235 INFO epoch # 1656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021425242390250787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,259 INFO epoch # 1657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02137976256199181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,283 INFO epoch # 1658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020136970590101555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,307 INFO epoch # 1659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020657930115703493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,331 INFO epoch # 1660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020634866232285276
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:30,331 INFO *** epoch 1660, rolling-avg-loss (window=10)= 0.02053241158137098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,358 INFO epoch # 1661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021623518521664664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,388 INFO epoch # 1662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.021369664173107594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,417 INFO epoch # 1663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020489489805186167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,442 INFO epoch # 1664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020330553292296827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,466 INFO epoch # 1665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019730819680262357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,490 INFO epoch # 1666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019621366227511317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,514 INFO epoch # 1667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019934852636652067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,539 INFO epoch # 1668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019344663378433324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,564 INFO epoch # 1669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019889729825081304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,589 INFO epoch # 1670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020401187182869762
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:30,589 INFO *** epoch 1670, rolling-avg-loss (window=10)= 0.02027358447230654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,614 INFO epoch # 1671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01972577258129604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,639 INFO epoch # 1672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020285462378524244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,664 INFO epoch # 1673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020041579147800803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,688 INFO epoch # 1674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02002373500727117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,712 INFO epoch # 1675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020772732299519703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,736 INFO epoch # 1676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020540737081319094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,761 INFO epoch # 1677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01971179651445709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,786 INFO epoch # 1678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0206986996345222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,811 INFO epoch # 1679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019744756049476564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,835 INFO epoch # 1680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020000775723019615
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:30,835 INFO *** epoch 1680, rolling-avg-loss (window=10)= 0.020154604641720652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,859 INFO epoch # 1681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01958045612263959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,883 INFO epoch # 1682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02038731618085876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,907 INFO epoch # 1683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020255115785403177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,930 INFO epoch # 1684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019972879235865548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,955 INFO epoch # 1685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01994218782056123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:30,979 INFO epoch # 1686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020079471491044387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,003 INFO epoch # 1687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019912740477593616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,028 INFO epoch # 1688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019719451433047652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,053 INFO epoch # 1689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02029091669828631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,078 INFO epoch # 1690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019651179405627772
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:31,078 INFO *** epoch 1690, rolling-avg-loss (window=10)= 0.019979171465092805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,102 INFO epoch # 1691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02029959337960463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,125 INFO epoch # 1692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019981047371402383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,149 INFO epoch # 1693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01974751497618854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,174 INFO epoch # 1694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0206416642467957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,198 INFO epoch # 1695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020184051303658634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,223 INFO epoch # 1696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020646051038056612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,247 INFO epoch # 1697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020130854565650225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,272 INFO epoch # 1698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01964593061711639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,297 INFO epoch # 1699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019168306869687513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,321 INFO epoch # 1700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019559507170924917
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:31,321 INFO *** epoch 1700, rolling-avg-loss (window=10)= 0.020000452153908554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,344 INFO epoch # 1701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019120855766232125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,368 INFO epoch # 1702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019821815163595602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,393 INFO epoch # 1703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0203550991427619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,417 INFO epoch # 1704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019729240622837096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,441 INFO epoch # 1705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019420507218455896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,466 INFO epoch # 1706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019258653090219013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,490 INFO epoch # 1707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01920221709588077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,515 INFO epoch # 1708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01954641775228083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,539 INFO epoch # 1709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019666904932819307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,564 INFO epoch # 1710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019297954422654584
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:31,564 INFO *** epoch 1710, rolling-avg-loss (window=10)= 0.019541966520773713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,588 INFO epoch # 1711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01994503499008715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,612 INFO epoch # 1712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01989921717904508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,642 INFO epoch # 1713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01998418930452317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,677 INFO epoch # 1714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.022065935976570472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,712 INFO epoch # 1715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020215810698573478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,741 INFO epoch # 1716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01982804454746656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,769 INFO epoch # 1717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01952078496105969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,796 INFO epoch # 1718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01961418622522615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,820 INFO epoch # 1719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01982997340383008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,846 INFO epoch # 1720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020487459929427132
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:31,846 INFO *** epoch 1720, rolling-avg-loss (window=10)= 0.020139063721580898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,872 INFO epoch # 1721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020564580016070977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,896 INFO epoch # 1722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02021178719587624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,921 INFO epoch # 1723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020562738936860114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,945 INFO epoch # 1724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020902015472529456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,970 INFO epoch # 1725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019958598888479173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:31,995 INFO epoch # 1726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01968251512153074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,019 INFO epoch # 1727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018892557767685503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,044 INFO epoch # 1728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01963049391633831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,070 INFO epoch # 1729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019892455457011238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,095 INFO epoch # 1730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020130156772211194
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:32,095 INFO *** epoch 1730, rolling-avg-loss (window=10)= 0.020042789954459295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,121 INFO epoch # 1731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019692985952133313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,146 INFO epoch # 1732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01898662859457545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,171 INFO epoch # 1733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018950068450067192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,194 INFO epoch # 1734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019783478055614978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,218 INFO epoch # 1735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019875825091730803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,244 INFO epoch # 1736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01950934631167911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,268 INFO epoch # 1737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019782184914220124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,293 INFO epoch # 1738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020013748260680586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,318 INFO epoch # 1739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019079573117778637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,344 INFO epoch # 1740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01917919222614728
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:32,344 INFO *** epoch 1740, rolling-avg-loss (window=10)= 0.01948530309746275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,369 INFO epoch # 1741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020993932150304317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,394 INFO epoch # 1742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01926646989886649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,420 INFO epoch # 1743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019351047318195924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,444 INFO epoch # 1744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019314022676553577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,470 INFO epoch # 1745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019471969368169084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,495 INFO epoch # 1746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019567704148357734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,519 INFO epoch # 1747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01992006038199179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,544 INFO epoch # 1748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01927831824286841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,569 INFO epoch # 1749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019399641023483127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,595 INFO epoch # 1750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019589714967878535
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:32,595 INFO *** epoch 1750, rolling-avg-loss (window=10)= 0.019615288017666897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,620 INFO epoch # 1751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01973285715212114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,654 INFO epoch # 1752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01937076434842311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,682 INFO epoch # 1753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018937818240374327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,713 INFO epoch # 1754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02004883249173872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,741 INFO epoch # 1755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0199572007113602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,768 INFO epoch # 1756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01897347456542775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,794 INFO epoch # 1757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018969694443512708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,821 INFO epoch # 1758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02054123772541061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,846 INFO epoch # 1759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019546415001968853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,871 INFO epoch # 1760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020426055998541415
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:32,871 INFO *** epoch 1760, rolling-avg-loss (window=10)= 0.019650435067887884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,895 INFO epoch # 1761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020532589551294222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,920 INFO epoch # 1762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019752719497773796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,945 INFO epoch # 1763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01952692194026895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,970 INFO epoch # 1764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01918197178747505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:32,994 INFO epoch # 1765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018533992144512013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,018 INFO epoch # 1766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01896678685443476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,045 INFO epoch # 1767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01898933050688356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,070 INFO epoch # 1768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019583267072448507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,095 INFO epoch # 1769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01912723228451796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,120 INFO epoch # 1770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019442172721028328
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:33,120 INFO *** epoch 1770, rolling-avg-loss (window=10)= 0.019363698436063715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,145 INFO epoch # 1771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018831023218808696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,172 INFO epoch # 1772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01879223171272315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,197 INFO epoch # 1773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0189948768238537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,222 INFO epoch # 1774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02093547605909407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,247 INFO epoch # 1775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020178668579319492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,272 INFO epoch # 1776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01968813326675445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,297 INFO epoch # 1777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019686965824803337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,322 INFO epoch # 1778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01882663188735023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,347 INFO epoch # 1779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02006869579781778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,372 INFO epoch # 1780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01930855054524727
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:33,372 INFO *** epoch 1780, rolling-avg-loss (window=10)= 0.019531125371577217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,398 INFO epoch # 1781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01894955159514211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,423 INFO epoch # 1782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018991276709130034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,449 INFO epoch # 1783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01914633988053538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,473 INFO epoch # 1784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019567736715544015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,496 INFO epoch # 1785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019769284743233584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,520 INFO epoch # 1786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019066567154368386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,544 INFO epoch # 1787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019109324173768982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,569 INFO epoch # 1788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01887122867628932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,594 INFO epoch # 1789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019081351201748475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,619 INFO epoch # 1790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018849478801712394
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:33,619 INFO *** epoch 1790, rolling-avg-loss (window=10)= 0.019140213965147267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,643 INFO epoch # 1791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018799502940964885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,667 INFO epoch # 1792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019323790882481262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,691 INFO epoch # 1793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018831314926501364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,715 INFO epoch # 1794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01887883269228041
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,738 INFO epoch # 1795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019150428212014958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,762 INFO epoch # 1796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020153178105829284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,786 INFO epoch # 1797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019276570295915008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,811 INFO epoch # 1798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01891499079647474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,836 INFO epoch # 1799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018632240753504448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,861 INFO epoch # 1800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01861624454613775
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:33,861 INFO *** epoch 1800, rolling-avg-loss (window=10)= 0.01905770941521041
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,885 INFO epoch # 1801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01900603386457078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,909 INFO epoch # 1802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019358107063453645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,933 INFO epoch # 1803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0186737817566609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,956 INFO epoch # 1804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018267369508976117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:33,980 INFO epoch # 1805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01873731819796376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,004 INFO epoch # 1806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019194966385839507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,029 INFO epoch # 1807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018757843790808693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,054 INFO epoch # 1808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018984169873874635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,079 INFO epoch # 1809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018947047705296427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,103 INFO epoch # 1810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01884881041769404
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:34,104 INFO *** epoch 1810, rolling-avg-loss (window=10)= 0.01887754485651385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,127 INFO epoch # 1811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018606789817567915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,151 INFO epoch # 1812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01910210627829656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,175 INFO epoch # 1813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01878819882404059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,199 INFO epoch # 1814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020059918490005657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,223 INFO epoch # 1815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018888503254856914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,248 INFO epoch # 1816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019212288549169898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,273 INFO epoch # 1817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018879800656577572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,297 INFO epoch # 1818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01950454988400452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,321 INFO epoch # 1819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019687622756464407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,345 INFO epoch # 1820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01955110824201256
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:34,345 INFO *** epoch 1820, rolling-avg-loss (window=10)= 0.01922808867529966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,369 INFO epoch # 1821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018780907499603927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,393 INFO epoch # 1822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018537759300670587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,417 INFO epoch # 1823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018715599639108405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,441 INFO epoch # 1824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019477342546451837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,466 INFO epoch # 1825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019705746235558763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,490 INFO epoch # 1826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018810333276633173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,515 INFO epoch # 1827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018777791876345873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,539 INFO epoch # 1828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018829335371265188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,564 INFO epoch # 1829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01898329413961619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,588 INFO epoch # 1830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01861600310076028
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:34,588 INFO *** epoch 1830, rolling-avg-loss (window=10)= 0.018923411298601422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,612 INFO epoch # 1831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018250087421620265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,637 INFO epoch # 1832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018472999217920005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,661 INFO epoch # 1833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018060305155813694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,685 INFO epoch # 1834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018001746720983647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,710 INFO epoch # 1835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018371224374277517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,735 INFO epoch # 1836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018774725089315325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,759 INFO epoch # 1837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01802569700521417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,782 INFO epoch # 1838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018365202966379002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,806 INFO epoch # 1839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019951300797401927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,830 INFO epoch # 1840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019516762084094808
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:34,830 INFO *** epoch 1840, rolling-avg-loss (window=10)= 0.018579005083302035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,854 INFO epoch # 1841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018721708125667647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,878 INFO epoch # 1842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01841742332908325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,902 INFO epoch # 1843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01874870093888603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,926 INFO epoch # 1844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018811675254255533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,951 INFO epoch # 1845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018474656535545364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,975 INFO epoch # 1846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01880167389754206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:34,999 INFO epoch # 1847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01846301927434979
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,024 INFO epoch # 1848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018683700007386506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,048 INFO epoch # 1849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01909790007630363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,073 INFO epoch # 1850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019145896774716675
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:35,073 INFO *** epoch 1850, rolling-avg-loss (window=10)= 0.01873663542137365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,097 INFO epoch # 1851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01877614841214381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,122 INFO epoch # 1852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01832312619080767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,147 INFO epoch # 1853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017993541943724267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,171 INFO epoch # 1854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018226687097921968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,196 INFO epoch # 1855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01851489077671431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,220 INFO epoch # 1856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017932660732185468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,243 INFO epoch # 1857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018314447486773133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,267 INFO epoch # 1858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018716833990765736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,292 INFO epoch # 1859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018450580348144285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,317 INFO epoch # 1860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018349041463807225
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:35,317 INFO *** epoch 1860, rolling-avg-loss (window=10)= 0.01835979584429879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,341 INFO epoch # 1861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018188144284067675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,366 INFO epoch # 1862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018395714840153232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,390 INFO epoch # 1863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018478441808838397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,414 INFO epoch # 1864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01907884524553083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,438 INFO epoch # 1865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01941470935707912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,462 INFO epoch # 1866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020203045045491308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,486 INFO epoch # 1867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018937401968287304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,509 INFO epoch # 1868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018985669739777222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,534 INFO epoch # 1869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01803306650253944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,559 INFO epoch # 1870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018420804146444425
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:35,559 INFO *** epoch 1870, rolling-avg-loss (window=10)= 0.018813584293820896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,584 INFO epoch # 1871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018710164091316983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,608 INFO epoch # 1872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01892761015915312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,633 INFO epoch # 1873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01838871969084721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,658 INFO epoch # 1874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018386124895187095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,682 INFO epoch # 1875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018111425189999864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,706 INFO epoch # 1876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018449527909979224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,730 INFO epoch # 1877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018802860242431052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,754 INFO epoch # 1878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018230042478535324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,778 INFO epoch # 1879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01749045480391942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,803 INFO epoch # 1880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017689829837763682
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:35,803 INFO *** epoch 1880, rolling-avg-loss (window=10)= 0.0183186759299133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,828 INFO epoch # 1881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017839844920672476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,852 INFO epoch # 1882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019153536879457533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,875 INFO epoch # 1883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01903857008437626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,899 INFO epoch # 1884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018433086370350793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,923 INFO epoch # 1885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01819134471588768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,947 INFO epoch # 1886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018448965740390122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,971 INFO epoch # 1887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019892271244316362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:35,995 INFO epoch # 1888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01850181055488065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,020 INFO epoch # 1889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018206666834885255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,045 INFO epoch # 1890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017981876502744853
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:36,045 INFO *** epoch 1890, rolling-avg-loss (window=10)= 0.0185687973847962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,069 INFO epoch # 1891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01820747641613707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,093 INFO epoch # 1892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017905852291733027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,118 INFO epoch # 1893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01801989387604408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,141 INFO epoch # 1894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017859781917650253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,165 INFO epoch # 1895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02010806923499331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,190 INFO epoch # 1896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019182232616003603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,214 INFO epoch # 1897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.020440627267817035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,239 INFO epoch # 1898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01837230898672715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,263 INFO epoch # 1899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018774790019961074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,288 INFO epoch # 1900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01794015589985065
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:36,288 INFO *** epoch 1900, rolling-avg-loss (window=10)= 0.018681118852691726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,311 INFO epoch # 1901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018100228073308244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,335 INFO epoch # 1902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018317074776859954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,359 INFO epoch # 1903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018361787108005956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,383 INFO epoch # 1904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018198379082605243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,408 INFO epoch # 1905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018415759841445833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,432 INFO epoch # 1906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01840051746694371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,457 INFO epoch # 1907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017958343698410317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,481 INFO epoch # 1908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017672955815214664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,506 INFO epoch # 1909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017781449714675546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,530 INFO epoch # 1910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01880120165878907
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:36,530 INFO *** epoch 1910, rolling-avg-loss (window=10)= 0.018200769723625852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,554 INFO epoch # 1911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018730251991655678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,578 INFO epoch # 1912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018451458803610876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,603 INFO epoch # 1913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019285627640783787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,628 INFO epoch # 1914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018644590250914916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,653 INFO epoch # 1915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01843177105183713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,677 INFO epoch # 1916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018862770753912628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,702 INFO epoch # 1917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01775383483618498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,726 INFO epoch # 1918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01835940236924216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,750 INFO epoch # 1919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01900139995268546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,774 INFO epoch # 1920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.02022696763742715
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:36,774 INFO *** epoch 1920, rolling-avg-loss (window=10)= 0.018774807528825477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,798 INFO epoch # 1921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018529211811255664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,822 INFO epoch # 1922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018125526752555743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,846 INFO epoch # 1923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017778315057512373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,871 INFO epoch # 1924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018896465131547302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,896 INFO epoch # 1925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01805545759270899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,920 INFO epoch # 1926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01818272698437795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,944 INFO epoch # 1927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018269749911269173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,968 INFO epoch # 1928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01801141328178346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:36,992 INFO epoch # 1929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017630962378461845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,016 INFO epoch # 1930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018115308310370892
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:37,016 INFO *** epoch 1930, rolling-avg-loss (window=10)= 0.01815951372118434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,040 INFO epoch # 1931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017738386726705357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,064 INFO epoch # 1932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017664908576989546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,089 INFO epoch # 1933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017450742263463326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,113 INFO epoch # 1934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01767483860021457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,138 INFO epoch # 1935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017881719541037455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,162 INFO epoch # 1936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017729327722918242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,186 INFO epoch # 1937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018100947025232017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,210 INFO epoch # 1938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017917122662765905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,233 INFO epoch # 1939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018666391872102395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,257 INFO epoch # 1940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017296548350714147
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:37,257 INFO *** epoch 1940, rolling-avg-loss (window=10)= 0.017812093334214296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,282 INFO epoch # 1941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019490654551191255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,306 INFO epoch # 1942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01847399113466963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,331 INFO epoch # 1943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018103829061146826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,355 INFO epoch # 1944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017776585154933855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,380 INFO epoch # 1945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017576190002728254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,404 INFO epoch # 1946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017750178376445547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,428 INFO epoch # 1947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01767808067961596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,452 INFO epoch # 1948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018457259109709412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,476 INFO epoch # 1949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017859793035313487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,501 INFO epoch # 1950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01826330620679073
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:37,501 INFO *** epoch 1950, rolling-avg-loss (window=10)= 0.018142986731254494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,525 INFO epoch # 1951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018228669185191393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,550 INFO epoch # 1952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01755578548181802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,574 INFO epoch # 1953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017844048677943647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,598 INFO epoch # 1954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018087309697875753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,622 INFO epoch # 1955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01788076211232692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,646 INFO epoch # 1956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017863809611299075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,670 INFO epoch # 1957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017586405330803245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,694 INFO epoch # 1958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01748092583147809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,718 INFO epoch # 1959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01799596266937442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,743 INFO epoch # 1960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018797518510837108
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:37,743 INFO *** epoch 1960, rolling-avg-loss (window=10)= 0.017932119710894767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,768 INFO epoch # 1961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.019043636391870677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,792 INFO epoch # 1962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01779868669109419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,816 INFO epoch # 1963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0175738572870614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,840 INFO epoch # 1964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017410599160939455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,864 INFO epoch # 1965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01757206801266875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,887 INFO epoch # 1966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017943465732969344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,911 INFO epoch # 1967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017762114526703954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,935 INFO epoch # 1968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01730259507894516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,960 INFO epoch # 1969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017588936985703185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:37,984 INFO epoch # 1970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01770859389216639
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:37,985 INFO *** epoch 1970, rolling-avg-loss (window=10)= 0.01777045537601225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,009 INFO epoch # 1971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017640901438426226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,033 INFO epoch # 1972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01749738688522484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,058 INFO epoch # 1973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01805015665013343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,082 INFO epoch # 1974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01765370913199149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,105 INFO epoch # 1975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017072437331080437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,129 INFO epoch # 1976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017064537154510617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,153 INFO epoch # 1977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017500855814432725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,177 INFO epoch # 1978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01774359174305573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,202 INFO epoch # 1979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01836120107327588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,226 INFO epoch # 1980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017105965845985338
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:38,226 INFO *** epoch 1980, rolling-avg-loss (window=10)= 0.01756907430681167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,251 INFO epoch # 1981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017564515612320974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,274 INFO epoch # 1982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01789683452807367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,298 INFO epoch # 1983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017412220360711217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,322 INFO epoch # 1984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017561911488883197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,346 INFO epoch # 1985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018157283950131387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,370 INFO epoch # 1986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018173653807025403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,394 INFO epoch # 1987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017464447941165417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,419 INFO epoch # 1988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017628157511353493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,444 INFO epoch # 1989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01715364563278854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,468 INFO epoch # 1990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017696377239190042
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:38,468 INFO *** epoch 1990, rolling-avg-loss (window=10)= 0.017670904807164333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,492 INFO epoch # 1991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01736660857568495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,516 INFO epoch # 1992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01757622142031323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,540 INFO epoch # 1993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017756645363988355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,564 INFO epoch # 1994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0179470791190397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,589 INFO epoch # 1995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017535521648824215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,614 INFO epoch # 1996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017319559890893288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,639 INFO epoch # 1997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017370300163747743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,664 INFO epoch # 1998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016946458068559878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,689 INFO epoch # 1999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017399769189069048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,713 INFO epoch # 2000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017423242010409012
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:38,713 INFO *** epoch 2000, rolling-avg-loss (window=10)= 0.017464140545052943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,737 INFO epoch # 2001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017352038441458717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,761 INFO epoch # 2002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01747331232763827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,785 INFO epoch # 2003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016973983685602434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,809 INFO epoch # 2004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01745900118839927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,834 INFO epoch # 2005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01732692816585768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,858 INFO epoch # 2006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017282641943893395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,882 INFO epoch # 2007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017563540939590894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,906 INFO epoch # 2008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017240088782273233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,930 INFO epoch # 2009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016959117259830236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,954 INFO epoch # 2010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017490910555352457
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:38,954 INFO *** epoch 2010, rolling-avg-loss (window=10)= 0.017312156328989657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:38,978 INFO epoch # 2011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018027076293947175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,002 INFO epoch # 2012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01821559394011274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,026 INFO epoch # 2013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017515501036541536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,051 INFO epoch # 2014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017230764642590657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,075 INFO epoch # 2015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017255097278393805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,100 INFO epoch # 2016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01765553731820546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,124 INFO epoch # 2017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017426277947379276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,148 INFO epoch # 2018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017519243352580816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,172 INFO epoch # 2019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017340829261229374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,196 INFO epoch # 2020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017330394475720823
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:39,196 INFO *** epoch 2020, rolling-avg-loss (window=10)= 0.017551631554670166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,220 INFO epoch # 2021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018112876132363454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,245 INFO epoch # 2022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017389902903232723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,269 INFO epoch # 2023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01725381615688093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,294 INFO epoch # 2024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018498860066756606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,319 INFO epoch # 2025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01778550649760291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,343 INFO epoch # 2026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017183461430249736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,367 INFO epoch # 2027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016844393278006464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,391 INFO epoch # 2028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01734962666523643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,414 INFO epoch # 2029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017463668627897277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,438 INFO epoch # 2030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017490657599410042
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:39,439 INFO *** epoch 2030, rolling-avg-loss (window=10)= 0.01753727693576366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,463 INFO epoch # 2031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01721328016719781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,488 INFO epoch # 2032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017226900294190273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,512 INFO epoch # 2033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017872969940071926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,536 INFO epoch # 2034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017530482247821055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,561 INFO epoch # 2035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01716554422455374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,585 INFO epoch # 2036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01714018324855715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,609 INFO epoch # 2037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01792540733003989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,633 INFO epoch # 2038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01802388360374607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,657 INFO epoch # 2039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016924172450671904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,682 INFO epoch # 2040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01661588680872228
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:39,682 INFO *** epoch 2040, rolling-avg-loss (window=10)= 0.01736387103155721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,707 INFO epoch # 2041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017211152066010982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,731 INFO epoch # 2042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0171443990257103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,756 INFO epoch # 2043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017309673683485016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,780 INFO epoch # 2044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01728630400612019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,804 INFO epoch # 2045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017840579675976187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,827 INFO epoch # 2046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01712989143561572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,851 INFO epoch # 2047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018093039718223736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,875 INFO epoch # 2048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017110584216425195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,899 INFO epoch # 2049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017450213781557977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,924 INFO epoch # 2050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018080442692735232
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:39,924 INFO *** epoch 2050, rolling-avg-loss (window=10)= 0.017465628030186052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,949 INFO epoch # 2051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016847901075379923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,974 INFO epoch # 2052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017504812945844606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:39,998 INFO epoch # 2053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017149284161860123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,023 INFO epoch # 2054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017677122668828815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,047 INFO epoch # 2055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01696181364241056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,071 INFO epoch # 2056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01730198365112301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,095 INFO epoch # 2057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016900838527362794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,120 INFO epoch # 2058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016620775684714317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,144 INFO epoch # 2059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016521931625902653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,169 INFO epoch # 2060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016775837866589427
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:40,169 INFO *** epoch 2060, rolling-avg-loss (window=10)= 0.01702623018500162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,193 INFO epoch # 2061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01682093835552223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,217 INFO epoch # 2062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016671673103701323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,241 INFO epoch # 2063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017115575974457897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,265 INFO epoch # 2064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016880792303709313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,289 INFO epoch # 2065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016815438953926787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,313 INFO epoch # 2066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017122021221439354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,337 INFO epoch # 2067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01718528039054945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,362 INFO epoch # 2068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01718718814663589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,387 INFO epoch # 2069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01776485951268114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,411 INFO epoch # 2070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016913767292862758
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:40,411 INFO *** epoch 2070, rolling-avg-loss (window=10)= 0.017047753525548614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,436 INFO epoch # 2071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016822570585645735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,460 INFO epoch # 2072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01680036514881067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,484 INFO epoch # 2073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016549188148928806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,507 INFO epoch # 2074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017028528702212498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,531 INFO epoch # 2075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017528021373436786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,557 INFO epoch # 2076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016781173675553873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,581 INFO epoch # 2077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01706168935925234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,606 INFO epoch # 2078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01768999577325303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,631 INFO epoch # 2079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017845891270553693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,656 INFO epoch # 2080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0174042247235775
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:40,656 INFO *** epoch 2080, rolling-avg-loss (window=10)= 0.017151164876122494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,681 INFO epoch # 2081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016928175144130364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,705 INFO epoch # 2082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016951632278505713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,729 INFO epoch # 2083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016927424556342885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,754 INFO epoch # 2084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016821298690047115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,779 INFO epoch # 2085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01750788016943261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,803 INFO epoch # 2086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0169735720846802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,828 INFO epoch # 2087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017089015484089032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,852 INFO epoch # 2088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016257476643659174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,876 INFO epoch # 2089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016182518520508893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,900 INFO epoch # 2090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01627286005532369
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:40,900 INFO *** epoch 2090, rolling-avg-loss (window=10)= 0.01679118536267197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,924 INFO epoch # 2091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016836922106449492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,948 INFO epoch # 2092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017140152340289205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,972 INFO epoch # 2093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01789504598127678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:40,996 INFO epoch # 2094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017575183475855738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,021 INFO epoch # 2095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017223143149749376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,046 INFO epoch # 2096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01858321501640603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,070 INFO epoch # 2097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017759614362148568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,094 INFO epoch # 2098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017226192489033565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,118 INFO epoch # 2099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01714133413042873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,142 INFO epoch # 2100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01678038912359625
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:41,142 INFO *** epoch 2100, rolling-avg-loss (window=10)= 0.017416119217523375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,166 INFO epoch # 2101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01763382469653152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,190 INFO epoch # 2102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01781756784475874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,214 INFO epoch # 2103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01617186785733793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,239 INFO epoch # 2104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016915618762141094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,264 INFO epoch # 2105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01654434809461236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,288 INFO epoch # 2106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016374552753404714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,312 INFO epoch # 2107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016897775334655307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,336 INFO epoch # 2108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01732000583433546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,360 INFO epoch # 2109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0163695385272149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,384 INFO epoch # 2110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01656770912813954
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:41,384 INFO *** epoch 2110, rolling-avg-loss (window=10)= 0.016861280883313158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,408 INFO epoch # 2111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01733102006255649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,432 INFO epoch # 2112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016445235058199614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,456 INFO epoch # 2113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016866677353391424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,481 INFO epoch # 2114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017964388040127233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,505 INFO epoch # 2115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01665366906672716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,530 INFO epoch # 2116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017134590831119567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,555 INFO epoch # 2117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016611850878689438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,579 INFO epoch # 2118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016069790406618267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,603 INFO epoch # 2119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01690600876463577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,627 INFO epoch # 2120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016851793130626902
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:41,628 INFO *** epoch 2120, rolling-avg-loss (window=10)= 0.016883502359269185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,652 INFO epoch # 2121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01631161708792206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,677 INFO epoch # 2122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01631932928285096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,702 INFO epoch # 2123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016376114159356803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,726 INFO epoch # 2124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017145136429462582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,750 INFO epoch # 2125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01712470076745376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,775 INFO epoch # 2126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016996709309751168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,798 INFO epoch # 2127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01696619752328843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,822 INFO epoch # 2128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016204802377615124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,846 INFO epoch # 2129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016318706620950252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,870 INFO epoch # 2130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016857443915796466
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:41,871 INFO *** epoch 2130, rolling-avg-loss (window=10)= 0.01666207574744476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,895 INFO epoch # 2131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01665265968767926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,919 INFO epoch # 2132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016765265056164935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,944 INFO epoch # 2133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016448824244434945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,969 INFO epoch # 2134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016508759363205172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:41,993 INFO epoch # 2135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016135846526594833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,017 INFO epoch # 2136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016611573300906457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,041 INFO epoch # 2137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01630925145582296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,066 INFO epoch # 2138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016415456906543113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,090 INFO epoch # 2139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01748141788993962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,115 INFO epoch # 2140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01785367343109101
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:42,115 INFO *** epoch 2140, rolling-avg-loss (window=10)= 0.01671827278623823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,140 INFO epoch # 2141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01765261296532117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,164 INFO epoch # 2142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016827648651087657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,188 INFO epoch # 2143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01629070405033417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,212 INFO epoch # 2144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016575499466853216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,236 INFO epoch # 2145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018119698273949325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,260 INFO epoch # 2146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016262301709502935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,284 INFO epoch # 2147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016592464948189445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,308 INFO epoch # 2148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015880857812589966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,333 INFO epoch # 2149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016846051847096533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,358 INFO epoch # 2150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0167150761699304
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:42,358 INFO *** epoch 2150, rolling-avg-loss (window=10)= 0.01677629158948548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,383 INFO epoch # 2151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016928967263083905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,407 INFO epoch # 2152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016499608755111694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,431 INFO epoch # 2153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016189863381441683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,455 INFO epoch # 2154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016377369582187384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,478 INFO epoch # 2155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01653241296298802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,503 INFO epoch # 2156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016193648742046207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,527 INFO epoch # 2157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015856077588978224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,552 INFO epoch # 2158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01611895120004192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,577 INFO epoch # 2159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01663187431404367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,602 INFO epoch # 2160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016578223236138
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:42,602 INFO *** epoch 2160, rolling-avg-loss (window=10)= 0.016390699702606072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,626 INFO epoch # 2161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016348381963325664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,650 INFO epoch # 2162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017995708491071127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,674 INFO epoch # 2163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01653961777628865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,698 INFO epoch # 2164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017366210580803454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,723 INFO epoch # 2165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0167780599440448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,748 INFO epoch # 2166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016214041330385953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,773 INFO epoch # 2167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016833713452797383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,797 INFO epoch # 2168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016691528377123177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,821 INFO epoch # 2169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016772059781942517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,846 INFO epoch # 2170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017675112147117034
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:42,846 INFO *** epoch 2170, rolling-avg-loss (window=10)= 0.016921443384489976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,870 INFO epoch # 2171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016563891811529174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,893 INFO epoch # 2172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01716491286060773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,917 INFO epoch # 2173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016553647263208404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,941 INFO epoch # 2174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01644906005822122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,965 INFO epoch # 2175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016040533562772907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:42,990 INFO epoch # 2176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016341664653737098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,015 INFO epoch # 2177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01664119481574744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,039 INFO epoch # 2178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016072608093963936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,064 INFO epoch # 2179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01586026440782007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,087 INFO epoch # 2180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016348454140825197
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:43,088 INFO *** epoch 2180, rolling-avg-loss (window=10)= 0.016403623166843316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,111 INFO epoch # 2181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015956247254507616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,135 INFO epoch # 2182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016419885694631375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,159 INFO epoch # 2183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01640991200110875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,183 INFO epoch # 2184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015603943291353062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,208 INFO epoch # 2185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015632776376151014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,232 INFO epoch # 2186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016001750889699906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,257 INFO epoch # 2187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01619215012760833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,281 INFO epoch # 2188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01645058965368662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,305 INFO epoch # 2189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016005485798814334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,329 INFO epoch # 2190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015938761411234736
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:43,329 INFO *** epoch 2190, rolling-avg-loss (window=10)= 0.016061150249879575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,353 INFO epoch # 2191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016101619403343648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,377 INFO epoch # 2192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015974671550793573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,401 INFO epoch # 2193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016118316707434133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,426 INFO epoch # 2194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016162697051186115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,450 INFO epoch # 2195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01591827202355489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,475 INFO epoch # 2196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015859601087868214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,499 INFO epoch # 2197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015790472156368196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,522 INFO epoch # 2198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016587872232776135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,546 INFO epoch # 2199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016392549849115312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,570 INFO epoch # 2200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016439552360679954
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:43,571 INFO *** epoch 2200, rolling-avg-loss (window=10)= 0.01613456244231202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,595 INFO epoch # 2201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016167788533493876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,620 INFO epoch # 2202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015675400849431753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,644 INFO epoch # 2203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016512224916368723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,669 INFO epoch # 2204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01630383776500821
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,694 INFO epoch # 2205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01637612091144547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,718 INFO epoch # 2206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016055618441896513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,742 INFO epoch # 2207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016887610021512955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,765 INFO epoch # 2208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016540263110073283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,789 INFO epoch # 2209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01613880075456109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,814 INFO epoch # 2210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01633986564411316
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:43,814 INFO *** epoch 2210, rolling-avg-loss (window=10)= 0.016299753094790502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,838 INFO epoch # 2211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015607639128575101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,863 INFO epoch # 2212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016253938956651837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,888 INFO epoch # 2213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016221999801928177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,912 INFO epoch # 2214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016427615366410464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,937 INFO epoch # 2215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01586479810066521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,961 INFO epoch # 2216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01667292442289181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:43,985 INFO epoch # 2217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01667552237631753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,009 INFO epoch # 2218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016224301056354307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,033 INFO epoch # 2219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01582627605239395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,059 INFO epoch # 2220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015871230367338285
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:44,059 INFO *** epoch 2220, rolling-avg-loss (window=10)= 0.016164624562952666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,085 INFO epoch # 2221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016511986264958978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,110 INFO epoch # 2222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01632765341491904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,135 INFO epoch # 2223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016562416160013527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,159 INFO epoch # 2224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01623279355408158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,184 INFO epoch # 2225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0158955518854782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,209 INFO epoch # 2226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015545826114248484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,233 INFO epoch # 2227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015298399535822682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,257 INFO epoch # 2228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015604035128490068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,282 INFO epoch # 2229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015754680760437623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,307 INFO epoch # 2230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016197657489101402
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:44,307 INFO *** epoch 2230, rolling-avg-loss (window=10)= 0.01599310003075516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,332 INFO epoch # 2231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01566029613604769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,356 INFO epoch # 2232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016029373786295764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,381 INFO epoch # 2233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015528295451076701
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,405 INFO epoch # 2234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015893445641268045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,429 INFO epoch # 2235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015583781510940753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,453 INFO epoch # 2236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016240470722550526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,477 INFO epoch # 2237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016121725318953395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,502 INFO epoch # 2238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015969157655490562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,527 INFO epoch # 2239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016010142921004444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,553 INFO epoch # 2240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015585406188620254
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:44,553 INFO *** epoch 2240, rolling-avg-loss (window=10)= 0.015862209533224814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,577 INFO epoch # 2241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016102264053188264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,602 INFO epoch # 2242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015441787632880732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,626 INFO epoch # 2243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016186899330932647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,650 INFO epoch # 2244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01555339276092127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,674 INFO epoch # 2245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015800217821379192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,698 INFO epoch # 2246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016037384237279184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,723 INFO epoch # 2247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01680884786765091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,748 INFO epoch # 2248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015876271500019357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,772 INFO epoch # 2249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017510803852928802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,797 INFO epoch # 2250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016662488196743652
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:44,797 INFO *** epoch 2250, rolling-avg-loss (window=10)= 0.016198035725392402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,821 INFO epoch # 2251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015562474945909344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,845 INFO epoch # 2252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015842092398088425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,869 INFO epoch # 2253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01589668428641744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,893 INFO epoch # 2254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016731759998947382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,917 INFO epoch # 2255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015976151029462926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,944 INFO epoch # 2256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015306754052289762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,971 INFO epoch # 2257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01573147578164935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:44,996 INFO epoch # 2258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01556748544680886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,020 INFO epoch # 2259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01596395626256708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,045 INFO epoch # 2260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015819343039765954
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:45,045 INFO *** epoch 2260, rolling-avg-loss (window=10)= 0.01583981772419065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,069 INFO epoch # 2261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015552044889773242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,093 INFO epoch # 2262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015477678898605518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,117 INFO epoch # 2263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014977062252000906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,141 INFO epoch # 2264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015172522922512144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,165 INFO epoch # 2265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015240010528941639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,190 INFO epoch # 2266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015815205304534175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,214 INFO epoch # 2267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01579569317982532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,238 INFO epoch # 2268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01564266619971022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,262 INFO epoch # 2269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017563777277246118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,287 INFO epoch # 2270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.017049686808604747
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:45,287 INFO *** epoch 2270, rolling-avg-loss (window=10)= 0.015828634826175404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,311 INFO epoch # 2271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016036649845773354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,335 INFO epoch # 2272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01623934446251951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,359 INFO epoch # 2273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016673091624397784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,384 INFO epoch # 2274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01581928922678344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,409 INFO epoch # 2275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01602349869790487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,433 INFO epoch # 2276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015564041925244965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,458 INFO epoch # 2277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015135382273001596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,482 INFO epoch # 2278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015206749812932685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,506 INFO epoch # 2279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015633317088941112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,531 INFO epoch # 2280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01616796207963489
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:45,531 INFO *** epoch 2280, rolling-avg-loss (window=10)= 0.01584993270371342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,559 INFO epoch # 2281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015453853397048078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,600 INFO epoch # 2282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015649372100597247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,643 INFO epoch # 2283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015827069233637303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,671 INFO epoch # 2284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015994023211533204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,696 INFO epoch # 2285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01568571814277675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,720 INFO epoch # 2286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01564578757097479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,745 INFO epoch # 2287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01571020661504008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,769 INFO epoch # 2288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015555166959529743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,794 INFO epoch # 2289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01548974409524817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,819 INFO epoch # 2290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015829191775992513
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:45,819 INFO *** epoch 2290, rolling-avg-loss (window=10)= 0.015684013310237787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,844 INFO epoch # 2291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015884381282376125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,869 INFO epoch # 2292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015732035652035847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,893 INFO epoch # 2293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015588930458761752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,918 INFO epoch # 2294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016326815748470835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,943 INFO epoch # 2295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01613647391786799
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,967 INFO epoch # 2296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015897699107881635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:45,992 INFO epoch # 2297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015436272442457266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,016 INFO epoch # 2298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01568406324076932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,041 INFO epoch # 2299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01587186148390174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,065 INFO epoch # 2300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01523297480889596
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:46,066 INFO *** epoch 2300, rolling-avg-loss (window=10)= 0.015779150814341846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,090 INFO epoch # 2301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015006722111138515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,115 INFO epoch # 2302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016015646047890186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,139 INFO epoch # 2303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015335488074924797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,164 INFO epoch # 2304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014833913402981125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,188 INFO epoch # 2305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01498206498217769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,213 INFO epoch # 2306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01595559692941606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,237 INFO epoch # 2307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01579158820095472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,261 INFO epoch # 2308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015316993405576795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,286 INFO epoch # 2309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01538009308569599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,310 INFO epoch # 2310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015230312725179829
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:46,310 INFO *** epoch 2310, rolling-avg-loss (window=10)= 0.01538484189659357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,335 INFO epoch # 2311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015211858684779145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,359 INFO epoch # 2312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01594645282602869
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,384 INFO epoch # 2313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016275191810564138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,408 INFO epoch # 2314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015693792141973972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,433 INFO epoch # 2315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015547367511317134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,457 INFO epoch # 2316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015084330181707628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,482 INFO epoch # 2317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01488447182055097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,506 INFO epoch # 2318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015647566426196136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,530 INFO epoch # 2319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015670997949200682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,555 INFO epoch # 2320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01507168223906774
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:46,555 INFO *** epoch 2320, rolling-avg-loss (window=10)= 0.015503371159138624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,579 INFO epoch # 2321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01540571721852757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,603 INFO epoch # 2322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015806656912900507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,630 INFO epoch # 2323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01579139308887534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,654 INFO epoch # 2324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015425857447553426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,679 INFO epoch # 2325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01587973252753727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,704 INFO epoch # 2326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.018249530607135966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,728 INFO epoch # 2327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015718204667791724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,753 INFO epoch # 2328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015113369838218205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,778 INFO epoch # 2329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01550803157442715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,802 INFO epoch # 2330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015259475621860474
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:46,802 INFO *** epoch 2330, rolling-avg-loss (window=10)= 0.015815796950482762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,827 INFO epoch # 2331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015113583081983961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,851 INFO epoch # 2332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01603981899097562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,875 INFO epoch # 2333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014927436510333791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,899 INFO epoch # 2334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015251177799655125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,924 INFO epoch # 2335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015414216628414579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,948 INFO epoch # 2336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014936074599972926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,973 INFO epoch # 2337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015328629800933413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:46,997 INFO epoch # 2338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015576274716295302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,022 INFO epoch # 2339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015100740507477894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,046 INFO epoch # 2340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015271572512574494
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:47,047 INFO *** epoch 2340, rolling-avg-loss (window=10)= 0.01529595251486171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,071 INFO epoch # 2341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014858129303320311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,096 INFO epoch # 2342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01543597578711342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,120 INFO epoch # 2343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016969487667665817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,145 INFO epoch # 2344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015840543841477484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,169 INFO epoch # 2345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014863955759210512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,194 INFO epoch # 2346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015293837583158165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,218 INFO epoch # 2347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015919897297862917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,242 INFO epoch # 2348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015229467753670178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,267 INFO epoch # 2349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014749243811820634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,290 INFO epoch # 2350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015197491040453315
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:47,290 INFO *** epoch 2350, rolling-avg-loss (window=10)= 0.015435802984575276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,314 INFO epoch # 2351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015236551262205467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,338 INFO epoch # 2352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015555779042188078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,362 INFO epoch # 2353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01505847137013916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,387 INFO epoch # 2354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015926364590995945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,411 INFO epoch # 2355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016020874463720247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,436 INFO epoch # 2356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015458638838026673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,460 INFO epoch # 2357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015498715714784339
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,484 INFO epoch # 2358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015126429760130122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,508 INFO epoch # 2359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014358615662786178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,532 INFO epoch # 2360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015009446840849705
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:47,533 INFO *** epoch 2360, rolling-avg-loss (window=10)= 0.015324988754582592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,557 INFO epoch # 2361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014372100107721053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,581 INFO epoch # 2362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015081926307175308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,606 INFO epoch # 2363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015904031257377937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,631 INFO epoch # 2364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01488603776670061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,656 INFO epoch # 2365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015382664540084079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,681 INFO epoch # 2366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014979924308136106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,705 INFO epoch # 2367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014843624405330047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,730 INFO epoch # 2368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01507600002514664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,754 INFO epoch # 2369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014623529830714688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,778 INFO epoch # 2370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014793845562962815
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:47,778 INFO *** epoch 2370, rolling-avg-loss (window=10)= 0.01499436841113493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,802 INFO epoch # 2371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015776762520545162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,827 INFO epoch # 2372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0162620003829943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,851 INFO epoch # 2373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014480191341135651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,876 INFO epoch # 2374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015125913108931854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,900 INFO epoch # 2375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015329435496823862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,925 INFO epoch # 2376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014728192269103602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,949 INFO epoch # 2377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015190130143309943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,973 INFO epoch # 2378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014532469431287609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:47,996 INFO epoch # 2379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015078665557666682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,020 INFO epoch # 2380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01494559801358264
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:48,021 INFO *** epoch 2380, rolling-avg-loss (window=10)= 0.015144935826538131
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,045 INFO epoch # 2381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014938786087441258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,070 INFO epoch # 2382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01538761805568356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,094 INFO epoch # 2383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01544225357065443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,118 INFO epoch # 2384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014781412246520631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,143 INFO epoch # 2385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0153007233311655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,167 INFO epoch # 2386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015202521200990304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,191 INFO epoch # 2387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015674055350245908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,215 INFO epoch # 2388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015107147075468674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,239 INFO epoch # 2389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015426867583300918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,263 INFO epoch # 2390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014993618839071132
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:48,263 INFO *** epoch 2390, rolling-avg-loss (window=10)= 0.015225500334054232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,288 INFO epoch # 2391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014930330886272714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,312 INFO epoch # 2392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01601529764593579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,338 INFO epoch # 2393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015998266520909965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,363 INFO epoch # 2394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015072300535393879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,388 INFO epoch # 2395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016058470879215747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,412 INFO epoch # 2396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015994320245226845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,437 INFO epoch # 2397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015673051617341116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,462 INFO epoch # 2398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015498897279030643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,487 INFO epoch # 2399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015473467981792055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,512 INFO epoch # 2400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014590391074307263
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:48,512 INFO *** epoch 2400, rolling-avg-loss (window=10)= 0.015530479466542601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,537 INFO epoch # 2401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01440024511248339
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,561 INFO epoch # 2402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014323435549158603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,586 INFO epoch # 2403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014512390553136356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,610 INFO epoch # 2404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014814984067925252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,635 INFO epoch # 2405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01579588271852117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,660 INFO epoch # 2406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014991283300332725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,685 INFO epoch # 2407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014688585739349946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,711 INFO epoch # 2408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015084433078300208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,737 INFO epoch # 2409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014872477535391226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,762 INFO epoch # 2410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014581411625840701
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:48,762 INFO *** epoch 2410, rolling-avg-loss (window=10)= 0.014806512928043958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,787 INFO epoch # 2411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015352998627349734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,812 INFO epoch # 2412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01475282947649248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,836 INFO epoch # 2413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01508811600797344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,862 INFO epoch # 2414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01449150952976197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,888 INFO epoch # 2415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014761419923161156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,914 INFO epoch # 2416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014320161717478186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,938 INFO epoch # 2417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014522577344905585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,963 INFO epoch # 2418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016764245650847442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:48,988 INFO epoch # 2419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0167356147285318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,013 INFO epoch # 2420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016064584808191285
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:49,013 INFO *** epoch 2420, rolling-avg-loss (window=10)= 0.015285405781469308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,038 INFO epoch # 2421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01526813762029633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,063 INFO epoch # 2422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01474304246949032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,088 INFO epoch # 2423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015115825008251704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,113 INFO epoch # 2424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01491022203117609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,138 INFO epoch # 2425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01466637325938791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,163 INFO epoch # 2426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0154260246490594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,188 INFO epoch # 2427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015420425988850184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,213 INFO epoch # 2428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014464400010183454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,238 INFO epoch # 2429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014817592149483971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,264 INFO epoch # 2430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014628004602855071
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:49,264 INFO *** epoch 2430, rolling-avg-loss (window=10)= 0.014946004778903444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,290 INFO epoch # 2431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01440838523558341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,315 INFO epoch # 2432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014535214868374169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,340 INFO epoch # 2433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014551026077242568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,365 INFO epoch # 2434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014575961831724271
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,391 INFO epoch # 2435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014524428537697531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,416 INFO epoch # 2436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01440850978542585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,441 INFO epoch # 2437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014623827388277277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,465 INFO epoch # 2438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014455579163040966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,490 INFO epoch # 2439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014704224377055652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,514 INFO epoch # 2440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014673787096398883
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:49,515 INFO *** epoch 2440, rolling-avg-loss (window=10)= 0.014546094436082058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,542 INFO epoch # 2441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014613820181693882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,571 INFO epoch # 2442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013953688845504075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,599 INFO epoch # 2443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014550647101714276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,628 INFO epoch # 2444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014915229839971289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,656 INFO epoch # 2445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015243244866724126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,685 INFO epoch # 2446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015062266233144328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,713 INFO epoch # 2447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01452082407195121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,741 INFO epoch # 2448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015085378516232595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,770 INFO epoch # 2449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014894723004545085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,799 INFO epoch # 2450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0141313515487127
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:49,799 INFO *** epoch 2450, rolling-avg-loss (window=10)= 0.014697117421019357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,827 INFO epoch # 2451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014741467297426425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,856 INFO epoch # 2452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014453949508606456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,884 INFO epoch # 2453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014680783409858122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,912 INFO epoch # 2454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014918543165549636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,941 INFO epoch # 2455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01477421514573507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,970 INFO epoch # 2456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01499185875582043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:49,998 INFO epoch # 2457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014697059377795085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,027 INFO epoch # 2458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014683636967674829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,055 INFO epoch # 2459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014464591702562757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,084 INFO epoch # 2460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014128616254311055
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:50,084 INFO *** epoch 2460, rolling-avg-loss (window=10)= 0.014653472158533987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,113 INFO epoch # 2461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014417670405237004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,142 INFO epoch # 2462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015072677109856158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,170 INFO epoch # 2463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015112270601093769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,199 INFO epoch # 2464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014525624661473557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,228 INFO epoch # 2465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014318539149826393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,256 INFO epoch # 2466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01540924004802946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,281 INFO epoch # 2467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01536205896991305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,305 INFO epoch # 2468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01473256955796387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,329 INFO epoch # 2469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01402412548486609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,353 INFO epoch # 2470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014582006609998643
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:50,354 INFO *** epoch 2470, rolling-avg-loss (window=10)= 0.0147556782598258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,379 INFO epoch # 2471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014124382185400464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,403 INFO epoch # 2472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014293779124272987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,428 INFO epoch # 2473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01428072624548804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,452 INFO epoch # 2474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014599740912672132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,477 INFO epoch # 2475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013927856081863865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,501 INFO epoch # 2476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014771250454941764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,526 INFO epoch # 2477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014531985521898605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,550 INFO epoch # 2478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014600564769352786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,575 INFO epoch # 2479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014277517213486135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,600 INFO epoch # 2480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01436766363622155
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:50,600 INFO *** epoch 2480, rolling-avg-loss (window=10)= 0.014377546614559833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,625 INFO epoch # 2481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014268645303673111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,651 INFO epoch # 2482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01470137458818499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,676 INFO epoch # 2483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014700197541969828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,701 INFO epoch # 2484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014356610976392403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,726 INFO epoch # 2485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014544060672051273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,750 INFO epoch # 2486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01453303438029252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,774 INFO epoch # 2487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01404363256006036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,798 INFO epoch # 2488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014520902986987494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,823 INFO epoch # 2489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014576062429114245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,848 INFO epoch # 2490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014233026246074587
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:50,848 INFO *** epoch 2490, rolling-avg-loss (window=10)= 0.014447754768480082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,873 INFO epoch # 2491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014592795865610242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,897 INFO epoch # 2492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014457340599619783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,922 INFO epoch # 2493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015170089303865097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,946 INFO epoch # 2494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01460913330083713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,971 INFO epoch # 2495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013787595657049678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:50,996 INFO epoch # 2496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014048008772078902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,020 INFO epoch # 2497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014771780493902043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,045 INFO epoch # 2498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014501942132483236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,070 INFO epoch # 2499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016214235001825728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,096 INFO epoch # 2500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015565489957225509
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:51,096 INFO *** epoch 2500, rolling-avg-loss (window=10)= 0.014771841108449734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,120 INFO epoch # 2501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014408543007448316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,145 INFO epoch # 2502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014230846194550395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,169 INFO epoch # 2503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013904901556088589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,194 INFO epoch # 2504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014073589292820543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,219 INFO epoch # 2505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014616885193390772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,244 INFO epoch # 2506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015316180535592139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,269 INFO epoch # 2507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01381261192727834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,294 INFO epoch # 2508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014440469472901896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,319 INFO epoch # 2509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014765185711439699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,343 INFO epoch # 2510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014275920344516635
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:51,343 INFO *** epoch 2510, rolling-avg-loss (window=10)= 0.014384513323602733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,368 INFO epoch # 2511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014032199629582465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,392 INFO epoch # 2512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01452546713699121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,417 INFO epoch # 2513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01503459780360572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,441 INFO epoch # 2514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014489946537651122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,466 INFO epoch # 2515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.016539443575311452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,491 INFO epoch # 2516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015101159864570946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,515 INFO epoch # 2517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01391232994501479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,539 INFO epoch # 2518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014734829543158412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,563 INFO epoch # 2519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01456822121690493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,588 INFO epoch # 2520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014560673909727484
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:51,588 INFO *** epoch 2520, rolling-avg-loss (window=10)= 0.014749886916251852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,612 INFO epoch # 2521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015006859044660814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,636 INFO epoch # 2522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014796441537328064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,661 INFO epoch # 2523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015152979147387668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,686 INFO epoch # 2524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01454985051532276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,710 INFO epoch # 2525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01479011558694765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,735 INFO epoch # 2526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014406421250896528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,759 INFO epoch # 2527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013900084217311814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,783 INFO epoch # 2528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014204677994712256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,807 INFO epoch # 2529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01466150366468355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,831 INFO epoch # 2530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014360120680066757
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:51,831 INFO *** epoch 2530, rolling-avg-loss (window=10)= 0.014582905363931786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,855 INFO epoch # 2531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014716768608195707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,879 INFO epoch # 2532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0141110214026412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,903 INFO epoch # 2533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014350953031680547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,928 INFO epoch # 2534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013996347188367508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,953 INFO epoch # 2535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013940981894847937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:51,977 INFO epoch # 2536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014088575728237629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,001 INFO epoch # 2537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014281092575401999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,026 INFO epoch # 2538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014557065645931289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,050 INFO epoch # 2539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014360476503497921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,074 INFO epoch # 2540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014081176283070818
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:52,074 INFO *** epoch 2540, rolling-avg-loss (window=10)= 0.014248445886187256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,099 INFO epoch # 2541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013905148967751302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,124 INFO epoch # 2542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014632011705543846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,148 INFO epoch # 2543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01445050144684501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,173 INFO epoch # 2544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014346801384817809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,197 INFO epoch # 2545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013841667183442041
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,221 INFO epoch # 2546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014441641498706304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,245 INFO epoch # 2547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014097854495048523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,269 INFO epoch # 2548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014198889213730581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,293 INFO epoch # 2549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013935134949861094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,318 INFO epoch # 2550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013974134169984609
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:52,318 INFO *** epoch 2550, rolling-avg-loss (window=10)= 0.014182378501573112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,343 INFO epoch # 2551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013641627971082926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,367 INFO epoch # 2552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013966229773359373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,392 INFO epoch # 2553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01392694677633699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,416 INFO epoch # 2554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013783859831164591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,441 INFO epoch # 2555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014318192203063518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,465 INFO epoch # 2556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014635911604273133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,489 INFO epoch # 2557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01580565978656523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,513 INFO epoch # 2558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014307741730590351
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,538 INFO epoch # 2559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0142938681965461
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,563 INFO epoch # 2560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014463845727732405
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:52,563 INFO *** epoch 2560, rolling-avg-loss (window=10)= 0.014314388360071461
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,588 INFO epoch # 2561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013980142059153877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,612 INFO epoch # 2562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01414924586424604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,637 INFO epoch # 2563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01414444156398531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,661 INFO epoch # 2564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013966138430987485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,686 INFO epoch # 2565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013584357293439098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,710 INFO epoch # 2566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013951352899312042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,734 INFO epoch # 2567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014321695984108374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,759 INFO epoch # 2568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014006461919052526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,784 INFO epoch # 2569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01382829254725948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,809 INFO epoch # 2570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013754319021245465
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:52,809 INFO *** epoch 2570, rolling-avg-loss (window=10)= 0.01396864475827897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,833 INFO epoch # 2571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013571615083492361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,858 INFO epoch # 2572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0137073472578777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,882 INFO epoch # 2573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013887735665775836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,906 INFO epoch # 2574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014193408555001952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,930 INFO epoch # 2575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015176303830230609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,955 INFO epoch # 2576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014093928009970114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:52,979 INFO epoch # 2577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013938801028416492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,004 INFO epoch # 2578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014728617927175947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,029 INFO epoch # 2579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01412002272263635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,053 INFO epoch # 2580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01397274604823906
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:53,053 INFO *** epoch 2580, rolling-avg-loss (window=10)= 0.014139052612881642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,078 INFO epoch # 2581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01427792408503592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,102 INFO epoch # 2582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01390348668792285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,127 INFO epoch # 2583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013668436135048978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,151 INFO epoch # 2584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014207309533958323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,175 INFO epoch # 2585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014116206541075371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,200 INFO epoch # 2586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013667887396877632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,224 INFO epoch # 2587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014043726754607633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,249 INFO epoch # 2588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013916575248003937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,273 INFO epoch # 2589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013985821104142815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,298 INFO epoch # 2590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015214186700177379
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:53,298 INFO *** epoch 2590, rolling-avg-loss (window=10)= 0.014100156018685084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,322 INFO epoch # 2591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015085014660144225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,346 INFO epoch # 2592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013977264548884705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,370 INFO epoch # 2593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014587647558073513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,395 INFO epoch # 2594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013772124380921014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,419 INFO epoch # 2595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013679993891855702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,444 INFO epoch # 2596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014041459042346105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,468 INFO epoch # 2597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013710342973354273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,493 INFO epoch # 2598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013994643886690028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,517 INFO epoch # 2599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01401707628974691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,541 INFO epoch # 2600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013418171423836611
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:53,542 INFO *** epoch 2600, rolling-avg-loss (window=10)= 0.014028373865585309
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,566 INFO epoch # 2601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013979457653476857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,590 INFO epoch # 2602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013495221312041394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,615 INFO epoch # 2603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013996709589264356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,640 INFO epoch # 2604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013853083000867628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,664 INFO epoch # 2605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014203609258402139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,689 INFO epoch # 2606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013822451670421287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,713 INFO epoch # 2607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014122343025519513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,737 INFO epoch # 2608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013648905864101835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,761 INFO epoch # 2609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013846386747900397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,785 INFO epoch # 2610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013820090825902298
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:53,785 INFO *** epoch 2610, rolling-avg-loss (window=10)= 0.013878825894789771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,809 INFO epoch # 2611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015024051259388216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,834 INFO epoch # 2612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01507211459102109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,859 INFO epoch # 2613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014132883530692197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,883 INFO epoch # 2614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014128437454928644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,908 INFO epoch # 2615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014563773074769415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,932 INFO epoch # 2616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013757444816292264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,957 INFO epoch # 2617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01366959915321786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:53,981 INFO epoch # 2618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014231238470529206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,004 INFO epoch # 2619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014228338346583769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,028 INFO epoch # 2620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013882631494197994
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:54,028 INFO *** epoch 2620, rolling-avg-loss (window=10)= 0.014269051219162065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,052 INFO epoch # 2621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013726967532420531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,077 INFO epoch # 2622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013321559919859283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,102 INFO epoch # 2623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013648358726641163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,127 INFO epoch # 2624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014242878824006766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,151 INFO epoch # 2625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0137585383199621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,175 INFO epoch # 2626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013929792228736915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,199 INFO epoch # 2627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013654063615831546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,223 INFO epoch # 2628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014120810679742135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,246 INFO epoch # 2629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01358503355004359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,271 INFO epoch # 2630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013621630991110578
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:54,271 INFO *** epoch 2630, rolling-avg-loss (window=10)= 0.01376096343883546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,296 INFO epoch # 2631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013502486704965122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,321 INFO epoch # 2632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013488644632161595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,345 INFO epoch # 2633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013695355548406951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,370 INFO epoch # 2634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014048500801436603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,395 INFO epoch # 2635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014281941519584507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,419 INFO epoch # 2636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01368818906485103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,444 INFO epoch # 2637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013799264954286627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,468 INFO epoch # 2638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013263138083857484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,492 INFO epoch # 2639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013269791757920757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,517 INFO epoch # 2640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013221994355262723
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:54,517 INFO *** epoch 2640, rolling-avg-loss (window=10)= 0.01362593074227334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,542 INFO epoch # 2641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014392082506674342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,567 INFO epoch # 2642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01480957432067953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,591 INFO epoch # 2643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01370834995759651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,616 INFO epoch # 2644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014442812564084306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,641 INFO epoch # 2645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014022329705767334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,666 INFO epoch # 2646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013804106798488647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,690 INFO epoch # 2647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013795747116091661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,715 INFO epoch # 2648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013676910471986048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,739 INFO epoch # 2649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013821886503137648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,764 INFO epoch # 2650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01475668724742718
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:54,764 INFO *** epoch 2650, rolling-avg-loss (window=10)= 0.01412304871919332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,788 INFO epoch # 2651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013729777085245587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,813 INFO epoch # 2652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013576375815318897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,837 INFO epoch # 2653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013244372719782405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,861 INFO epoch # 2654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013573592092143372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,885 INFO epoch # 2655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013772713951766491
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,910 INFO epoch # 2656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013680173964530695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,935 INFO epoch # 2657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013986690421006642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,959 INFO epoch # 2658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013527544244425371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:54,983 INFO epoch # 2659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01311235100729391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,008 INFO epoch # 2660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013191949590691365
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:55,009 INFO *** epoch 2660, rolling-avg-loss (window=10)= 0.013539554089220474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,033 INFO epoch # 2661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01366341847460717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,057 INFO epoch # 2662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013159960391931236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,081 INFO epoch # 2663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014306271870736964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,106 INFO epoch # 2664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014132534292002674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,130 INFO epoch # 2665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014113324097706936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,155 INFO epoch # 2666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013440619775792584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,179 INFO epoch # 2667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013348321197554469
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,203 INFO epoch # 2668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013653302172315307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,228 INFO epoch # 2669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01384784393303562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,252 INFO epoch # 2670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013028462955844589
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:55,252 INFO *** epoch 2670, rolling-avg-loss (window=10)= 0.013669405916152755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,276 INFO epoch # 2671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013532666911487468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,300 INFO epoch # 2672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01397601630014833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,324 INFO epoch # 2673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013846708054188639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,348 INFO epoch # 2674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014536618677084334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,373 INFO epoch # 2675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013589062276878394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,398 INFO epoch # 2676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01319983071880415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,423 INFO epoch # 2677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013174394436646253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,447 INFO epoch # 2678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014291447630967014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,472 INFO epoch # 2679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014045400224858895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,496 INFO epoch # 2680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013579675607616082
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:55,496 INFO *** epoch 2680, rolling-avg-loss (window=10)= 0.013777182083867957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,520 INFO epoch # 2681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013674265414010733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,545 INFO epoch # 2682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013822322653140873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,569 INFO epoch # 2683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013878523735911585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,595 INFO epoch # 2684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013390087697189301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,620 INFO epoch # 2685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01436715315503534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,646 INFO epoch # 2686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013372448884183541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,671 INFO epoch # 2687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013143839474651031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,695 INFO epoch # 2688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013578542304458097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,720 INFO epoch # 2689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013914094946812838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,744 INFO epoch # 2690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015287362257367931
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:55,744 INFO *** epoch 2690, rolling-avg-loss (window=10)= 0.013842864052276128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,769 INFO epoch # 2691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.015778176544699818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,794 INFO epoch # 2692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014109767012996599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,819 INFO epoch # 2693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01320019039849285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,843 INFO epoch # 2694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013183675517211668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,868 INFO epoch # 2695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013341723766643554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,892 INFO epoch # 2696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013237281105830334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,916 INFO epoch # 2697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013290038084960543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,940 INFO epoch # 2698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013429813552647829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,964 INFO epoch # 2699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013432613064651377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:55,989 INFO epoch # 2700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01287831970694242
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:55,989 INFO *** epoch 2700, rolling-avg-loss (window=10)= 0.0135881598755077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,013 INFO epoch # 2701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012931241202750243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,038 INFO epoch # 2702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013793847610941157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,063 INFO epoch # 2703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013941914105089381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,087 INFO epoch # 2704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013381982746068388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,112 INFO epoch # 2705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013864973196177743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,136 INFO epoch # 2706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01370102068176493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,160 INFO epoch # 2707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013289889902807772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,184 INFO epoch # 2708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012972364245797507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,209 INFO epoch # 2709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012892245591501705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,234 INFO epoch # 2710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01275085895031225
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:56,234 INFO *** epoch 2710, rolling-avg-loss (window=10)= 0.013352033823321108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,258 INFO epoch # 2711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012775787952705286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,283 INFO epoch # 2712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013073474605334923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,307 INFO epoch # 2713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013367055158596486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,331 INFO epoch # 2714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01350360200740397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,355 INFO epoch # 2715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01342969863617327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,380 INFO epoch # 2716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013167700584745035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,405 INFO epoch # 2717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013768353630439378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,429 INFO epoch # 2718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013628272965434007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,454 INFO epoch # 2719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013772314807283692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,478 INFO epoch # 2720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013825743284542114
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:56,478 INFO *** epoch 2720, rolling-avg-loss (window=10)= 0.013431200363265816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,503 INFO epoch # 2721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013929868204286322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,527 INFO epoch # 2722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014642187394201756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,552 INFO epoch # 2723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014169745583785698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,576 INFO epoch # 2724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013532410026527941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,602 INFO epoch # 2725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013390741805778816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,626 INFO epoch # 2726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012830989508074708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,651 INFO epoch # 2727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013315687872818671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,675 INFO epoch # 2728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013314195311977528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,700 INFO epoch # 2729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012921888031996787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,725 INFO epoch # 2730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013006603083340451
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:56,725 INFO *** epoch 2730, rolling-avg-loss (window=10)= 0.013505431682278867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,749 INFO epoch # 2731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014159185957396403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,773 INFO epoch # 2732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013553415978094563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,798 INFO epoch # 2733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013039856930845417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,822 INFO epoch # 2734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0150383866566699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,847 INFO epoch # 2735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014070576915401034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,871 INFO epoch # 2736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012562430390971713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,896 INFO epoch # 2737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01279442927625496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,920 INFO epoch # 2738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012944753412739374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,945 INFO epoch # 2739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013119718452799134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,969 INFO epoch # 2740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013429864615318365
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:56,969 INFO *** epoch 2740, rolling-avg-loss (window=10)= 0.013471261858649086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:56,994 INFO epoch # 2741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01268196277669631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,018 INFO epoch # 2742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013165029653464444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,042 INFO epoch # 2743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012888291377748828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,067 INFO epoch # 2744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012744806932460051
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,091 INFO epoch # 2745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01356021389074158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,116 INFO epoch # 2746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013109432140481658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,140 INFO epoch # 2747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012720504615572281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,165 INFO epoch # 2748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012932103534694761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,189 INFO epoch # 2749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01383514110057149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,214 INFO epoch # 2750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013051061352598481
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:57,214 INFO *** epoch 2750, rolling-avg-loss (window=10)= 0.013068854737502989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,238 INFO epoch # 2751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013026355256442912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,262 INFO epoch # 2752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013453595747705549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,285 INFO epoch # 2753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0135661681269994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,310 INFO epoch # 2754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013814197343890555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,335 INFO epoch # 2755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013339604309294373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,359 INFO epoch # 2756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013403283170191571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,385 INFO epoch # 2757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013603198181954212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,409 INFO epoch # 2758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012959974133991636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,434 INFO epoch # 2759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01321619922237005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,458 INFO epoch # 2760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013244268484413624
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:57,458 INFO *** epoch 2760, rolling-avg-loss (window=10)= 0.013362684397725389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,482 INFO epoch # 2761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013420589079032652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,506 INFO epoch # 2762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012708152149571106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,530 INFO epoch # 2763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013353926711715758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,555 INFO epoch # 2764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012981230393052101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,580 INFO epoch # 2765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014024801028426737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,605 INFO epoch # 2766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01380135235376656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,630 INFO epoch # 2767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01335215121798683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,654 INFO epoch # 2768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013243744382634759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,678 INFO epoch # 2769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013002042382140644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,703 INFO epoch # 2770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01304236298892647
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:57,703 INFO *** epoch 2770, rolling-avg-loss (window=10)= 0.013293035268725361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,726 INFO epoch # 2771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01293212766177021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,750 INFO epoch # 2772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012718473939457908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,776 INFO epoch # 2773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012684704372077249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,801 INFO epoch # 2774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013392139953793958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,825 INFO epoch # 2775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013601062877569348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,850 INFO epoch # 2776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013947693150839768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,875 INFO epoch # 2777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013749920763075352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,899 INFO epoch # 2778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013077670882921666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,924 INFO epoch # 2779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013445343414787203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,948 INFO epoch # 2780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012971269592526369
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:57,948 INFO *** epoch 2780, rolling-avg-loss (window=10)= 0.013252040660881903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,972 INFO epoch # 2781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013220252076280303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:57,997 INFO epoch # 2782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012731193957733922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,021 INFO epoch # 2783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012857518551754765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,046 INFO epoch # 2784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01353253457637038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,070 INFO epoch # 2785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013582145431428216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,095 INFO epoch # 2786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014250904554501176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,119 INFO epoch # 2787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013521809451049194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,143 INFO epoch # 2788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012967080212547444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,168 INFO epoch # 2789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013151504390407354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,193 INFO epoch # 2790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012747690299875103
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:58,193 INFO *** epoch 2790, rolling-avg-loss (window=10)= 0.013256263350194785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,217 INFO epoch # 2791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012938498970470391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,242 INFO epoch # 2792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013573862335761078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,267 INFO epoch # 2793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01321202096005436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,291 INFO epoch # 2794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01280681807838846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,316 INFO epoch # 2795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012494491413235664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,340 INFO epoch # 2796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013154192944057286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,365 INFO epoch # 2797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013224772177636623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,390 INFO epoch # 2798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01281472176196985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,414 INFO epoch # 2799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013759109351667576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,439 INFO epoch # 2800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012873080864665098
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:58,439 INFO *** epoch 2800, rolling-avg-loss (window=10)= 0.01308515688579064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,463 INFO epoch # 2801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01278170598379802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,488 INFO epoch # 2802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012508830390288495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,513 INFO epoch # 2803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013205668452428654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,537 INFO epoch # 2804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012702840031124651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,561 INFO epoch # 2805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013892882008804008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,587 INFO epoch # 2806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013116395770339295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,611 INFO epoch # 2807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013821195054333657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,636 INFO epoch # 2808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013554852193919942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,661 INFO epoch # 2809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01298871063045226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,686 INFO epoch # 2810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012895298277726397
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:58,686 INFO *** epoch 2810, rolling-avg-loss (window=10)= 0.013146837879321537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,710 INFO epoch # 2811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012590924787218682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,735 INFO epoch # 2812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01269302457512822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,759 INFO epoch # 2813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012817448339774273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,783 INFO epoch # 2814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01326897284889128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,808 INFO epoch # 2815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013238826402812265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,832 INFO epoch # 2816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013083161393296905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,857 INFO epoch # 2817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013097190574626438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,881 INFO epoch # 2818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013056005263933912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,906 INFO epoch # 2819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013425603552605025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,930 INFO epoch # 2820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012491464804043062
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:58,931 INFO *** epoch 2820, rolling-avg-loss (window=10)= 0.012976262254233007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,955 INFO epoch # 2821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012720348022412509
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:58,979 INFO epoch # 2822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012947890572831966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,003 INFO epoch # 2823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012942141824169084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,028 INFO epoch # 2824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01290353387594223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,052 INFO epoch # 2825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012765143474098295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,076 INFO epoch # 2826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012527280545327812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,101 INFO epoch # 2827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012722072817268781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,125 INFO epoch # 2828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.014117869432084262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,150 INFO epoch # 2829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013574425393017009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,174 INFO epoch # 2830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013259109575301409
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:59,174 INFO *** epoch 2830, rolling-avg-loss (window=10)= 0.013047981553245335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,199 INFO epoch # 2831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012871996106696315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,223 INFO epoch # 2832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012732445175061002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,247 INFO epoch # 2833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012531811153166927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,271 INFO epoch # 2834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013517526720534079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,295 INFO epoch # 2835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013325036590686068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,320 INFO epoch # 2836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012365240225335583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,344 INFO epoch # 2837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012857168287155218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,370 INFO epoch # 2838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013204626753577031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,399 INFO epoch # 2839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013273378455778584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,426 INFO epoch # 2840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012518929899670184
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:59,426 INFO *** epoch 2840, rolling-avg-loss (window=10)= 0.0129198159367661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,451 INFO epoch # 2841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012518360919784755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,475 INFO epoch # 2842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012089853247744031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,499 INFO epoch # 2843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012524135585408658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,524 INFO epoch # 2844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012684961795457639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,549 INFO epoch # 2845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01281010007369332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,573 INFO epoch # 2846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012277621251996607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,599 INFO epoch # 2847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013996679146657698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,623 INFO epoch # 2848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013205673691118136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,648 INFO epoch # 2849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01371418331109453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,672 INFO epoch # 2850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012347966810921207
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:59,672 INFO *** epoch 2850, rolling-avg-loss (window=10)= 0.012816953583387658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,696 INFO epoch # 2851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01259306026622653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,721 INFO epoch # 2852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012248616898432374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,745 INFO epoch # 2853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012960370877408423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,770 INFO epoch # 2854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013629352441057563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,795 INFO epoch # 2855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012566962541313842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,819 INFO epoch # 2856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012503785692388192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,843 INFO epoch # 2857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012790878157829866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,868 INFO epoch # 2858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01299173611914739
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,892 INFO epoch # 2859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013604453430161811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,916 INFO epoch # 2860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01284639500954654
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:38:59,917 INFO *** epoch 2860, rolling-avg-loss (window=10)= 0.012873561143351253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,941 INFO epoch # 2861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012660708802286536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,966 INFO epoch # 2862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012548029946628958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:38:59,990 INFO epoch # 2863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012892079364974052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,015 INFO epoch # 2864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01268193060241174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,039 INFO epoch # 2865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01260966139670927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,063 INFO epoch # 2866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012574834865517914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,088 INFO epoch # 2867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012653248020797037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,113 INFO epoch # 2868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013166101911338046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,137 INFO epoch # 2869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01303924790408928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,162 INFO epoch # 2870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012811486754799262
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:00,162 INFO *** epoch 2870, rolling-avg-loss (window=10)= 0.01276373295695521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,187 INFO epoch # 2871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0133797723101452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,211 INFO epoch # 2872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012135987082729116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,236 INFO epoch # 2873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012368130788672715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,260 INFO epoch # 2874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012860565810115077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,284 INFO epoch # 2875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013134635271853767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,309 INFO epoch # 2876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013115003996063024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,333 INFO epoch # 2877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012370184267638251
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,358 INFO epoch # 2878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012660087464610115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,383 INFO epoch # 2879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013087754035950638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,408 INFO epoch # 2880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012429021604475565
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:00,408 INFO *** epoch 2880, rolling-avg-loss (window=10)= 0.012754114263225346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,432 INFO epoch # 2881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01232949594850652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,456 INFO epoch # 2882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01205001755442936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,481 INFO epoch # 2883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01217073448060546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,505 INFO epoch # 2884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011960714371525683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,529 INFO epoch # 2885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012283820047741756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,553 INFO epoch # 2886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012372961835353635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,578 INFO epoch # 2887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01244244106055703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,604 INFO epoch # 2888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012508608342614025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,629 INFO epoch # 2889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012624978058738634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,654 INFO epoch # 2890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013178569191950373
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:00,654 INFO *** epoch 2890, rolling-avg-loss (window=10)= 0.012392234089202248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,678 INFO epoch # 2891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012890817524748854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,703 INFO epoch # 2892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012794725291314535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,727 INFO epoch # 2893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012562400981551036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,752 INFO epoch # 2894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012821158627048135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,776 INFO epoch # 2895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012715244563878514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,800 INFO epoch # 2896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012352681573247537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,825 INFO epoch # 2897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012557380527141504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,852 INFO epoch # 2898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013528136681998149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,878 INFO epoch # 2899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012998673060792498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,902 INFO epoch # 2900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012842700991313905
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:00,902 INFO *** epoch 2900, rolling-avg-loss (window=10)= 0.012806391982303466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,927 INFO epoch # 2901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012619120287126862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,951 INFO epoch # 2902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012551719366456382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:00,976 INFO epoch # 2903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0123508618271444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,002 INFO epoch # 2904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012432377043296583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,027 INFO epoch # 2905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011896072232048027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,051 INFO epoch # 2906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012315926171140745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,076 INFO epoch # 2907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012925975766847841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,101 INFO epoch # 2908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012736036704154685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,126 INFO epoch # 2909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012894353931187652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,150 INFO epoch # 2910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012815368245355785
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:01,150 INFO *** epoch 2910, rolling-avg-loss (window=10)= 0.012553781157475897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,175 INFO epoch # 2911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012789655826054513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,200 INFO epoch # 2912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012536109788925387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,226 INFO epoch # 2913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012120678533392493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,253 INFO epoch # 2914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012287859251955524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,279 INFO epoch # 2915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012078225467121229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,304 INFO epoch # 2916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012228449922986329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,329 INFO epoch # 2917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012388800314511172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,354 INFO epoch # 2918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012348257092526183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,378 INFO epoch # 2919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012403385451762006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,403 INFO epoch # 2920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011873168725287542
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:01,403 INFO *** epoch 2920, rolling-avg-loss (window=10)= 0.012305459037452237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,429 INFO epoch # 2921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011990732076810673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,454 INFO epoch # 2922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01210808553150855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,479 INFO epoch # 2923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012227218030602671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,504 INFO epoch # 2924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012507898558396846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,528 INFO epoch # 2925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012279450034839101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,553 INFO epoch # 2926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012082083441782743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,577 INFO epoch # 2927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012247641236172058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,602 INFO epoch # 2928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01220750593347475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,627 INFO epoch # 2929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012278988171601668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,654 INFO epoch # 2930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012473610244342126
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:01,654 INFO *** epoch 2930, rolling-avg-loss (window=10)= 0.012240321325953119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,680 INFO epoch # 2931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012549848557682708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,706 INFO epoch # 2932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012604002738953568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,731 INFO epoch # 2933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013760683592408895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,755 INFO epoch # 2934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013353576345252804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,780 INFO epoch # 2935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012523404060630128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,805 INFO epoch # 2936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012482930949772708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,829 INFO epoch # 2937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012387483948259614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,854 INFO epoch # 2938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012910658493638039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,880 INFO epoch # 2939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012817903872928582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,906 INFO epoch # 2940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012363407557131723
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:01,906 INFO *** epoch 2940, rolling-avg-loss (window=10)= 0.012775390011665878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,932 INFO epoch # 2941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012342298970906995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,957 INFO epoch # 2942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012172043789178133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:01,981 INFO epoch # 2943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012443650717614219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,006 INFO epoch # 2944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012610550373210572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,031 INFO epoch # 2945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012342313755652867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,055 INFO epoch # 2946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01389868302794639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,082 INFO epoch # 2947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012997212703339756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,109 INFO epoch # 2948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012317503991653211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,136 INFO epoch # 2949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012538565599243157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,161 INFO epoch # 2950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012157777026004624
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:02,161 INFO *** epoch 2950, rolling-avg-loss (window=10)= 0.012582059995474992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,186 INFO epoch # 2951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012423391701304354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,210 INFO epoch # 2952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012331842095591128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,234 INFO epoch # 2953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012642297355341725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,259 INFO epoch # 2954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012251549327629618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,283 INFO epoch # 2955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01222499564755708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,310 INFO epoch # 2956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012634748971322551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,336 INFO epoch # 2957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012043342518154532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,363 INFO epoch # 2958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011522038708790205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,388 INFO epoch # 2959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011912296133232303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,413 INFO epoch # 2960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012469949535443448
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:02,413 INFO *** epoch 2960, rolling-avg-loss (window=10)= 0.012245645199436694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,438 INFO epoch # 2961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012626186216948554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,462 INFO epoch # 2962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013694860113901086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,486 INFO epoch # 2963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012280173497856595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,509 INFO epoch # 2964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011820486251963302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,534 INFO epoch # 2965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011778836022131145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,559 INFO epoch # 2966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011985118093434721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,585 INFO epoch # 2967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012796079899999313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,609 INFO epoch # 2968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012483354366850108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,634 INFO epoch # 2969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012040238187182695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,658 INFO epoch # 2970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012300125148613006
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:02,658 INFO *** epoch 2970, rolling-avg-loss (window=10)= 0.012380545779888053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,683 INFO epoch # 2971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012561148192617111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,707 INFO epoch # 2972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012488160573411733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,731 INFO epoch # 2973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012108630369766615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,755 INFO epoch # 2974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012168052548076957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,780 INFO epoch # 2975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01231362487305887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,804 INFO epoch # 2976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013160487898858264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,829 INFO epoch # 2977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012039773908327334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,853 INFO epoch # 2978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012640985820326023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,877 INFO epoch # 2979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012381559718050994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,902 INFO epoch # 2980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012009395824861713
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:02,902 INFO *** epoch 2980, rolling-avg-loss (window=10)= 0.012387181972735561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,926 INFO epoch # 2981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01230247609782964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,951 INFO epoch # 2982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012184630744741298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,975 INFO epoch # 2983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012269414597540163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:02,999 INFO epoch # 2984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011896135489223525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,024 INFO epoch # 2985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012339264751062728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,048 INFO epoch # 2986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012605286756297573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,073 INFO epoch # 2987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012285947988857515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,097 INFO epoch # 2988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01174415901186876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,122 INFO epoch # 2989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012645327835343778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,146 INFO epoch # 2990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012223268233356066
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:03,146 INFO *** epoch 2990, rolling-avg-loss (window=10)= 0.012249591150612105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,171 INFO epoch # 2991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011938247553189285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,195 INFO epoch # 2992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011592500180995557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,220 INFO epoch # 2993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01176752833998762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,245 INFO epoch # 2994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01189258648082614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,270 INFO epoch # 2995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012019841131404974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,295 INFO epoch # 2996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012672079115873203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,319 INFO epoch # 2997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013050383771769702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,343 INFO epoch # 2998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012463122970075347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,367 INFO epoch # 2999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011960077041294426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,392 INFO epoch # 3000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012178943637991324
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:03,392 INFO *** epoch 3000, rolling-avg-loss (window=10)= 0.012153531022340758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,416 INFO epoch # 3001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012336273313849233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,441 INFO epoch # 3002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011812760974862613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,465 INFO epoch # 3003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012328959870501421
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,489 INFO epoch # 3004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012229922824189998
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,514 INFO epoch # 3005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01269376625714358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,538 INFO epoch # 3006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011892226248164661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,562 INFO epoch # 3007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01265589875401929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,586 INFO epoch # 3008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01286754960892722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,611 INFO epoch # 3009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013340213452465832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,636 INFO epoch # 3010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01319016344496049
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:03,636 INFO *** epoch 3010, rolling-avg-loss (window=10)= 0.012534773474908435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,660 INFO epoch # 3011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01230682288587559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,685 INFO epoch # 3012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01201019155269023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,709 INFO epoch # 3013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011506313720019534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,734 INFO epoch # 3014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012265896031749435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,758 INFO epoch # 3015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011514608340803534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,782 INFO epoch # 3016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011808415423729457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,807 INFO epoch # 3017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011594796043937095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,831 INFO epoch # 3018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0116278784989845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,855 INFO epoch # 3019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01219111961836461
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,880 INFO epoch # 3020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012149409885751083
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:03,880 INFO *** epoch 3020, rolling-avg-loss (window=10)= 0.011897545200190507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,905 INFO epoch # 3021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011990819999482483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,930 INFO epoch # 3022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011822361630038358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,954 INFO epoch # 3023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012030131081701256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:03,978 INFO epoch # 3024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011838402133435011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,003 INFO epoch # 3025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011903011807589792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,027 INFO epoch # 3026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01227755811123643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,051 INFO epoch # 3027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01249850427848287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,076 INFO epoch # 3028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01174265485315118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,101 INFO epoch # 3029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012417173638823442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,128 INFO epoch # 3030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011784196729422547
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:04,128 INFO *** epoch 3030, rolling-avg-loss (window=10)= 0.012030481426336337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,153 INFO epoch # 3031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011908898450201377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,177 INFO epoch # 3032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011764510985813104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,202 INFO epoch # 3033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011924029895453714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,226 INFO epoch # 3034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011760451045120135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,250 INFO epoch # 3035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011896824653376825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,275 INFO epoch # 3036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01208344686892815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,300 INFO epoch # 3037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012463060265872627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,324 INFO epoch # 3038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011931053071748465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,349 INFO epoch # 3039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012041253401548602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,373 INFO epoch # 3040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011697813344653696
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:04,373 INFO *** epoch 3040, rolling-avg-loss (window=10)= 0.01194713419827167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,398 INFO epoch # 3041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011737405278836377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,423 INFO epoch # 3042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011269975046161562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,447 INFO epoch # 3043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01165255653904751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,471 INFO epoch # 3044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011751568192266859
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,496 INFO epoch # 3045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011931679473491386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,520 INFO epoch # 3046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01200982162845321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,545 INFO epoch # 3047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012318613946263213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,570 INFO epoch # 3048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012086614908184856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,595 INFO epoch # 3049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011885815372806974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,620 INFO epoch # 3050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013013154282816686
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:04,620 INFO *** epoch 3050, rolling-avg-loss (window=10)= 0.011965720466832863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,645 INFO epoch # 3051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0115524560897029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,669 INFO epoch # 3052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011675569170620292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,693 INFO epoch # 3053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01168259848782327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,717 INFO epoch # 3054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01151929704064969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,742 INFO epoch # 3055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011685928184306249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,767 INFO epoch # 3056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012299516223720275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,791 INFO epoch # 3057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013644658436533064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,816 INFO epoch # 3058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012347152151050977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,840 INFO epoch # 3059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012078657498932444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,865 INFO epoch # 3060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011774952901760116
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:04,865 INFO *** epoch 3060, rolling-avg-loss (window=10)= 0.012026078618509928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,888 INFO epoch # 3061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011618077332968824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,913 INFO epoch # 3062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011523589520948008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,937 INFO epoch # 3063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011637477233307436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,962 INFO epoch # 3064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012040197965689003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:04,987 INFO epoch # 3065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012536839072708972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,011 INFO epoch # 3066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0117996318731457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,036 INFO epoch # 3067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011508673371281475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,060 INFO epoch # 3068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011563306266907603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,085 INFO epoch # 3069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011747830503736623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,109 INFO epoch # 3070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011937162082176656
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:05,109 INFO *** epoch 3070, rolling-avg-loss (window=10)= 0.01179127852228703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,134 INFO epoch # 3071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011598977274843492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,158 INFO epoch # 3072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011611942332820036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,183 INFO epoch # 3073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011550148934475146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,207 INFO epoch # 3074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012019058660371229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,231 INFO epoch # 3075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011555064455023967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,256 INFO epoch # 3076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011830862058559433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,280 INFO epoch # 3077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011899695877218619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,304 INFO epoch # 3078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011418739988585003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,328 INFO epoch # 3079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011327546380925924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,353 INFO epoch # 3080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011753752100048587
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:05,353 INFO *** epoch 3080, rolling-avg-loss (window=10)= 0.011656578806287144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,377 INFO epoch # 3081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011721890739863738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,402 INFO epoch # 3082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011556598561583087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,426 INFO epoch # 3083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011693718828610145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,450 INFO epoch # 3084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012169004592578858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,475 INFO epoch # 3085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011835951183456928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,499 INFO epoch # 3086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012072236771928146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,523 INFO epoch # 3087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011996750021353364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,546 INFO epoch # 3088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011639345626463182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,571 INFO epoch # 3089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011781509885622654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,596 INFO epoch # 3090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011933205561945215
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:05,596 INFO *** epoch 3090, rolling-avg-loss (window=10)= 0.011840021177340532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,621 INFO epoch # 3091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012026669122860767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,646 INFO epoch # 3092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011631893314188346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,670 INFO epoch # 3093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011567105873837136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,695 INFO epoch # 3094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011934845548239537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,719 INFO epoch # 3095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011314953022520058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,743 INFO epoch # 3096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011253589516854845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,767 INFO epoch # 3097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01176791696343571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,791 INFO epoch # 3098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011879386751388665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,816 INFO epoch # 3099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012125038876547478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,840 INFO epoch # 3100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012087989365682006
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:05,840 INFO *** epoch 3100, rolling-avg-loss (window=10)= 0.011758938835555455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,865 INFO epoch # 3101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01182511544902809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,889 INFO epoch # 3102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011957339957007207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,914 INFO epoch # 3103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011165299561980646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,938 INFO epoch # 3104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011019359895726666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,962 INFO epoch # 3105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011197421845281497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:05,986 INFO epoch # 3106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011387475504307076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,010 INFO epoch # 3107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011416932291467674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,034 INFO epoch # 3108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012043436494423077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,059 INFO epoch # 3109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011290478898445144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,084 INFO epoch # 3110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011741050431737676
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:06,084 INFO *** epoch 3110, rolling-avg-loss (window=10)= 0.011504391032940476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,109 INFO epoch # 3111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011612720001721755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,134 INFO epoch # 3112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011911283203517087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,158 INFO epoch # 3113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011655077047180384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,182 INFO epoch # 3114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011679201139486395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,206 INFO epoch # 3115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011304723433568142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,231 INFO epoch # 3116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01185436725791078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,255 INFO epoch # 3117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011531126816407777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,279 INFO epoch # 3118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011370570951839909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,304 INFO epoch # 3119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011970213396125473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,328 INFO epoch # 3120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011494807724375278
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:06,329 INFO *** epoch 3120, rolling-avg-loss (window=10)= 0.011638409097213299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,353 INFO epoch # 3121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011741736874682829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,377 INFO epoch # 3122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01223563778330572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,401 INFO epoch # 3123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011168843033374287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,426 INFO epoch # 3124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011022466671420261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,450 INFO epoch # 3125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011694486893247813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,475 INFO epoch # 3126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012545743738883175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,500 INFO epoch # 3127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01228778219956439
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,524 INFO epoch # 3128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012296440312638879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,548 INFO epoch # 3129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01136877188400831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,573 INFO epoch # 3130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011517242179252207
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:06,573 INFO *** epoch 3130, rolling-avg-loss (window=10)= 0.011787915157037787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,598 INFO epoch # 3131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011179054956301115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,623 INFO epoch # 3132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011412062944145873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,647 INFO epoch # 3133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011857279314426705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,671 INFO epoch # 3134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011493525715195574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,696 INFO epoch # 3135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010934707534033805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,720 INFO epoch # 3136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011320261473883875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,745 INFO epoch # 3137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011754980965633877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,769 INFO epoch # 3138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011735315332771279
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,793 INFO epoch # 3139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012361044093267992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,818 INFO epoch # 3140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01139920749119483
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:06,818 INFO *** epoch 3140, rolling-avg-loss (window=10)= 0.011544743982085492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,842 INFO epoch # 3141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011320588513626717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,867 INFO epoch # 3142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011686719139106572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,891 INFO epoch # 3143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011491686906083487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,915 INFO epoch # 3144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010995951495715417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,940 INFO epoch # 3145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011314721326925792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,964 INFO epoch # 3146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011431341830757447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:06,990 INFO epoch # 3147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012527586906799115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,014 INFO epoch # 3148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011777951280237176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,038 INFO epoch # 3149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011979031158261932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,063 INFO epoch # 3150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011748113800422288
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:07,063 INFO *** epoch 3150, rolling-avg-loss (window=10)= 0.011627369235793595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,087 INFO epoch # 3151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011224442307138816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,111 INFO epoch # 3152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011353995723766275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,136 INFO epoch # 3153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011226851973333396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,160 INFO epoch # 3154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01152266256394796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,185 INFO epoch # 3155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011437330220360309
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,209 INFO epoch # 3156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011283726271358319
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,234 INFO epoch # 3157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01186581036017742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,257 INFO epoch # 3158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012058840016834438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,281 INFO epoch # 3159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011861965453135781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,305 INFO epoch # 3160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011025061830878258
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:07,305 INFO *** epoch 3160, rolling-avg-loss (window=10)= 0.011486068672093097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,330 INFO epoch # 3161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011062852892791852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,354 INFO epoch # 3162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011104422403150238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,379 INFO epoch # 3163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011205748960492201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,403 INFO epoch # 3164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01130252999428194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,428 INFO epoch # 3165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012582337600179017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,452 INFO epoch # 3166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012707226705970243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,476 INFO epoch # 3167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011868413843330927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,501 INFO epoch # 3168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011307158798445016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,525 INFO epoch # 3169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011533542550751008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,549 INFO epoch # 3170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011438536064815708
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:07,549 INFO *** epoch 3170, rolling-avg-loss (window=10)= 0.011611276981420815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,574 INFO epoch # 3171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01164081206661649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,599 INFO epoch # 3172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011727883262210526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,625 INFO epoch # 3173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011700979535817169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,649 INFO epoch # 3174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011510588083183393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,674 INFO epoch # 3175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011147033743327484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,698 INFO epoch # 3176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0112954706128221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,723 INFO epoch # 3177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01129590070922859
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,747 INFO epoch # 3178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011557369973161258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,771 INFO epoch # 3179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01176625772495754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,795 INFO epoch # 3180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010982569641782902
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:07,796 INFO *** epoch 3180, rolling-avg-loss (window=10)= 0.011462486535310745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,820 INFO epoch # 3181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011519664200022817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,845 INFO epoch # 3182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011510670214192942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,869 INFO epoch # 3183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013169815909350291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,894 INFO epoch # 3184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012808473024051636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,918 INFO epoch # 3185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011709648504620418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,943 INFO epoch # 3186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011031320507754572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,967 INFO epoch # 3187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011157772692968138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:07,991 INFO epoch # 3188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011002132145222276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,016 INFO epoch # 3189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010947816088446416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,041 INFO epoch # 3190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011364610865712166
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:08,041 INFO *** epoch 3190, rolling-avg-loss (window=10)= 0.011622192415234166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,065 INFO epoch # 3191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012775136536220089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,090 INFO epoch # 3192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012438492456567474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,114 INFO epoch # 3193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011742243659682572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,138 INFO epoch # 3194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01117125284508802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,163 INFO epoch # 3195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011243791508604772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,187 INFO epoch # 3196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01110239485569764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,211 INFO epoch # 3197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01087576913414523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,236 INFO epoch # 3198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010987413465045393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,260 INFO epoch # 3199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011476611718535423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,285 INFO epoch # 3200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010657083432306536
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:08,285 INFO *** epoch 3200, rolling-avg-loss (window=10)= 0.011447018961189315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,310 INFO epoch # 3201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011176715765031986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,334 INFO epoch # 3202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011000611950294115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,358 INFO epoch # 3203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011363951256498694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,383 INFO epoch # 3204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011629798871581443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,407 INFO epoch # 3205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011515914360643364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,431 INFO epoch # 3206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01088799002172891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,456 INFO epoch # 3207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011215645776246674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,480 INFO epoch # 3208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011680454525048845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,505 INFO epoch # 3209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011214230384211987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,529 INFO epoch # 3210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010930318836471997
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:08,529 INFO *** epoch 3210, rolling-avg-loss (window=10)= 0.011261563174775802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,554 INFO epoch # 3211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01118508014769759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,578 INFO epoch # 3212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012344273593043908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,603 INFO epoch # 3213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011310333546134643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,628 INFO epoch # 3214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012239889430929907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,652 INFO epoch # 3215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011269138456555083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,677 INFO epoch # 3216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011104071862064302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,702 INFO epoch # 3217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01145535017712973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,726 INFO epoch # 3218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011198311185580678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,751 INFO epoch # 3219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011395517227356322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,775 INFO epoch # 3220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012122652609832585
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:08,775 INFO *** epoch 3220, rolling-avg-loss (window=10)= 0.011562461823632474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,799 INFO epoch # 3221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011424650379922241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,824 INFO epoch # 3222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011201297762454487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,848 INFO epoch # 3223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010480940152774565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,873 INFO epoch # 3224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011367319151759148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,897 INFO epoch # 3225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011643716861726716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,922 INFO epoch # 3226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010945103829726577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,946 INFO epoch # 3227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01085572288138792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,970 INFO epoch # 3228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011547117464942858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:08,994 INFO epoch # 3229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011607507156440988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,019 INFO epoch # 3230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012186043473775499
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:09,019 INFO *** epoch 3230, rolling-avg-loss (window=10)= 0.0113259419114911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,043 INFO epoch # 3231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012740769830998033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,067 INFO epoch # 3232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011716782973962836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,092 INFO epoch # 3233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011197542655281723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,117 INFO epoch # 3234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010908736658166163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,141 INFO epoch # 3235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010641484550433233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,166 INFO epoch # 3236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011060140968766063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,190 INFO epoch # 3237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011863308660394978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,215 INFO epoch # 3238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011247283197008073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,239 INFO epoch # 3239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010778171475976706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,263 INFO epoch # 3240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010998580459272489
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:09,263 INFO *** epoch 3240, rolling-avg-loss (window=10)= 0.011315280143026029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,287 INFO epoch # 3241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010970076997182332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,311 INFO epoch # 3242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011412438572733663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,336 INFO epoch # 3243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01096667734964285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,360 INFO epoch # 3244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010650906915543601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,385 INFO epoch # 3245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010977832629578188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,409 INFO epoch # 3246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011237805665587075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,434 INFO epoch # 3247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010855109023395926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,458 INFO epoch # 3248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010900807435973547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,482 INFO epoch # 3249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010851122322492301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,506 INFO epoch # 3250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010954613506328315
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:09,506 INFO *** epoch 3250, rolling-avg-loss (window=10)= 0.01097773904184578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,531 INFO epoch # 3251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011040345139917918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,555 INFO epoch # 3252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0110511266393587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,580 INFO epoch # 3253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011132529936730862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,605 INFO epoch # 3254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01119734562234953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,630 INFO epoch # 3255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011866234832268674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,655 INFO epoch # 3256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011258590428042226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,679 INFO epoch # 3257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011394932589610107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,703 INFO epoch # 3258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01124730973970145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,727 INFO epoch # 3259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01104606244189199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,752 INFO epoch # 3260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010978349746437743
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:09,752 INFO *** epoch 3260, rolling-avg-loss (window=10)= 0.01122128271163092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,777 INFO epoch # 3261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011684009077725932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,802 INFO epoch # 3262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010987727073370479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,826 INFO epoch # 3263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01065705441578757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,851 INFO epoch # 3264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01055362558690831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,875 INFO epoch # 3265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010989271861035377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,899 INFO epoch # 3266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01070427690865472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,923 INFO epoch # 3267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011481652312795632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,947 INFO epoch # 3268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010948714436381124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,972 INFO epoch # 3269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01080350816482678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:09,997 INFO epoch # 3270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011316264717606828
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:09,997 INFO *** epoch 3270, rolling-avg-loss (window=10)= 0.011012610455509275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,021 INFO epoch # 3271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010937858474790119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,046 INFO epoch # 3272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010606806259602308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,070 INFO epoch # 3273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010996850018273108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,095 INFO epoch # 3274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011771944438805804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,119 INFO epoch # 3275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.013143322168616578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,143 INFO epoch # 3276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012669426665524952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,168 INFO epoch # 3277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01183925024815835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,192 INFO epoch # 3278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010740420344518498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,217 INFO epoch # 3279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011059010779717937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,241 INFO epoch # 3280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010677290396415628
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:10,241 INFO *** epoch 3280, rolling-avg-loss (window=10)= 0.011444217979442329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,266 INFO epoch # 3281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010896051040617749
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,290 INFO epoch # 3282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010712611925555393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,314 INFO epoch # 3283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011325841856887564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,339 INFO epoch # 3284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011549335977178998
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,363 INFO epoch # 3285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01166293810820207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,388 INFO epoch # 3286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010901276822551154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,412 INFO epoch # 3287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011236427177209407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,436 INFO epoch # 3288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011324823019094765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,461 INFO epoch # 3289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010829815284523647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,486 INFO epoch # 3290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010430435519083403
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:10,486 INFO *** epoch 3290, rolling-avg-loss (window=10)= 0.011086955673090416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,510 INFO epoch # 3291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010458648190251552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,534 INFO epoch # 3292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010877531734877266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,558 INFO epoch # 3293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01130755063786637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,584 INFO epoch # 3294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01163019280647859
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,608 INFO epoch # 3295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010944197514618281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,633 INFO epoch # 3296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010534863104112446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,659 INFO epoch # 3297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010871824459172785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,684 INFO epoch # 3298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011999897149507888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,708 INFO epoch # 3299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011209245945792645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,733 INFO epoch # 3300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012365868635242805
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:10,733 INFO *** epoch 3300, rolling-avg-loss (window=10)= 0.011219982017792063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,757 INFO epoch # 3301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011546306821401231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,781 INFO epoch # 3302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010549177648499608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,806 INFO epoch # 3303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010397311787528452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,830 INFO epoch # 3304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010574299965810496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,855 INFO epoch # 3305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011001100632711314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,879 INFO epoch # 3306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010454361690790392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,904 INFO epoch # 3307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011227526643779129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,928 INFO epoch # 3308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01117703404452186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,952 INFO epoch # 3309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0116368336166488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:10,977 INFO epoch # 3310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011444119678344578
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:10,977 INFO *** epoch 3310, rolling-avg-loss (window=10)= 0.011000807253003586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,001 INFO epoch # 3311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011206832961761393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,025 INFO epoch # 3312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010946213456918485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,049 INFO epoch # 3313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01122487845714204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,073 INFO epoch # 3314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012086719245417044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,098 INFO epoch # 3315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011315601084788796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,123 INFO epoch # 3316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01109843128506327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,147 INFO epoch # 3317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011127999780001119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,171 INFO epoch # 3318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010882803908316419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,196 INFO epoch # 3319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01059807607816765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,221 INFO epoch # 3320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010805934842210263
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:11,221 INFO *** epoch 3320, rolling-avg-loss (window=10)= 0.011129349109978648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,245 INFO epoch # 3321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010368220784584992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,269 INFO epoch # 3322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010457046053488739
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,294 INFO epoch # 3323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010449142311699688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,318 INFO epoch # 3324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010425800675875507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,343 INFO epoch # 3325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010444447296322323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,367 INFO epoch # 3326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010680068444344215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,392 INFO epoch # 3327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010735448318882845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,416 INFO epoch # 3328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010445943014929071
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,440 INFO epoch # 3329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010463198021170683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,464 INFO epoch # 3330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011377144401194528
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:11,464 INFO *** epoch 3330, rolling-avg-loss (window=10)= 0.010584645932249259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,489 INFO epoch # 3331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011376171896699816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,513 INFO epoch # 3332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011568243658985011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,537 INFO epoch # 3333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010992523733875714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,565 INFO epoch # 3334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010868841942283325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,590 INFO epoch # 3335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010895190411247313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,614 INFO epoch # 3336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011041731020668522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,638 INFO epoch # 3337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010679481318220496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,663 INFO epoch # 3338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010848842182895169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,687 INFO epoch # 3339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010675269062630832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,712 INFO epoch # 3340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011406571327825077
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:11,712 INFO *** epoch 3340, rolling-avg-loss (window=10)= 0.011035286655533128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,736 INFO epoch # 3341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0107719585794257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,761 INFO epoch # 3342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010434203504701145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,785 INFO epoch # 3343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010274984582792968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,809 INFO epoch # 3344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01118221196520608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,834 INFO epoch # 3345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010838179689017124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,858 INFO epoch # 3346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010925273396424018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,882 INFO epoch # 3347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01083303191990126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,906 INFO epoch # 3348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010769702086690813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,930 INFO epoch # 3349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010683682849048637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,954 INFO epoch # 3350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010871835067518987
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:11,954 INFO *** epoch 3350, rolling-avg-loss (window=10)= 0.010758506364072672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:11,979 INFO epoch # 3351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010957886974210851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,003 INFO epoch # 3352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010596515538054518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,028 INFO epoch # 3353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0108957607371849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,052 INFO epoch # 3354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010390939540229738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,077 INFO epoch # 3355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010255040557240136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,101 INFO epoch # 3356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010672972006432246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,125 INFO epoch # 3357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010519721399759874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,150 INFO epoch # 3358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010438318204251118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,175 INFO epoch # 3359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010167411339352839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,200 INFO epoch # 3360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01105395560443867
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:12,200 INFO *** epoch 3360, rolling-avg-loss (window=10)= 0.01059485219011549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,224 INFO epoch # 3361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010777173221867997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,248 INFO epoch # 3362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010316681087715551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,273 INFO epoch # 3363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010716583070461638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,297 INFO epoch # 3364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010703216379624791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,321 INFO epoch # 3365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010646127950167283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,346 INFO epoch # 3366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011011018461431377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,370 INFO epoch # 3367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011080136653617956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,395 INFO epoch # 3368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010608697746647522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,419 INFO epoch # 3369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0100944106670795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,444 INFO epoch # 3370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010312420097761787
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:12,444 INFO *** epoch 3370, rolling-avg-loss (window=10)= 0.01062664653363754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,469 INFO epoch # 3371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011029313725885004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,493 INFO epoch # 3372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010613689257297665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,518 INFO epoch # 3373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010474014183273539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,542 INFO epoch # 3374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010871266160393134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,566 INFO epoch # 3375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01042180648073554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,591 INFO epoch # 3376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010828040845808573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,616 INFO epoch # 3377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010544986951572355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,642 INFO epoch # 3378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010402643500128761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,666 INFO epoch # 3379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010476533389010001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,691 INFO epoch # 3380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011105896890512668
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:12,691 INFO *** epoch 3380, rolling-avg-loss (window=10)= 0.010676819138461724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,716 INFO epoch # 3381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011089909734437242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,740 INFO epoch # 3382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011063547179219313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,764 INFO epoch # 3383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010984257576637901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,789 INFO epoch # 3384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010635936865583062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,814 INFO epoch # 3385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010628690564772114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,839 INFO epoch # 3386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010967465961584821
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,864 INFO epoch # 3387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01067710750066908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,888 INFO epoch # 3388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010292983497492969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,913 INFO epoch # 3389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010548014775849879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,937 INFO epoch # 3390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010387309099314734
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:12,937 INFO *** epoch 3390, rolling-avg-loss (window=10)= 0.010727522275556112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,962 INFO epoch # 3391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01026052790984977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:12,985 INFO epoch # 3392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010198101022979245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,010 INFO epoch # 3393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011031846559490077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,034 INFO epoch # 3394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012360891123535112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,059 INFO epoch # 3395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.012730223155813292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,084 INFO epoch # 3396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010929270982160233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,108 INFO epoch # 3397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010634079691953957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,133 INFO epoch # 3398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010271787425153889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,157 INFO epoch # 3399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010231911728624254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,182 INFO epoch # 3400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010359361331211403
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:13,182 INFO *** epoch 3400, rolling-avg-loss (window=10)= 0.010900800093077123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,206 INFO epoch # 3401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010245236393529922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,230 INFO epoch # 3402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010125943823368289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,254 INFO epoch # 3403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011174576735356823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,279 INFO epoch # 3404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010576850560028106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,304 INFO epoch # 3405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010357283666962758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,328 INFO epoch # 3406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010658026207238436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,353 INFO epoch # 3407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010084154695505276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,377 INFO epoch # 3408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011004568805219606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,402 INFO epoch # 3409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010502348945010453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,426 INFO epoch # 3410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010620892207953148
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:13,426 INFO *** epoch 3410, rolling-avg-loss (window=10)= 0.01053498820401728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,450 INFO epoch # 3411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010282791234203614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,474 INFO epoch # 3412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010229842329863459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,499 INFO epoch # 3413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00995640737528447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,524 INFO epoch # 3414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010169065702939406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,548 INFO epoch # 3415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010692674972233362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,573 INFO epoch # 3416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010788399828015827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,597 INFO epoch # 3417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010640267064445652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,622 INFO epoch # 3418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010635920683853328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,646 INFO epoch # 3419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010500616044737399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,671 INFO epoch # 3420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010726203341619112
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:13,671 INFO *** epoch 3420, rolling-avg-loss (window=10)= 0.010462218857719563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,695 INFO epoch # 3421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010627520576235838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,720 INFO epoch # 3422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011035905699827708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,744 INFO epoch # 3423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010612392739858478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,769 INFO epoch # 3424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010688751295674592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,793 INFO epoch # 3425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011046428349800408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,817 INFO epoch # 3426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01141472370363772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,842 INFO epoch # 3427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010835949156899005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,866 INFO epoch # 3428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010734142269939184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,890 INFO epoch # 3429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010959931074467022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,915 INFO epoch # 3430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010326079922378995
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:13,915 INFO *** epoch 3430, rolling-avg-loss (window=10)= 0.010828182478871896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,939 INFO epoch # 3431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010124758788151667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,964 INFO epoch # 3432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010032573220087215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:13,988 INFO epoch # 3433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010546606063144282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,013 INFO epoch # 3434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010665960900951177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,037 INFO epoch # 3435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010640704640536569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,061 INFO epoch # 3436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011238742896239273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,086 INFO epoch # 3437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011367428829544224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,110 INFO epoch # 3438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0110088569635991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,135 INFO epoch # 3439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01103066059295088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,159 INFO epoch # 3440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010644272028002888
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:14,159 INFO *** epoch 3440, rolling-avg-loss (window=10)= 0.010730056492320728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,185 INFO epoch # 3441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010850052392925136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,210 INFO epoch # 3442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010537382753682323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,234 INFO epoch # 3443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010990524882799946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,259 INFO epoch # 3444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010566582772298716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,283 INFO epoch # 3445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010559825459495187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,307 INFO epoch # 3446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010199712181929499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,331 INFO epoch # 3447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010175351519137621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,356 INFO epoch # 3448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010594101448077708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,381 INFO epoch # 3449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01042496573063545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,405 INFO epoch # 3450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009994773514335975
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:14,405 INFO *** epoch 3450, rolling-avg-loss (window=10)= 0.010489327265531756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,430 INFO epoch # 3451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010720529535319656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,454 INFO epoch # 3452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010493055939150508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,478 INFO epoch # 3453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010127274479600601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,503 INFO epoch # 3454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00991347385570407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,527 INFO epoch # 3455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01056221804174129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,551 INFO epoch # 3456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010241584343020804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,576 INFO epoch # 3457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010058596337330528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,601 INFO epoch # 3458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010165658357436769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,626 INFO epoch # 3459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010390604948042892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,650 INFO epoch # 3460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010187026622588746
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:14,651 INFO *** epoch 3460, rolling-avg-loss (window=10)= 0.010286002245993586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,675 INFO epoch # 3461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010820009279996157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,699 INFO epoch # 3462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010233419365249574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,724 INFO epoch # 3463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010096112760948017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,747 INFO epoch # 3464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009939351861248724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,771 INFO epoch # 3465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010574212603387423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,795 INFO epoch # 3466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010797641487442888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,820 INFO epoch # 3467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010340069798985496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,844 INFO epoch # 3468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009929580381140113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,869 INFO epoch # 3469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009754159487783909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,893 INFO epoch # 3470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00998543921741657
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:14,894 INFO *** epoch 3470, rolling-avg-loss (window=10)= 0.010246999624359886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,918 INFO epoch # 3471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01046187499014195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,942 INFO epoch # 3472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00999611169390846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,966 INFO epoch # 3473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010437052987981588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:14,990 INFO epoch # 3474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010869695179280825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,014 INFO epoch # 3475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010660334999556653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,038 INFO epoch # 3476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0109553602305823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,063 INFO epoch # 3477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010067794544738717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,089 INFO epoch # 3478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010025233452324755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,114 INFO epoch # 3479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010583109047729522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,138 INFO epoch # 3480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010892936857999302
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:15,138 INFO *** epoch 3480, rolling-avg-loss (window=10)= 0.010494950398424407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,162 INFO epoch # 3481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011027725471649319
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,187 INFO epoch # 3482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010577749148069415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,211 INFO epoch # 3483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011855969452881254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,235 INFO epoch # 3484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010815655477927066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,260 INFO epoch # 3485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010651368596882094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,286 INFO epoch # 3486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010275961452862248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,310 INFO epoch # 3487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009925324106006883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,335 INFO epoch # 3488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010746188963821623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,359 INFO epoch # 3489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010297093031113036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,384 INFO epoch # 3490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010385194065747783
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:15,384 INFO *** epoch 3490, rolling-avg-loss (window=10)= 0.010655822976696072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,408 INFO epoch # 3491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009682391551905312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,433 INFO epoch # 3492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01004287542309612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,457 INFO epoch # 3493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010831889696419239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,482 INFO epoch # 3494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009975173699785955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,506 INFO epoch # 3495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010440767291584052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,531 INFO epoch # 3496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010604591341689229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,555 INFO epoch # 3497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010250485735014081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,580 INFO epoch # 3498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010359333420637995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,604 INFO epoch # 3499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010614442202495411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,629 INFO epoch # 3500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010244762743241154
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:15,629 INFO *** epoch 3500, rolling-avg-loss (window=10)= 0.010304671310586854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,654 INFO epoch # 3501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010396316894912161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,679 INFO epoch # 3502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010565068441792391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,703 INFO epoch # 3503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009989442260120995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,728 INFO epoch # 3504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01025293002021499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,752 INFO epoch # 3505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009558968507917598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,777 INFO epoch # 3506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009863799663435202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,801 INFO epoch # 3507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010365652182372287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,826 INFO epoch # 3508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010960491810692474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,851 INFO epoch # 3509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010795977985253558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,875 INFO epoch # 3510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010868611832847819
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:15,875 INFO *** epoch 3510, rolling-avg-loss (window=10)= 0.010361725959955947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,901 INFO epoch # 3511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010507138722459786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,925 INFO epoch # 3512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011214217331144027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,950 INFO epoch # 3513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010826671117683873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,975 INFO epoch # 3514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01043298291915562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:15,999 INFO epoch # 3515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010134102078154683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,024 INFO epoch # 3516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010079960382427089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,048 INFO epoch # 3517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009798061626497656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,073 INFO epoch # 3518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009917177303577773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,098 INFO epoch # 3519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00988811171555426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,123 INFO epoch # 3520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009594075607310515
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:16,124 INFO *** epoch 3520, rolling-avg-loss (window=10)= 0.010239249880396529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,158 INFO epoch # 3521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009673803302575834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,187 INFO epoch # 3522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010474196475115605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,216 INFO epoch # 3523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010676242454792373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,244 INFO epoch # 3524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009749136312166229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,269 INFO epoch # 3525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010036761843366548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,293 INFO epoch # 3526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009996614084229805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,318 INFO epoch # 3527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00998309429269284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,343 INFO epoch # 3528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009707156030344777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,369 INFO epoch # 3529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009854607662418857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,394 INFO epoch # 3530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009999189947848208
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:16,395 INFO *** epoch 3530, rolling-avg-loss (window=10)= 0.010015080240555108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,423 INFO epoch # 3531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010528382350457832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,453 INFO epoch # 3532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010332519857911393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,481 INFO epoch # 3533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010535703913774341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,506 INFO epoch # 3534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010340551918488927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,531 INFO epoch # 3535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010238978276902344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,556 INFO epoch # 3536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01028141364804469
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,581 INFO epoch # 3537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0100060968688922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,606 INFO epoch # 3538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01024698252149392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,640 INFO epoch # 3539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010319100896595046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,668 INFO epoch # 3540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01015044073574245
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:16,668 INFO *** epoch 3540, rolling-avg-loss (window=10)= 0.010298017098830315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,697 INFO epoch # 3541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011075319082010537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,726 INFO epoch # 3542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010579388617770746
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,768 INFO epoch # 3543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010527278340305202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,813 INFO epoch # 3544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010330182820325717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,856 INFO epoch # 3545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010422877938253805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,899 INFO epoch # 3546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010260295508487616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,943 INFO epoch # 3547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009662721015047282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:16,987 INFO epoch # 3548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010660144747816958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,019 INFO epoch # 3549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01081892114598304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,046 INFO epoch # 3550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0102854740107432
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:17,047 INFO *** epoch 3550, rolling-avg-loss (window=10)= 0.01046226032267441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,072 INFO epoch # 3551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010263836069498211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,098 INFO epoch # 3552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01003408178803511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,124 INFO epoch # 3553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010765709273982793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,149 INFO epoch # 3554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009838709753239527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,174 INFO epoch # 3555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010340483815525658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,199 INFO epoch # 3556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010175336254178546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,229 INFO epoch # 3557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010032934602349997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,257 INFO epoch # 3558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00998011037154356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,298 INFO epoch # 3559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010112393196322955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,322 INFO epoch # 3560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01021484297234565
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:17,323 INFO *** epoch 3560, rolling-avg-loss (window=10)= 0.010175843809702202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,351 INFO epoch # 3561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010029615499661304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,379 INFO epoch # 3562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009774822581675835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,408 INFO epoch # 3563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009881646314170212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,435 INFO epoch # 3564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010051247256342322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,462 INFO epoch # 3565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010246521880617365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,490 INFO epoch # 3566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00990033608104568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,518 INFO epoch # 3567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00984165706904605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,544 INFO epoch # 3568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010464134771609679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,568 INFO epoch # 3569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010478488038643263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,592 INFO epoch # 3570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009803951266803779
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:17,592 INFO *** epoch 3570, rolling-avg-loss (window=10)= 0.01004724207596155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,616 INFO epoch # 3571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010239579612971283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,640 INFO epoch # 3572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009711484090075828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,664 INFO epoch # 3573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009729892022733111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,688 INFO epoch # 3574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01031657075509429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,711 INFO epoch # 3575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010118410034920089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,735 INFO epoch # 3576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00976698509475682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,759 INFO epoch # 3577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009817812519031577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,783 INFO epoch # 3578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010015276449848898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,806 INFO epoch # 3579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010580227521131746
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,831 INFO epoch # 3580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009857769284280948
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:17,831 INFO *** epoch 3580, rolling-avg-loss (window=10)= 0.010015400738484458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,855 INFO epoch # 3581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009993438914534636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,880 INFO epoch # 3582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009513541277556214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,904 INFO epoch # 3583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009479098531301133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,928 INFO epoch # 3584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009712428960483521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,952 INFO epoch # 3585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010171509748033714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,975 INFO epoch # 3586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010061412845971063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:17,999 INFO epoch # 3587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01031041276291944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,023 INFO epoch # 3588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010047839241451584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,047 INFO epoch # 3589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009966230252757668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,071 INFO epoch # 3590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010017787368269637
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:18,071 INFO *** epoch 3590, rolling-avg-loss (window=10)= 0.009927369990327862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,095 INFO epoch # 3591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00979636395641137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,119 INFO epoch # 3592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009919768301188014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,143 INFO epoch # 3593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0103095390368253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,167 INFO epoch # 3594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010927591036306694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,190 INFO epoch # 3595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009688016376458108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,214 INFO epoch # 3596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01014954422134906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,238 INFO epoch # 3597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010599092696793377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,262 INFO epoch # 3598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009933864392223768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,285 INFO epoch # 3599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009860945385298692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,309 INFO epoch # 3600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009846876753726974
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:18,309 INFO *** epoch 3600, rolling-avg-loss (window=10)= 0.010103160215658135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,333 INFO epoch # 3601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009634930072934367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,357 INFO epoch # 3602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010102981323143467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,382 INFO epoch # 3603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010694915516069159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,406 INFO epoch # 3604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010669962925021537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,431 INFO epoch # 3605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010320406669052318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,456 INFO epoch # 3606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009998687426559627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,480 INFO epoch # 3607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009994249092414975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,505 INFO epoch # 3608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00954890742286807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,529 INFO epoch # 3609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01011924774502404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,553 INFO epoch # 3610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010117543904925697
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:18,553 INFO *** epoch 3610, rolling-avg-loss (window=10)= 0.010120183209801325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,577 INFO epoch # 3611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009838172831223346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,601 INFO epoch # 3612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010116061952430755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,625 INFO epoch # 3613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009823835920542479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,649 INFO epoch # 3614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009566300504957326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,673 INFO epoch # 3615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01037874691246543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,698 INFO epoch # 3616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010200708762567956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,722 INFO epoch # 3617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009632949368096888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,746 INFO epoch # 3618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009602266844012775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,770 INFO epoch # 3619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00923752296512248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,794 INFO epoch # 3620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009934180357959121
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:18,794 INFO *** epoch 3620, rolling-avg-loss (window=10)= 0.009833074641937856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,819 INFO epoch # 3621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009577557008014992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,843 INFO epoch # 3622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010165637504542246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,867 INFO epoch # 3623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009367119360831566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,891 INFO epoch # 3624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009491925404290669
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,915 INFO epoch # 3625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009368947852635756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,940 INFO epoch # 3626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00954436454048846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:18,973 INFO epoch # 3627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009956408997823019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,010 INFO epoch # 3628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009560239384882152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,039 INFO epoch # 3629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009516983685898595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,068 INFO epoch # 3630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009878529745037667
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:19,068 INFO *** epoch 3630, rolling-avg-loss (window=10)= 0.009642771348444512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,096 INFO epoch # 3631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009578549768775702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,124 INFO epoch # 3632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010131280330824666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,157 INFO epoch # 3633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009859929014055524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,189 INFO epoch # 3634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009486417271546088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,218 INFO epoch # 3635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009756963161635213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,248 INFO epoch # 3636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010120729173650034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,276 INFO epoch # 3637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010458448072313331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,302 INFO epoch # 3638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009757799198268913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,326 INFO epoch # 3639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010269466278259642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,350 INFO epoch # 3640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010803016091813333
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:19,350 INFO *** epoch 3640, rolling-avg-loss (window=10)= 0.010022259836114244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,374 INFO epoch # 3641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010276754655933473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,399 INFO epoch # 3642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010149729525437579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,423 INFO epoch # 3643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009540760554955341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,447 INFO epoch # 3644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009686761652119458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,472 INFO epoch # 3645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010447969092638232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,495 INFO epoch # 3646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010382375898188911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,519 INFO epoch # 3647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009893336988170631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,544 INFO epoch # 3648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010353450459660962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,570 INFO epoch # 3649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010203175450442359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,595 INFO epoch # 3650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009651969958213158
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:19,595 INFO *** epoch 3650, rolling-avg-loss (window=10)= 0.010058628423576011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,623 INFO epoch # 3651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009568185232637916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,651 INFO epoch # 3652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009670380284660496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,679 INFO epoch # 3653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009926878279657103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,706 INFO epoch # 3654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009898994263494387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,730 INFO epoch # 3655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009416356755536981
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,756 INFO epoch # 3656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009928751271218061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,781 INFO epoch # 3657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010153050840017386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,807 INFO epoch # 3658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01019652807735838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,831 INFO epoch # 3659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01004529964120593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,857 INFO epoch # 3660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010010196609073319
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:19,857 INFO *** epoch 3660, rolling-avg-loss (window=10)= 0.009881462125485995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,882 INFO epoch # 3661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01011298548837658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,914 INFO epoch # 3662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010068387884530239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,938 INFO epoch # 3663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009523137829091866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,962 INFO epoch # 3664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00973454417544417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:19,987 INFO epoch # 3665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009507500617473852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,011 INFO epoch # 3666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009900789373205043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,035 INFO epoch # 3667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009809065755689517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,059 INFO epoch # 3668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0095773972425377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,083 INFO epoch # 3669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009569032554281875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,107 INFO epoch # 3670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009313188391388394
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:20,107 INFO *** epoch 3670, rolling-avg-loss (window=10)= 0.009711602931201924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,131 INFO epoch # 3671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009468764779740013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,158 INFO epoch # 3672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00925104710040614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,185 INFO epoch # 3673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01051524153444916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,209 INFO epoch # 3674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0096643964498071
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,234 INFO epoch # 3675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009553781201248057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,258 INFO epoch # 3676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009502689841610845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,282 INFO epoch # 3677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009597398311598226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,306 INFO epoch # 3678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009637520415708423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,330 INFO epoch # 3679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00978281325660646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,355 INFO epoch # 3680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011503856774652377
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:20,355 INFO *** epoch 3680, rolling-avg-loss (window=10)= 0.00984775096658268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,378 INFO epoch # 3681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010089752817293629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,403 INFO epoch # 3682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009803757930058055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,427 INFO epoch # 3683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00978528532141354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,452 INFO epoch # 3684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009285170919611119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,476 INFO epoch # 3685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009440420937607996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,500 INFO epoch # 3686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01013490225886926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,524 INFO epoch # 3687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009461326451855712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,548 INFO epoch # 3688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010066669769003056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,572 INFO epoch # 3689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00994856761826668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,596 INFO epoch # 3690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010250710503896698
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:20,596 INFO *** epoch 3690, rolling-avg-loss (window=10)= 0.009826656452787574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,621 INFO epoch # 3691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011805486865341663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,650 INFO epoch # 3692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009601228492101654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,679 INFO epoch # 3693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009512636504950933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,707 INFO epoch # 3694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009580983372870833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,735 INFO epoch # 3695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010131889343028888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,762 INFO epoch # 3696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009748702330398373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,786 INFO epoch # 3697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01038472201616969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,810 INFO epoch # 3698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009555535827530548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,833 INFO epoch # 3699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009433916333364323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,864 INFO epoch # 3700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009108664962695912
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:20,865 INFO *** epoch 3700, rolling-avg-loss (window=10)= 0.009886376604845282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,892 INFO epoch # 3701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009373075765324757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,920 INFO epoch # 3702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00939279532758519
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,946 INFO epoch # 3703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00950200560328085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,972 INFO epoch # 3704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009444787559914403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:20,997 INFO epoch # 3705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009099818329559639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,023 INFO epoch # 3706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009421712631592527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,047 INFO epoch # 3707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010095881640154403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,071 INFO epoch # 3708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009496163635049015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,095 INFO epoch # 3709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009117794048506767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,119 INFO epoch # 3710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009230902534909546
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:21,119 INFO *** epoch 3710, rolling-avg-loss (window=10)= 0.00941749370758771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,143 INFO epoch # 3711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00983730802545324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,167 INFO epoch # 3712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00976155573152937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,191 INFO epoch # 3713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00933663263276685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,214 INFO epoch # 3714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009921461503836326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,238 INFO epoch # 3715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010076662380015478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,262 INFO epoch # 3716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009507134047453292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,287 INFO epoch # 3717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009764062342583202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,311 INFO epoch # 3718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009762127563590184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,334 INFO epoch # 3719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009885716455755755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,359 INFO epoch # 3720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010096087644342333
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:21,359 INFO *** epoch 3720, rolling-avg-loss (window=10)= 0.009794874832732604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,383 INFO epoch # 3721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009359136340208352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,407 INFO epoch # 3722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009192256344249472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,431 INFO epoch # 3723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00903360235679429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,455 INFO epoch # 3724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009521243628114462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,479 INFO epoch # 3725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009401712959515862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,503 INFO epoch # 3726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009600297125871293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,527 INFO epoch # 3727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009415658823854756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,551 INFO epoch # 3728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009548212576191872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,575 INFO epoch # 3729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010179811157286167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,599 INFO epoch # 3730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009770602351636626
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:21,599 INFO *** epoch 3730, rolling-avg-loss (window=10)= 0.009502253366372315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,624 INFO epoch # 3731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010172952323046047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,648 INFO epoch # 3732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009717459390230943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,671 INFO epoch # 3733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009200101150781848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,695 INFO epoch # 3734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009506895126833115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,719 INFO epoch # 3735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009560988255543634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,743 INFO epoch # 3736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009561728140397463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,767 INFO epoch # 3737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010129781425348483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,791 INFO epoch # 3738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009062719997018576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,815 INFO epoch # 3739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009434785504708998
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,839 INFO epoch # 3740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00937660779163707
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:21,839 INFO *** epoch 3740, rolling-avg-loss (window=10)= 0.009572401910554617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,864 INFO epoch # 3741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00953591027064249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,888 INFO epoch # 3742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009270661132177338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,912 INFO epoch # 3743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008936231432016939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,936 INFO epoch # 3744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009375675013870932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,962 INFO epoch # 3745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009209390824253205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:21,987 INFO epoch # 3746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009172484220471233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,012 INFO epoch # 3747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009250980147044174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,037 INFO epoch # 3748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009178408021398354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,062 INFO epoch # 3749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009122610557824373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,087 INFO epoch # 3750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00916551880800398
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:22,087 INFO *** epoch 3750, rolling-avg-loss (window=10)= 0.009221787042770302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,111 INFO epoch # 3751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010263591684633866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,135 INFO epoch # 3752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009590276662493125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,159 INFO epoch # 3753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009811562398681417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,184 INFO epoch # 3754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00967127706098836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,207 INFO epoch # 3755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009398451700690202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,232 INFO epoch # 3756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009661856689490378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,255 INFO epoch # 3757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010104127701197285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,279 INFO epoch # 3758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009759141074027866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,302 INFO epoch # 3759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009301780912210234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,326 INFO epoch # 3760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01004131673835218
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:22,326 INFO *** epoch 3760, rolling-avg-loss (window=10)= 0.00976033826227649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,351 INFO epoch # 3761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009869665023870766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,375 INFO epoch # 3762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0096331500026281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,399 INFO epoch # 3763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009083968761842698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,423 INFO epoch # 3764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00950424857728649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,447 INFO epoch # 3765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009545860717480537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,470 INFO epoch # 3766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009551254086545669
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,494 INFO epoch # 3767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009421465867490042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,518 INFO epoch # 3768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009735382802318782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,542 INFO epoch # 3769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009070225154573563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,566 INFO epoch # 3770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009304757331847213
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:22,566 INFO *** epoch 3770, rolling-avg-loss (window=10)= 0.009471997832588386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,590 INFO epoch # 3771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009287062464863993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,614 INFO epoch # 3772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009239370920113288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,639 INFO epoch # 3773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009317911542893853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,665 INFO epoch # 3774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009480545813858043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,691 INFO epoch # 3775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009259692662453745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,719 INFO epoch # 3776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009064617057447322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,747 INFO epoch # 3777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010047467279946432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,775 INFO epoch # 3778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00952272693393752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,800 INFO epoch # 3779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009302325146563817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,824 INFO epoch # 3780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0091551186342258
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:22,825 INFO *** epoch 3780, rolling-avg-loss (window=10)= 0.009367683845630382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,849 INFO epoch # 3781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009118492300331127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,873 INFO epoch # 3782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009827787289395928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,896 INFO epoch # 3783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010004960146034136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,920 INFO epoch # 3784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00971692424354842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,943 INFO epoch # 3785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009644610414397903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,968 INFO epoch # 3786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009456244588363916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:22,991 INFO epoch # 3787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008767280232859775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,016 INFO epoch # 3788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009008342938614078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,040 INFO epoch # 3789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009552618736051954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,063 INFO epoch # 3790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009671072504715994
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:23,063 INFO *** epoch 3790, rolling-avg-loss (window=10)= 0.009476833339431324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,087 INFO epoch # 3791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009290691261412576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,111 INFO epoch # 3792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009223095839843154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,134 INFO epoch # 3793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010301191578037106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,158 INFO epoch # 3794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009983203970477916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,182 INFO epoch # 3795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009973802822059952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,206 INFO epoch # 3796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009471661993302405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,230 INFO epoch # 3797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008950285919127055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,254 INFO epoch # 3798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009250769166101236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,278 INFO epoch # 3799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010159014964301605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,301 INFO epoch # 3800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009662861717515625
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:23,301 INFO *** epoch 3800, rolling-avg-loss (window=10)= 0.009626657923217864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,325 INFO epoch # 3801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009022385806019884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,349 INFO epoch # 3802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00929752092633862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,373 INFO epoch # 3803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010148134155315347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,397 INFO epoch # 3804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009050403532455675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,421 INFO epoch # 3805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00971379796101246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,445 INFO epoch # 3806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01096094463719055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,470 INFO epoch # 3807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009863912484433968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,494 INFO epoch # 3808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009456330197281204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,517 INFO epoch # 3809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009045976519701071
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,541 INFO epoch # 3810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00919715446070768
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:23,541 INFO *** epoch 3810, rolling-avg-loss (window=10)= 0.009575656068045646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,565 INFO epoch # 3811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00998369937587995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,589 INFO epoch # 3812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009699270274722949
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,613 INFO epoch # 3813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009397864283528179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,637 INFO epoch # 3814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009927489591063932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,661 INFO epoch # 3815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009651155662140809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,685 INFO epoch # 3816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009756369036040269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,709 INFO epoch # 3817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009196070532198064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,733 INFO epoch # 3818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009292971080867574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,756 INFO epoch # 3819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008865385541866999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,780 INFO epoch # 3820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009161186411802191
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:23,780 INFO *** epoch 3820, rolling-avg-loss (window=10)= 0.009493146179011092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,803 INFO epoch # 3821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008912232180591673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,827 INFO epoch # 3822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009561778977513313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,851 INFO epoch # 3823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009963060379959643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,875 INFO epoch # 3824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008901202832930721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,899 INFO epoch # 3825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009155921412457246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,922 INFO epoch # 3826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009122675663093105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,946 INFO epoch # 3827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009050765220308676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,970 INFO epoch # 3828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008701472223037854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:23,996 INFO epoch # 3829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009025718842167407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,020 INFO epoch # 3830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010175675604841672
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:24,020 INFO *** epoch 3830, rolling-avg-loss (window=10)= 0.00925705033369013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,044 INFO epoch # 3831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009421898386790417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,067 INFO epoch # 3832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009391000770847313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,091 INFO epoch # 3833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00886352075758623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,115 INFO epoch # 3834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009391216954099946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,139 INFO epoch # 3835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009059498610440642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,162 INFO epoch # 3836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008928190029109828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,186 INFO epoch # 3837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00884258674341254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,210 INFO epoch # 3838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009459377615712583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,234 INFO epoch # 3839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009166936230030842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,258 INFO epoch # 3840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009000253354315646
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:24,258 INFO *** epoch 3840, rolling-avg-loss (window=10)= 0.009152447945234598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,282 INFO epoch # 3841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009343143217847683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,306 INFO epoch # 3842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009265410772059113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,330 INFO epoch # 3843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008979863399872556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,354 INFO epoch # 3844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008846537093631923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,378 INFO epoch # 3845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009290455607697368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,402 INFO epoch # 3846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009205978909449186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,425 INFO epoch # 3847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008751069573918357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,449 INFO epoch # 3848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008805182078504004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,472 INFO epoch # 3849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009665924269938841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,496 INFO epoch # 3850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00921037106309086
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:24,496 INFO *** epoch 3850, rolling-avg-loss (window=10)= 0.00913639359860099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,520 INFO epoch # 3851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009135360989603214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,544 INFO epoch # 3852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009288210058002733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,568 INFO epoch # 3853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009127195502514951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,591 INFO epoch # 3854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009161771522485651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,616 INFO epoch # 3855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00896805764932651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,640 INFO epoch # 3856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00887152443465311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,664 INFO epoch # 3857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009451518417336047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,687 INFO epoch # 3858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008953083874075674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,711 INFO epoch # 3859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008899599451979157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,735 INFO epoch # 3860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009415057458681986
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:24,735 INFO *** epoch 3860, rolling-avg-loss (window=10)= 0.009127137935865903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,759 INFO epoch # 3861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009532792595564388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,783 INFO epoch # 3862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010176646683248691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,808 INFO epoch # 3863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009302569851570297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,832 INFO epoch # 3864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009275611315388232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,856 INFO epoch # 3865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009397497095051222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,880 INFO epoch # 3866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009417442619451322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,904 INFO epoch # 3867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009394529712153599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,928 INFO epoch # 3868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009654970257543027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,952 INFO epoch # 3869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009412205254193395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:24,976 INFO epoch # 3870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00962942573823966
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:24,976 INFO *** epoch 3870, rolling-avg-loss (window=10)= 0.009519369112240383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,000 INFO epoch # 3871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00926761602750048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,024 INFO epoch # 3872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009003783008665778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,047 INFO epoch # 3873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009231120791810099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,071 INFO epoch # 3874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009680081959231757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,095 INFO epoch # 3875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009646440666983835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,119 INFO epoch # 3876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00986517732962966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,143 INFO epoch # 3877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008880466019036248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,168 INFO epoch # 3878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009230514398950618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,192 INFO epoch # 3879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008819240902084857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,216 INFO epoch # 3880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008735601819353178
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:25,216 INFO *** epoch 3880, rolling-avg-loss (window=10)= 0.009236004292324651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,239 INFO epoch # 3881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008907691721105948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,263 INFO epoch # 3882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00915168372739572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,287 INFO epoch # 3883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009114981148741208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,310 INFO epoch # 3884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009315787734522019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,334 INFO epoch # 3885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009150461351964623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,358 INFO epoch # 3886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008566264572436921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,383 INFO epoch # 3887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008566937409341335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,407 INFO epoch # 3888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009041011857334524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,431 INFO epoch # 3889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009436421125428751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,454 INFO epoch # 3890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008908392541343346
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:25,455 INFO *** epoch 3890, rolling-avg-loss (window=10)= 0.009015963318961439
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,478 INFO epoch # 3891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00878016730712261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,502 INFO epoch # 3892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00878557410032954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,525 INFO epoch # 3893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008840375929139555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,549 INFO epoch # 3894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00894423751742579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,573 INFO epoch # 3895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00899191663484089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,597 INFO epoch # 3896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008905768016120419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,621 INFO epoch # 3897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008829269288980868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,645 INFO epoch # 3898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009170981997158378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,668 INFO epoch # 3899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009254109085304663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,692 INFO epoch # 3900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009442545371712185
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:25,692 INFO *** epoch 3900, rolling-avg-loss (window=10)= 0.00899449452481349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,716 INFO epoch # 3901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009017432224936783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,739 INFO epoch # 3902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009034261165652424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,762 INFO epoch # 3903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009063163117389195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,786 INFO epoch # 3904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008632575743831694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,810 INFO epoch # 3905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009664301862358116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,834 INFO epoch # 3906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00922944810736226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,858 INFO epoch # 3907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008904870759579353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,882 INFO epoch # 3908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009023569480632432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,906 INFO epoch # 3909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009287173204938881
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,929 INFO epoch # 3910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010070480522699654
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:25,929 INFO *** epoch 3910, rolling-avg-loss (window=10)= 0.009192727618938079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,952 INFO epoch # 3911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009688202058896422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:25,976 INFO epoch # 3912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009586705498804804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,000 INFO epoch # 3913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008801268268143758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,024 INFO epoch # 3914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009077904505829792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,048 INFO epoch # 3915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00899059804942226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,072 INFO epoch # 3916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009064905476407148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,096 INFO epoch # 3917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009467532639973797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,119 INFO epoch # 3918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008929035328037571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,143 INFO epoch # 3919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009268065332435071
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,167 INFO epoch # 3920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008771063323365524
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:26,167 INFO *** epoch 3920, rolling-avg-loss (window=10)= 0.009164528048131615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,191 INFO epoch # 3921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009021551260957494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,215 INFO epoch # 3922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008792445907602087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,238 INFO epoch # 3923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009121493130805902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,262 INFO epoch # 3924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008875270970747806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,286 INFO epoch # 3925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00891237015457591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,310 INFO epoch # 3926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00864274994819425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,333 INFO epoch # 3927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00849840205773944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,358 INFO epoch # 3928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00909085915191099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,381 INFO epoch # 3929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008663655855343677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,405 INFO epoch # 3930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00867352235945873
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:26,405 INFO *** epoch 3930, rolling-avg-loss (window=10)= 0.008829232079733629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,429 INFO epoch # 3931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008895213053619955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,453 INFO epoch # 3932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008614640384621453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,477 INFO epoch # 3933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008526472644007299
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,500 INFO epoch # 3934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009255843673599884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,524 INFO epoch # 3935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008953695025411434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,547 INFO epoch # 3936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009107239631703123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,571 INFO epoch # 3937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008824258446111344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,595 INFO epoch # 3938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008966357374447398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,618 INFO epoch # 3939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009009868801513221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,642 INFO epoch # 3940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008706424923730083
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:26,642 INFO *** epoch 3940, rolling-avg-loss (window=10)= 0.00888600139587652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,667 INFO epoch # 3941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008923690562369302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,690 INFO epoch # 3942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009126031989580952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,714 INFO epoch # 3943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00897355611959938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,738 INFO epoch # 3944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0088762842133292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,761 INFO epoch # 3945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008883314614649862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,785 INFO epoch # 3946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00923964395042276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,808 INFO epoch # 3947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00930490485188784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,832 INFO epoch # 3948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009161773821688257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,857 INFO epoch # 3949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009265103028155863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,880 INFO epoch # 3950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00882701374212047
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:26,881 INFO *** epoch 3950, rolling-avg-loss (window=10)= 0.009058131689380388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,904 INFO epoch # 3951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008817995054414496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,928 INFO epoch # 3952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008536912195268087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,952 INFO epoch # 3953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009282069193432108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:26,976 INFO epoch # 3954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009221843662089668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,000 INFO epoch # 3955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008785110876488034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,023 INFO epoch # 3956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009047906860359944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,047 INFO epoch # 3957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008770967469899915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,071 INFO epoch # 3958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009050744993146509
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,095 INFO epoch # 3959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008800747717032209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,119 INFO epoch # 3960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008799780945992097
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:27,119 INFO *** epoch 3960, rolling-avg-loss (window=10)= 0.008911407896812306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,143 INFO epoch # 3961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009050872817169875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,166 INFO epoch # 3962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008750087406951934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,190 INFO epoch # 3963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008782657096162438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,213 INFO epoch # 3964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008652581396745518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,237 INFO epoch # 3965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008753591486311052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,261 INFO epoch # 3966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009133747982559726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,285 INFO epoch # 3967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008812860141915735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,309 INFO epoch # 3968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00890592680661939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,333 INFO epoch # 3969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008485657868732233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,357 INFO epoch # 3970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008408567424339708
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:27,357 INFO *** epoch 3970, rolling-avg-loss (window=10)= 0.008773655042750761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,381 INFO epoch # 3971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00902698737627361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,405 INFO epoch # 3972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00864796220412245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,428 INFO epoch # 3973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008911914381315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,452 INFO epoch # 3974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00917383782507386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,475 INFO epoch # 3975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008835847329464741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,499 INFO epoch # 3976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009050598338944837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,523 INFO epoch # 3977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0090490064030746
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,547 INFO epoch # 3978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009085333134862594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,571 INFO epoch # 3979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008632039294752758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,595 INFO epoch # 3980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0087027546542231
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:27,595 INFO *** epoch 3980, rolling-avg-loss (window=10)= 0.008911628094210755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,619 INFO epoch # 3981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009759476452018134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,642 INFO epoch # 3982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009504460278549232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,666 INFO epoch # 3983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008893912599887699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,690 INFO epoch # 3984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009053485890035518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,713 INFO epoch # 3985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009715779189718887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,737 INFO epoch # 3986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.01030341166188009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,761 INFO epoch # 3987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010218120689387433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,785 INFO epoch # 3988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009438665438210592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,808 INFO epoch # 3989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009184013193589635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,832 INFO epoch # 3990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00867124788055662
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:27,832 INFO *** epoch 3990, rolling-avg-loss (window=10)= 0.009474257327383384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,856 INFO epoch # 3991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008550437109079212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,881 INFO epoch # 3992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008617282008344773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,904 INFO epoch # 3993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00884469086304307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,928 INFO epoch # 3994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008817265552352183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,952 INFO epoch # 3995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010530029991059564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:27,976 INFO epoch # 3996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009762108631548472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,000 INFO epoch # 3997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009049793909071013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,024 INFO epoch # 3998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008979311372968368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,048 INFO epoch # 3999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008871982521668542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,074 INFO epoch # 4000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008882082911441103
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:28,074 INFO *** epoch 4000, rolling-avg-loss (window=10)= 0.00909049848705763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,102 INFO epoch # 4001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009181299988995306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,129 INFO epoch # 4002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008781342941801995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,156 INFO epoch # 4003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009137432512943633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,180 INFO epoch # 4004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00848440171830589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,203 INFO epoch # 4005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008603656649938785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,227 INFO epoch # 4006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00878420897788601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,251 INFO epoch # 4007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009958048816770315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,274 INFO epoch # 4008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008805068755464163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,298 INFO epoch # 4009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008626662878668867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,322 INFO epoch # 4010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009557022582157515
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:28,322 INFO *** epoch 4010, rolling-avg-loss (window=10)= 0.008991914582293248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,346 INFO epoch # 4011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009651354819652624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,370 INFO epoch # 4012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00886221930704778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,394 INFO epoch # 4013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00893685850314796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,418 INFO epoch # 4014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009240327577572316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,441 INFO epoch # 4015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009251283496269025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,465 INFO epoch # 4016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.011773255493608303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,489 INFO epoch # 4017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009596647447324358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,512 INFO epoch # 4018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008721170546778012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,536 INFO epoch # 4019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008791725544142537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,561 INFO epoch # 4020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00884539320395561
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:28,561 INFO *** epoch 4020, rolling-avg-loss (window=10)= 0.009367023593949853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,585 INFO epoch # 4021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010097487262100913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,609 INFO epoch # 4022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009527775822789408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,633 INFO epoch # 4023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008928924158681184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,656 INFO epoch # 4024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008873732425854541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,680 INFO epoch # 4025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008573932165745646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,704 INFO epoch # 4026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008368487942789216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,727 INFO epoch # 4027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008300615241751075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,751 INFO epoch # 4028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008532603664207272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,774 INFO epoch # 4029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008908296469599009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,798 INFO epoch # 4030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009530952200293541
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:28,798 INFO *** epoch 4030, rolling-avg-loss (window=10)= 0.008964280735381181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,822 INFO epoch # 4031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009285962078138255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,847 INFO epoch # 4032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008704515283170622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,871 INFO epoch # 4033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008827069963444956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,895 INFO epoch # 4034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00844661103474209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,919 INFO epoch # 4035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00907195368927205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,943 INFO epoch # 4036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009009531859192066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,966 INFO epoch # 4037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008603028007200919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:28,990 INFO epoch # 4038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008204762983950786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,013 INFO epoch # 4039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008955569952377118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,037 INFO epoch # 4040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008605166316556279
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:29,037 INFO *** epoch 4040, rolling-avg-loss (window=10)= 0.008771417116804514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,062 INFO epoch # 4041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008159854260156862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,086 INFO epoch # 4042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008965120017819572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,109 INFO epoch # 4043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00825894404988503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,133 INFO epoch # 4044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008812134125037119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,157 INFO epoch # 4045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009345849663077388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,180 INFO epoch # 4046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009834105068875942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,204 INFO epoch # 4047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008853686551447026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,228 INFO epoch # 4048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008508405757311266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,252 INFO epoch # 4049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008181821394828148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,275 INFO epoch # 4050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008554853731766343
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:29,276 INFO *** epoch 4050, rolling-avg-loss (window=10)= 0.00874747746202047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,300 INFO epoch # 4051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00972564572293777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,323 INFO epoch # 4052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00904209163491032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,347 INFO epoch # 4053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008894217586203013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,371 INFO epoch # 4054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00844177279213909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,395 INFO epoch # 4055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00832184789760504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,419 INFO epoch # 4056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008776724615017883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,443 INFO epoch # 4057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009213135272148065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,466 INFO epoch # 4058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008857070206431672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,490 INFO epoch # 4059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008449558008578606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,514 INFO epoch # 4060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009092385109397583
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:29,514 INFO *** epoch 4060, rolling-avg-loss (window=10)= 0.008881444884536904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,538 INFO epoch # 4061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008936746788094752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,561 INFO epoch # 4062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008946220303187147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,585 INFO epoch # 4063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008374654076760635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,609 INFO epoch # 4064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00875154254026711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,633 INFO epoch # 4065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008204863363062032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,657 INFO epoch # 4066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00855255718488479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,681 INFO epoch # 4067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00960672425571829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,705 INFO epoch # 4068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009141253292909823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,729 INFO epoch # 4069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00839017640100792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,753 INFO epoch # 4070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010455938216182403
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:29,753 INFO *** epoch 4070, rolling-avg-loss (window=10)= 0.008936067642207491
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,777 INFO epoch # 4071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009025284372910392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,800 INFO epoch # 4072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00908838571922388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,823 INFO epoch # 4073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008816706016659737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,848 INFO epoch # 4074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008805807505268604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,874 INFO epoch # 4075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008979578953585587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,898 INFO epoch # 4076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008793326080194674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,922 INFO epoch # 4077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008372173972020391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,946 INFO epoch # 4078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008541630231775343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,969 INFO epoch # 4079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008336452032381203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:29,993 INFO epoch # 4080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008556144661270082
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:29,993 INFO *** epoch 4080, rolling-avg-loss (window=10)= 0.00873154895452899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,017 INFO epoch # 4081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008469628737657331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,040 INFO epoch # 4082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009486543451203033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,064 INFO epoch # 4083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009027648520714138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,088 INFO epoch # 4084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009340926466393284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,112 INFO epoch # 4085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008523331591277383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,136 INFO epoch # 4086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008528561906132381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,160 INFO epoch # 4087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008087142567092087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,184 INFO epoch # 4088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008387201560253743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,207 INFO epoch # 4089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00835602395090973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,230 INFO epoch # 4090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008912753721233457
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:30,231 INFO *** epoch 4090, rolling-avg-loss (window=10)= 0.008711976247286656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,254 INFO epoch # 4091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008453456219285727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,278 INFO epoch # 4092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008674889599205926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,301 INFO epoch # 4093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00853128349990584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,325 INFO epoch # 4094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009549074253300205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,349 INFO epoch # 4095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00883448452805169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,373 INFO epoch # 4096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008708026900421828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,397 INFO epoch # 4097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008441509722615592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,420 INFO epoch # 4098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00858591291034827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,444 INFO epoch # 4099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008391227842366789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,467 INFO epoch # 4100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008427689048403408
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:30,467 INFO *** epoch 4100, rolling-avg-loss (window=10)= 0.008659755452390528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,490 INFO epoch # 4101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008376565987418871
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,514 INFO epoch # 4102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008397813413466793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,538 INFO epoch # 4103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008620212262030691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,562 INFO epoch # 4104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008790242864051834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,586 INFO epoch # 4105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008971289782493841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,609 INFO epoch # 4106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008307693875394762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,633 INFO epoch # 4107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008243708129157312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,658 INFO epoch # 4108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008458468408207409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,681 INFO epoch # 4109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008337366569321603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,705 INFO epoch # 4110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008591832884121686
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:30,705 INFO *** epoch 4110, rolling-avg-loss (window=10)= 0.00850951941756648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,729 INFO epoch # 4111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008717493270523846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,753 INFO epoch # 4112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008477388517349027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,777 INFO epoch # 4113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008438240554824006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,800 INFO epoch # 4114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00947917862504255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,824 INFO epoch # 4115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008892953206668608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,848 INFO epoch # 4116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008495812639012001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,871 INFO epoch # 4117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008488036874041427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,895 INFO epoch # 4118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008709604138857685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,919 INFO epoch # 4119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008913951583963353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,943 INFO epoch # 4120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008167631902324501
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:30,943 INFO *** epoch 4120, rolling-avg-loss (window=10)= 0.0086780291312607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,967 INFO epoch # 4121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008832671606796794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:30,991 INFO epoch # 4122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0084259506984381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,015 INFO epoch # 4123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008425608408288099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,039 INFO epoch # 4124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008760995740885846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,062 INFO epoch # 4125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00834176792704966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,086 INFO epoch # 4126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00852554298035102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,109 INFO epoch # 4127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008960258623119444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,133 INFO epoch # 4128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009424422038136981
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,157 INFO epoch # 4129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009360031646792777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,181 INFO epoch # 4130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008573240163968876
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:31,181 INFO *** epoch 4130, rolling-avg-loss (window=10)= 0.00876304898338276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,205 INFO epoch # 4131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008140783495036885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,229 INFO epoch # 4132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00832072549383156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,252 INFO epoch # 4133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008966402005171403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,276 INFO epoch # 4134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009483202724368311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,299 INFO epoch # 4135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008787866514467169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,323 INFO epoch # 4136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008665253320941702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,346 INFO epoch # 4137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008651351585285738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,370 INFO epoch # 4138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008284190262202173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,394 INFO epoch # 4139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008380396771826781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,419 INFO epoch # 4140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008443832703051157
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:31,419 INFO *** epoch 4140, rolling-avg-loss (window=10)= 0.008612400487618289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,443 INFO epoch # 4141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008184591097233351
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,466 INFO epoch # 4142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008393543255806435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,490 INFO epoch # 4143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00881931582262041
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,513 INFO epoch # 4144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008602205656643491
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,536 INFO epoch # 4145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008327017436386086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,560 INFO epoch # 4146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008385625951632392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,584 INFO epoch # 4147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008307465875986964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,607 INFO epoch # 4148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008196529626729898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,633 INFO epoch # 4149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00843772868392989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,657 INFO epoch # 4150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008166083702235483
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:31,657 INFO *** epoch 4150, rolling-avg-loss (window=10)= 0.00838201071092044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,681 INFO epoch # 4151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00892018506419845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,705 INFO epoch # 4152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008456110648694448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,728 INFO epoch # 4153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008191554268705659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,752 INFO epoch # 4154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008852084138197824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,776 INFO epoch # 4155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008202074721339159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,800 INFO epoch # 4156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008914657795685343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,824 INFO epoch # 4157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008716818163520657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,848 INFO epoch # 4158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0080682379193604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,872 INFO epoch # 4159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008228954626247287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,896 INFO epoch # 4160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008448031949228607
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:31,896 INFO *** epoch 4160, rolling-avg-loss (window=10)= 0.008499870929517784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,920 INFO epoch # 4161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008576376123528462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,944 INFO epoch # 4162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008881042987923138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,968 INFO epoch # 4163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009274490839743521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:31,991 INFO epoch # 4164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009158080603810959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,015 INFO epoch # 4165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008512115746270865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,039 INFO epoch # 4166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008597193227615207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,063 INFO epoch # 4167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008969976501248311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,086 INFO epoch # 4168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008540814837033395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,110 INFO epoch # 4169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008138999110087752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,134 INFO epoch # 4170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008181148383300751
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:32,134 INFO *** epoch 4170, rolling-avg-loss (window=10)= 0.008683023836056236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,157 INFO epoch # 4171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008084047818556428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,181 INFO epoch # 4172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008194560556148645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,204 INFO epoch # 4173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008663324057124555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,228 INFO epoch # 4174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007970120233949274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,252 INFO epoch # 4175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00813318229484139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,276 INFO epoch # 4176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008181835190043785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,300 INFO epoch # 4177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008130653128318954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,323 INFO epoch # 4178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008624742913525552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,347 INFO epoch # 4179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008366268812096678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,371 INFO epoch # 4180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008328837706358172
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:32,371 INFO *** epoch 4180, rolling-avg-loss (window=10)= 0.008267757271096344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,395 INFO epoch # 4181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008515284076565877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,419 INFO epoch # 4182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009113099142268766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,443 INFO epoch # 4183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008506160258548334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,467 INFO epoch # 4184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008282021859486122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,491 INFO epoch # 4185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00834409970411798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,514 INFO epoch # 4186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008349637559149414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,538 INFO epoch # 4187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008142638944264036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,562 INFO epoch # 4188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008694609481608495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,586 INFO epoch # 4189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008522405732946936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,610 INFO epoch # 4190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008765124592173379
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:32,610 INFO *** epoch 4190, rolling-avg-loss (window=10)= 0.008523508135112934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,633 INFO epoch # 4191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00886177085340023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,657 INFO epoch # 4192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00873176370805595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,681 INFO epoch # 4193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00817680977343116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,705 INFO epoch # 4194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008132414062856697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,729 INFO epoch # 4195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008022426169191021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,753 INFO epoch # 4196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008174496309948154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,776 INFO epoch # 4197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008738384116441011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,800 INFO epoch # 4198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008772561523073819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,823 INFO epoch # 4199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008617932093329728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,846 INFO epoch # 4200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008325155227794312
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:32,846 INFO *** epoch 4200, rolling-avg-loss (window=10)= 0.008455371383752209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,870 INFO epoch # 4201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008014193925191648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,894 INFO epoch # 4202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008249297381553333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,918 INFO epoch # 4203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009006879685330205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,942 INFO epoch # 4204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008419675032200757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,966 INFO epoch # 4205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007894441827374976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:32,990 INFO epoch # 4206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008037431311095133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,013 INFO epoch # 4207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008355315614608116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,037 INFO epoch # 4208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008043272391660139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,060 INFO epoch # 4209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008588392862293404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,084 INFO epoch # 4210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00820631766691804
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:33,084 INFO *** epoch 4210, rolling-avg-loss (window=10)= 0.008281521769822575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,108 INFO epoch # 4211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008016198386030737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,132 INFO epoch # 4212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008314971448271535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,156 INFO epoch # 4213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008391589879465755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,179 INFO epoch # 4214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009048474697920028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,203 INFO epoch # 4215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00840983101807069
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,227 INFO epoch # 4216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008243854696047492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,251 INFO epoch # 4217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008451885150861926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,274 INFO epoch # 4218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00862715112452861
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,298 INFO epoch # 4219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009126116419793107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,322 INFO epoch # 4220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008231466490542516
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:33,322 INFO *** epoch 4220, rolling-avg-loss (window=10)= 0.00848615393115324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,346 INFO epoch # 4221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008381248189834878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,370 INFO epoch # 4222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008459858814603649
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,394 INFO epoch # 4223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008564220683183521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,418 INFO epoch # 4224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008313316495332401
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,441 INFO epoch # 4225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007887656400271226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,465 INFO epoch # 4226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007944285222038161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,488 INFO epoch # 4227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008309719130920712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,512 INFO epoch # 4228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008149283945385832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,536 INFO epoch # 4229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008374664044822566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,560 INFO epoch # 4230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008401338825933635
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:33,560 INFO *** epoch 4230, rolling-avg-loss (window=10)= 0.008278559175232658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,584 INFO epoch # 4231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008310664547025226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,608 INFO epoch # 4232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008122715866193175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,632 INFO epoch # 4233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008638145052827895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,655 INFO epoch # 4234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00809426860359963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,679 INFO epoch # 4235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008424154366366565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,702 INFO epoch # 4236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00796929484931752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,727 INFO epoch # 4237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0078456242554239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,750 INFO epoch # 4238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00788639031816274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,774 INFO epoch # 4239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009121453200350516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,798 INFO epoch # 4240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00841113764909096
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:33,798 INFO *** epoch 4240, rolling-avg-loss (window=10)= 0.008282384870835813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,821 INFO epoch # 4241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008281809801701456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,846 INFO epoch # 4242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008377247097087093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,869 INFO epoch # 4243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007986222211911809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,893 INFO epoch # 4244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00832642475143075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,917 INFO epoch # 4245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007747661969915498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,941 INFO epoch # 4246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008739967612200417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,965 INFO epoch # 4247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008752901114348788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:33,988 INFO epoch # 4248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007972074454301037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,012 INFO epoch # 4249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008541141141904518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,036 INFO epoch # 4250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00839443600125378
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:34,036 INFO *** epoch 4250, rolling-avg-loss (window=10)= 0.008311988615605514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,060 INFO epoch # 4251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008935589576140046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,084 INFO epoch # 4252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0089247303258162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,108 INFO epoch # 4253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008810363193333615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,132 INFO epoch # 4254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008690198505064473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,156 INFO epoch # 4255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008933482931752224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,180 INFO epoch # 4256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008558712957892567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,204 INFO epoch # 4257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008199694486393128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,228 INFO epoch # 4258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008131321548717096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,251 INFO epoch # 4259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008176887582521886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,275 INFO epoch # 4260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008536223002010956
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:34,275 INFO *** epoch 4260, rolling-avg-loss (window=10)= 0.00858972041096422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,298 INFO epoch # 4261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008053158991970122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,322 INFO epoch # 4262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008427420645602979
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,346 INFO epoch # 4263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008433197406702675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,370 INFO epoch # 4264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008365032968868036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,394 INFO epoch # 4265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008699018726474606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,417 INFO epoch # 4266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009036389521497767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,441 INFO epoch # 4267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008168756467057392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,465 INFO epoch # 4268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008411695474933367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,489 INFO epoch # 4269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008497412709402852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,512 INFO epoch # 4270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00841390123241581
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:34,512 INFO *** epoch 4270, rolling-avg-loss (window=10)= 0.00845059841449256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,536 INFO epoch # 4271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008551921127946116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,561 INFO epoch # 4272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00777480687975185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,586 INFO epoch # 4273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007808609771018382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,610 INFO epoch # 4274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007872804315411486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,634 INFO epoch # 4275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008244430093327537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,658 INFO epoch # 4276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008303494985739235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,681 INFO epoch # 4277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008055455793510191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,705 INFO epoch # 4278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007942949763673823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,728 INFO epoch # 4279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007992184910108335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,752 INFO epoch # 4280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008405960725212935
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:34,752 INFO *** epoch 4280, rolling-avg-loss (window=10)= 0.00809526183656999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,776 INFO epoch # 4281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008129510904836934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,800 INFO epoch # 4282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00828629985335283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,823 INFO epoch # 4283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008168734384526033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,848 INFO epoch # 4284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008233018925238866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,872 INFO epoch # 4285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009076654045202304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,895 INFO epoch # 4286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007812472009391058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,919 INFO epoch # 4287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008426031155977398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,943 INFO epoch # 4288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00816860615304904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,966 INFO epoch # 4289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007863551851187367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:34,989 INFO epoch # 4290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007881624020228628
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:34,990 INFO *** epoch 4290, rolling-avg-loss (window=10)= 0.008204650330299046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,013 INFO epoch # 4291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00816160609247163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,037 INFO epoch # 4292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008146249325363897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,061 INFO epoch # 4293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008665793662657961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,085 INFO epoch # 4294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007939814553537872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,109 INFO epoch # 4295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007839288082323037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,132 INFO epoch # 4296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008128087356453761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,156 INFO epoch # 4297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008211179701902438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,179 INFO epoch # 4298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008463934122119099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,203 INFO epoch # 4299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008357499958947301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,226 INFO epoch # 4300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007991530597792007
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:35,227 INFO *** epoch 4300, rolling-avg-loss (window=10)= 0.0081904983453569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,251 INFO epoch # 4301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008100561710307375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,274 INFO epoch # 4302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009184463720885105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,298 INFO epoch # 4303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009043774152814876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,322 INFO epoch # 4304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008633560239104554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,346 INFO epoch # 4305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008537112909834832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,370 INFO epoch # 4306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008560139496694319
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,393 INFO epoch # 4307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00807001676002983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,417 INFO epoch # 4308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008498647817759775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,441 INFO epoch # 4309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008015444342163391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,465 INFO epoch # 4310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00790256754407892
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:35,465 INFO *** epoch 4310, rolling-avg-loss (window=10)= 0.008454628869367297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,489 INFO epoch # 4311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007876643743657041
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,513 INFO epoch # 4312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008401153318118304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,537 INFO epoch # 4313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007866522682888899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,561 INFO epoch # 4314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007989726233063266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,585 INFO epoch # 4315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007878916163463145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,609 INFO epoch # 4316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007965616052388214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,633 INFO epoch # 4317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008249330072430894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,658 INFO epoch # 4318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007889890519436449
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,682 INFO epoch # 4319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008037526953557972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,705 INFO epoch # 4320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00811438625532901
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:35,706 INFO *** epoch 4320, rolling-avg-loss (window=10)= 0.00802697119943332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,729 INFO epoch # 4321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008079358020040672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,753 INFO epoch # 4322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008152713438903447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,777 INFO epoch # 4323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008082395674136933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,800 INFO epoch # 4324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007991147678694688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,824 INFO epoch # 4325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00785730190546019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,848 INFO epoch # 4326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007715727399045136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,872 INFO epoch # 4327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008267361226899084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,896 INFO epoch # 4328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008040836546570063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,920 INFO epoch # 4329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008008930220967159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,944 INFO epoch # 4330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007986969569174107
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:35,944 INFO *** epoch 4330, rolling-avg-loss (window=10)= 0.008018274167989147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,968 INFO epoch # 4331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008146761058014818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:35,992 INFO epoch # 4332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008316412458952982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,015 INFO epoch # 4333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008316924453538377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,039 INFO epoch # 4334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008329949763719924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,063 INFO epoch # 4335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008145807587425224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,087 INFO epoch # 4336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008305832445330452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,111 INFO epoch # 4337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009533715710858814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,134 INFO epoch # 4338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009425865195225924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,158 INFO epoch # 4339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009746306997840293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,182 INFO epoch # 4340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008135735224641394
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:36,182 INFO *** epoch 4340, rolling-avg-loss (window=10)= 0.00864033108955482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,205 INFO epoch # 4341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007718570464930963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,229 INFO epoch # 4342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007718950517300982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,252 INFO epoch # 4343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008353025092219468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,275 INFO epoch # 4344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0082218054230907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,299 INFO epoch # 4345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00825820978207048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,323 INFO epoch # 4346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008370957832084969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,347 INFO epoch # 4347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008492439272231422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,371 INFO epoch # 4348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008046040456974879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,395 INFO epoch # 4349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008568167992052622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,418 INFO epoch # 4350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00839462447038386
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:36,418 INFO *** epoch 4350, rolling-avg-loss (window=10)= 0.008214279130334035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,442 INFO epoch # 4351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007710906938882545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,465 INFO epoch # 4352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008213237320887856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,489 INFO epoch # 4353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007999080487934407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,512 INFO epoch # 4354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008448747183138039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,536 INFO epoch # 4355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007959286107507069
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,560 INFO epoch # 4356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007731717618298717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,584 INFO epoch # 4357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00794285727897659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,608 INFO epoch # 4358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008765641920035705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,632 INFO epoch # 4359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009060679920366965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,656 INFO epoch # 4360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008508240614901297
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:36,656 INFO *** epoch 4360, rolling-avg-loss (window=10)= 0.008234039539092918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,679 INFO epoch # 4361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008352175704203546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,703 INFO epoch # 4362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00880065613455372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,727 INFO epoch # 4363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008143065373587888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,751 INFO epoch # 4364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008365063127712347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,775 INFO epoch # 4365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008183702499081846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,799 INFO epoch # 4366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008037110899749678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,823 INFO epoch # 4367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007627053433679976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,848 INFO epoch # 4368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076902199507458135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,871 INFO epoch # 4369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007809402668499388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,895 INFO epoch # 4370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007843711202440318
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:36,895 INFO *** epoch 4370, rolling-avg-loss (window=10)= 0.008085216099425451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,919 INFO epoch # 4371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007880173216108233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,943 INFO epoch # 4372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007508964350563474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,967 INFO epoch # 4373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00827867204498034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:36,991 INFO epoch # 4374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008134841671562754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,015 INFO epoch # 4375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008311353958561085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,039 INFO epoch # 4376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00788475514491438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,063 INFO epoch # 4377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00796272503794171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,086 INFO epoch # 4378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008161636229488067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,110 INFO epoch # 4379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0078090414936013985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,134 INFO epoch # 4380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007785228342982009
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:37,134 INFO *** epoch 4380, rolling-avg-loss (window=10)= 0.007971739149070345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,158 INFO epoch # 4381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007586088257085066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,182 INFO epoch # 4382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007784234396240208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,206 INFO epoch # 4383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076998499789624475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,230 INFO epoch # 4384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007875105038692709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,254 INFO epoch # 4385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007570385481812991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,277 INFO epoch # 4386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007638300616235938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,301 INFO epoch # 4387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00805939783458598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,324 INFO epoch # 4388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0082865735021187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,349 INFO epoch # 4389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007922052005596925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,372 INFO epoch # 4390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007844311177905183
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:37,373 INFO *** epoch 4390, rolling-avg-loss (window=10)= 0.007826629828923615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,397 INFO epoch # 4391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008206360958865844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,420 INFO epoch # 4392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008064074121648446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,444 INFO epoch # 4393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007722939881205093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,468 INFO epoch # 4394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007818856043741107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,491 INFO epoch # 4395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007976984394190367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,515 INFO epoch # 4396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00822823001362849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,538 INFO epoch # 4397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00800436062127119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,562 INFO epoch # 4398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0080804983881535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,586 INFO epoch # 4399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008035134844249114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,610 INFO epoch # 4400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008126210450427607
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:37,610 INFO *** epoch 4400, rolling-avg-loss (window=10)= 0.008026364971738077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,634 INFO epoch # 4401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007847751629014965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,657 INFO epoch # 4402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007733451406238601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,681 INFO epoch # 4403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007979998306836933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,705 INFO epoch # 4404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007579993784020189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,728 INFO epoch # 4405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007813031523255631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,751 INFO epoch # 4406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007681681432586629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,774 INFO epoch # 4407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008037167914153542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,798 INFO epoch # 4408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008082518746959977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,822 INFO epoch # 4409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007919354378827848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,846 INFO epoch # 4410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007975200409418903
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:37,846 INFO *** epoch 4410, rolling-avg-loss (window=10)= 0.007865014953131322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,870 INFO epoch # 4411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007871517685998697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,893 INFO epoch # 4412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007887591869803146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,917 INFO epoch # 4413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007999178378668148
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,940 INFO epoch # 4414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008604980146628805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,963 INFO epoch # 4415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008208271785406396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:37,987 INFO epoch # 4416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007882112913648598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,011 INFO epoch # 4417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007810003895428963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,035 INFO epoch # 4418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007800407693139277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,059 INFO epoch # 4419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008626612441730686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,083 INFO epoch # 4420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00796154682757333
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:38,083 INFO *** epoch 4420, rolling-avg-loss (window=10)= 0.008065222363802605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,107 INFO epoch # 4421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008718296587176155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,130 INFO epoch # 4422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008207606777432375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,154 INFO epoch # 4423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007871800182329025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,177 INFO epoch # 4424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009152050246484578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,200 INFO epoch # 4425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00858094527211506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,224 INFO epoch # 4426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007946809411805589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,248 INFO epoch # 4427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007788096874719486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,272 INFO epoch # 4428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008287235723400954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,295 INFO epoch # 4429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008354061137652025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,319 INFO epoch # 4430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008007891250599641
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:38,319 INFO *** epoch 4430, rolling-avg-loss (window=10)= 0.008291479346371488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,343 INFO epoch # 4431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008312050849781372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,367 INFO epoch # 4432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008387446025153622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,390 INFO epoch # 4433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007465774746378884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,414 INFO epoch # 4434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007360170588071924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,438 INFO epoch # 4435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007432405327563174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,461 INFO epoch # 4436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007999644309165888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,485 INFO epoch # 4437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008040432978305034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,509 INFO epoch # 4438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007742408473859541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,532 INFO epoch # 4439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007999225264939014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,556 INFO epoch # 4440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007841193932108581
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:38,556 INFO *** epoch 4440, rolling-avg-loss (window=10)= 0.007858075249532703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,580 INFO epoch # 4441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007858528777433094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,604 INFO epoch # 4442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007642603915883228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,627 INFO epoch # 4443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007482974637241568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,651 INFO epoch # 4444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007814417454937939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,675 INFO epoch # 4445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007563448561995756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,698 INFO epoch # 4446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007859399207518436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,722 INFO epoch # 4447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008968060879851691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,745 INFO epoch # 4448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008010919656953774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,769 INFO epoch # 4449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007742494119156618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,792 INFO epoch # 4450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076239086338318884
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:38,792 INFO *** epoch 4450, rolling-avg-loss (window=10)= 0.007856675584480399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,815 INFO epoch # 4451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00823450007737847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,838 INFO epoch # 4452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008247711695730686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,863 INFO epoch # 4453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007848912689951248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,887 INFO epoch # 4454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007977452514751349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,910 INFO epoch # 4455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008321130066178739
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,934 INFO epoch # 4456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007670050348679069
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,958 INFO epoch # 4457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007807287001924124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:38,981 INFO epoch # 4458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007884736769483425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,005 INFO epoch # 4459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007964754768181592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,028 INFO epoch # 4460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007925937010440975
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:39,028 INFO *** epoch 4460, rolling-avg-loss (window=10)= 0.007988247294269968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,052 INFO epoch # 4461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076324726032908075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,076 INFO epoch # 4462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00788395442214096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,100 INFO epoch # 4463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007989283905772027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,123 INFO epoch # 4464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007662547766813077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,147 INFO epoch # 4465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007713528088061139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,170 INFO epoch # 4466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008349699462996796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,194 INFO epoch # 4467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007578066171845421
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,217 INFO epoch # 4468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00802054037922062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,240 INFO epoch # 4469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007995581203431357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,263 INFO epoch # 4470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007799552964570466
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:39,263 INFO *** epoch 4470, rolling-avg-loss (window=10)= 0.007862522696814266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,287 INFO epoch # 4471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007431612219079398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,311 INFO epoch # 4472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007818352583853994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,335 INFO epoch # 4473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008157143238349818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,361 INFO epoch # 4474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007761374235997209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,385 INFO epoch # 4475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073166519578080624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,409 INFO epoch # 4476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00766953553102212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,432 INFO epoch # 4477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007745377282844856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,456 INFO epoch # 4478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007668345599086024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,480 INFO epoch # 4479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008635204430902377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,504 INFO epoch # 4480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00921009638113901
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:39,504 INFO *** epoch 4480, rolling-avg-loss (window=10)= 0.007941369346008286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,528 INFO epoch # 4481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077664795753662474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,552 INFO epoch # 4482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008256027722381987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,575 INFO epoch # 4483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007860188496124465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,599 INFO epoch # 4484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007978608344274107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,623 INFO epoch # 4485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007754282429232262
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,647 INFO epoch # 4486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007682953029870987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,670 INFO epoch # 4487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007729524419119116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,694 INFO epoch # 4488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008066040601988789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,718 INFO epoch # 4489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00835867179557681
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,742 INFO epoch # 4490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007648955142940395
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:39,742 INFO *** epoch 4490, rolling-avg-loss (window=10)= 0.007910173155687516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,766 INFO epoch # 4491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007876820614910685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,790 INFO epoch # 4492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007641887714271434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,814 INFO epoch # 4493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007607144958456047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,838 INFO epoch # 4494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007892004323366564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,862 INFO epoch # 4495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008382148167584091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,885 INFO epoch # 4496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00826984839659417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,909 INFO epoch # 4497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007386718760244548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,933 INFO epoch # 4498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007204581947007682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,957 INFO epoch # 4499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0076263922528596595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:39,981 INFO epoch # 4500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00836010258353781
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:39,981 INFO *** epoch 4500, rolling-avg-loss (window=10)= 0.00782476497188327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,005 INFO epoch # 4501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007948053869768046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,029 INFO epoch # 4502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007754927224596031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,053 INFO epoch # 4503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077851439564256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,076 INFO epoch # 4504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00759661132178735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,100 INFO epoch # 4505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007474090802134015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,124 INFO epoch # 4506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007321998942643404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,147 INFO epoch # 4507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007414607236569282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,171 INFO epoch # 4508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007441711437422782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,195 INFO epoch # 4509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007740747765637934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,219 INFO epoch # 4510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007869979395763949
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:40,219 INFO *** epoch 4510, rolling-avg-loss (window=10)= 0.007634787195274839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,243 INFO epoch # 4511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008153013070113957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,267 INFO epoch # 4512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008451311550743412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,290 INFO epoch # 4513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00782395178976003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,314 INFO epoch # 4514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074361364604556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,338 INFO epoch # 4515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007529829177656211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,362 INFO epoch # 4516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008139255682181101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,386 INFO epoch # 4517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007731500169029459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,410 INFO epoch # 4518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007810981769580394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,434 INFO epoch # 4519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007720261288341135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,457 INFO epoch # 4520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007655296947632451
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:40,457 INFO *** epoch 4520, rolling-avg-loss (window=10)= 0.007845153790549375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,481 INFO epoch # 4521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007371568848611787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,505 INFO epoch # 4522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00798100147221703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,528 INFO epoch # 4523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007962736191984732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,552 INFO epoch # 4524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007711814105277881
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,576 INFO epoch # 4525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008318034757394344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,600 INFO epoch # 4526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007679249858483672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,624 INFO epoch # 4527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007693350438785274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,648 INFO epoch # 4528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007827006964362226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,672 INFO epoch # 4529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008769473504798952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,696 INFO epoch # 4530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007960090981214307
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:40,696 INFO *** epoch 4530, rolling-avg-loss (window=10)= 0.007927432712313021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,719 INFO epoch # 4531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007573142742330674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,743 INFO epoch # 4532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007646936268429272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,767 INFO epoch # 4533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007721707166638225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,791 INFO epoch # 4534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007625003621797077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,815 INFO epoch # 4535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008084228116786107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,842 INFO epoch # 4536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077206677524372935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,867 INFO epoch # 4537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007856590404117014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,891 INFO epoch # 4538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008120540500385687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,915 INFO epoch # 4539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007726440439000726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,938 INFO epoch # 4540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007884113692853134
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:40,938 INFO *** epoch 4540, rolling-avg-loss (window=10)= 0.007795937070477521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,962 INFO epoch # 4541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008613071775471326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:40,986 INFO epoch # 4542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008030328383028973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,010 INFO epoch # 4543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007263990424689837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,034 INFO epoch # 4544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007629331841599196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,057 INFO epoch # 4545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007763471672660671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,081 INFO epoch # 4546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00784585929068271
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,106 INFO epoch # 4547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007803055290423799
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,130 INFO epoch # 4548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0078082872641971335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,155 INFO epoch # 4549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007580646524729673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,178 INFO epoch # 4550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007579149962111842
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:41,178 INFO *** epoch 4550, rolling-avg-loss (window=10)= 0.007791719242959516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,202 INFO epoch # 4551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075372134378994815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,226 INFO epoch # 4552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008289842589874752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,250 INFO epoch # 4553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007949782055220567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,274 INFO epoch # 4554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007519257647800259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,298 INFO epoch # 4555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007284951832843944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,321 INFO epoch # 4556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007134577070246451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,348 INFO epoch # 4557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008037260369746946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,372 INFO epoch # 4558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075182663422310725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,395 INFO epoch # 4559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007811713876435533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,420 INFO epoch # 4560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007392288120172452
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:41,420 INFO *** epoch 4560, rolling-avg-loss (window=10)= 0.007647515334247146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,453 INFO epoch # 4561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007285268446139526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,496 INFO epoch # 4562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007864352366595995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,536 INFO epoch # 4563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007799901664839126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,577 INFO epoch # 4564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007560353646113072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,621 INFO epoch # 4565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007764472778944764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,666 INFO epoch # 4566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007961852425069083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,709 INFO epoch # 4567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007730550001724623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,750 INFO epoch # 4568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007851673071854748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,776 INFO epoch # 4569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007900879660155624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,800 INFO epoch # 4570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007848230859963223
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:41,800 INFO *** epoch 4570, rolling-avg-loss (window=10)= 0.007756753492139978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,824 INFO epoch # 4571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077511388444690965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,849 INFO epoch # 4572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007404303811199497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,873 INFO epoch # 4573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00760719038953539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,897 INFO epoch # 4574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007560935635410715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,922 INFO epoch # 4575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007626205951964948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,946 INFO epoch # 4576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007573710601718631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,969 INFO epoch # 4577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007261599581397604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:41,993 INFO epoch # 4578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00757324028381845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,017 INFO epoch # 4579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007662476011319086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,041 INFO epoch # 4580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007394738815492019
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:42,041 INFO *** epoch 4580, rolling-avg-loss (window=10)= 0.007541553992632543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,065 INFO epoch # 4581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072999724361579865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,089 INFO epoch # 4582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008063387256697752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,113 INFO epoch # 4583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007731805555522442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,138 INFO epoch # 4584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009965446457499638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,162 INFO epoch # 4585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007832506351405755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,186 INFO epoch # 4586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007301934885617811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,210 INFO epoch # 4587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00768808793509379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,233 INFO epoch # 4588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007939923074445687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,256 INFO epoch # 4589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007679621594434138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,280 INFO epoch # 4590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007888894564530347
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:42,280 INFO *** epoch 4590, rolling-avg-loss (window=10)= 0.007939158011140535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,303 INFO epoch # 4591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009349341213237494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,327 INFO epoch # 4592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008810447296127677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,352 INFO epoch # 4593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008010069628653582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,377 INFO epoch # 4594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007822903891792521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,401 INFO epoch # 4595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074377954078954645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,425 INFO epoch # 4596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007492742734029889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,449 INFO epoch # 4597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008157931035384536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,473 INFO epoch # 4598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073548735017539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,497 INFO epoch # 4599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007664866927370895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,521 INFO epoch # 4600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007317593204788864
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:42,521 INFO *** epoch 4600, rolling-avg-loss (window=10)= 0.007941856484103482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,545 INFO epoch # 4601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006969834954361431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,571 INFO epoch # 4602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073349389276700094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,598 INFO epoch # 4603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007901328397565521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,622 INFO epoch # 4604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008628770643554162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,646 INFO epoch # 4605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007492306969652418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,670 INFO epoch # 4606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007439342727593612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,694 INFO epoch # 4607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007345722624449991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,718 INFO epoch # 4608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007128634286345914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,742 INFO epoch # 4609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007299533848708961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,767 INFO epoch # 4610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074503065188764594
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:42,767 INFO *** epoch 4610, rolling-avg-loss (window=10)= 0.007499071989877848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,791 INFO epoch # 4611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007645976889762096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,815 INFO epoch # 4612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008403392326727044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,840 INFO epoch # 4613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008353738812729716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,863 INFO epoch # 4614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007613033754751086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,887 INFO epoch # 4615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007567203210783191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,911 INFO epoch # 4616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007321319877519272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,936 INFO epoch # 4617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073652300125104375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,960 INFO epoch # 4618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007339567833696492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:42,984 INFO epoch # 4619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074296434031566605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,008 INFO epoch # 4620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007452588753949385
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:43,009 INFO *** epoch 4620, rolling-avg-loss (window=10)= 0.007649169487558538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,033 INFO epoch # 4621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007286242143891286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,056 INFO epoch # 4622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007227400114061311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,080 INFO epoch # 4623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007443533890182152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,104 INFO epoch # 4624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007618458628712688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,127 INFO epoch # 4625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007755546590487938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,151 INFO epoch # 4626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007636715607077349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,174 INFO epoch # 4627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00766262122488115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,198 INFO epoch # 4628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071279248732025735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,222 INFO epoch # 4629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007249630922160577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,246 INFO epoch # 4630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007752391458780039
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:43,246 INFO *** epoch 4630, rolling-avg-loss (window=10)= 0.007476046545343706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,270 INFO epoch # 4631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007758886044030078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,293 INFO epoch # 4632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007822700958058704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,317 INFO epoch # 4633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007305822160560638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,342 INFO epoch # 4634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007157161657232791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,366 INFO epoch # 4635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074241800321033224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,390 INFO epoch # 4636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007388238918792922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,413 INFO epoch # 4637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007602314028190449
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,437 INFO epoch # 4638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007487280177883804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,461 INFO epoch # 4639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007662111063837074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,485 INFO epoch # 4640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00815839569986565
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:43,485 INFO *** epoch 4640, rolling-avg-loss (window=10)= 0.007576709074055543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,509 INFO epoch # 4641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008427143220615108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,532 INFO epoch # 4642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007717376844084356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,556 INFO epoch # 4643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007340706652030349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,579 INFO epoch # 4644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007505562862206716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,603 INFO epoch # 4645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007694920052017551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,627 INFO epoch # 4646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00744621306512272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,651 INFO epoch # 4647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00727800652384758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,675 INFO epoch # 4648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007269980444107205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,699 INFO epoch # 4649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007552736111392733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,723 INFO epoch # 4650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007505751396820415
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:43,723 INFO *** epoch 4650, rolling-avg-loss (window=10)= 0.007573839717224473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,747 INFO epoch # 4651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007793494631187059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,770 INFO epoch # 4652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007951072278956417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,794 INFO epoch # 4653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00785233182250522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,818 INFO epoch # 4654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073707323463167995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,843 INFO epoch # 4655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007405126925732475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,867 INFO epoch # 4656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007501322870666627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,891 INFO epoch # 4657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007654872024431825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,914 INFO epoch # 4658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00725596505071735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,938 INFO epoch # 4659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007975904583872762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,962 INFO epoch # 4660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007479364758182783
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:43,962 INFO *** epoch 4660, rolling-avg-loss (window=10)= 0.0076240187292569315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:43,985 INFO epoch # 4661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007693074170674663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,009 INFO epoch # 4662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007502702828787733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,033 INFO epoch # 4663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007377201021881774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,056 INFO epoch # 4664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007691987157159019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,080 INFO epoch # 4665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007705886499024928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,107 INFO epoch # 4666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00788645204011118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,130 INFO epoch # 4667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007872002242947929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,154 INFO epoch # 4668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073870889318641275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,178 INFO epoch # 4669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006956138931855094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,202 INFO epoch # 4670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007854577990656253
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:44,202 INFO *** epoch 4670, rolling-avg-loss (window=10)= 0.00759271118149627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,225 INFO epoch # 4671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075622792792273685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,249 INFO epoch # 4672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007046285150863696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,273 INFO epoch # 4673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069478692530537955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,297 INFO epoch # 4674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00709401259518927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,321 INFO epoch # 4675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00707186438376084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,345 INFO epoch # 4676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007362478201685008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,369 INFO epoch # 4677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007457985637302045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,393 INFO epoch # 4678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007253441246575676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,417 INFO epoch # 4679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007301855745026842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,440 INFO epoch # 4680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007445217197528109
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:44,441 INFO *** epoch 4680, rolling-avg-loss (window=10)= 0.007254328869021265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,465 INFO epoch # 4681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00802029713668162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,490 INFO epoch # 4682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007858370619942434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,517 INFO epoch # 4683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007892560970503837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,544 INFO epoch # 4684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075447042036103085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,572 INFO epoch # 4685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072705549173406325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,599 INFO epoch # 4686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006943567335838452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,623 INFO epoch # 4687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007715188403381035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,646 INFO epoch # 4688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007352832712058444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,670 INFO epoch # 4689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007250823233334813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,694 INFO epoch # 4690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007599595810461324
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:44,694 INFO *** epoch 4690, rolling-avg-loss (window=10)= 0.00754484953431529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,718 INFO epoch # 4691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007829562047845684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,742 INFO epoch # 4692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007915920396044385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,765 INFO epoch # 4693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007782808446791023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,789 INFO epoch # 4694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071712033823132515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,813 INFO epoch # 4695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007237385725602508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,837 INFO epoch # 4696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007451498255250044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,861 INFO epoch # 4697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071333342712023295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,884 INFO epoch # 4698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007594032562337816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,908 INFO epoch # 4699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007519150516600348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,932 INFO epoch # 4700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007685091411985923
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:44,932 INFO *** epoch 4700, rolling-avg-loss (window=10)= 0.007531998701597331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,956 INFO epoch # 4701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007837798453692812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:44,980 INFO epoch # 4702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00782189381425269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,004 INFO epoch # 4703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008475278809783049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,028 INFO epoch # 4704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007316155104490463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,052 INFO epoch # 4705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007203722947451752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,075 INFO epoch # 4706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072178023619926535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,099 INFO epoch # 4707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00725994429376442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,123 INFO epoch # 4708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073447699105599895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,147 INFO epoch # 4709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007298430416994961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,171 INFO epoch # 4710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071368177741533145
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:45,171 INFO *** epoch 4710, rolling-avg-loss (window=10)= 0.007491261388713611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,195 INFO epoch # 4711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007846862688893452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,218 INFO epoch # 4712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00788394103437895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,242 INFO epoch # 4713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007375775014224928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,265 INFO epoch # 4714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007309506283490919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,289 INFO epoch # 4715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00725721367780352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,312 INFO epoch # 4716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007456773295416497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,337 INFO epoch # 4717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007103282230673358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,362 INFO epoch # 4718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006965110958844889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,386 INFO epoch # 4719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006896233564475551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,410 INFO epoch # 4720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007121182919945568
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:45,410 INFO *** epoch 4720, rolling-avg-loss (window=10)= 0.007321588166814763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,434 INFO epoch # 4721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007146353469579481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,457 INFO epoch # 4722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007143012815504335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,481 INFO epoch # 4723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007600371478474699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,504 INFO epoch # 4724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008394380842219107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,528 INFO epoch # 4725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007943130782223307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,552 INFO epoch # 4726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007271510607097298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,576 INFO epoch # 4727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007407819248328451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,600 INFO epoch # 4728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007840854086680338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,624 INFO epoch # 4729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007141723210224882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,647 INFO epoch # 4730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007600017706863582
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:45,648 INFO *** epoch 4730, rolling-avg-loss (window=10)= 0.007548917424719548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,671 INFO epoch # 4731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007377796158834826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,695 INFO epoch # 4732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073717950654099695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,719 INFO epoch # 4733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007346344711550046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,743 INFO epoch # 4734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007261127066158224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,767 INFO epoch # 4735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007486608359613456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,791 INFO epoch # 4736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075943075062241405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,815 INFO epoch # 4737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075739170497399755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,840 INFO epoch # 4738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00792136498785112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,864 INFO epoch # 4739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007648820916074328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,888 INFO epoch # 4740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007246116394526325
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:45,888 INFO *** epoch 4740, rolling-avg-loss (window=10)= 0.007482819821598241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,912 INFO epoch # 4741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00698146357171936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,936 INFO epoch # 4742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067854828157578595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,960 INFO epoch # 4743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007058337403577752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:45,984 INFO epoch # 4744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007704602699959651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,009 INFO epoch # 4745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007503547036321834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,033 INFO epoch # 4746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007304637838387862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,057 INFO epoch # 4747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007452035635651555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,081 INFO epoch # 4748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007580385710753035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,105 INFO epoch # 4749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007566799446067307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,129 INFO epoch # 4750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007679824833758175
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:46,129 INFO *** epoch 4750, rolling-avg-loss (window=10)= 0.007361711699195439
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,152 INFO epoch # 4751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007700049922277685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,176 INFO epoch # 4752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007509635426686145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,200 INFO epoch # 4753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007057489623548463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,224 INFO epoch # 4754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007530574272095691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,248 INFO epoch # 4755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007282513346581254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,272 INFO epoch # 4756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007337252980505582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,296 INFO epoch # 4757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007520746607042383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,319 INFO epoch # 4758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007328017571126111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,344 INFO epoch # 4759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007319944823393598
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,368 INFO epoch # 4760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007436555220920127
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:46,368 INFO *** epoch 4760, rolling-avg-loss (window=10)= 0.007402277979417704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,391 INFO epoch # 4761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007181195804150775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,415 INFO epoch # 4762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007109156671504024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,439 INFO epoch # 4763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072074262498063035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,463 INFO epoch # 4764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007002728569204919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,487 INFO epoch # 4765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069104631402296945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,511 INFO epoch # 4766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007210453753941692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,535 INFO epoch # 4767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006992480208282359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,559 INFO epoch # 4768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007260936843522359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,583 INFO epoch # 4769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007183126734162215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,610 INFO epoch # 4770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007489181436540093
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:46,611 INFO *** epoch 4770, rolling-avg-loss (window=10)= 0.007154714941134443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,641 INFO epoch # 4771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007255194061144721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,678 INFO epoch # 4772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007215751618787181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,711 INFO epoch # 4773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006971172333578579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,736 INFO epoch # 4774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007648744962352794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,762 INFO epoch # 4775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006851436068245675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,792 INFO epoch # 4776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007326673861825839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,822 INFO epoch # 4777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007383177238807548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,853 INFO epoch # 4778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007344927187659778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,882 INFO epoch # 4779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007410064172290731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,910 INFO epoch # 4780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007513589611335192
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:46,910 INFO *** epoch 4780, rolling-avg-loss (window=10)= 0.007292073111602804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,936 INFO epoch # 4781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072985210790648125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,960 INFO epoch # 4782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007138292050512973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:46,985 INFO epoch # 4783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073181174011551775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,013 INFO epoch # 4784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007405047261272557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,048 INFO epoch # 4785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007266318767506164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,091 INFO epoch # 4786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007703350966039579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,134 INFO epoch # 4787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007358569448115304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,167 INFO epoch # 4788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007393882297037635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,193 INFO epoch # 4789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007851021895476151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,219 INFO epoch # 4790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007901386234152596
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:47,219 INFO *** epoch 4790, rolling-avg-loss (window=10)= 0.007463450740033295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,245 INFO epoch # 4791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007084952827426605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,270 INFO epoch # 4792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007158994900237303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,297 INFO epoch # 4793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007086190074915066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,325 INFO epoch # 4794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007356067908403929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,352 INFO epoch # 4795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071058422909118235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,378 INFO epoch # 4796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00679933052015258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,405 INFO epoch # 4797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006943502543435898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,429 INFO epoch # 4798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007285545856575482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,454 INFO epoch # 4799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070367441439884715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,480 INFO epoch # 4800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071541276629432105
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:47,480 INFO *** epoch 4800, rolling-avg-loss (window=10)= 0.007101129872899037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,504 INFO epoch # 4801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006837954679212999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,528 INFO epoch # 4802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007073555294482503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,553 INFO epoch # 4803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006951663359359372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,577 INFO epoch # 4804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007326960418140516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,604 INFO epoch # 4805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071674449209240265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,630 INFO epoch # 4806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008230284860474057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,657 INFO epoch # 4807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007837422410375439
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,686 INFO epoch # 4808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007633607849129476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,714 INFO epoch # 4809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006986485939705744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,741 INFO epoch # 4810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00694172375369817
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:47,741 INFO *** epoch 4810, rolling-avg-loss (window=10)= 0.00729871034855023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,766 INFO epoch # 4811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006698367287754081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,790 INFO epoch # 4812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007173961141234031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,814 INFO epoch # 4813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007236477809783537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,838 INFO epoch # 4814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007636419890332036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,862 INFO epoch # 4815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075262912578182295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,887 INFO epoch # 4816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007247255591209978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,911 INFO epoch # 4817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007363225806329865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,935 INFO epoch # 4818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006881318709929474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,959 INFO epoch # 4819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072785783049766906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:47,984 INFO epoch # 4820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073967859352706
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:47,984 INFO *** epoch 4820, rolling-avg-loss (window=10)= 0.007243868173463852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,008 INFO epoch # 4821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007027132567600347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,032 INFO epoch # 4822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008857273904141039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,056 INFO epoch # 4823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007056804206513334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,081 INFO epoch # 4824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007255474403791595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,109 INFO epoch # 4825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007514467048167717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,137 INFO epoch # 4826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071071140191634186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,162 INFO epoch # 4827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070738108770456165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,186 INFO epoch # 4828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007035998562059831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,209 INFO epoch # 4829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006723793048877269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,233 INFO epoch # 4830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006976576769375242
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:48,233 INFO *** epoch 4830, rolling-avg-loss (window=10)= 0.007262844540673541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,257 INFO epoch # 4831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00697427707928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,280 INFO epoch # 4832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006697023040032946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,304 INFO epoch # 4833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006865056588139851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,328 INFO epoch # 4834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007292938767932355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,354 INFO epoch # 4835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007276375217770692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,378 INFO epoch # 4836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071913244828465395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,402 INFO epoch # 4837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007281742320628837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,426 INFO epoch # 4838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007580276404041797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,449 INFO epoch # 4839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007128378681954928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,473 INFO epoch # 4840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006944015498447698
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:48,473 INFO *** epoch 4840, rolling-avg-loss (window=10)= 0.007123140808107564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,497 INFO epoch # 4841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006970095695578493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,520 INFO epoch # 4842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071050942278816365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,545 INFO epoch # 4843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007342603064898867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,571 INFO epoch # 4844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007589018270664383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,596 INFO epoch # 4845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007714717503404245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,621 INFO epoch # 4846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007290388013643678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,645 INFO epoch # 4847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006988477864069864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,669 INFO epoch # 4848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007091029001458082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,692 INFO epoch # 4849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007042398334306199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,716 INFO epoch # 4850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006919305102201179
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:48,716 INFO *** epoch 4850, rolling-avg-loss (window=10)= 0.007205312707810663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,740 INFO epoch # 4851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070911915972828865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,764 INFO epoch # 4852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007075543362589087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,790 INFO epoch # 4853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007099643400579225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,814 INFO epoch # 4854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068779379726038314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,840 INFO epoch # 4855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00670840385282645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,865 INFO epoch # 4856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00709965738496976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,890 INFO epoch # 4857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007944168188259937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,914 INFO epoch # 4858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007521550134697463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,938 INFO epoch # 4859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007362713382462971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,963 INFO epoch # 4860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007057884176902007
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:48,963 INFO *** epoch 4860, rolling-avg-loss (window=10)= 0.007183869345317362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:48,988 INFO epoch # 4861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007422449496516492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,012 INFO epoch # 4862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007172181394707877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,036 INFO epoch # 4863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070319663936970755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,061 INFO epoch # 4864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007065978250466287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,085 INFO epoch # 4865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007554678755695932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,109 INFO epoch # 4866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007314553688047454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,133 INFO epoch # 4867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007037071685772389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,157 INFO epoch # 4868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070524387047044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,181 INFO epoch # 4869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007507434893341269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,206 INFO epoch # 4870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007762064342387021
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:49,207 INFO *** epoch 4870, rolling-avg-loss (window=10)= 0.007292081760533619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,230 INFO epoch # 4871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00734081427071942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,254 INFO epoch # 4872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007010051609540824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,278 INFO epoch # 4873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006783179240301251
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,302 INFO epoch # 4874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007081002742779674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,325 INFO epoch # 4875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006859219916805159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,352 INFO epoch # 4876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007084064891387243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,377 INFO epoch # 4877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007055803034745622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,401 INFO epoch # 4878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068992768283351324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,425 INFO epoch # 4879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006494071029010229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,451 INFO epoch # 4880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006937224374269135
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:49,451 INFO *** epoch 4880, rolling-avg-loss (window=10)= 0.006954470793789369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,476 INFO epoch # 4881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007422681483149063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,501 INFO epoch # 4882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007180245898780413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,526 INFO epoch # 4883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007394775821012445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,550 INFO epoch # 4884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007375306267931592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,574 INFO epoch # 4885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007075649984471966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,598 INFO epoch # 4886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007250797752931248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,623 INFO epoch # 4887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007111822866136208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,648 INFO epoch # 4888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007238743273774162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,673 INFO epoch # 4889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006974815471039619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,698 INFO epoch # 4890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008290290272270795
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:49,699 INFO *** epoch 4890, rolling-avg-loss (window=10)= 0.007331512909149751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,723 INFO epoch # 4891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007288402128324378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,748 INFO epoch # 4892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006939436352695338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,772 INFO epoch # 4893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007002022859524004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,795 INFO epoch # 4894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006707745880703442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,819 INFO epoch # 4895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007326189246668946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,846 INFO epoch # 4896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00726379279512912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,870 INFO epoch # 4897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072297632723348215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,894 INFO epoch # 4898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007597229850944132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,918 INFO epoch # 4899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070274485915433615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,942 INFO epoch # 4900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007088113103236537
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:49,942 INFO *** epoch 4900, rolling-avg-loss (window=10)= 0.0071470144081104085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,966 INFO epoch # 4901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069364369119284675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:49,989 INFO epoch # 4902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00725014584895689
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,013 INFO epoch # 4903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007266311404237058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,037 INFO epoch # 4904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007238337529997807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,061 INFO epoch # 4905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00683867802581517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,086 INFO epoch # 4906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007166870163928252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,110 INFO epoch # 4907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007477010309230536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,134 INFO epoch # 4908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007032039386103861
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,158 INFO epoch # 4909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007045255319098942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,182 INFO epoch # 4910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006901501532411203
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:50,182 INFO *** epoch 4910, rolling-avg-loss (window=10)= 0.007115258643170819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,207 INFO epoch # 4911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007027896601357497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,231 INFO epoch # 4912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006705675576085923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,255 INFO epoch # 4913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006678022073174361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,279 INFO epoch # 4914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006755030306521803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,303 INFO epoch # 4915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007148083925130777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,328 INFO epoch # 4916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007073172186210286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,354 INFO epoch # 4917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0077038223535055295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,378 INFO epoch # 4918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008435185176494997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,402 INFO epoch # 4919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072819735323719215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,427 INFO epoch # 4920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006814580578065943
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:50,427 INFO *** epoch 4920, rolling-avg-loss (window=10)= 0.007162344230891904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,450 INFO epoch # 4921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007011364832578693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,475 INFO epoch # 4922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006795066605263855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,500 INFO epoch # 4923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006879968190332875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,525 INFO epoch # 4924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006751582077413332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,549 INFO epoch # 4925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067507627318263985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,573 INFO epoch # 4926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006733899084792938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,597 INFO epoch # 4927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007084117634803988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,621 INFO epoch # 4928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007074435445247218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,646 INFO epoch # 4929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006934637960512191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,670 INFO epoch # 4930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006571169615199324
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:50,670 INFO *** epoch 4930, rolling-avg-loss (window=10)= 0.006858700417797081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,694 INFO epoch # 4931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00681751129013719
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,718 INFO epoch # 4932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006794613436795771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,742 INFO epoch # 4933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006789881699660327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,766 INFO epoch # 4934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00688989766058512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,791 INFO epoch # 4935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006887346120493021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,815 INFO epoch # 4936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00726602970098611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,840 INFO epoch # 4937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00690185627900064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,865 INFO epoch # 4938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007508535192755517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,889 INFO epoch # 4939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00940267565601971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,913 INFO epoch # 4940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007302764926862437
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:50,913 INFO *** epoch 4940, rolling-avg-loss (window=10)= 0.0072561111963295845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,938 INFO epoch # 4941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007108814967068611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,962 INFO epoch # 4942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068583123429561965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:50,986 INFO epoch # 4943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006998697259405162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,010 INFO epoch # 4944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006674251526419539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,034 INFO epoch # 4945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008072241260379087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,058 INFO epoch # 4946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008696024371602107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,082 INFO epoch # 4947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007379537884844467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,105 INFO epoch # 4948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00683800017577596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,129 INFO epoch # 4949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067914235187345184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,153 INFO epoch # 4950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006508449747343548
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:51,153 INFO *** epoch 4950, rolling-avg-loss (window=10)= 0.0071925753054529196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,177 INFO epoch # 4951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006878979009343311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,202 INFO epoch # 4952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006703902181470767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,226 INFO epoch # 4953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007019315809884574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,250 INFO epoch # 4954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006648417314863764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,274 INFO epoch # 4955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006559224573720712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,297 INFO epoch # 4956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066897447177325375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,321 INFO epoch # 4957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006718879558320623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,346 INFO epoch # 4958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070734993714722805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,371 INFO epoch # 4959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00683609233237803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,395 INFO epoch # 4960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007006863379501738
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:51,395 INFO *** epoch 4960, rolling-avg-loss (window=10)= 0.0068134918248688335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,419 INFO epoch # 4961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007266142696607858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,443 INFO epoch # 4962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006838706984126475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,467 INFO epoch # 4963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006906000395247247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,491 INFO epoch # 4964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006944828695850447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,515 INFO epoch # 4965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074555610117386095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,538 INFO epoch # 4966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075290394452167675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,562 INFO epoch # 4967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007068551451084204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,586 INFO epoch # 4968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068911026464775205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,610 INFO epoch # 4969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006589467950107064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,634 INFO epoch # 4970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007018951822828967
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:51,635 INFO *** epoch 4970, rolling-avg-loss (window=10)= 0.007050835309928516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,659 INFO epoch # 4971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006836138287326321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,683 INFO epoch # 4972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006816749621066265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,707 INFO epoch # 4973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00744126988138305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,730 INFO epoch # 4974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006829949994425988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,754 INFO epoch # 4975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007077194306475576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,778 INFO epoch # 4976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00745332889346173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,802 INFO epoch # 4977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007747544885205571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,826 INFO epoch # 4978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008507619313604664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,852 INFO epoch # 4979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.009028453008795623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,876 INFO epoch # 4980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008280403628305066
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:51,876 INFO *** epoch 4980, rolling-avg-loss (window=10)= 0.007601865182004985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,900 INFO epoch # 4981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008033783924474847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,924 INFO epoch # 4982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006817019260779489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,948 INFO epoch # 4983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006839304740424268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,972 INFO epoch # 4984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006509200022264849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:51,995 INFO epoch # 4985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006547320190293249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,019 INFO epoch # 4986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006600299326237291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,044 INFO epoch # 4987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072181471332442015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,068 INFO epoch # 4988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006631063864915632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,092 INFO epoch # 4989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006812497718783561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,116 INFO epoch # 4990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00832679909945
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:52,117 INFO *** epoch 4990, rolling-avg-loss (window=10)= 0.007033543528086739
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,141 INFO epoch # 4991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007650062078027986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,166 INFO epoch # 4992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007512607982789632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,195 INFO epoch # 4993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00696713898651069
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,223 INFO epoch # 4994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006362953819916584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,251 INFO epoch # 4995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069474843112402596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,281 INFO epoch # 4996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006762829645595048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,309 INFO epoch # 4997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006506296667794231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,337 INFO epoch # 4998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006664658270892687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,364 INFO epoch # 4999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00683859056152869
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,388 INFO epoch # 5000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006738115545886103
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:52,388 INFO *** epoch 5000, rolling-avg-loss (window=10)= 0.006895073787018191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,412 INFO epoch # 5001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006611225748201832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,437 INFO epoch # 5002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006776604350307025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,461 INFO epoch # 5003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006647723541391315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,485 INFO epoch # 5004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006811275299696717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,509 INFO epoch # 5005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006693446834105998
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,533 INFO epoch # 5006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006821617964305915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,556 INFO epoch # 5007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006952494899451267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,581 INFO epoch # 5008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006900428859808017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,607 INFO epoch # 5009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006634454744926188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,632 INFO epoch # 5010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006808555634052027
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:52,632 INFO *** epoch 5010, rolling-avg-loss (window=10)= 0.00676578278762463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,656 INFO epoch # 5011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006539005513332086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,681 INFO epoch # 5012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006686051907308865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,705 INFO epoch # 5013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006823657029599417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,729 INFO epoch # 5014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006988194632867817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,753 INFO epoch # 5015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.010246433288557455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,777 INFO epoch # 5016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008144907682435587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,801 INFO epoch # 5017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008048889234487433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,825 INFO epoch # 5018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070798609594930895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,849 INFO epoch # 5019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006634168312302791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,873 INFO epoch # 5020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068856313155265525
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:52,873 INFO *** epoch 5020, rolling-avg-loss (window=10)= 0.007407679987591109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,897 INFO epoch # 5021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006877136976982001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,921 INFO epoch # 5022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068617886790889315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,946 INFO epoch # 5023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007337745846598409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,969 INFO epoch # 5024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007291562251339201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:52,994 INFO epoch # 5025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007126976110157557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,018 INFO epoch # 5026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00684957395424135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,042 INFO epoch # 5027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007173126679845154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,066 INFO epoch # 5028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007364031305769458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,090 INFO epoch # 5029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007554481970146298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,114 INFO epoch # 5030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074329382259747945
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:53,114 INFO *** epoch 5030, rolling-avg-loss (window=10)= 0.0071869362000143155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,138 INFO epoch # 5031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006799246635637246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,162 INFO epoch # 5032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006842693765065633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,186 INFO epoch # 5033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006690307425742503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,210 INFO epoch # 5034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006960960265132599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,234 INFO epoch # 5035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00677709008596139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,257 INFO epoch # 5036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007052312255837023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,281 INFO epoch # 5037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007033165173197631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,305 INFO epoch # 5038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007141831607441418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,329 INFO epoch # 5039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006861402085633017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,355 INFO epoch # 5040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006422156686312519
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:53,355 INFO *** epoch 5040, rolling-avg-loss (window=10)= 0.006858116598596098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,379 INFO epoch # 5041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066320893238298595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,403 INFO epoch # 5042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006638849765295163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,426 INFO epoch # 5043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006559605542861391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,450 INFO epoch # 5044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006497141999716405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,474 INFO epoch # 5045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066031738388119265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,498 INFO epoch # 5046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006780701485695317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,522 INFO epoch # 5047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006973553419811651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,546 INFO epoch # 5048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007030097287497483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,570 INFO epoch # 5049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007315086688322481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,595 INFO epoch # 5050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006818916845077183
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:53,595 INFO *** epoch 5050, rolling-avg-loss (window=10)= 0.006784921619691886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,619 INFO epoch # 5051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006896359314850997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,643 INFO epoch # 5052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006397905075573362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,667 INFO epoch # 5053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006490140323876403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,690 INFO epoch # 5054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006796171750465874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,715 INFO epoch # 5055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006709565372148063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,739 INFO epoch # 5056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006818511719757225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,763 INFO epoch # 5057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007086731849994976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,787 INFO epoch # 5058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069876694469712675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,811 INFO epoch # 5059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006869742515846156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,835 INFO epoch # 5060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006413808099750895
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:53,835 INFO *** epoch 5060, rolling-avg-loss (window=10)= 0.006746660546923522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,859 INFO epoch # 5061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065447983288322575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,883 INFO epoch # 5062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075109606550540775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,907 INFO epoch # 5063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007222108717542142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,931 INFO epoch # 5064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006594904873054475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,956 INFO epoch # 5065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006468823026807513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:53,981 INFO epoch # 5066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066385238387738355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,005 INFO epoch # 5067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007063123986881692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,030 INFO epoch # 5068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072723311895970255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,054 INFO epoch # 5069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006789347462472506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,078 INFO epoch # 5070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007067011916660704
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:54,078 INFO *** epoch 5070, rolling-avg-loss (window=10)= 0.006917193399567622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,102 INFO epoch # 5071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067647310643224046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,126 INFO epoch # 5072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006848534008895513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,150 INFO epoch # 5073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006568923556187656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,175 INFO epoch # 5074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006852372971479781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,199 INFO epoch # 5075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006817921967012808
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,227 INFO epoch # 5076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063556389504810795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,255 INFO epoch # 5077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006867742908070795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,283 INFO epoch # 5078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007042746496153995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,310 INFO epoch # 5079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006818311114329845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,336 INFO epoch # 5080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067090680822730064
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:54,336 INFO *** epoch 5080, rolling-avg-loss (window=10)= 0.0067645991119206885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,362 INFO epoch # 5081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006823816838732455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,386 INFO epoch # 5082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00658993617980741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,410 INFO epoch # 5083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070063819803181104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,434 INFO epoch # 5084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072310237810597755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,459 INFO epoch # 5085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00693226300063543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,483 INFO epoch # 5086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067505501210689545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,507 INFO epoch # 5087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007385841257928405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,530 INFO epoch # 5088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007059617521008477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,554 INFO epoch # 5089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006852869468275458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,578 INFO epoch # 5090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006783566452213563
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:54,578 INFO *** epoch 5090, rolling-avg-loss (window=10)= 0.006941586660104804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,602 INFO epoch # 5091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006879429689433891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,626 INFO epoch # 5092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006992164024268277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,650 INFO epoch # 5093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007502015097998083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,675 INFO epoch # 5094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007001290978223551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,699 INFO epoch # 5095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070012089345254935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,722 INFO epoch # 5096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00680297667713603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,746 INFO epoch # 5097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006502203541458584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,770 INFO epoch # 5098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006954185890208464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,794 INFO epoch # 5099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006585538205399644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,818 INFO epoch # 5100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006545207688759547
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:54,818 INFO *** epoch 5100, rolling-avg-loss (window=10)= 0.006876622072741157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,843 INFO epoch # 5101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006494444991403725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,867 INFO epoch # 5102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006477507566160057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,891 INFO epoch # 5103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006527203855512198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,915 INFO epoch # 5104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066836798141594045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,939 INFO epoch # 5105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006794717723096255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,963 INFO epoch # 5106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007178469393693376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:54,987 INFO epoch # 5107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006639752769842744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,011 INFO epoch # 5108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006684265601506922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,035 INFO epoch # 5109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066473016631789505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,059 INFO epoch # 5110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006834642837930005
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:55,059 INFO *** epoch 5110, rolling-avg-loss (window=10)= 0.006696198621648363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,083 INFO epoch # 5111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007388980324321892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,107 INFO epoch # 5112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006931957403139677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,131 INFO epoch # 5113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006748011102899909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,155 INFO epoch # 5114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006767016609956045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,180 INFO epoch # 5115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006724452941853087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,204 INFO epoch # 5116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00686430414498318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,228 INFO epoch # 5117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006529251084430143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,253 INFO epoch # 5118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006503498654637951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,278 INFO epoch # 5119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00649946357589215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,303 INFO epoch # 5120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006621561165957246
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:55,303 INFO *** epoch 5120, rolling-avg-loss (window=10)= 0.006757849700807128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,327 INFO epoch # 5121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00700582852732623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,353 INFO epoch # 5122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007200740285043139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,377 INFO epoch # 5123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006807353529438842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,401 INFO epoch # 5124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006573657417902723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,426 INFO epoch # 5125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006448903302953113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,450 INFO epoch # 5126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006574337436177302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,475 INFO epoch # 5127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067915805338998325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,499 INFO epoch # 5128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006589764416276012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,523 INFO epoch # 5129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066472488033468835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,547 INFO epoch # 5130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006269048040849157
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:55,547 INFO *** epoch 5130, rolling-avg-loss (window=10)= 0.0066908462293213235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,571 INFO epoch # 5131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006554159757797606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,595 INFO epoch # 5132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006984476174693555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,619 INFO epoch # 5133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006664549924607854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,643 INFO epoch # 5134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067682769149541855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,667 INFO epoch # 5135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006500589530332945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,691 INFO epoch # 5136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066069064632756636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,715 INFO epoch # 5137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064957805807353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,739 INFO epoch # 5138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067963024994242005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,763 INFO epoch # 5139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006776936468668282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,788 INFO epoch # 5140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006494611661764793
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:55,788 INFO *** epoch 5140, rolling-avg-loss (window=10)= 0.006664258997625438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,812 INFO epoch # 5141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006693881998216966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,836 INFO epoch # 5142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00656164348765742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,860 INFO epoch # 5143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006319242675090209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,884 INFO epoch # 5144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006734874885296449
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,908 INFO epoch # 5145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006986537599004805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,932 INFO epoch # 5146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006911644653882831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,956 INFO epoch # 5147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006937176105566323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:55,981 INFO epoch # 5148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006793816355639137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,005 INFO epoch # 5149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006896296836202964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,028 INFO epoch # 5150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006890423159347847
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:56,029 INFO *** epoch 5150, rolling-avg-loss (window=10)= 0.006772553775590495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,052 INFO epoch # 5151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006611231081478763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,076 INFO epoch # 5152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006923692959389882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,099 INFO epoch # 5153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066958032475668006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,123 INFO epoch # 5154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006867816500744084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,148 INFO epoch # 5155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00629466243844945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,172 INFO epoch # 5156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066654527581704315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,196 INFO epoch # 5157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006658801285084337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,220 INFO epoch # 5158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006436475785449147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,244 INFO epoch # 5159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006717870244756341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,267 INFO epoch # 5160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007020991462923121
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:56,268 INFO *** epoch 5160, rolling-avg-loss (window=10)= 0.006689279776401236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,291 INFO epoch # 5161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068145409095450304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,315 INFO epoch # 5162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006873290549265221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,340 INFO epoch # 5163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0072481320312363096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,364 INFO epoch # 5164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006676284581772052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,389 INFO epoch # 5165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006545743599417619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,413 INFO epoch # 5166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006577350759471301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,437 INFO epoch # 5167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006667638765065931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,460 INFO epoch # 5168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064662212680559605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,484 INFO epoch # 5169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006380451595759951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,508 INFO epoch # 5170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006634869692788925
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:56,508 INFO *** epoch 5170, rolling-avg-loss (window=10)= 0.00668845237523783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,532 INFO epoch # 5171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006399017889634706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,556 INFO epoch # 5172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006720658559061121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,580 INFO epoch # 5173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006577199339517392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,604 INFO epoch # 5174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006576411644346081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,628 INFO epoch # 5175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006498594259028323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,652 INFO epoch # 5176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006562344737176318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,675 INFO epoch # 5177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006611673867155332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,699 INFO epoch # 5178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00681819085002644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,723 INFO epoch # 5179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0073585694408393465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,747 INFO epoch # 5180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006771738815587014
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:56,747 INFO *** epoch 5180, rolling-avg-loss (window=10)= 0.006689439940237208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,771 INFO epoch # 5181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006695711868815124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,795 INFO epoch # 5182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007080961389874574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,820 INFO epoch # 5183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00685173489182489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,845 INFO epoch # 5184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006786124031350482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,869 INFO epoch # 5185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070391203626059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,893 INFO epoch # 5186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067011206556344405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,916 INFO epoch # 5187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006801548588555306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,940 INFO epoch # 5188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006374110071192263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,964 INFO epoch # 5189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006469150583143346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:56,987 INFO epoch # 5190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006705649801006075
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:56,988 INFO *** epoch 5190, rolling-avg-loss (window=10)= 0.00675052322440024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,012 INFO epoch # 5191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006638722341449466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,036 INFO epoch # 5192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063709287060191855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,060 INFO epoch # 5193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063286603581218515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,084 INFO epoch # 5194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006673203250102233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,108 INFO epoch # 5195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006401412974810228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,131 INFO epoch # 5196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006818627996835858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,155 INFO epoch # 5197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006830475183960516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,179 INFO epoch # 5198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006722061982145533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,203 INFO epoch # 5199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061733039692626335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,227 INFO epoch # 5200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064379916148027405
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:57,227 INFO *** epoch 5200, rolling-avg-loss (window=10)= 0.006539538837751024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,251 INFO epoch # 5201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006348245347908232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,276 INFO epoch # 5202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006639046318014152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,300 INFO epoch # 5203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006850041638244875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,324 INFO epoch # 5204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006356524856528267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,348 INFO epoch # 5205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006049499403161462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,372 INFO epoch # 5206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006277780135860667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,396 INFO epoch # 5207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006778805240173824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,420 INFO epoch # 5208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006652078038314357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,444 INFO epoch # 5209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00633358091727132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,468 INFO epoch # 5210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006233780848560855
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:57,468 INFO *** epoch 5210, rolling-avg-loss (window=10)= 0.006451938274403801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,492 INFO epoch # 5211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006650105577136856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,516 INFO epoch # 5212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006922471438883804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,540 INFO epoch # 5213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006795083907491062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,563 INFO epoch # 5214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006567864809767343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,587 INFO epoch # 5215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006500717856397387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,611 INFO epoch # 5216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00690771073277574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,635 INFO epoch # 5217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067101032109349035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,659 INFO epoch # 5218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007201469168649055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,683 INFO epoch # 5219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006645343943091575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,707 INFO epoch # 5220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067842079006368294
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:57,707 INFO *** epoch 5220, rolling-avg-loss (window=10)= 0.006768507854576456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,731 INFO epoch # 5221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006978619770961814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,755 INFO epoch # 5222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006910546842846088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,779 INFO epoch # 5223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00721316940325778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,802 INFO epoch # 5224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006908321600349154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,826 INFO epoch # 5225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006890608383400831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,850 INFO epoch # 5226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006719372024235781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,874 INFO epoch # 5227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0074206626013619825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,898 INFO epoch # 5228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007501264750317205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,922 INFO epoch # 5229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00844791955751134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,946 INFO epoch # 5230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069769600668223575
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:57,946 INFO *** epoch 5230, rolling-avg-loss (window=10)= 0.007196744500106433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,970 INFO epoch # 5231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006309457301540533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:57,994 INFO epoch # 5232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065453492279630154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,018 INFO epoch # 5233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006722526999510592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,042 INFO epoch # 5234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007239861253765412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,066 INFO epoch # 5235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008294528517581057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,090 INFO epoch # 5236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068799299915554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,114 INFO epoch # 5237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063706122018629685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,138 INFO epoch # 5238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006910013209562749
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,162 INFO epoch # 5239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064752447651699185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,186 INFO epoch # 5240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007578739045129623
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:58,186 INFO *** epoch 5240, rolling-avg-loss (window=10)= 0.0069326262513641265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,210 INFO epoch # 5241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00666493261087453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,233 INFO epoch # 5242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006603637993976008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,257 INFO epoch # 5243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062838530575390905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,281 INFO epoch # 5244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006330197225906886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,305 INFO epoch # 5245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006286458512477111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,329 INFO epoch # 5246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007368871491053142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,354 INFO epoch # 5247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007227151108963881
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,378 INFO epoch # 5248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00695576163707301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,402 INFO epoch # 5249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006424513157980982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,426 INFO epoch # 5250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006480038755398709
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:58,426 INFO *** epoch 5250, rolling-avg-loss (window=10)= 0.006662541555124335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,450 INFO epoch # 5251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006266366493946407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,473 INFO epoch # 5252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006719696561049204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,497 INFO epoch # 5253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006937014404684305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,521 INFO epoch # 5254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006512955726066139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,546 INFO epoch # 5255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006222249401616864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,570 INFO epoch # 5256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006296488114458043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,593 INFO epoch # 5257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006235659071535338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,618 INFO epoch # 5258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006273085295106284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,641 INFO epoch # 5259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063640317603130825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,665 INFO epoch # 5260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006501955875137355
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:58,665 INFO *** epoch 5260, rolling-avg-loss (window=10)= 0.006432950270391302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,689 INFO epoch # 5261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064054533941089176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,712 INFO epoch # 5262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006579600878467318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,737 INFO epoch # 5263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062956776964711025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,761 INFO epoch # 5264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006564948831510264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,785 INFO epoch # 5265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006677844110527076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,811 INFO epoch # 5266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006823278978117742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,839 INFO epoch # 5267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006552706952788867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,867 INFO epoch # 5268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006241275696083903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,891 INFO epoch # 5269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006643383152550086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,915 INFO epoch # 5270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061297777720028535
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:58,915 INFO *** epoch 5270, rolling-avg-loss (window=10)= 0.006491394746262813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,940 INFO epoch # 5271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006321890665276442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,966 INFO epoch # 5272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006190690808580257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:58,991 INFO epoch # 5273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006201919350132812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,015 INFO epoch # 5274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006585947150597349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,039 INFO epoch # 5275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006623188288358506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,062 INFO epoch # 5276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006368509602907579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,086 INFO epoch # 5277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006371615061652847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,110 INFO epoch # 5278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006498954804555979
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,135 INFO epoch # 5279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063869651858112775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,160 INFO epoch # 5280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006322498113149777
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:59,160 INFO *** epoch 5280, rolling-avg-loss (window=10)= 0.006387217903102283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,185 INFO epoch # 5281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006189199586515315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,210 INFO epoch # 5282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006064426830562297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,235 INFO epoch # 5283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006311460507276934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,259 INFO epoch # 5284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006571701633220073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,283 INFO epoch # 5285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006502231590275187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,306 INFO epoch # 5286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066869331858470105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,330 INFO epoch # 5287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006494119712442625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,356 INFO epoch # 5288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006127928594651166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,380 INFO epoch # 5289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006745763625076506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,404 INFO epoch # 5290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063075630241655745
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:59,404 INFO *** epoch 5290, rolling-avg-loss (window=10)= 0.006400132829003269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,428 INFO epoch # 5291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006440506633225596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,452 INFO epoch # 5292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006550507991050836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,476 INFO epoch # 5293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006352377888106275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,500 INFO epoch # 5294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00635544050601311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,524 INFO epoch # 5295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007207463168015238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,547 INFO epoch # 5296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006666968583886046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,572 INFO epoch # 5297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063250677194446325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,597 INFO epoch # 5298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006634046105318703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,623 INFO epoch # 5299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007197816848929506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,648 INFO epoch # 5300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0071080456909839995
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:59,648 INFO *** epoch 5300, rolling-avg-loss (window=10)= 0.006683824113497394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,672 INFO epoch # 5301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006772319713491015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,696 INFO epoch # 5302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006885501534270588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,719 INFO epoch # 5303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00635722044899012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,744 INFO epoch # 5304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065144387481268495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,768 INFO epoch # 5305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065163077306351624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,793 INFO epoch # 5306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065898264219868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,818 INFO epoch # 5307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006409616056771483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,844 INFO epoch # 5308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006397219083737582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,869 INFO epoch # 5309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061743113765260205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,893 INFO epoch # 5310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006378872159984894
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:39:59,893 INFO *** epoch 5310, rolling-avg-loss (window=10)= 0.0064995633274520515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,917 INFO epoch # 5311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006419925928639714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,940 INFO epoch # 5312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006309109303401783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,963 INFO epoch # 5313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006744391423126217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:39:59,987 INFO epoch # 5314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006840807443950325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,011 INFO epoch # 5315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006788310172851197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,034 INFO epoch # 5316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006394401716534048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,058 INFO epoch # 5317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062914945228840224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,082 INFO epoch # 5318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006825251621194184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,106 INFO epoch # 5319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066951928893104196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,130 INFO epoch # 5320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006762152996088844
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:00,130 INFO *** epoch 5320, rolling-avg-loss (window=10)= 0.0066071038017980754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,153 INFO epoch # 5321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006556863278092351
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,177 INFO epoch # 5322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007001860780292191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,201 INFO epoch # 5323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006730307737598196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,224 INFO epoch # 5324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006667786270554643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,248 INFO epoch # 5325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00630815214390168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,272 INFO epoch # 5326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006413324932509568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,296 INFO epoch # 5327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006402441453246865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,320 INFO epoch # 5328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005981901555060176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,345 INFO epoch # 5329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006362893640471157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,368 INFO epoch # 5330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006410042638890445
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:00,369 INFO *** epoch 5330, rolling-avg-loss (window=10)= 0.006483557443061727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,392 INFO epoch # 5331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006118018696724903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,416 INFO epoch # 5332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063626866904087365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,439 INFO epoch # 5333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006405778040061705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,463 INFO epoch # 5334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006437383191951085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,487 INFO epoch # 5335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00675877549656434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,511 INFO epoch # 5336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006852656464616302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,535 INFO epoch # 5337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006505049728730228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,559 INFO epoch # 5338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006410679590771906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,583 INFO epoch # 5339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006821408685937058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,606 INFO epoch # 5340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006754481299140025
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:00,606 INFO *** epoch 5340, rolling-avg-loss (window=10)= 0.0065426917884906285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,630 INFO epoch # 5341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006232459665625356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,654 INFO epoch # 5342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006428931046684738
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,678 INFO epoch # 5343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006653142292634584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,702 INFO epoch # 5344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006547022334416397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,726 INFO epoch # 5345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006363928958307952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,750 INFO epoch # 5346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006392483599483967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,774 INFO epoch # 5347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063714487696415745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,797 INFO epoch # 5348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006775953239412047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,821 INFO epoch # 5349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006755514521501027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,845 INFO epoch # 5350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006399638667062391
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:00,846 INFO *** epoch 5350, rolling-avg-loss (window=10)= 0.006492052309477003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,871 INFO epoch # 5351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006346882706566248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,894 INFO epoch # 5352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00635033209255198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,918 INFO epoch # 5353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066959588875761256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,942 INFO epoch # 5354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068716927053174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,966 INFO epoch # 5355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0075403387600090355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:00,989 INFO epoch # 5356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006243152791284956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,013 INFO epoch # 5357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006383975742210168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,037 INFO epoch # 5358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006224306205695029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,061 INFO epoch # 5359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006127253276645206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,085 INFO epoch # 5360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006288081189268269
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:01,085 INFO *** epoch 5360, rolling-avg-loss (window=10)= 0.006507197435712442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,110 INFO epoch # 5361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006345270696328953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,134 INFO epoch # 5362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006190602489368757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,158 INFO epoch # 5363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00658054395171348
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,182 INFO epoch # 5364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006725147410179488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,205 INFO epoch # 5365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006288233205850702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,229 INFO epoch # 5366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006139916979009286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,253 INFO epoch # 5367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006396257755113766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,276 INFO epoch # 5368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00697515660431236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,300 INFO epoch # 5369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00921923850546591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,324 INFO epoch # 5370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007004158593190368
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:01,325 INFO *** epoch 5370, rolling-avg-loss (window=10)= 0.006786452619053307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,350 INFO epoch # 5371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007028383020951878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,374 INFO epoch # 5372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006589348631678149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,398 INFO epoch # 5373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006630992196733132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,422 INFO epoch # 5374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006474079942563549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,445 INFO epoch # 5375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006354893354000524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,469 INFO epoch # 5376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006757492359611206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,493 INFO epoch # 5377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0069900732196401805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,517 INFO epoch # 5378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006495791698398534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,541 INFO epoch # 5379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006648482492892072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,565 INFO epoch # 5380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006860128181870095
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:01,565 INFO *** epoch 5380, rolling-avg-loss (window=10)= 0.006682966509833932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,589 INFO epoch # 5381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063581755457562394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,613 INFO epoch # 5382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006611872275243513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,637 INFO epoch # 5383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006807066049077548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,661 INFO epoch # 5384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006771195701730903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,684 INFO epoch # 5385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006900317006511614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,708 INFO epoch # 5386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006323195084405597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,732 INFO epoch # 5387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006131642410764471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,756 INFO epoch # 5388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063896958527038805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,780 INFO epoch # 5389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006037067367287818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,804 INFO epoch # 5390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006019207547069527
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:01,804 INFO *** epoch 5390, rolling-avg-loss (window=10)= 0.006434943484055111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,828 INFO epoch # 5391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0059806288772961125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,852 INFO epoch # 5392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065144981854246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,876 INFO epoch # 5393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006457769428379834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,900 INFO epoch # 5394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006703021041175816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,924 INFO epoch # 5395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061900195651105605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,947 INFO epoch # 5396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063385997418663464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,971 INFO epoch # 5397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005928050726652145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:01,995 INFO epoch # 5398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005996601415972691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,019 INFO epoch # 5399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058464147805352695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,043 INFO epoch # 5400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006183146106195636
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:02,043 INFO *** epoch 5400, rolling-avg-loss (window=10)= 0.0062138749868609015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,067 INFO epoch # 5401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065903379654628225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,091 INFO epoch # 5402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062680141782038845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,114 INFO epoch # 5403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006602987203223165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,138 INFO epoch # 5404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006283122318563983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,162 INFO epoch # 5405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064953867622534744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,186 INFO epoch # 5406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00596100873008254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,210 INFO epoch # 5407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006062095249944832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,233 INFO epoch # 5408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006463471640017815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,257 INFO epoch # 5409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006578439519216772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,281 INFO epoch # 5410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00613801293366123
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:02,281 INFO *** epoch 5410, rolling-avg-loss (window=10)= 0.006344287650063052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,305 INFO epoch # 5411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006534125248435885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,328 INFO epoch # 5412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006556871310749557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,353 INFO epoch # 5413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006227941390534397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,377 INFO epoch # 5414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064530416784691624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,401 INFO epoch # 5415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006862971145892516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,425 INFO epoch # 5416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00669306830241112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,449 INFO epoch # 5417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062788925133645535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,472 INFO epoch # 5418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006188634441059548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,496 INFO epoch # 5419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006229568651178852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,520 INFO epoch # 5420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007963671938341577
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:02,520 INFO *** epoch 5420, rolling-avg-loss (window=10)= 0.006598878662043717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,543 INFO epoch # 5421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008080438492470421
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,567 INFO epoch # 5422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006817375739046838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,591 INFO epoch # 5423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00681799823360052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,615 INFO epoch # 5424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006148523207230028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,639 INFO epoch # 5425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006400438185664825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,663 INFO epoch # 5426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006340621075651143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,687 INFO epoch # 5427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006390118942363188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,710 INFO epoch # 5428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063672959950054064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,734 INFO epoch # 5429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006698179538943805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,758 INFO epoch # 5430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006704097977490164
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:02,758 INFO *** epoch 5430, rolling-avg-loss (window=10)= 0.006676508738746634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,782 INFO epoch # 5431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006354848010232672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,806 INFO epoch # 5432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006317508421489038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,829 INFO epoch # 5433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062814380653435364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,854 INFO epoch # 5434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064082164608407766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,878 INFO epoch # 5435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00613221062667435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,902 INFO epoch # 5436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005900753472815268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,926 INFO epoch # 5437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065796766357379965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,949 INFO epoch # 5438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006694093106489163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,972 INFO epoch # 5439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006365747511154041
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:02,996 INFO epoch # 5440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0067265210309415124
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:02,996 INFO *** epoch 5440, rolling-avg-loss (window=10)= 0.0063761013341718355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,019 INFO epoch # 5441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006510232567961793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,043 INFO epoch # 5442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0068984464451204985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,067 INFO epoch # 5443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065284498341497965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,091 INFO epoch # 5444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006592031430045608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,114 INFO epoch # 5445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006039119361958001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,138 INFO epoch # 5446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006573858867341187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,161 INFO epoch # 5447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005989847355522215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,185 INFO epoch # 5448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006070763280149549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,209 INFO epoch # 5449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006327822513412684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,233 INFO epoch # 5450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007613385831064079
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:03,233 INFO *** epoch 5450, rolling-avg-loss (window=10)= 0.006514395748672542
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,257 INFO epoch # 5451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0070869834526092745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,280 INFO epoch # 5452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00633122961880872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,304 INFO epoch # 5453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006225131830433384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,328 INFO epoch # 5454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063233600376406685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,352 INFO epoch # 5455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006249149832001422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,376 INFO epoch # 5456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006555825566465501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,400 INFO epoch # 5457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006744823622284457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,423 INFO epoch # 5458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006323109242657665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,447 INFO epoch # 5459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006183244622661732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,471 INFO epoch # 5460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006191805921844207
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:03,471 INFO *** epoch 5460, rolling-avg-loss (window=10)= 0.006421466374740703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,495 INFO epoch # 5461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065028530661948025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,519 INFO epoch # 5462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006569260316609871
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,543 INFO epoch # 5463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005811228744278196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,566 INFO epoch # 5464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006181245407788083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,590 INFO epoch # 5465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006245866883546114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,614 INFO epoch # 5466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006277954576944467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,637 INFO epoch # 5467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006337153274216689
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,661 INFO epoch # 5468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061043167297611944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,685 INFO epoch # 5469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006018748303176835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,709 INFO epoch # 5470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006239914990146644
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:03,709 INFO *** epoch 5470, rolling-avg-loss (window=10)= 0.00622885422926629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,733 INFO epoch # 5471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006309114738542121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,757 INFO epoch # 5472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005892628061701544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,780 INFO epoch # 5473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006534384381666314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,808 INFO epoch # 5474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006189716477820184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,832 INFO epoch # 5475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005761965112469625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,856 INFO epoch # 5476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005775214645836968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,880 INFO epoch # 5477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061926611815579236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,904 INFO epoch # 5478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007706541218794882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,928 INFO epoch # 5479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006763184530427679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,953 INFO epoch # 5480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006492120795883238
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:03,953 INFO *** epoch 5480, rolling-avg-loss (window=10)= 0.0063617531144700475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:03,976 INFO epoch # 5481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0064743131515569985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,000 INFO epoch # 5482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006047505004971754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,024 INFO epoch # 5483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006156502946396358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,048 INFO epoch # 5484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006467739804065786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,071 INFO epoch # 5485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006527988371090032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,095 INFO epoch # 5486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006084380263928324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,119 INFO epoch # 5487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062358536961255595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,143 INFO epoch # 5488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005949508071353193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,167 INFO epoch # 5489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006323369532765355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,191 INFO epoch # 5490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006297715328400955
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:04,191 INFO *** epoch 5490, rolling-avg-loss (window=10)= 0.0062564876170654316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,215 INFO epoch # 5491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006849240569863468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,238 INFO epoch # 5492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006921918342413846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,262 INFO epoch # 5493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006493783490441274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,286 INFO epoch # 5494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006303681289864471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,310 INFO epoch # 5495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005865105224074796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,335 INFO epoch # 5496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006106634282332379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,359 INFO epoch # 5497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006728895248670597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,383 INFO epoch # 5498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006341582877212204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,407 INFO epoch # 5499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006357874306559097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,431 INFO epoch # 5500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005965154923615046
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:04,431 INFO *** epoch 5500, rolling-avg-loss (window=10)= 0.0063933870555047175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,455 INFO epoch # 5501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006331046992272604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,479 INFO epoch # 5502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006468275754741626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,502 INFO epoch # 5503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006033443241904024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,526 INFO epoch # 5504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005783640895970166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,550 INFO epoch # 5505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0059846436633961275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,574 INFO epoch # 5506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060539936312125064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,598 INFO epoch # 5507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006150804867502302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,622 INFO epoch # 5508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006193253961100709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,646 INFO epoch # 5509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006252719882468227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,670 INFO epoch # 5510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005990623598336242
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:04,670 INFO *** epoch 5510, rolling-avg-loss (window=10)= 0.0061242446488904536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,693 INFO epoch # 5511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006088973779696971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,717 INFO epoch # 5512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006283404938585591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,741 INFO epoch # 5513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00605739613820333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,765 INFO epoch # 5514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005805261986097321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,789 INFO epoch # 5515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006170842141727917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,813 INFO epoch # 5516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006061364183551632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,838 INFO epoch # 5517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006237855297513306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,862 INFO epoch # 5518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006574023500434123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,885 INFO epoch # 5519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006268289573199581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,908 INFO epoch # 5520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006027087620168459
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:04,909 INFO *** epoch 5520, rolling-avg-loss (window=10)= 0.006157449915917823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,932 INFO epoch # 5521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007641052456165198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,956 INFO epoch # 5522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006458445081079844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:04,980 INFO epoch # 5523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005881949015019927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,004 INFO epoch # 5524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005917918751947582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,027 INFO epoch # 5525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006680940088699572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,051 INFO epoch # 5526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006845737982075661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,075 INFO epoch # 5527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006637007143581286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,098 INFO epoch # 5528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00650636044156272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,121 INFO epoch # 5529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005908916195039637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,146 INFO epoch # 5530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006091018956794869
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:05,146 INFO *** epoch 5530, rolling-avg-loss (window=10)= 0.00645693461119663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,170 INFO epoch # 5531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005974279389192816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,193 INFO epoch # 5532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006313269892416429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,217 INFO epoch # 5533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006065725166990887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,241 INFO epoch # 5534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005923400625761133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,265 INFO epoch # 5535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006224137570825405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,289 INFO epoch # 5536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005828508757986128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,312 INFO epoch # 5537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00610961631173268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,336 INFO epoch # 5538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060065168036089744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,360 INFO epoch # 5539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006208375620190054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,384 INFO epoch # 5540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062320589713635854
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:05,384 INFO *** epoch 5540, rolling-avg-loss (window=10)= 0.006088588911006809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,408 INFO epoch # 5541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006535239415825345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,431 INFO epoch # 5542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0059982819911965635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,455 INFO epoch # 5543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005843947910761926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,479 INFO epoch # 5544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006132074282504618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,503 INFO epoch # 5545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00615445377479773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,526 INFO epoch # 5546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005846355481480714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,550 INFO epoch # 5547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005970443678961601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,573 INFO epoch # 5548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006076358287828043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,597 INFO epoch # 5549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006015723782184068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,621 INFO epoch # 5550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005968868041236419
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:05,621 INFO *** epoch 5550, rolling-avg-loss (window=10)= 0.0060541746646777025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,645 INFO epoch # 5551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006094981785281561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,669 INFO epoch # 5552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006103655679908115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,693 INFO epoch # 5553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006940815605048556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,716 INFO epoch # 5554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00645403157977853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,740 INFO epoch # 5555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063765295562916435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,764 INFO epoch # 5556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060150777353555895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,787 INFO epoch # 5557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006272103095398052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,811 INFO epoch # 5558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006202848693646956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,835 INFO epoch # 5559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00678864090150455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,861 INFO epoch # 5560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006463860008807387
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:05,861 INFO *** epoch 5560, rolling-avg-loss (window=10)= 0.006371254464102094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,885 INFO epoch # 5561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006318253377685323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,909 INFO epoch # 5562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00675336363929091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,932 INFO epoch # 5563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006152719193778466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,956 INFO epoch # 5564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006163835561892483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:05,979 INFO epoch # 5565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006357438200211618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,003 INFO epoch # 5566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007096311826899182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,026 INFO epoch # 5567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006257432643906213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,050 INFO epoch # 5568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005853067137650214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,074 INFO epoch # 5569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006080610859498847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,098 INFO epoch # 5570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006258750276174396
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:06,098 INFO *** epoch 5570, rolling-avg-loss (window=10)= 0.0063291782716987655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,122 INFO epoch # 5571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065486239836900495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,146 INFO epoch # 5572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006987709552049637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,169 INFO epoch # 5573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007098061694705393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,192 INFO epoch # 5574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006544061470776796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,216 INFO epoch # 5575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061196574388304725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,239 INFO epoch # 5576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005910884021432139
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,263 INFO epoch # 5577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005798738973680884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,287 INFO epoch # 5578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006159248907351866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,311 INFO epoch # 5579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005928658811171772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,335 INFO epoch # 5580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006006536081258673
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:06,335 INFO *** epoch 5580, rolling-avg-loss (window=10)= 0.006310218093494769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,359 INFO epoch # 5581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006166947270685341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,383 INFO epoch # 5582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007038634146738332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,407 INFO epoch # 5583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007584622559079435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,431 INFO epoch # 5584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006517111462017056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,455 INFO epoch # 5585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006123739221948199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,479 INFO epoch # 5586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006695422831398901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,503 INFO epoch # 5587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006129208632046357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,527 INFO epoch # 5588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006046151822374668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,551 INFO epoch # 5589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005832260911120102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,574 INFO epoch # 5590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006081647614337271
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:06,574 INFO *** epoch 5590, rolling-avg-loss (window=10)= 0.006421574647174566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,598 INFO epoch # 5591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061676790137426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,622 INFO epoch # 5592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00584371414151974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,646 INFO epoch # 5593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005760569867561571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,669 INFO epoch # 5594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006044529494829476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,693 INFO epoch # 5595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058089335725526325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,717 INFO epoch # 5596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00605924405681435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,741 INFO epoch # 5597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006253710649616551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,765 INFO epoch # 5598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006212020613020286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,789 INFO epoch # 5599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00580282359442208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,812 INFO epoch # 5600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005838609045895282
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:06,812 INFO *** epoch 5600, rolling-avg-loss (window=10)= 0.0059791834049974565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,837 INFO epoch # 5601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006046601301932242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,861 INFO epoch # 5602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005887248949875357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,886 INFO epoch # 5603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057768562764977105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,910 INFO epoch # 5604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006037224266037811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,934 INFO epoch # 5605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006109703303081915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,958 INFO epoch # 5606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005843610146257561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:06,981 INFO epoch # 5607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006211215084476862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,005 INFO epoch # 5608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006261403745156713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,029 INFO epoch # 5609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006713496128213592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,052 INFO epoch # 5610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062065263409749605
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:07,053 INFO *** epoch 5610, rolling-avg-loss (window=10)= 0.006109388554250472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,076 INFO epoch # 5611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006415116826246958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,100 INFO epoch # 5612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006332670724077616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,124 INFO epoch # 5613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006339191211736761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,148 INFO epoch # 5614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058226797700626776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,172 INFO epoch # 5615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006058149614545982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,196 INFO epoch # 5616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006538169298437424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,220 INFO epoch # 5617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006461383534769993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,243 INFO epoch # 5618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006377904021064751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,267 INFO epoch # 5619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00744015202508308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,291 INFO epoch # 5620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007128499630198348
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:07,291 INFO *** epoch 5620, rolling-avg-loss (window=10)= 0.006491391665622359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,315 INFO epoch # 5621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065013402127078734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,340 INFO epoch # 5622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006862418995297048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,364 INFO epoch # 5623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00648071376781445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,388 INFO epoch # 5624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00607086031959625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,412 INFO epoch # 5625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006848592522146646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,436 INFO epoch # 5626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006040014610334765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,460 INFO epoch # 5627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060045245045330375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,484 INFO epoch # 5628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006026068083883729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,507 INFO epoch # 5629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006120081488916185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,531 INFO epoch # 5630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057792874140432104
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:07,531 INFO *** epoch 5630, rolling-avg-loss (window=10)= 0.00627339019192732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,555 INFO epoch # 5631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0059678139732568525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,579 INFO epoch # 5632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00634345505386591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,603 INFO epoch # 5633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062377558861044236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,627 INFO epoch # 5634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006021584231348243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,651 INFO epoch # 5635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006351470074150711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,675 INFO epoch # 5636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006110183669079561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,698 INFO epoch # 5637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005866310399142094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,722 INFO epoch # 5638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006179008581966627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,746 INFO epoch # 5639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006091787385230418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,770 INFO epoch # 5640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005945266442722641
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:07,770 INFO *** epoch 5640, rolling-avg-loss (window=10)= 0.0061114635696867484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,794 INFO epoch # 5641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006103140680352226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,818 INFO epoch # 5642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006725045619532466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,842 INFO epoch # 5643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061506498386734165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,866 INFO epoch # 5644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006331447861157358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,890 INFO epoch # 5645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006182121731399093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,914 INFO epoch # 5646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006129616085672751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,937 INFO epoch # 5647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006030697499227244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,961 INFO epoch # 5648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005595333299424965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:07,985 INFO epoch # 5649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057513804495101795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,009 INFO epoch # 5650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005907669692533091
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:08,009 INFO *** epoch 5650, rolling-avg-loss (window=10)= 0.006090710275748279
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,033 INFO epoch # 5651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005913773362408392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,057 INFO epoch # 5652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061474726389860734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,080 INFO epoch # 5653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006226457066077273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,104 INFO epoch # 5654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00626277677656617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,128 INFO epoch # 5655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054989631826174445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,151 INFO epoch # 5656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006162192319607129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,175 INFO epoch # 5657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005836464180902112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,198 INFO epoch # 5658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062696011300431564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,222 INFO epoch # 5659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00611480807128828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,246 INFO epoch # 5660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006500266339571681
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:08,246 INFO *** epoch 5660, rolling-avg-loss (window=10)= 0.006093277506806771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,270 INFO epoch # 5661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007049940366414376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,294 INFO epoch # 5662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00839419803378405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,318 INFO epoch # 5663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006127417578682071
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,342 INFO epoch # 5664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005940964831097517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,366 INFO epoch # 5665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006702422448142897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,390 INFO epoch # 5666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006858241795271169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,413 INFO epoch # 5667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006491521795396693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,438 INFO epoch # 5668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006028471965692006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,462 INFO epoch # 5669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005873368711036164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,485 INFO epoch # 5670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005714839811844286
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:08,485 INFO *** epoch 5670, rolling-avg-loss (window=10)= 0.006518138733736123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,509 INFO epoch # 5671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005798682999738958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,533 INFO epoch # 5672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005959810110653052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,557 INFO epoch # 5673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005731047960580327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,581 INFO epoch # 5674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005873095957213081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,605 INFO epoch # 5675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006051438547729049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,629 INFO epoch # 5676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005972085753455758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,653 INFO epoch # 5677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058845065723289736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,676 INFO epoch # 5678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005644734701490961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,700 INFO epoch # 5679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058875985705526546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,724 INFO epoch # 5680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005906139798753429
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:08,724 INFO *** epoch 5680, rolling-avg-loss (window=10)= 0.005870914097249625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,748 INFO epoch # 5681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005587333449511789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,772 INFO epoch # 5682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005874214839423075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,795 INFO epoch # 5683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006178006457048468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,819 INFO epoch # 5684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006244608710403554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,843 INFO epoch # 5685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006750402710167691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,867 INFO epoch # 5686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007181642242358066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,891 INFO epoch # 5687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006172539400722599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,915 INFO epoch # 5688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005961491850030143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,938 INFO epoch # 5689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006117533521319274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,962 INFO epoch # 5690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005958192690741271
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:08,962 INFO *** epoch 5690, rolling-avg-loss (window=10)= 0.006202596587172593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:08,986 INFO epoch # 5691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006028149902704172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,009 INFO epoch # 5692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006002872010867577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,033 INFO epoch # 5693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005922612945141736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,057 INFO epoch # 5694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00589844834030373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,081 INFO epoch # 5695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005955858148809057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,105 INFO epoch # 5696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005873430782230571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,129 INFO epoch # 5697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006258585912291892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,152 INFO epoch # 5698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005948453406745102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,176 INFO epoch # 5699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006059240789909381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,199 INFO epoch # 5700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00585143813805189
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:09,200 INFO *** epoch 5700, rolling-avg-loss (window=10)= 0.005979909037705511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,223 INFO epoch # 5701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006124888386693783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,247 INFO epoch # 5702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006152936060971115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,271 INFO epoch # 5703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005673019964888226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,295 INFO epoch # 5704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006151586909254547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,319 INFO epoch # 5705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006000854438752867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,343 INFO epoch # 5706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005805498614790849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,367 INFO epoch # 5707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005853026181284804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,391 INFO epoch # 5708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006626685521041509
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,415 INFO epoch # 5709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006173526584461797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,438 INFO epoch # 5710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006200489991897484
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:09,439 INFO *** epoch 5710, rolling-avg-loss (window=10)= 0.0060762512654036985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,462 INFO epoch # 5711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00592575467453571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,486 INFO epoch # 5712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005724016882595606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,510 INFO epoch # 5713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00581750590208685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,534 INFO epoch # 5714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005700850444554817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,558 INFO epoch # 5715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006073638542147819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,582 INFO epoch # 5716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058021998484036885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,606 INFO epoch # 5717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005715813458664343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,630 INFO epoch # 5718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005635028035612777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,654 INFO epoch # 5719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006070564064430073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,677 INFO epoch # 5720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006004320297506638
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:09,677 INFO *** epoch 5720, rolling-avg-loss (window=10)= 0.005846969215053832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,701 INFO epoch # 5721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005809254915220663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,725 INFO epoch # 5722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005785854846180882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,749 INFO epoch # 5723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060133568695164286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,774 INFO epoch # 5724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006021803033945616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,798 INFO epoch # 5725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005975108120765071
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,822 INFO epoch # 5726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0055748642480466515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,847 INFO epoch # 5727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00598500577325467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,872 INFO epoch # 5728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005837398246512748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,896 INFO epoch # 5729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061023261660011485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,920 INFO epoch # 5730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00604176995693706
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:09,920 INFO *** epoch 5730, rolling-avg-loss (window=10)= 0.005914674217638094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,944 INFO epoch # 5731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006203670091053937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,969 INFO epoch # 5732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060301757548586465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:09,994 INFO epoch # 5733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00623443099175347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,017 INFO epoch # 5734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006185864229337312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,041 INFO epoch # 5735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005868773274414707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,066 INFO epoch # 5736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006161255012557376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,089 INFO epoch # 5737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006205315206898376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,113 INFO epoch # 5738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005770989191660192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,138 INFO epoch # 5739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007057978575176094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,163 INFO epoch # 5740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065314185922034085
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:10,163 INFO *** epoch 5740, rolling-avg-loss (window=10)= 0.006224987091991352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,187 INFO epoch # 5741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006119123216194566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,211 INFO epoch # 5742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006783045748306904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,234 INFO epoch # 5743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006614852478378452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,258 INFO epoch # 5744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006161519078887068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,282 INFO epoch # 5745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006208567916473839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,306 INFO epoch # 5746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005813646544993389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,330 INFO epoch # 5747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005793583295599092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,355 INFO epoch # 5748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006147586973384023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,380 INFO epoch # 5749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006489868559583556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,404 INFO epoch # 5750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00650500858318992
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:10,405 INFO *** epoch 5750, rolling-avg-loss (window=10)= 0.006263680239499081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,428 INFO epoch # 5751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006308524440100882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,452 INFO epoch # 5752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006038504565367475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,476 INFO epoch # 5753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005899440686334856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,500 INFO epoch # 5754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005597072551609017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,524 INFO epoch # 5755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00582420798309613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,548 INFO epoch # 5756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005727725292672403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,573 INFO epoch # 5757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006073994860344101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,597 INFO epoch # 5758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005900728421693202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,621 INFO epoch # 5759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006894012440170627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,645 INFO epoch # 5760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006197699152835412
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:10,645 INFO *** epoch 5760, rolling-avg-loss (window=10)= 0.006046191039422411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,669 INFO epoch # 5761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005833680246723816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,693 INFO epoch # 5762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005920620715187397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,717 INFO epoch # 5763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006325535774521995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,741 INFO epoch # 5764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00600161785405362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,765 INFO epoch # 5765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005659475878928788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,789 INFO epoch # 5766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058708704527816735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,813 INFO epoch # 5767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006132115806394722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,839 INFO epoch # 5768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007402499613817781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,865 INFO epoch # 5769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006348301722027827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,889 INFO epoch # 5770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005968904217297677
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:10,889 INFO *** epoch 5770, rolling-avg-loss (window=10)= 0.006146362228173529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,913 INFO epoch # 5771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005698857676179614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,937 INFO epoch # 5772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005728242154873442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,961 INFO epoch # 5773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005524983578652609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:10,985 INFO epoch # 5774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006320842228888068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,009 INFO epoch # 5775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006358835074934177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,033 INFO epoch # 5776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007459008105797693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,057 INFO epoch # 5777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006983713348745368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,081 INFO epoch # 5778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006513124346383847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,104 INFO epoch # 5779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006096425204304978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,128 INFO epoch # 5780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005882817902602255
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:11,128 INFO *** epoch 5780, rolling-avg-loss (window=10)= 0.006256684962136205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,152 INFO epoch # 5781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005929171027673874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,176 INFO epoch # 5782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005972347171336878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,199 INFO epoch # 5783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060506080990307964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,223 INFO epoch # 5784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005882224955712445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,247 INFO epoch # 5785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058813451760215685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,271 INFO epoch # 5786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005719903332646936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,295 INFO epoch # 5787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005790079550934024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,320 INFO epoch # 5788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006672054747468792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,345 INFO epoch # 5789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005749016258050688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,371 INFO epoch # 5790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0059935835233773105
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:11,371 INFO *** epoch 5790, rolling-avg-loss (window=10)= 0.005964033384225331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,396 INFO epoch # 5791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056305997422896326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,421 INFO epoch # 5792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005727086114347912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,446 INFO epoch # 5793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063500736869173124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,471 INFO epoch # 5794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006020512577379122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,496 INFO epoch # 5795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005726566269004252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,521 INFO epoch # 5796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005422480026027188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,545 INFO epoch # 5797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005814955176902004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,570 INFO epoch # 5798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006432255540858023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,594 INFO epoch # 5799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060240439779590815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,619 INFO epoch # 5800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005888973879336845
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:11,619 INFO *** epoch 5800, rolling-avg-loss (window=10)= 0.005903754699102137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,645 INFO epoch # 5801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00546341097651748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,670 INFO epoch # 5802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005549300498387311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,695 INFO epoch # 5803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005854526440089103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,720 INFO epoch # 5804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00598829249793198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,745 INFO epoch # 5805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00636860316444654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,770 INFO epoch # 5806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005751303368015215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,794 INFO epoch # 5807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005618747396511026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,819 INFO epoch # 5808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005709083598048892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,845 INFO epoch # 5809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00568426818063017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,875 INFO epoch # 5810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005927548714680597
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:11,875 INFO *** epoch 5810, rolling-avg-loss (window=10)= 0.005791508483525831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,901 INFO epoch # 5811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00566034954317729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,929 INFO epoch # 5812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006168950792925898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,956 INFO epoch # 5813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005788838672742713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:11,983 INFO epoch # 5814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057534757143002935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,010 INFO epoch # 5815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006217846821527928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,036 INFO epoch # 5816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005954989246674813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,063 INFO epoch # 5817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00580538888607407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,089 INFO epoch # 5818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060368219201336615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,116 INFO epoch # 5819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005777061443950515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,141 INFO epoch # 5820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006309552394668572
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:12,141 INFO *** epoch 5820, rolling-avg-loss (window=10)= 0.005947327543617575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,166 INFO epoch # 5821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005920805448113242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,191 INFO epoch # 5822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005935552493610885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,216 INFO epoch # 5823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006195652109454386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,240 INFO epoch # 5824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005713499020203017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,264 INFO epoch # 5825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006301908433670178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,289 INFO epoch # 5826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00547296013974119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,314 INFO epoch # 5827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005355670007702429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,340 INFO epoch # 5828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0055177171307150275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,365 INFO epoch # 5829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005569421162363142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,389 INFO epoch # 5830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005649314720358234
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:12,390 INFO *** epoch 5830, rolling-avg-loss (window=10)= 0.005763250066593173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,413 INFO epoch # 5831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005804853579320479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,437 INFO epoch # 5832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005941485913353972
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,461 INFO epoch # 5833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058130272082053125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,485 INFO epoch # 5834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005821759586979169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,509 INFO epoch # 5835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005918553426454309
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,533 INFO epoch # 5836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005611089622107102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,557 INFO epoch # 5837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005695724859833717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,581 INFO epoch # 5838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00578723647777224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,605 INFO epoch # 5839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006682373146759346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,629 INFO epoch # 5840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.008381648782233242
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:12,629 INFO *** epoch 5840, rolling-avg-loss (window=10)= 0.006145775260301889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,652 INFO epoch # 5841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006472413442679681
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,676 INFO epoch # 5842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005950072969426401
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,700 INFO epoch # 5843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00601070328411879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,724 INFO epoch # 5844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005874020695046056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,748 INFO epoch # 5845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00548735358461272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,772 INFO epoch # 5846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005486004003614653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,796 INFO epoch # 5847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00599874863110017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,820 INFO epoch # 5848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00587908158195205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,845 INFO epoch # 5849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006417105170839932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,869 INFO epoch # 5850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005797261830593925
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:12,869 INFO *** epoch 5850, rolling-avg-loss (window=10)= 0.0059372765193984375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,892 INFO epoch # 5851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00589501944341464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,916 INFO epoch # 5852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006120879814261571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,940 INFO epoch # 5853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005655299253703561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,964 INFO epoch # 5854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005975992069579661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:12,988 INFO epoch # 5855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006451461646065582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,012 INFO epoch # 5856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006050169358786661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,037 INFO epoch # 5857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00613522687490331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,061 INFO epoch # 5858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005822445265948772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,085 INFO epoch # 5859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005665943535859697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,108 INFO epoch # 5860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005807311536045745
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:13,108 INFO *** epoch 5860, rolling-avg-loss (window=10)= 0.0059579748798569195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,132 INFO epoch # 5861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058308217412559316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,156 INFO epoch # 5862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005900577940337826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,180 INFO epoch # 5863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006298673812125344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,203 INFO epoch # 5864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005979268462397158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,227 INFO epoch # 5865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005611067223071586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,251 INFO epoch # 5866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005798269194201566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,275 INFO epoch # 5867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005686309341399465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,299 INFO epoch # 5868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00599342991336016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,323 INFO epoch # 5869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006125045496446546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,347 INFO epoch # 5870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005850971749168821
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:13,347 INFO *** epoch 5870, rolling-avg-loss (window=10)= 0.0059074434873764405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,371 INFO epoch # 5871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006025947455782443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,395 INFO epoch # 5872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058235423639416695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,419 INFO epoch # 5873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005616654649202246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,443 INFO epoch # 5874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005797241363325156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,467 INFO epoch # 5875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006030485208611935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,491 INFO epoch # 5876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00589199233945692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,515 INFO epoch # 5877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005814820709929336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,539 INFO epoch # 5878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005875544295122381
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,562 INFO epoch # 5879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005605578480754048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,586 INFO epoch # 5880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054105684903333895
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:13,586 INFO *** epoch 5880, rolling-avg-loss (window=10)= 0.005789237535645952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,611 INFO epoch # 5881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005973958774120547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,635 INFO epoch # 5882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006021021596097853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,659 INFO epoch # 5883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006325418609776534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,683 INFO epoch # 5884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005676381242665229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,707 INFO epoch # 5885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005515914133866318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,730 INFO epoch # 5886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054785215252195485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,754 INFO epoch # 5887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005918546266912017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,778 INFO epoch # 5888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00549263134598732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,802 INFO epoch # 5889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005841698744916357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,827 INFO epoch # 5890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005767149319581222
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:13,827 INFO *** epoch 5890, rolling-avg-loss (window=10)= 0.005801124155914295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,852 INFO epoch # 5891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005840373665705556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,878 INFO epoch # 5892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005586042221693788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,903 INFO epoch # 5893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005813230942294467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,927 INFO epoch # 5894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005984678049571812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,950 INFO epoch # 5895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005783909124147613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,974 INFO epoch # 5896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006064950153813697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:13,998 INFO epoch # 5897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005838845070684329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,022 INFO epoch # 5898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005508548485522624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,045 INFO epoch # 5899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005856386516825296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,069 INFO epoch # 5900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005849243112606928
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:14,069 INFO *** epoch 5900, rolling-avg-loss (window=10)= 0.005812620734286611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,093 INFO epoch # 5901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057562445508665405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,117 INFO epoch # 5902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005705095980374608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,141 INFO epoch # 5903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056638112146174535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,165 INFO epoch # 5904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005704767972929403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,189 INFO epoch # 5905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006150136381620541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,212 INFO epoch # 5906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005811982060549781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,236 INFO epoch # 5907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005619784176815301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,260 INFO epoch # 5908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060944546276004985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,284 INFO epoch # 5909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006298880798567552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,308 INFO epoch # 5910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006230222308658995
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:14,308 INFO *** epoch 5910, rolling-avg-loss (window=10)= 0.005903538007260067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,332 INFO epoch # 5911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005601127078989521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,358 INFO epoch # 5912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005465519210702041
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,382 INFO epoch # 5913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061562302653328516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,406 INFO epoch # 5914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00626896748872241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,430 INFO epoch # 5915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057935968798119575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,453 INFO epoch # 5916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005632954242173582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,477 INFO epoch # 5917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005793317053758074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,501 INFO epoch # 5918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005789291964902077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,525 INFO epoch # 5919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057710693654371426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,549 INFO epoch # 5920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056738518323982134
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:14,549 INFO *** epoch 5920, rolling-avg-loss (window=10)= 0.005794592538222787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,573 INFO epoch # 5921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005318734194588615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,596 INFO epoch # 5922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005615632893750444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,620 INFO epoch # 5923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005907251674216241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,644 INFO epoch # 5924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005672015417076182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,667 INFO epoch # 5925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005343069497030228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,691 INFO epoch # 5926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006006589537719265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,715 INFO epoch # 5927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0063139038102235645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,739 INFO epoch # 5928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005976230022497475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,763 INFO epoch # 5929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005775108904344961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,787 INFO epoch # 5930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005867479994776659
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:14,787 INFO *** epoch 5930, rolling-avg-loss (window=10)= 0.005779601594622363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,810 INFO epoch # 5931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005460314088850282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,834 INFO epoch # 5932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005607167615380604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,859 INFO epoch # 5933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005708469048840925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,882 INFO epoch # 5934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060372439183993265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,906 INFO epoch # 5935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005561309910262935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,930 INFO epoch # 5936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005798184567538556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,954 INFO epoch # 5937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062477790415869094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:14,978 INFO epoch # 5938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00634280928352382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,002 INFO epoch # 5939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00616007578355493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,026 INFO epoch # 5940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005478818507981487
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:15,026 INFO *** epoch 5940, rolling-avg-loss (window=10)= 0.005840217176591977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,050 INFO epoch # 5941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058402438153279945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,074 INFO epoch # 5942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006061613850761205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,097 INFO epoch # 5943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005607362181763165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,120 INFO epoch # 5944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005474239449540619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,144 INFO epoch # 5945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00595293862716062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,168 INFO epoch # 5946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006931658674147911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,192 INFO epoch # 5947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00663554084894713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,216 INFO epoch # 5948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005860688790562563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,240 INFO epoch # 5949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00574002115172334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,264 INFO epoch # 5950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00580791434913408
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:15,264 INFO *** epoch 5950, rolling-avg-loss (window=10)= 0.005991222173906863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,287 INFO epoch # 5951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005375558212108444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,311 INFO epoch # 5952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005744689202401787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,335 INFO epoch # 5953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052981061598984525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,360 INFO epoch # 5954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005515125245437957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,384 INFO epoch # 5955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005409025332482997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,408 INFO epoch # 5956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006309967960987706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,432 INFO epoch # 5957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005914131208555773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,456 INFO epoch # 5958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005490264244144782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,480 INFO epoch # 5959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053926166365272366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,504 INFO epoch # 5960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005579141274210997
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:15,504 INFO *** epoch 5960, rolling-avg-loss (window=10)= 0.005602862547675613
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,527 INFO epoch # 5961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005876834700757172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,551 INFO epoch # 5962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005712692240194883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,575 INFO epoch # 5963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006432463451346848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,599 INFO epoch # 5964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057562031943234615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,623 INFO epoch # 5965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005847132924827747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,647 INFO epoch # 5966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005498678125150036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,671 INFO epoch # 5967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005346047240891494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,695 INFO epoch # 5968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005718108513974585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,718 INFO epoch # 5969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005666566445142962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,742 INFO epoch # 5970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006035793558112346
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:15,742 INFO *** epoch 5970, rolling-avg-loss (window=10)= 0.005789052039472153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,766 INFO epoch # 5971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005961168673820794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,790 INFO epoch # 5972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00629155314527452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,814 INFO epoch # 5973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005818561570777092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,838 INFO epoch # 5974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005631340543914121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,862 INFO epoch # 5975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006165910737763625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,886 INFO epoch # 5976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057024171110242605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,910 INFO epoch # 5977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0055410767236026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,934 INFO epoch # 5978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005517854951904155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,958 INFO epoch # 5979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005458479274238925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:15,981 INFO epoch # 5980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005981646048894618
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:15,981 INFO *** epoch 5980, rolling-avg-loss (window=10)= 0.005807000878121471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,005 INFO epoch # 5981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006095344404457137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,029 INFO epoch # 5982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057047926529776305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,053 INFO epoch # 5983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054941123016760685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,078 INFO epoch # 5984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005607583745586453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,102 INFO epoch # 5985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005539880912692752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,127 INFO epoch # 5986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005453031611978076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,152 INFO epoch # 5987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005641200383251999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,176 INFO epoch # 5988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005608789047983009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,201 INFO epoch # 5989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005514507582120132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,226 INFO epoch # 5990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005527687520952895
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:16,226 INFO *** epoch 5990, rolling-avg-loss (window=10)= 0.005618693016367615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,251 INFO epoch # 5991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005971057966235094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,275 INFO epoch # 5992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005817011588078458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,300 INFO epoch # 5993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006161417149996851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,325 INFO epoch # 5994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006518588881590404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,350 INFO epoch # 5995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00618100848805625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,374 INFO epoch # 5996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066450943777454086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,399 INFO epoch # 5997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060850014560855925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,424 INFO epoch # 5998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005806243600090966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,448 INFO epoch # 5999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005892412598768715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,472 INFO epoch # 6000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005473461467772722
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:16,473 INFO *** epoch 6000, rolling-avg-loss (window=10)= 0.006055129757442046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,496 INFO epoch # 6001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005526452201593202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,520 INFO epoch # 6002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005660374452418182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,544 INFO epoch # 6003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005313545319950208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,568 INFO epoch # 6004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005667816651111934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,591 INFO epoch # 6005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0055495293490821496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,621 INFO epoch # 6006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005812555238662753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,655 INFO epoch # 6007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005907378625124693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,685 INFO epoch # 6008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005850882349477615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,711 INFO epoch # 6009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053165700228419155
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,736 INFO epoch # 6010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005640454975946341
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:16,736 INFO *** epoch 6010, rolling-avg-loss (window=10)= 0.0056245559186209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,760 INFO epoch # 6011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005969556812488008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,784 INFO epoch # 6012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005817043223942164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,808 INFO epoch # 6013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006226320467249025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,832 INFO epoch # 6014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005414747793111019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,856 INFO epoch # 6015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006063740132958628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,880 INFO epoch # 6016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0059603090267046355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,905 INFO epoch # 6017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006024550850270316
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,929 INFO epoch # 6018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056936068867798895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,954 INFO epoch # 6019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005442347814096138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:16,979 INFO epoch # 6020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005611971871985588
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:16,979 INFO *** epoch 6020, rolling-avg-loss (window=10)= 0.005822419487958541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,003 INFO epoch # 6021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005442349767690757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,028 INFO epoch # 6022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054015696223359555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,053 INFO epoch # 6023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005176288501388626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,077 INFO epoch # 6024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005796486875624396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,102 INFO epoch # 6025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007165120892750565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,126 INFO epoch # 6026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006070327599445591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,150 INFO epoch # 6027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057442897887085564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,173 INFO epoch # 6028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005688002420356497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,197 INFO epoch # 6029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005594096212007571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,221 INFO epoch # 6030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005757616134360433
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:17,221 INFO *** epoch 6030, rolling-avg-loss (window=10)= 0.005783614781466895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,245 INFO epoch # 6031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005547605658648536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,269 INFO epoch # 6032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052191246577422135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,293 INFO epoch # 6033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052267879218561575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,317 INFO epoch # 6034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005234089861914981
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,342 INFO epoch # 6035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005632664375298191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,366 INFO epoch # 6036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056882732824306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,390 INFO epoch # 6037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0055706623461446725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,413 INFO epoch # 6038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056141635941457935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,437 INFO epoch # 6039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005975123945972882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,460 INFO epoch # 6040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005925233839661814
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:17,461 INFO *** epoch 6040, rolling-avg-loss (window=10)= 0.005563372948381584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,484 INFO epoch # 6041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007113766150723677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,508 INFO epoch # 6042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006181178388942499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,532 INFO epoch # 6043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006283756360062398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,556 INFO epoch # 6044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005985341100313235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,580 INFO epoch # 6045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005618917755782604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,604 INFO epoch # 6046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005538657780562062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,628 INFO epoch # 6047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005183386820135638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,652 INFO epoch # 6048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005550198249693494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,675 INFO epoch # 6049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005575141396548133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,699 INFO epoch # 6050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005744730719015934
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:17,699 INFO *** epoch 6050, rolling-avg-loss (window=10)= 0.005877507472177967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,723 INFO epoch # 6051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005408956014434807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,747 INFO epoch # 6052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051614344120025635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,771 INFO epoch # 6053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005529386202397291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,795 INFO epoch # 6054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005495409721333999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,819 INFO epoch # 6055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005545426633034367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,843 INFO epoch # 6056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005912925873417407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,867 INFO epoch # 6057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006030493525031488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,891 INFO epoch # 6058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005768986640759977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,915 INFO epoch # 6059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005662874689733144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,939 INFO epoch # 6060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006200911127962172
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:17,939 INFO *** epoch 6060, rolling-avg-loss (window=10)= 0.005671680484010721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,963 INFO epoch # 6061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005642993644869421
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:17,987 INFO epoch # 6062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00530315755167976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,011 INFO epoch # 6063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005564158127526753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,036 INFO epoch # 6064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00561320308042923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,060 INFO epoch # 6065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005650517174217384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,084 INFO epoch # 6066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005608096929790918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,108 INFO epoch # 6067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005772538061137311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,132 INFO epoch # 6068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005237941484665498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,155 INFO epoch # 6069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00538099545519799
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,179 INFO epoch # 6070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005802571686217561
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:18,179 INFO *** epoch 6070, rolling-avg-loss (window=10)= 0.005557617319573183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,204 INFO epoch # 6071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005887182014703285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,228 INFO epoch # 6072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005389897291024681
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,251 INFO epoch # 6073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00553794544248376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,275 INFO epoch # 6074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006001208756060805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,299 INFO epoch # 6075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005686522556061391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,323 INFO epoch # 6076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005589597480138764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,347 INFO epoch # 6077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005703198345145211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,371 INFO epoch # 6078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005483504377480131
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,395 INFO epoch # 6079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005604088968539145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,419 INFO epoch # 6080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054879327362868935
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:18,419 INFO *** epoch 6080, rolling-avg-loss (window=10)= 0.005637107796792407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,443 INFO epoch # 6081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006432061476516537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,466 INFO epoch # 6082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0062275358504848555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,490 INFO epoch # 6083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006055867670511361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,514 INFO epoch # 6084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005708225529815536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,537 INFO epoch # 6085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054439470477518626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,561 INFO epoch # 6086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00554786880093161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,585 INFO epoch # 6087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005732192148570903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,609 INFO epoch # 6088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005653129192069173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,633 INFO epoch # 6089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005815297401568387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,656 INFO epoch # 6090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005585059676377568
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:18,656 INFO *** epoch 6090, rolling-avg-loss (window=10)= 0.005820118479459779
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,680 INFO epoch # 6091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005568679582211189
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,704 INFO epoch # 6092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006255692285776604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,728 INFO epoch # 6093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006137157870398369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,751 INFO epoch # 6094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005595833878032863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,775 INFO epoch # 6095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058787332418432925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,799 INFO epoch # 6096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00631762677221559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,823 INFO epoch # 6097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006099708174588159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,848 INFO epoch # 6098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00557501853472786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,872 INFO epoch # 6099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00557778294023592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,897 INFO epoch # 6100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005580837751040235
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:18,897 INFO *** epoch 6100, rolling-avg-loss (window=10)= 0.0058587071031070085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,921 INFO epoch # 6101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005629770559608005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,944 INFO epoch # 6102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005645734370773425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,968 INFO epoch # 6103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005684723138983827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:18,992 INFO epoch # 6104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005295555245538708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,015 INFO epoch # 6105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005725734034058405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,039 INFO epoch # 6106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005751870732638054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,063 INFO epoch # 6107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005753330606239615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,087 INFO epoch # 6108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005394550506025553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,111 INFO epoch # 6109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005140049746842124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,135 INFO epoch # 6110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005389927071519196
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:19,135 INFO *** epoch 6110, rolling-avg-loss (window=10)= 0.005541124601222691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,159 INFO epoch # 6111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005774817036581226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,182 INFO epoch # 6112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005637330745230429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,206 INFO epoch # 6113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005701569505617954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,230 INFO epoch # 6114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053238410473568365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,254 INFO epoch # 6115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005269592176773585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,278 INFO epoch # 6116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005655306609696709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,302 INFO epoch # 6117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005681703027221374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,326 INFO epoch # 6118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006204614895978011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,350 INFO epoch # 6119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005651155428495258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,374 INFO epoch # 6120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005357122194254771
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:19,374 INFO *** epoch 6120, rolling-avg-loss (window=10)= 0.005625705266720615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,397 INFO epoch # 6121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005715348990634084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,421 INFO epoch # 6122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00570810789213283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,445 INFO epoch # 6123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050950021723110694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,469 INFO epoch # 6124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005344177421648055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,493 INFO epoch # 6125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005313197572831996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,517 INFO epoch # 6126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056587902654428035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,541 INFO epoch # 6127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005738763728004415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,564 INFO epoch # 6128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005463612935272977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,588 INFO epoch # 6129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005446780276542995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,612 INFO epoch # 6130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057913139535230584
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:19,612 INFO *** epoch 6130, rolling-avg-loss (window=10)= 0.005527509520834428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,635 INFO epoch # 6131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005756601342000067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,659 INFO epoch # 6132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005963973562757019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,683 INFO epoch # 6133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054529221160919406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,707 INFO epoch # 6134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057914979552151635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,731 INFO epoch # 6135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00539672212471487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,754 INFO epoch # 6136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058139304019277915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,778 INFO epoch # 6137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005855349307239521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,802 INFO epoch # 6138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005413960643636528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,826 INFO epoch # 6139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005372027517296374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,849 INFO epoch # 6140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005447937364806421
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:19,849 INFO *** epoch 6140, rolling-avg-loss (window=10)= 0.005626492233568569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,874 INFO epoch # 6141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054375096879084595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,898 INFO epoch # 6142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005663767988153268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,921 INFO epoch # 6143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005830861882714089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,945 INFO epoch # 6144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006034324615029618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,969 INFO epoch # 6145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005727207739255391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:19,993 INFO epoch # 6146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005922194970480632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,018 INFO epoch # 6147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058770531104528345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,042 INFO epoch # 6148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006371624433086254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,065 INFO epoch # 6149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005357348076358903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,089 INFO epoch # 6150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00531210461849696
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:20,089 INFO *** epoch 6150, rolling-avg-loss (window=10)= 0.005753399712193641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,113 INFO epoch # 6151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005765891160990577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,137 INFO epoch # 6152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005386573866417166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,160 INFO epoch # 6153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005374492080591153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,184 INFO epoch # 6154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005211538526054937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,208 INFO epoch # 6155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054956610438239295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,231 INFO epoch # 6156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005414882056356873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,255 INFO epoch # 6157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053223678696667776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,278 INFO epoch # 6158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005781903448223602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,302 INFO epoch # 6159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005682781113137025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,326 INFO epoch # 6160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005624350320431404
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:20,326 INFO *** epoch 6160, rolling-avg-loss (window=10)= 0.005506044148569344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,351 INFO epoch # 6161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005367729841964319
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,375 INFO epoch # 6162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005281112928059883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,399 INFO epoch # 6163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051299871993251145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,423 INFO epoch # 6164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005226304841926321
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,446 INFO epoch # 6165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005414084349467885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,470 INFO epoch # 6166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005285468716465402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,494 INFO epoch # 6167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005395714928454254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,518 INFO epoch # 6168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005427621450508013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,542 INFO epoch # 6169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005587788647972047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,566 INFO epoch # 6170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005934478394920006
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:20,566 INFO *** epoch 6170, rolling-avg-loss (window=10)= 0.005405029129906325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,590 INFO epoch # 6171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005882053912500851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,614 INFO epoch # 6172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005428421383840032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,640 INFO epoch # 6173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005541949605685659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,663 INFO epoch # 6174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005626741563901305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,687 INFO epoch # 6175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005744843987486092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,711 INFO epoch # 6176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005805449865874834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,735 INFO epoch # 6177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0061803972712368704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,759 INFO epoch # 6178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005907581085921265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,783 INFO epoch # 6179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005371939481847221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,807 INFO epoch # 6180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005297058363794349
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:20,807 INFO *** epoch 6180, rolling-avg-loss (window=10)= 0.005678643652208848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,831 INFO epoch # 6181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005399595080234576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,859 INFO epoch # 6182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005891954941034783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,883 INFO epoch # 6183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005418901251687203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,907 INFO epoch # 6184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005486675006977748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,930 INFO epoch # 6185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005565886131080333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,954 INFO epoch # 6186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005251622475043405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:20,978 INFO epoch # 6187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005431428777228575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,002 INFO epoch # 6188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005298518990457524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,026 INFO epoch # 6189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005379981263104128
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,050 INFO epoch # 6190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005344638157112058
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:21,050 INFO *** epoch 6190, rolling-avg-loss (window=10)= 0.005446920207396034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,074 INFO epoch # 6191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005079670198028907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,098 INFO epoch # 6192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005178541352506727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,122 INFO epoch # 6193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005517757017514668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,146 INFO epoch # 6194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054823852915433235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,170 INFO epoch # 6195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005455367871036287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,194 INFO epoch # 6196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006066977060982026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,218 INFO epoch # 6197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.007265121108503081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,242 INFO epoch # 6198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00894887228787411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,266 INFO epoch # 6199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0065837298752740026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,290 INFO epoch # 6200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005302695921272971
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:21,290 INFO *** epoch 6200, rolling-avg-loss (window=10)= 0.006088111798453611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,314 INFO epoch # 6201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005240267899353057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,340 INFO epoch # 6202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004991721398255322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,366 INFO epoch # 6203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005262769562250469
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,390 INFO epoch # 6204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005204059365496505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,414 INFO epoch # 6205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053170751489233226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,440 INFO epoch # 6206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005329469109710772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,468 INFO epoch # 6207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005390880585764535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,496 INFO epoch # 6208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005247907163720811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,521 INFO epoch # 6209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005441709399747197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,545 INFO epoch # 6210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005754052835982293
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:21,545 INFO *** epoch 6210, rolling-avg-loss (window=10)= 0.005317991246920428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,569 INFO epoch # 6211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00546501598728355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,593 INFO epoch # 6212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005223255560849793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,617 INFO epoch # 6213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005255989366560243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,641 INFO epoch # 6214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005317206407198682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,665 INFO epoch # 6215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051369896391406655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,688 INFO epoch # 6216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053757182831759565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,712 INFO epoch # 6217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005960551898169797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,735 INFO epoch # 6218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056294996829819866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,759 INFO epoch # 6219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005910477935685776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,783 INFO epoch # 6220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005299965450831223
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:21,783 INFO *** epoch 6220, rolling-avg-loss (window=10)= 0.005457467021187767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,806 INFO epoch # 6221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005399048299295828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,830 INFO epoch # 6222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005405677904491313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,856 INFO epoch # 6223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005398789333412424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,880 INFO epoch # 6224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005795281089376658
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,904 INFO epoch # 6225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00532093000219902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,928 INFO epoch # 6226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0055043237007339485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,952 INFO epoch # 6227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051255722137284465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:21,976 INFO epoch # 6228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00530127886304399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,000 INFO epoch # 6229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005585169477853924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,023 INFO epoch # 6230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005562087237194646
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:22,024 INFO *** epoch 6230, rolling-avg-loss (window=10)= 0.0054398158121330194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,047 INFO epoch # 6231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005548547313082963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,072 INFO epoch # 6232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005401579437602777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,096 INFO epoch # 6233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054258567179203965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,119 INFO epoch # 6234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005581437093496788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,143 INFO epoch # 6235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005322337157849688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,167 INFO epoch # 6236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00542880583088845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,191 INFO epoch # 6237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005456455328385346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,215 INFO epoch # 6238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005387218792748172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,238 INFO epoch # 6239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005180782594834454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,262 INFO epoch # 6240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005639813323796261
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:22,262 INFO *** epoch 6240, rolling-avg-loss (window=10)= 0.00543728335906053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,286 INFO epoch # 6241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006174371570523363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,310 INFO epoch # 6242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005692920552974101
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,334 INFO epoch # 6243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005416373896878213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,358 INFO epoch # 6244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005362077412428334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,382 INFO epoch # 6245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005272010013868567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,405 INFO epoch # 6246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005537689219636377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,429 INFO epoch # 6247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005467254261020571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,453 INFO epoch # 6248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005646771372994408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,477 INFO epoch # 6249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005625304562272504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,501 INFO epoch # 6250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052783807986998
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:22,501 INFO *** epoch 6250, rolling-avg-loss (window=10)= 0.005547315366129624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,525 INFO epoch # 6251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005276574578601867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,549 INFO epoch # 6252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053267131734173745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,573 INFO epoch # 6253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005243784769845661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,597 INFO epoch # 6254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005661914336087648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,620 INFO epoch # 6255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005203022810746916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,644 INFO epoch # 6256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0055197600995597895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,668 INFO epoch # 6257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005764757137512788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,692 INFO epoch # 6258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006201521187904291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,716 INFO epoch # 6259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005055330060713459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,740 INFO epoch # 6260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052352188067743555
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:22,740 INFO *** epoch 6260, rolling-avg-loss (window=10)= 0.005448859696116415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,764 INFO epoch # 6261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005134767015988473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,788 INFO epoch # 6262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005382388277212158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,811 INFO epoch # 6263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005592501722276211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,835 INFO epoch # 6264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005994520637614187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,859 INFO epoch # 6265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00592946786491666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,883 INFO epoch # 6266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005860771387233399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,908 INFO epoch # 6267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056522290396969765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,932 INFO epoch # 6268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005374483152991161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,955 INFO epoch # 6269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005486921996634919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:22,979 INFO epoch # 6270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005942040988884401
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:22,979 INFO *** epoch 6270, rolling-avg-loss (window=10)= 0.005635009208344854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,003 INFO epoch # 6271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054630625527352095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,027 INFO epoch # 6272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005213964053837117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,051 INFO epoch # 6273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005223237909376621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,074 INFO epoch # 6274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051775279571302235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,098 INFO epoch # 6275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005282964099023957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,122 INFO epoch # 6276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005433240949059837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,146 INFO epoch # 6277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005551069640205242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,170 INFO epoch # 6278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005342375665350119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,193 INFO epoch # 6279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005205979374295566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,217 INFO epoch # 6280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053942978047416545
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:23,217 INFO *** epoch 6280, rolling-avg-loss (window=10)= 0.005328772000575554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,241 INFO epoch # 6281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00498657392381574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,265 INFO epoch # 6282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005479848288814537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,288 INFO epoch # 6283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005606726757832803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,312 INFO epoch # 6284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005382197123253718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,336 INFO epoch # 6285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005470776304719038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,361 INFO epoch # 6286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005368179430661257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,385 INFO epoch # 6287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005527261659153737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,409 INFO epoch # 6288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005432307130831759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,433 INFO epoch # 6289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005176394795853412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,456 INFO epoch # 6290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005100835755001754
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:23,457 INFO *** epoch 6290, rolling-avg-loss (window=10)= 0.005353110116993775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,480 INFO epoch # 6291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005060770272393711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,504 INFO epoch # 6292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005388618752476759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,528 INFO epoch # 6293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005741208631661721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,551 INFO epoch # 6294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005542974060517736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,575 INFO epoch # 6295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00542200847849017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,599 INFO epoch # 6296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005275137817079667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,623 INFO epoch # 6297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004998845746740699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,647 INFO epoch # 6298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005099909285490867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,671 INFO epoch # 6299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005160744443855947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,695 INFO epoch # 6300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005282359219563659
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:23,695 INFO *** epoch 6300, rolling-avg-loss (window=10)= 0.005297257670827093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,718 INFO epoch # 6301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005222696741839172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,742 INFO epoch # 6302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053711356522399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,766 INFO epoch # 6303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00563028050964931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,790 INFO epoch # 6304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006233314503333531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,813 INFO epoch # 6305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005569335407926701
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,837 INFO epoch # 6306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058778047387022525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,861 INFO epoch # 6307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005533974814170506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,885 INFO epoch # 6308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006106174128944986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,909 INFO epoch # 6309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005479075618495699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,933 INFO epoch # 6310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005318038209225051
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:23,933 INFO *** epoch 6310, rolling-avg-loss (window=10)= 0.005634183032452711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,956 INFO epoch # 6311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054309102924889885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:23,980 INFO epoch # 6312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005377940156904515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,004 INFO epoch # 6313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005388126155594364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,028 INFO epoch # 6314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005245113861747086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,051 INFO epoch # 6315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005369274229451548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,075 INFO epoch # 6316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005158457257493865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,099 INFO epoch # 6317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052627514087362215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,123 INFO epoch # 6318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005293620211887173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,147 INFO epoch # 6319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005748766612668987
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,171 INFO epoch # 6320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005666349472448928
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:24,171 INFO *** epoch 6320, rolling-avg-loss (window=10)= 0.005394130965942168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,195 INFO epoch # 6321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005332350236130878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,219 INFO epoch # 6322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050770402449416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,243 INFO epoch # 6323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005772789045295212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,267 INFO epoch # 6324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053444377990672365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,290 INFO epoch # 6325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051480736656230874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,314 INFO epoch # 6326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00529492627538275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,338 INFO epoch # 6327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005325203463144135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,362 INFO epoch # 6328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00527085862995591
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,387 INFO epoch # 6329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005375735599955078
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,411 INFO epoch # 6330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005239387261099182
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:24,411 INFO *** epoch 6330, rolling-avg-loss (window=10)= 0.005318080222059507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,435 INFO epoch # 6331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005358303002140019
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,460 INFO epoch # 6332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005659244598064106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,483 INFO epoch # 6333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006026391951309051
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,507 INFO epoch # 6334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005371971157728694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,531 INFO epoch # 6335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005240034137386829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,555 INFO epoch # 6336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005092688683362212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,579 INFO epoch # 6337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004884361627773615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,602 INFO epoch # 6338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005368306956370361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,626 INFO epoch # 6339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005528998997760937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,650 INFO epoch # 6340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005475289435707964
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:24,650 INFO *** epoch 6340, rolling-avg-loss (window=10)= 0.005400559054760379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,674 INFO epoch # 6341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005994233433739282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,698 INFO epoch # 6342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005456063168821856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,722 INFO epoch # 6343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005125595402205363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,746 INFO epoch # 6344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005595958005869761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,770 INFO epoch # 6345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00582874720566906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,794 INFO epoch # 6346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005862770907697268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,818 INFO epoch # 6347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005919884977629408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,843 INFO epoch # 6348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006039796877303161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,868 INFO epoch # 6349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005237757148279343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,892 INFO epoch # 6350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005119599401950836
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:24,892 INFO *** epoch 6350, rolling-avg-loss (window=10)= 0.005618040652916534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,916 INFO epoch # 6351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005233571959252004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,940 INFO epoch # 6352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054012225664337166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,965 INFO epoch # 6353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005453739457152551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:24,988 INFO epoch # 6354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005210709074162878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,013 INFO epoch # 6355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005517661804333329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,037 INFO epoch # 6356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004978727796697058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,065 INFO epoch # 6357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004959081441484159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,092 INFO epoch # 6358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00505322413664544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,120 INFO epoch # 6359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005047685299359728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,144 INFO epoch # 6360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005356907422537915
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:25,144 INFO *** epoch 6360, rolling-avg-loss (window=10)= 0.0052212530958058775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,168 INFO epoch # 6361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005980791931506246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,192 INFO epoch # 6362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0066556234523886815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,216 INFO epoch # 6363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006062832624593284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,240 INFO epoch # 6364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005711249032174237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,265 INFO epoch # 6365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005638335183903109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,289 INFO epoch # 6366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058609145125956275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,313 INFO epoch # 6367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005428583892353345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,338 INFO epoch # 6368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053072014125064015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,362 INFO epoch # 6369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005481975473230705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,386 INFO epoch # 6370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052522513033181895
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:25,386 INFO *** epoch 6370, rolling-avg-loss (window=10)= 0.005737975881856983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,409 INFO epoch # 6371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005164120258996263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,433 INFO epoch # 6372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005470293675898574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,457 INFO epoch # 6373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005359648814192042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,481 INFO epoch # 6374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005473697216075379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,505 INFO epoch # 6375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005088457306555938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,529 INFO epoch # 6376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005436755040136632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,553 INFO epoch # 6377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005530358350370079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,576 INFO epoch # 6378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004903589942841791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,600 INFO epoch # 6379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004970878973836079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,624 INFO epoch # 6380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005099089059513062
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:25,624 INFO *** epoch 6380, rolling-avg-loss (window=10)= 0.005249688863841584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,648 INFO epoch # 6381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005188439201447181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,671 INFO epoch # 6382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005681814378476702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,695 INFO epoch # 6383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0055323803462670185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,719 INFO epoch # 6384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00514780190133024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,744 INFO epoch # 6385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004968774344888516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,768 INFO epoch # 6386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00542203489021631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,791 INFO epoch # 6387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005851803463883698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,815 INFO epoch # 6388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006167816245579161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,839 INFO epoch # 6389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005429971330158878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,865 INFO epoch # 6390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005552844937483314
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:25,865 INFO *** epoch 6390, rolling-avg-loss (window=10)= 0.005494368103973102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,889 INFO epoch # 6391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00580625584552763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,913 INFO epoch # 6392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006058127044525463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,938 INFO epoch # 6393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00538132436122396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,962 INFO epoch # 6394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051618137149489485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:25,986 INFO epoch # 6395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005476162474224111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,010 INFO epoch # 6396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005626422229397576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,034 INFO epoch # 6397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006217206464498304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,058 INFO epoch # 6398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006389305111952126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,081 INFO epoch # 6399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005365221230022144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,105 INFO epoch # 6400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004880117769062053
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:26,105 INFO *** epoch 6400, rolling-avg-loss (window=10)= 0.005636195624538232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,129 INFO epoch # 6401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004901736632746179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,153 INFO epoch # 6402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005132384983880911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,177 INFO epoch # 6403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005055393907241523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,200 INFO epoch # 6404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00485236183158122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,224 INFO epoch # 6405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005011221255699638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,248 INFO epoch # 6406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005050352297985228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,271 INFO epoch # 6407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00517347861386952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,295 INFO epoch # 6408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005231681039731484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,318 INFO epoch # 6409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005097838082292583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,343 INFO epoch # 6410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005740628417697735
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:26,343 INFO *** epoch 6410, rolling-avg-loss (window=10)= 0.005124707706272602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,367 INFO epoch # 6411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0055165818812383804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,391 INFO epoch # 6412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005084397445898503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,415 INFO epoch # 6413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005217688871198334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,438 INFO epoch # 6414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005789605638710782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,462 INFO epoch # 6415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005983575647405814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,486 INFO epoch # 6416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005720334425859619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,510 INFO epoch # 6417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005477174439874943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,533 INFO epoch # 6418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005589497879554983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,557 INFO epoch # 6419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005297310206515249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,581 INFO epoch # 6420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005408429009548854
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:26,581 INFO *** epoch 6420, rolling-avg-loss (window=10)= 0.005508459544580546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,605 INFO epoch # 6421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005086072625999805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,629 INFO epoch # 6422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005255172054603463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,652 INFO epoch # 6423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004895638703601435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,676 INFO epoch # 6424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005003982318157796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,700 INFO epoch # 6425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004960913225659169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,723 INFO epoch # 6426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005096196247905027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,747 INFO epoch # 6427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00497877963425708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,771 INFO epoch # 6428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005174204787181225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,795 INFO epoch # 6429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005213282704062294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,819 INFO epoch # 6430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00530156080640154
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:26,819 INFO *** epoch 6430, rolling-avg-loss (window=10)= 0.005096580310782884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,844 INFO epoch # 6431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005160552842426114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,867 INFO epoch # 6432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005514183321793098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,891 INFO epoch # 6433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052217278207535855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,914 INFO epoch # 6434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005300807890307624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,938 INFO epoch # 6435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052772917806578334
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,962 INFO epoch # 6436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005302442303218413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:26,986 INFO epoch # 6437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005201298285101075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,009 INFO epoch # 6438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004982036873116158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,034 INFO epoch # 6439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005622878717986168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,058 INFO epoch # 6440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005375134067435283
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:27,058 INFO *** epoch 6440, rolling-avg-loss (window=10)= 0.005295835390279535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,082 INFO epoch # 6441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005187688359001186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,106 INFO epoch # 6442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005094705949886702
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,130 INFO epoch # 6443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005126194933836814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,154 INFO epoch # 6444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057662535618874244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,178 INFO epoch # 6445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005709484186809277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,201 INFO epoch # 6446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005301496719766874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,225 INFO epoch # 6447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00510741576727014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,249 INFO epoch # 6448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00536547619412886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,273 INFO epoch # 6449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058460603722778615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,296 INFO epoch # 6450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005443189489596989
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:27,296 INFO *** epoch 6450, rolling-avg-loss (window=10)= 0.005394796553446213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,320 INFO epoch # 6451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005321597796864808
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,344 INFO epoch # 6452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005482219428813551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,368 INFO epoch # 6453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060054402420064434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,392 INFO epoch # 6454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005238452322373632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,416 INFO epoch # 6455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004817867971723899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,440 INFO epoch # 6456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005242674393230118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,464 INFO epoch # 6457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005387740977312205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,488 INFO epoch # 6458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005314664158504456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,512 INFO epoch # 6459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005290839675581083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,535 INFO epoch # 6460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005285526509396732
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:27,535 INFO *** epoch 6460, rolling-avg-loss (window=10)= 0.005338702347580692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,559 INFO epoch # 6461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005256637894490268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,583 INFO epoch # 6462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005164313486602623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,607 INFO epoch # 6463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004918175545753911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,630 INFO epoch # 6464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005591144694335526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,654 INFO epoch # 6465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005275875773804728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,678 INFO epoch # 6466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005182150322070811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,702 INFO epoch # 6467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005493485499755479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,726 INFO epoch # 6468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005551908252527937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,749 INFO epoch # 6469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005136647683684714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,773 INFO epoch # 6470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005282376441755332
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:27,773 INFO *** epoch 6470, rolling-avg-loss (window=10)= 0.005285271559478133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,797 INFO epoch # 6471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053144572375458665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,820 INFO epoch # 6472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005395978361775633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,845 INFO epoch # 6473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005187391470826697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,869 INFO epoch # 6474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005949997881543823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,892 INFO epoch # 6475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005315567454090342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,916 INFO epoch # 6476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005378701520385221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,940 INFO epoch # 6477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005382974944950547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,963 INFO epoch # 6478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005458773703139741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:27,987 INFO epoch # 6479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005460875298012979
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,011 INFO epoch # 6480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005436341765744146
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:28,011 INFO *** epoch 6480, rolling-avg-loss (window=10)= 0.0054281059638014995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,035 INFO epoch # 6481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005218464917561505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,059 INFO epoch # 6482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005678911693394184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,084 INFO epoch # 6483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00530682496537338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,108 INFO epoch # 6484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005253853070826153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,132 INFO epoch # 6485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004884543166554067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,155 INFO epoch # 6486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049465659612906165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,179 INFO epoch # 6487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005039310752181336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,203 INFO epoch # 6488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049977641938312445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,226 INFO epoch # 6489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005028070379921701
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,250 INFO epoch # 6490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048491333436686546
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:28,250 INFO *** epoch 6490, rolling-avg-loss (window=10)= 0.005120344244460284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,274 INFO epoch # 6491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005026352017011959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,298 INFO epoch # 6492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00527906404749956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,322 INFO epoch # 6493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005155250779353082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,347 INFO epoch # 6494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00547875085612759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,371 INFO epoch # 6495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004958738223649561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,395 INFO epoch # 6496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005662417672283482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,419 INFO epoch # 6497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005221466788498219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,442 INFO epoch # 6498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005585888553468976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,466 INFO epoch # 6499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005787940332083963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,490 INFO epoch # 6500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005697183463780675
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:28,490 INFO *** epoch 6500, rolling-avg-loss (window=10)= 0.005385305273375707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,514 INFO epoch # 6501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006129618857812602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,538 INFO epoch # 6502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005631067004287615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,562 INFO epoch # 6503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005170341675693635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,586 INFO epoch # 6504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005145000650372822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,609 INFO epoch # 6505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051252313060103916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,633 INFO epoch # 6506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005200571700697765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,657 INFO epoch # 6507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005509779461135622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,681 INFO epoch # 6508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00529957671824377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,705 INFO epoch # 6509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005465609938255511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,729 INFO epoch # 6510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005764089750300627
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:28,729 INFO *** epoch 6510, rolling-avg-loss (window=10)= 0.005444088706281036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,753 INFO epoch # 6511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005468738709168974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,777 INFO epoch # 6512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005066517391242087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,801 INFO epoch # 6513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005076069486676715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,824 INFO epoch # 6514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004854594546486624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,848 INFO epoch # 6515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005098210160213057
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,872 INFO epoch # 6516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005465454305522144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,896 INFO epoch # 6517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005370260802010307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,920 INFO epoch # 6518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005113672224979382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,944 INFO epoch # 6519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051140818613930605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,968 INFO epoch # 6520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054885710051166825
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:28,968 INFO *** epoch 6520, rolling-avg-loss (window=10)= 0.005211617049280903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:28,992 INFO epoch # 6521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005135838444402907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,016 INFO epoch # 6522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006011692741594743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,040 INFO epoch # 6523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0055794203144614585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,063 INFO epoch # 6524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005741952758398838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,087 INFO epoch # 6525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005476509435538901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,111 INFO epoch # 6526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005046615689934697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,135 INFO epoch # 6527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004619497743988177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,159 INFO epoch # 6528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048708051872381475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,183 INFO epoch # 6529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005230278577073477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,206 INFO epoch # 6530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005240039052296197
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:29,207 INFO *** epoch 6530, rolling-avg-loss (window=10)= 0.005295264994492755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,230 INFO epoch # 6531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048882948904065415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,254 INFO epoch # 6532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004824339579499792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,278 INFO epoch # 6533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004966386673913803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,301 INFO epoch # 6534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004946615168591961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,325 INFO epoch # 6535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051158760470571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,349 INFO epoch # 6536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005179330684768502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,373 INFO epoch # 6537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005011008521250915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,397 INFO epoch # 6538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005010340340959374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,421 INFO epoch # 6539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005190852039959282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,445 INFO epoch # 6540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051797036358038895
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:29,445 INFO *** epoch 6540, rolling-avg-loss (window=10)= 0.005031274758221116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,469 INFO epoch # 6541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049152115425386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,493 INFO epoch # 6542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005162796747754328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,516 INFO epoch # 6543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00520846377912676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,540 INFO epoch # 6544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00510258835129207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,564 INFO epoch # 6545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004967311426298693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,588 INFO epoch # 6546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005412915532360785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,612 INFO epoch # 6547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00544047588846297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,636 INFO epoch # 6548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004923828018945642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,660 INFO epoch # 6549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005371142222429626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,684 INFO epoch # 6550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005088669684482738
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:29,684 INFO *** epoch 6550, rolling-avg-loss (window=10)= 0.005159340319369221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,708 INFO epoch # 6551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050514239628682844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,732 INFO epoch # 6552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004811481085198466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,756 INFO epoch # 6553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005107411299832165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,780 INFO epoch # 6554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004821007023565471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,804 INFO epoch # 6555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048007260338636115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,828 INFO epoch # 6556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050894242522190325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,852 INFO epoch # 6557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005001543213438708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,875 INFO epoch # 6558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005203562581300503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,899 INFO epoch # 6559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005053735651017632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,923 INFO epoch # 6560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005140070112247486
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:29,923 INFO *** epoch 6560, rolling-avg-loss (window=10)= 0.005008038521555136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,946 INFO epoch # 6561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004905404908640776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,970 INFO epoch # 6562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004936194614856504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:29,994 INFO epoch # 6563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005239923906628974
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,018 INFO epoch # 6564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005313319918059278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,042 INFO epoch # 6565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005258289187622722
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,066 INFO epoch # 6566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005462577159050852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,090 INFO epoch # 6567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005258540957584046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,113 INFO epoch # 6568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005115505475259852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,137 INFO epoch # 6569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049217591440537944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,161 INFO epoch # 6570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00539484919863753
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:30,161 INFO *** epoch 6570, rolling-avg-loss (window=10)= 0.005180636447039433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,185 INFO epoch # 6571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005309618969477015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,209 INFO epoch # 6572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005183689048863016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,232 INFO epoch # 6573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005062229411123553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,256 INFO epoch # 6574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004993905444280244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,280 INFO epoch # 6575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005136746640346246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,304 INFO epoch # 6576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005265989544568583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,328 INFO epoch # 6577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005156269129656721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,352 INFO epoch # 6578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005135329185577575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,376 INFO epoch # 6579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004963322309777141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,399 INFO epoch # 6580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005154982485692017
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:30,399 INFO *** epoch 6580, rolling-avg-loss (window=10)= 0.005136208216936211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,423 INFO epoch # 6581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005406259522715118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,447 INFO epoch # 6582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005414610022853594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,471 INFO epoch # 6583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005597332397883292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,495 INFO epoch # 6584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005021220385970082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,518 INFO epoch # 6585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004998969976441003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,542 INFO epoch # 6586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005242107312369626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,566 INFO epoch # 6587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005916270180023275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,590 INFO epoch # 6588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005594313224719372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,613 INFO epoch # 6589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005415071915194858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,637 INFO epoch # 6590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004991295510990312
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:30,637 INFO *** epoch 6590, rolling-avg-loss (window=10)= 0.0053597450449160535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,661 INFO epoch # 6591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005365974640881177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,685 INFO epoch # 6592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056448030736646615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,709 INFO epoch # 6593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005342090254998766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,733 INFO epoch # 6594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005357920737878885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,756 INFO epoch # 6595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005344913195585832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,780 INFO epoch # 6596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00534094981776434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,804 INFO epoch # 6597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004996227035007905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,827 INFO epoch # 6598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005198280567128677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,853 INFO epoch # 6599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005465571623062715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,877 INFO epoch # 6600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005130963152623735
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:30,878 INFO *** epoch 6600, rolling-avg-loss (window=10)= 0.00531876940985967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,902 INFO epoch # 6601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005128613658598624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,925 INFO epoch # 6602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005173036173800938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,949 INFO epoch # 6603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056430763434036635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,973 INFO epoch # 6604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005458452884340659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:30,996 INFO epoch # 6605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005378161025873851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,020 INFO epoch # 6606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005180610351089854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,043 INFO epoch # 6607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004925173649098724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,067 INFO epoch # 6608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004839355009607971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,091 INFO epoch # 6609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047892093934933655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,115 INFO epoch # 6610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00497876060398994
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:31,115 INFO *** epoch 6610, rolling-avg-loss (window=10)= 0.005149444909329759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,139 INFO epoch # 6611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004867248026130255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,163 INFO epoch # 6612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048422433756059036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,186 INFO epoch # 6613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005242341387202032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,210 INFO epoch # 6614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005031676264479756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,233 INFO epoch # 6615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004793653155502398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,257 INFO epoch # 6616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004791315277543617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,281 INFO epoch # 6617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004918743048619945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,305 INFO epoch # 6618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005191486452531535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,329 INFO epoch # 6619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005153142916242359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,353 INFO epoch # 6620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004844519826292526
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:31,353 INFO *** epoch 6620, rolling-avg-loss (window=10)= 0.004967636973015032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,378 INFO epoch # 6621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004958549619914265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,401 INFO epoch # 6622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004777841779286973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,425 INFO epoch # 6623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005285355986416107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,448 INFO epoch # 6624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005371313673094846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,472 INFO epoch # 6625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005497367958014365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,496 INFO epoch # 6626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005260861915303394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,519 INFO epoch # 6627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005370868675527163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,543 INFO epoch # 6628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005171117278223392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,567 INFO epoch # 6629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053564211339107715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,591 INFO epoch # 6630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005172145647520665
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:31,591 INFO *** epoch 6630, rolling-avg-loss (window=10)= 0.005222184366721195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,615 INFO epoch # 6631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00502454148590914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,639 INFO epoch # 6632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005629712759400718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,663 INFO epoch # 6633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005114059502375312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,686 INFO epoch # 6634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005166613569599576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,710 INFO epoch # 6635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00478225225378992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,734 INFO epoch # 6636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005375097753130831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,758 INFO epoch # 6637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005206321944569936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,782 INFO epoch # 6638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005278618176816963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,805 INFO epoch # 6639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005085371394670801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,829 INFO epoch # 6640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005715058759960812
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:31,829 INFO *** epoch 6640, rolling-avg-loss (window=10)= 0.005237764760022401
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,853 INFO epoch # 6641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00561574095627293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,877 INFO epoch # 6642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049278421138296835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,901 INFO epoch # 6643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005229037258686731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,925 INFO epoch # 6644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00501887040809379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,949 INFO epoch # 6645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005257856930256821
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,973 INFO epoch # 6646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005091881757834926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:31,997 INFO epoch # 6647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004857416690356331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,021 INFO epoch # 6648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005261272301140707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,045 INFO epoch # 6649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005333319379133172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,068 INFO epoch # 6650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004908445705950726
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:32,068 INFO *** epoch 6650, rolling-avg-loss (window=10)= 0.005150168350155582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,092 INFO epoch # 6651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050045407260768116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,116 INFO epoch # 6652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004819724665139802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,140 INFO epoch # 6653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004722869656688999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,163 INFO epoch # 6654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048913444188656285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,188 INFO epoch # 6655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004823578346986324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,211 INFO epoch # 6656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004869535747275222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,235 INFO epoch # 6657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004950154368998483
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,259 INFO epoch # 6658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004710784742201213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,282 INFO epoch # 6659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004908937560685445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,306 INFO epoch # 6660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005045041260018479
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:32,306 INFO *** epoch 6660, rolling-avg-loss (window=10)= 0.004874651149293641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,330 INFO epoch # 6661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004971999158442486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,354 INFO epoch # 6662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005006910701922607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,378 INFO epoch # 6663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050597439767443575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,402 INFO epoch # 6664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006571447673195507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,426 INFO epoch # 6665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005702225891582202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,450 INFO epoch # 6666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005666057200869545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,473 INFO epoch # 6667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00552956552564865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,496 INFO epoch # 6668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005172348188352771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,520 INFO epoch # 6669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004900503299722914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,543 INFO epoch # 6670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005692788174201269
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:32,543 INFO *** epoch 6670, rolling-avg-loss (window=10)= 0.005427358979068231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,567 INFO epoch # 6671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00510523500997806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,592 INFO epoch # 6672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0046706836110388394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,615 INFO epoch # 6673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004661263072193833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,639 INFO epoch # 6674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00488654084620066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,663 INFO epoch # 6675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004908318009256618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,686 INFO epoch # 6676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004741437784105074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,710 INFO epoch # 6677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005145387971424498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,734 INFO epoch # 6678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004939940445183311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,758 INFO epoch # 6679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005391033686464652
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,782 INFO epoch # 6680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005137276370078325
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:32,782 INFO *** epoch 6680, rolling-avg-loss (window=10)= 0.004958711680592387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,806 INFO epoch # 6681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005196532889385708
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,830 INFO epoch # 6682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005001197576348204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,855 INFO epoch # 6683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005032296423451044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,879 INFO epoch # 6684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00490081687894417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,902 INFO epoch # 6685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005005075043300167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,926 INFO epoch # 6686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005329078579961788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,950 INFO epoch # 6687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005273500260955188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,974 INFO epoch # 6688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005374199274228886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:32,998 INFO epoch # 6689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005116978914884385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,022 INFO epoch # 6690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005167735809664009
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:33,022 INFO *** epoch 6690, rolling-avg-loss (window=10)= 0.005139741165112355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,046 INFO epoch # 6691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004698258038843051
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,069 INFO epoch # 6692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004867236333666369
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,093 INFO epoch # 6693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004730062384624034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,117 INFO epoch # 6694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004860889552219305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,140 INFO epoch # 6695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005078199024865171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,164 INFO epoch # 6696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004815078878891654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,188 INFO epoch # 6697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004840542438614648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,211 INFO epoch # 6698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004758632101584226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,235 INFO epoch # 6699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005156174229341559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,259 INFO epoch # 6700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005396458909672219
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:33,259 INFO *** epoch 6700, rolling-avg-loss (window=10)= 0.004920153189232224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,283 INFO epoch # 6701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0058104523122892715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,306 INFO epoch # 6702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00504970033216523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,330 INFO epoch # 6703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047052516165422276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,354 INFO epoch # 6704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004747797876916593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,378 INFO epoch # 6705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005041265132604167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,402 INFO epoch # 6706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004970217298250645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,426 INFO epoch # 6707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005284792005113559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,450 INFO epoch # 6708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004934044758556411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,474 INFO epoch # 6709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005195061363338027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,498 INFO epoch # 6710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004981180500180926
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:33,498 INFO *** epoch 6710, rolling-avg-loss (window=10)= 0.0050719763195957055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,521 INFO epoch # 6711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005078316848084796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,545 INFO epoch # 6712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00480076720850775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,569 INFO epoch # 6713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00476622924907133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,592 INFO epoch # 6714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045201783104857896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,616 INFO epoch # 6715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004829266843444202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,641 INFO epoch # 6716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005050586085417308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,665 INFO epoch # 6717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005417352062067948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,689 INFO epoch # 6718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005663304669724312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,713 INFO epoch # 6719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005318618073943071
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,737 INFO epoch # 6720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005247499117103871
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:33,737 INFO *** epoch 6720, rolling-avg-loss (window=10)= 0.005069211846785038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,760 INFO epoch # 6721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004694912913691951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,784 INFO epoch # 6722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004912848162348382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,808 INFO epoch # 6723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004758876493724529
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,831 INFO epoch # 6724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00504784347867826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,856 INFO epoch # 6725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050418249193171505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,880 INFO epoch # 6726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0057013775876839645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,904 INFO epoch # 6727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005497987593116704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,928 INFO epoch # 6728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005958359233773081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,952 INFO epoch # 6729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00570206228439929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,976 INFO epoch # 6730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005220908504270483
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:33,976 INFO *** epoch 6730, rolling-avg-loss (window=10)= 0.00525370011710038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:33,999 INFO epoch # 6731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005305643688188866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,023 INFO epoch # 6732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005035665657487698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,047 INFO epoch # 6733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047474420789512806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,071 INFO epoch # 6734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00498303968925029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,094 INFO epoch # 6735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005415769330284093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,118 INFO epoch # 6736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004828466418985045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,142 INFO epoch # 6737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00547810807620408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,166 INFO epoch # 6738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005052199609053787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,189 INFO epoch # 6739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048556715992162935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,213 INFO epoch # 6740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005069674509286415
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:34,213 INFO *** epoch 6740, rolling-avg-loss (window=10)= 0.005077168065690784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,236 INFO epoch # 6741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050720645594992675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,260 INFO epoch # 6742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005393992600147612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,284 INFO epoch # 6743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005125454314111266
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,308 INFO epoch # 6744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004684993575210683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,331 INFO epoch # 6745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004718015268736053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,356 INFO epoch # 6746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005050852341810241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,379 INFO epoch # 6747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005592044522927608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,403 INFO epoch # 6748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004930744256853359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,427 INFO epoch # 6749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049237484272453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,450 INFO epoch # 6750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005077166453702375
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:34,450 INFO *** epoch 6750, rolling-avg-loss (window=10)= 0.005056907632024377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,474 INFO epoch # 6751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005435728489828762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,498 INFO epoch # 6752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00505781944229966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,522 INFO epoch # 6753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050595007196534425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,546 INFO epoch # 6754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005243898973276373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,570 INFO epoch # 6755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049706895915733185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,594 INFO epoch # 6756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004690253343142103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,618 INFO epoch # 6757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004609314943081699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,641 INFO epoch # 6758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00483772048028186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,665 INFO epoch # 6759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004905049710941967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,688 INFO epoch # 6760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004696462543506641
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:34,689 INFO *** epoch 6760, rolling-avg-loss (window=10)= 0.004950643823758582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,712 INFO epoch # 6761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004772455053171143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,736 INFO epoch # 6762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00491506680555176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,760 INFO epoch # 6763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005108249923068797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,784 INFO epoch # 6764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005415161998826079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,808 INFO epoch # 6765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004927272260829341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,831 INFO epoch # 6766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005048708302638261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,855 INFO epoch # 6767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004872787663771305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,879 INFO epoch # 6768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004897759776213206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,903 INFO epoch # 6769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005134767990966793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,927 INFO epoch # 6770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004822314127522986
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:34,927 INFO *** epoch 6770, rolling-avg-loss (window=10)= 0.0049914543902559675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,951 INFO epoch # 6771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004829065415833611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,975 INFO epoch # 6772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004593647794536082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:34,998 INFO epoch # 6773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004626895213732496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,022 INFO epoch # 6774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004610655152646359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,046 INFO epoch # 6775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00486549103516154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,070 INFO epoch # 6776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004962578117556404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,094 INFO epoch # 6777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00505680240894435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,117 INFO epoch # 6778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004740025695355143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,142 INFO epoch # 6779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00494086022808915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,167 INFO epoch # 6780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005025832953833742
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:35,167 INFO *** epoch 6780, rolling-avg-loss (window=10)= 0.004825185401568888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,191 INFO epoch # 6781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005015486243792111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,215 INFO epoch # 6782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005055311186879408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,239 INFO epoch # 6783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004983656581316609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,263 INFO epoch # 6784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005015096554416232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,286 INFO epoch # 6785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051955281451228075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,310 INFO epoch # 6786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00528307866625255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,334 INFO epoch # 6787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004880556196440011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,360 INFO epoch # 6788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005078532696643379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,384 INFO epoch # 6789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004965870866726618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,409 INFO epoch # 6790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004693068978667725
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:35,409 INFO *** epoch 6790, rolling-avg-loss (window=10)= 0.005016618611625745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,433 INFO epoch # 6791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004707947482529562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,456 INFO epoch # 6792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005222995503572747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,480 INFO epoch # 6793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005781589476100635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,504 INFO epoch # 6794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00507309399836231
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,527 INFO epoch # 6795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049745990145311225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,551 INFO epoch # 6796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004605856076523196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,575 INFO epoch # 6797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00447385483130347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,599 INFO epoch # 6798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004499454444157891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,622 INFO epoch # 6799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004813680818188004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,646 INFO epoch # 6800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005606152932159603
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:35,646 INFO *** epoch 6800, rolling-avg-loss (window=10)= 0.004975922457742854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,670 INFO epoch # 6801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051799423672491685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,694 INFO epoch # 6802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005109286452352535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,717 INFO epoch # 6803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004978913719241973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,741 INFO epoch # 6804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004745027079479769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,764 INFO epoch # 6805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00489894520433154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,788 INFO epoch # 6806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004528633318841457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,812 INFO epoch # 6807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004923625754599925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,836 INFO epoch # 6808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054052750710980035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,861 INFO epoch # 6809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005437228945083916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,885 INFO epoch # 6810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0056828055894584395
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:35,885 INFO *** epoch 6810, rolling-avg-loss (window=10)= 0.005088968350173672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,909 INFO epoch # 6811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050730325892800465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,933 INFO epoch # 6812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00520142047025729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,957 INFO epoch # 6813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004918061058560852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:35,981 INFO epoch # 6814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004933296106173657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,005 INFO epoch # 6815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049443973548477516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,030 INFO epoch # 6816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005692983795597684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,054 INFO epoch # 6817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005352610081899911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,077 INFO epoch # 6818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005069169666967355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,101 INFO epoch # 6819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004787691905221436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,124 INFO epoch # 6820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005549950539716519
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:36,125 INFO *** epoch 6820, rolling-avg-loss (window=10)= 0.00515226135685225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,148 INFO epoch # 6821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005024078986025415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,172 INFO epoch # 6822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004599352309014648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,196 INFO epoch # 6823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048565338147454895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,219 INFO epoch # 6824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005175074991711881
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,243 INFO epoch # 6825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00480815287301084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,267 INFO epoch # 6826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004787789504916873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,291 INFO epoch # 6827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004902290471363813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,315 INFO epoch # 6828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004781513871421339
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,338 INFO epoch # 6829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004973353563400451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,362 INFO epoch # 6830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004669701233069645
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:36,362 INFO *** epoch 6830, rolling-avg-loss (window=10)= 0.004857784161868039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,386 INFO epoch # 6831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005145460105268285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,410 INFO epoch # 6832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0046146599779604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,434 INFO epoch # 6833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004913330696581397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,457 INFO epoch # 6834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004966146923834458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,481 INFO epoch # 6835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005167871044250205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,505 INFO epoch # 6836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005410939498688094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,528 INFO epoch # 6837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005071776744443923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,552 INFO epoch # 6838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053110595617908984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,576 INFO epoch # 6839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005350489460397512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,599 INFO epoch # 6840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0054037004156271
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:36,600 INFO *** epoch 6840, rolling-avg-loss (window=10)= 0.005135543442884227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,623 INFO epoch # 6841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005283024918753654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,647 INFO epoch # 6842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0053207752425805666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,671 INFO epoch # 6843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004780620340170572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,695 INFO epoch # 6844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00485036877944367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,719 INFO epoch # 6845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004874321341048926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,742 INFO epoch # 6846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004783936597959837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,766 INFO epoch # 6847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004848393400607165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,789 INFO epoch # 6848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00479008911861456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,813 INFO epoch # 6849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050191528353025205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,837 INFO epoch # 6850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050138220030930825
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:36,837 INFO *** epoch 6850, rolling-avg-loss (window=10)= 0.0049564504577574555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,862 INFO epoch # 6851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005300658260239288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,886 INFO epoch # 6852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005184092617128044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,910 INFO epoch # 6853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052591245985240676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,934 INFO epoch # 6854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005091950995847583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,957 INFO epoch # 6855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004648277012165636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:36,981 INFO epoch # 6856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052779054203710984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,006 INFO epoch # 6857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005274344279314391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,030 INFO epoch # 6858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00486683343842742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,054 INFO epoch # 6859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004513923155172961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,077 INFO epoch # 6860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005088944722956512
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:37,078 INFO *** epoch 6860, rolling-avg-loss (window=10)= 0.0050506054500147005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,101 INFO epoch # 6861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004687527925852919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,125 INFO epoch # 6862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004661300845327787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,149 INFO epoch # 6863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052592030333471484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,173 INFO epoch # 6864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005051160005677957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,197 INFO epoch # 6865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00454844217892969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,220 INFO epoch # 6866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005233576244791038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,244 INFO epoch # 6867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004948183890519431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,268 INFO epoch # 6868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004596448612574022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,292 INFO epoch # 6869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00483896361765801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,316 INFO epoch # 6870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00445694598602131
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:37,316 INFO *** epoch 6870, rolling-avg-loss (window=10)= 0.0048281752340699315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,340 INFO epoch # 6871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004576483923301566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,364 INFO epoch # 6872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004550264384306502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,388 INFO epoch # 6873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004752917688165326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,412 INFO epoch # 6874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004763626857311465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,436 INFO epoch # 6875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005264231604087399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,459 INFO epoch # 6876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004798571462742984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,483 INFO epoch # 6877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005058253889728803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,507 INFO epoch # 6878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005227991096035112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,531 INFO epoch # 6879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00516182176215807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,555 INFO epoch # 6880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004754894274810795
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:37,555 INFO *** epoch 6880, rolling-avg-loss (window=10)= 0.004890905694264802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,579 INFO epoch # 6881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004808406694792211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,603 INFO epoch # 6882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005074266460724175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,626 INFO epoch # 6883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005068749051133636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,650 INFO epoch # 6884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004933511052513495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,674 INFO epoch # 6885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004827617285627639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,697 INFO epoch # 6886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004783793574461015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,721 INFO epoch # 6887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004799277878191788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,745 INFO epoch # 6888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005179514460905921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,769 INFO epoch # 6889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005692255806934554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,793 INFO epoch # 6890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005113582345074974
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:37,793 INFO *** epoch 6890, rolling-avg-loss (window=10)= 0.005028097461035941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,816 INFO epoch # 6891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004861928398895543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,840 INFO epoch # 6892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004740128271805588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,865 INFO epoch # 6893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005021206903620623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,889 INFO epoch # 6894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005245330787147395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,913 INFO epoch # 6895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005329102059477009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,937 INFO epoch # 6896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005218374397372827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,961 INFO epoch # 6897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005077573376183864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:37,984 INFO epoch # 6898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004855863408010919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,008 INFO epoch # 6899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004715548413514625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,032 INFO epoch # 6900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005084502081444953
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:38,032 INFO *** epoch 6900, rolling-avg-loss (window=10)= 0.005014955809747335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,056 INFO epoch # 6901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004912963726383168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,080 INFO epoch # 6902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004661907587433234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,103 INFO epoch # 6903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004710559245722834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,127 INFO epoch # 6904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004812224498891737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,151 INFO epoch # 6905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004729156502435217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,175 INFO epoch # 6906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005085807420982746
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,199 INFO epoch # 6907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005072515501524322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,222 INFO epoch # 6908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00594318108051084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,246 INFO epoch # 6909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005890661115699913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,270 INFO epoch # 6910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047906464751577005
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:38,270 INFO *** epoch 6910, rolling-avg-loss (window=10)= 0.0050609623154741715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,294 INFO epoch # 6911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004852033354836749
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,317 INFO epoch # 6912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0046729760979360435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,341 INFO epoch # 6913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004922185093164444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,365 INFO epoch # 6914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005005425569834188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,389 INFO epoch # 6915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004922229301882908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,413 INFO epoch # 6916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004755352754727937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,437 INFO epoch # 6917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047804270216147415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,462 INFO epoch # 6918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004626666792319156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,486 INFO epoch # 6919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004449738786206581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,510 INFO epoch # 6920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004740536034660181
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:38,510 INFO *** epoch 6920, rolling-avg-loss (window=10)= 0.004772757080718293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,534 INFO epoch # 6921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004989577959349845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,558 INFO epoch # 6922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045085168749210425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,582 INFO epoch # 6923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004506408447923604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,606 INFO epoch # 6924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004751408734591678
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,630 INFO epoch # 6925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004791456030943664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,654 INFO epoch # 6926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004801359187695198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,678 INFO epoch # 6927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047622612764826044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,701 INFO epoch # 6928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004688024826464243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,725 INFO epoch # 6929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004770854931848589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,748 INFO epoch # 6930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004720346158137545
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:38,748 INFO *** epoch 6930, rolling-avg-loss (window=10)= 0.004729021442835801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,773 INFO epoch # 6931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004711865978606511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,797 INFO epoch # 6932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004551943042315543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,821 INFO epoch # 6933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005272930080536753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,845 INFO epoch # 6934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005027815823268611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,869 INFO epoch # 6935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004838011373067275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,893 INFO epoch # 6936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004569209060719004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,917 INFO epoch # 6937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048444095700688194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,941 INFO epoch # 6938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004758088354719803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,964 INFO epoch # 6939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004837349544686731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:38,988 INFO epoch # 6940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004604549176292494
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:38,988 INFO *** epoch 6940, rolling-avg-loss (window=10)= 0.004801617200428154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,012 INFO epoch # 6941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047628698230255395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,036 INFO epoch # 6942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00505504799366463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,060 INFO epoch # 6943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004707143714767881
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,083 INFO epoch # 6944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004662723866204033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,107 INFO epoch # 6945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004671925446018577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,131 INFO epoch # 6946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005200730040087365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,155 INFO epoch # 6947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004866501491051167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,178 INFO epoch # 6948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004679713129007723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,202 INFO epoch # 6949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004726903258415405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,226 INFO epoch # 6950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049841682412079535
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:39,226 INFO *** epoch 6950, rolling-avg-loss (window=10)= 0.0048317727003450274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,250 INFO epoch # 6951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048637908839737065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,274 INFO epoch # 6952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005415101448306814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,297 INFO epoch # 6953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005300129079842009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,321 INFO epoch # 6954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00545537984726252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,345 INFO epoch # 6955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005146651361428667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,369 INFO epoch # 6956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004881985973042902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,393 INFO epoch # 6957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005277368225506507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,417 INFO epoch # 6958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004544502480712254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,440 INFO epoch # 6959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004540729511063546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,464 INFO epoch # 6960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004520330268860562
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:39,464 INFO *** epoch 6960, rolling-avg-loss (window=10)= 0.004994596907999948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,488 INFO epoch # 6961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004766381498484407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,512 INFO epoch # 6962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004744292156829033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,536 INFO epoch # 6963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004388207926240284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,559 INFO epoch # 6964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004571592158754356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,583 INFO epoch # 6965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004659458791138604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,607 INFO epoch # 6966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050839279720094055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,631 INFO epoch # 6967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005188338225707412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,656 INFO epoch # 6968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004935585919156438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,680 INFO epoch # 6969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004909908813715447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,704 INFO epoch # 6970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004633680007827934
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:39,704 INFO *** epoch 6970, rolling-avg-loss (window=10)= 0.004788137346986332
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,728 INFO epoch # 6971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004838403765461408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,752 INFO epoch # 6972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005113041246659122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,775 INFO epoch # 6973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004862560686888173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,799 INFO epoch # 6974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004557214517262764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,823 INFO epoch # 6975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004484221077291295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,847 INFO epoch # 6976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004829159763175994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,871 INFO epoch # 6977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005070468054327648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,895 INFO epoch # 6978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0046653665231133346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,919 INFO epoch # 6979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004516588298429269
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,942 INFO epoch # 6980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004479911956877913
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:39,943 INFO *** epoch 6980, rolling-avg-loss (window=10)= 0.004741693588948692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,966 INFO epoch # 6981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004886348549916875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:39,990 INFO epoch # 6982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004825454518140759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,014 INFO epoch # 6983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004744229225252639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,037 INFO epoch # 6984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004528571396804182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,062 INFO epoch # 6985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047748389770276845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,086 INFO epoch # 6986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004784191187354736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,110 INFO epoch # 6987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004519046520726988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,134 INFO epoch # 6988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004667608460295014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,158 INFO epoch # 6989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004582985573506448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,181 INFO epoch # 6990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004188041311863344
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:40,182 INFO *** epoch 6990, rolling-avg-loss (window=10)= 0.004650131572088867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,205 INFO epoch # 6991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004565053121041274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,228 INFO epoch # 6992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004617779566615354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,252 INFO epoch # 6993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004773616965394467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,276 INFO epoch # 6994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004994942479243036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,299 INFO epoch # 6995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004803214207640849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,323 INFO epoch # 6996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004659723890654277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,348 INFO epoch # 6997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00449782738724025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,372 INFO epoch # 6998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004434130762092536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,396 INFO epoch # 6999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004774209788592998
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,420 INFO epoch # 7000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004990978006389923
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:40,420 INFO *** epoch 7000, rolling-avg-loss (window=10)= 0.004711147617490497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,443 INFO epoch # 7001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004913219738227781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,467 INFO epoch # 7002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005063573509687558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,491 INFO epoch # 7003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004882966604782268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,514 INFO epoch # 7004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047417964015039615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,538 INFO epoch # 7005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004813989042304456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,563 INFO epoch # 7006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004869035434239777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,586 INFO epoch # 7007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005226676446909551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,610 INFO epoch # 7008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005352972671971656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,634 INFO epoch # 7009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005060053259512642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,657 INFO epoch # 7010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0060043965495424345
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:40,658 INFO *** epoch 7010, rolling-avg-loss (window=10)= 0.005092867965868208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,681 INFO epoch # 7011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005130612953507807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,705 INFO epoch # 7012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048280932096531615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,729 INFO epoch # 7013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004685755593527574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,753 INFO epoch # 7014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00460812145320233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,777 INFO epoch # 7015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004478667484363541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,801 INFO epoch # 7016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004635384211724158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,824 INFO epoch # 7017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004649773130950052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,850 INFO epoch # 7018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004917861399007961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,874 INFO epoch # 7019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005008514908695361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,898 INFO epoch # 7020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047753234903211705
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:40,898 INFO *** epoch 7020, rolling-avg-loss (window=10)= 0.004771810783495312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,923 INFO epoch # 7021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004443693978828378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,947 INFO epoch # 7022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004836462831008248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,971 INFO epoch # 7023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004896265752904583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:40,994 INFO epoch # 7024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004636725461750757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,018 INFO epoch # 7025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004561273133731447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,042 INFO epoch # 7026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004858088941546157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,066 INFO epoch # 7027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005551883994485252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,090 INFO epoch # 7028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005765549627540167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,114 INFO epoch # 7029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005146555260580499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,138 INFO epoch # 7030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004777796602866147
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:41,138 INFO *** epoch 7030, rolling-avg-loss (window=10)= 0.004947429558524163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,162 INFO epoch # 7031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00512422680913005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,186 INFO epoch # 7032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004946377302985638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,210 INFO epoch # 7033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004630582945537753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,234 INFO epoch # 7034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004520064019743586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,259 INFO epoch # 7035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004465844285732601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,282 INFO epoch # 7036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00461931548488792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,307 INFO epoch # 7037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004586275877954904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,331 INFO epoch # 7038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004432146364706568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,356 INFO epoch # 7039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004842359419853892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,381 INFO epoch # 7040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004619488383468706
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:41,381 INFO *** epoch 7040, rolling-avg-loss (window=10)= 0.004678668089400162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,405 INFO epoch # 7041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047423247306142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,430 INFO epoch # 7042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004627168280421756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,454 INFO epoch # 7043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047808939998503774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,478 INFO epoch # 7044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004669737165386323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,501 INFO epoch # 7045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049611386566539295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,525 INFO epoch # 7046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004811250444618054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,549 INFO epoch # 7047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004869501637585927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,574 INFO epoch # 7048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00465155304846121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,598 INFO epoch # 7049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005008458982047159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,622 INFO epoch # 7050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048178012075368315
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:41,622 INFO *** epoch 7050, rolling-avg-loss (window=10)= 0.004793982815317577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,647 INFO epoch # 7051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005182628799957456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,671 INFO epoch # 7052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004678428238548804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,695 INFO epoch # 7053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004775511974003166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,719 INFO epoch # 7054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004371894039650215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,743 INFO epoch # 7055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004555594197881874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,767 INFO epoch # 7056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004676704469602555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,791 INFO epoch # 7057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004429171407537069
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,814 INFO epoch # 7058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004651467683288502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,838 INFO epoch # 7059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005216997516981792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,863 INFO epoch # 7060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005521422572201118
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:41,863 INFO *** epoch 7060, rolling-avg-loss (window=10)= 0.004805982089965255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,887 INFO epoch # 7061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052147438618703745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,911 INFO epoch # 7062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004605300488037756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,935 INFO epoch # 7063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004547511591226794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,959 INFO epoch # 7064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004682711736677447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:41,983 INFO epoch # 7065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004947637607983779
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,006 INFO epoch # 7066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045769885218760464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,031 INFO epoch # 7067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005064179713372141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,055 INFO epoch # 7068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004950173031829763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,079 INFO epoch # 7069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004823869141546311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,103 INFO epoch # 7070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004991277390217874
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:42,103 INFO *** epoch 7070, rolling-avg-loss (window=10)= 0.0048404393084638285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,127 INFO epoch # 7071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00487785080622416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,151 INFO epoch # 7072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004833339025935857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,175 INFO epoch # 7073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005125510127982125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,198 INFO epoch # 7074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048533138433413114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,222 INFO epoch # 7075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005207047768635675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,246 INFO epoch # 7076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005198693626880413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,270 INFO epoch # 7077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005594662419753149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,294 INFO epoch # 7078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004822874212550232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,318 INFO epoch # 7079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004789292113855481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,342 INFO epoch # 7080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004920557432342321
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:42,342 INFO *** epoch 7080, rolling-avg-loss (window=10)= 0.005022314137750073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,368 INFO epoch # 7081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004724071492091753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,394 INFO epoch # 7082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005135369377967436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,418 INFO epoch # 7083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004743729787151096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,443 INFO epoch # 7084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004470384708838537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,467 INFO epoch # 7085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004849135868425947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,491 INFO epoch # 7086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004579230535455281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,515 INFO epoch # 7087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004597701998136472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,539 INFO epoch # 7088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004580839253321756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,562 INFO epoch # 7089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045783231435052585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,586 INFO epoch # 7090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044274535539443605
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:42,586 INFO *** epoch 7090, rolling-avg-loss (window=10)= 0.00466862397188379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,610 INFO epoch # 7091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004418828248162754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,633 INFO epoch # 7092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00477804660476977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,657 INFO epoch # 7093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004836460138903931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,683 INFO epoch # 7094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004774092329171253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,714 INFO epoch # 7095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004840983834583312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,743 INFO epoch # 7096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004681091726524755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,771 INFO epoch # 7097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004654689462768147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,796 INFO epoch # 7098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050704291425063275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,820 INFO epoch # 7099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004671527029131539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,844 INFO epoch # 7100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004666874279791955
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:42,845 INFO *** epoch 7100, rolling-avg-loss (window=10)= 0.004739302279631374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,869 INFO epoch # 7101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00440050372708356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,895 INFO epoch # 7102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004415963754581753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,919 INFO epoch # 7103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004341170093539404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,943 INFO epoch # 7104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004430086162756197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,967 INFO epoch # 7105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004489423721679486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:42,991 INFO epoch # 7106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004611184522218537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,015 INFO epoch # 7107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004475153560633771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,039 INFO epoch # 7108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004765394704008941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,076 INFO epoch # 7109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005345942634448875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,117 INFO epoch # 7110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004740391439554514
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:43,118 INFO *** epoch 7110, rolling-avg-loss (window=10)= 0.004601521432050504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,147 INFO epoch # 7111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004447569463081891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,175 INFO epoch # 7112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004555425468424801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,203 INFO epoch # 7113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004505178982071811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,233 INFO epoch # 7114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004845850977289956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,261 INFO epoch # 7115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004688529730628943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,288 INFO epoch # 7116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004632410091289785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,317 INFO epoch # 7117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049746392978704534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,343 INFO epoch # 7118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050614475112524815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,368 INFO epoch # 7119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00512283588614082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,393 INFO epoch # 7120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004705826358986087
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:43,393 INFO *** epoch 7120, rolling-avg-loss (window=10)= 0.004753971376703703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,417 INFO epoch # 7121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004894537814834621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,441 INFO epoch # 7122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050576506910147145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,465 INFO epoch # 7123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004676723918237258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,489 INFO epoch # 7124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004572860121697886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,513 INFO epoch # 7125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00481861199659761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,537 INFO epoch # 7126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047586083310307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,561 INFO epoch # 7127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00433361301838886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,585 INFO epoch # 7128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004445217306056293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,608 INFO epoch # 7129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004759410618135007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,632 INFO epoch # 7130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004754897083330434
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:43,632 INFO *** epoch 7130, rolling-avg-loss (window=10)= 0.0047072130899323385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,656 INFO epoch # 7131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005131898280524183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,680 INFO epoch # 7132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004805468859558459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,705 INFO epoch # 7133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004444403006345965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,729 INFO epoch # 7134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004294542130082846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,753 INFO epoch # 7135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005075093402410857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,777 INFO epoch # 7136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005203255714150146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,801 INFO epoch # 7137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045750202880299184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,824 INFO epoch # 7138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004730185428343248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,848 INFO epoch # 7139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00516135833458975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,872 INFO epoch # 7140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00492538174148649
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:43,872 INFO *** epoch 7140, rolling-avg-loss (window=10)= 0.004834660718552186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,896 INFO epoch # 7141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005011043631384382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,920 INFO epoch # 7142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004598839339450933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,944 INFO epoch # 7143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004743652913020924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,968 INFO epoch # 7144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004784606862813234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:43,992 INFO epoch # 7145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00487939317827113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,016 INFO epoch # 7146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004739748386782594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,040 INFO epoch # 7147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004550588710117154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,064 INFO epoch # 7148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004786635130585637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,088 INFO epoch # 7149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004569648801407311
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,112 INFO epoch # 7150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004603898705681786
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:44,112 INFO *** epoch 7150, rolling-avg-loss (window=10)= 0.0047268055659515085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,136 INFO epoch # 7151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004562952206470072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,160 INFO epoch # 7152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00497062533031567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,184 INFO epoch # 7153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004891691482043825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,208 INFO epoch # 7154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005196468115173047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,232 INFO epoch # 7155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004798918904270977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,255 INFO epoch # 7156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004874511716479901
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,279 INFO epoch # 7157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005133840939379297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,303 INFO epoch # 7158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00464043610918452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,327 INFO epoch # 7159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004530518199317157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,351 INFO epoch # 7160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004304897607653402
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:44,351 INFO *** epoch 7160, rolling-avg-loss (window=10)= 0.004790486061028787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,377 INFO epoch # 7161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004724210215499625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,402 INFO epoch # 7162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005148710366484011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,426 INFO epoch # 7163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004648999674827792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,450 INFO epoch # 7164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004862036992562935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,474 INFO epoch # 7165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004590900196490111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,498 INFO epoch # 7166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004825980977329891
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,521 INFO epoch # 7167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00429778486432042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,545 INFO epoch # 7168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004272561604011571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,569 INFO epoch # 7169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004441378288902342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,593 INFO epoch # 7170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0046537723756046034
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:44,594 INFO *** epoch 7170, rolling-avg-loss (window=10)= 0.00464663355560333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,618 INFO epoch # 7171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004598730585712474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,643 INFO epoch # 7172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048355928374803625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,667 INFO epoch # 7173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004840882756980136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,691 INFO epoch # 7174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005129932185809594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,715 INFO epoch # 7175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00466280039108824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,739 INFO epoch # 7176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004553983551886631
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,765 INFO epoch # 7177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004895077923720237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,789 INFO epoch # 7178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004865250033617485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,814 INFO epoch # 7179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004555382169201039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,839 INFO epoch # 7180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004692169241025113
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:44,839 INFO *** epoch 7180, rolling-avg-loss (window=10)= 0.004762980167652131
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,863 INFO epoch # 7181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004408375476486981
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,889 INFO epoch # 7182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00429274933776469
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,913 INFO epoch # 7183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004507490353717003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,937 INFO epoch # 7184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047116413988987915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,960 INFO epoch # 7185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004506031757046003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:44,984 INFO epoch # 7186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004682645456341561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,008 INFO epoch # 7187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004865225571847986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,032 INFO epoch # 7188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00471602963807527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,056 INFO epoch # 7189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004945959692122415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,080 INFO epoch # 7190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005060643976321444
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:45,080 INFO *** epoch 7190, rolling-avg-loss (window=10)= 0.0046696792658622146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,104 INFO epoch # 7191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004785387864103541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,129 INFO epoch # 7192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004773404500156175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,153 INFO epoch # 7193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004725967984995805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,177 INFO epoch # 7194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00442423590720864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,201 INFO epoch # 7195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004598522715241415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,225 INFO epoch # 7196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042844264789891895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,249 INFO epoch # 7197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004466781709197676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,273 INFO epoch # 7198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004839507288124878
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,297 INFO epoch # 7199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004343894983321661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,321 INFO epoch # 7200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004609385101502994
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:45,321 INFO *** epoch 7200, rolling-avg-loss (window=10)= 0.0045851514532841975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,344 INFO epoch # 7201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004727896688564215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,368 INFO epoch # 7202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047801500841160305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,393 INFO epoch # 7203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004974249724909896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,417 INFO epoch # 7204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004987845244613709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,441 INFO epoch # 7205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004992217793187592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,465 INFO epoch # 7206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005223633568675723
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,489 INFO epoch # 7207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004857655883824918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,513 INFO epoch # 7208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005004586426366586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,537 INFO epoch # 7209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004651605864637531
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,561 INFO epoch # 7210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004323022676544497
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:45,561 INFO *** epoch 7210, rolling-avg-loss (window=10)= 0.00485228639554407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,584 INFO epoch # 7211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004667651795898564
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,608 INFO epoch # 7212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004454698861081852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,631 INFO epoch # 7213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051411281201581005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,656 INFO epoch # 7214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005770828276581597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,680 INFO epoch # 7215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004995116476493422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,705 INFO epoch # 7216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004526817123405635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,729 INFO epoch # 7217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004702387421275489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,753 INFO epoch # 7218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004875435559370089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,777 INFO epoch # 7219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004325868765590712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,800 INFO epoch # 7220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004747909311845433
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:45,800 INFO *** epoch 7220, rolling-avg-loss (window=10)= 0.004820784171170089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,824 INFO epoch # 7221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005088855265057646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,848 INFO epoch # 7222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004585351882269606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,873 INFO epoch # 7223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00444537461589789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,899 INFO epoch # 7224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004771675558004063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,925 INFO epoch # 7225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004457540017028805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,950 INFO epoch # 7226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004307116323616356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,973 INFO epoch # 7227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004482721255044453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:45,997 INFO epoch # 7228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004386033993796445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,021 INFO epoch # 7229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004077487978065619
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,044 INFO epoch # 7230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004283467453205958
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:46,044 INFO *** epoch 7230, rolling-avg-loss (window=10)= 0.004488562434198684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,068 INFO epoch # 7231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004222406692861114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,092 INFO epoch # 7232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004500013652432244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,116 INFO epoch # 7233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004794856569787953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,140 INFO epoch # 7234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004504736898525152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,164 INFO epoch # 7235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004517821180343162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,188 INFO epoch # 7236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004672668015700765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,212 INFO epoch # 7237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004633998170902487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,236 INFO epoch # 7238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044570024983840995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,259 INFO epoch # 7239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004575465136440471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,283 INFO epoch # 7240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048831421008799225
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:46,283 INFO *** epoch 7240, rolling-avg-loss (window=10)= 0.004576211091625737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,307 INFO epoch # 7241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048236598377116024
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,331 INFO epoch # 7242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004677852673921734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,355 INFO epoch # 7243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004618140126694925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,380 INFO epoch # 7244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004702549718786031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,404 INFO epoch # 7245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00512440234888345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,428 INFO epoch # 7246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00492339015181642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,452 INFO epoch # 7247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005569287321122829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,476 INFO epoch # 7248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005066464553237893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,500 INFO epoch # 7249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005584670692769578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,524 INFO epoch # 7250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005140547735209111
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:46,524 INFO *** epoch 7250, rolling-avg-loss (window=10)= 0.0050230965160153575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,548 INFO epoch # 7251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004670078422350343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,572 INFO epoch # 7252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004336580248491373
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,597 INFO epoch # 7253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004302117602492217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,621 INFO epoch # 7254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004362772884633159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,645 INFO epoch # 7255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004493719869060442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,669 INFO epoch # 7256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004455466129002161
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,692 INFO epoch # 7257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004488671198487282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,716 INFO epoch # 7258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004547419805021491
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,740 INFO epoch # 7259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045321901416173205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,764 INFO epoch # 7260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004514047170232516
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:46,764 INFO *** epoch 7260, rolling-avg-loss (window=10)= 0.00447030634713883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,788 INFO epoch # 7261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004233058509271359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,812 INFO epoch # 7262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004483198004891165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,836 INFO epoch # 7263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004452199413208291
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,860 INFO epoch # 7264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004411919835547451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,885 INFO epoch # 7265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004654195472539868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,908 INFO epoch # 7266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004602385983162094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,932 INFO epoch # 7267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004492234198551159
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,956 INFO epoch # 7268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045395674897008575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:46,980 INFO epoch # 7269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004500252842262853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,004 INFO epoch # 7270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00517127821512986
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:47,005 INFO *** epoch 7270, rolling-avg-loss (window=10)= 0.004554028996426496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,029 INFO epoch # 7271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005708641612727661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,052 INFO epoch # 7272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005074003587651532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,076 INFO epoch # 7273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047467535332543775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,099 INFO epoch # 7274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004090604063094361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,123 INFO epoch # 7275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004073678290296812
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,147 INFO epoch # 7276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00450010942586232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,171 INFO epoch # 7277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004521818671491928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,195 INFO epoch # 7278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004837486834730953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,219 INFO epoch # 7279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004726892577309627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,242 INFO epoch # 7280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004870752003625967
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:47,243 INFO *** epoch 7280, rolling-avg-loss (window=10)= 0.004715074060004554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,266 INFO epoch # 7281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004757595601404319
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,290 INFO epoch # 7282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0046790293490630575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,313 INFO epoch # 7283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00451663693820592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,337 INFO epoch # 7284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005029871113947593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,360 INFO epoch # 7285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0046399478014791384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,385 INFO epoch # 7286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005117015338328201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,410 INFO epoch # 7287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043406786062405445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,434 INFO epoch # 7288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00446274929709034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,458 INFO epoch # 7289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004435134578670841
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,482 INFO epoch # 7290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004323000302974833
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:47,482 INFO *** epoch 7290, rolling-avg-loss (window=10)= 0.004630165892740479
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,506 INFO epoch # 7291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004476169418921927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,529 INFO epoch # 7292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00436402202831232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,553 INFO epoch # 7293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043620409778668545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,576 INFO epoch # 7294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044240973656997085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,600 INFO epoch # 7295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004415843475726433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,624 INFO epoch # 7296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004381302878755378
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,648 INFO epoch # 7297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004672671471780632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,672 INFO epoch # 7298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004638733640604187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,696 INFO epoch # 7299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004575039718474727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,720 INFO epoch # 7300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00482006849051686
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:47,720 INFO *** epoch 7300, rolling-avg-loss (window=10)= 0.004512998946665903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,744 INFO epoch # 7301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004757508577313274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,767 INFO epoch # 7302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004642337218683679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,791 INFO epoch # 7303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004475597757846117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,815 INFO epoch # 7304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004304502635932295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,839 INFO epoch # 7305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004919224324112292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,863 INFO epoch # 7306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004595945327309892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,888 INFO epoch # 7307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004629156494047493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,912 INFO epoch # 7308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004637784244550858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,935 INFO epoch # 7309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004889487616310362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,959 INFO epoch # 7310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004683026152633829
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:47,959 INFO *** epoch 7310, rolling-avg-loss (window=10)= 0.004653457034874009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:47,983 INFO epoch # 7311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004499004688113928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,006 INFO epoch # 7312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004497193134739064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,030 INFO epoch # 7313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00434776383190183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,054 INFO epoch # 7314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004231951919791754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,078 INFO epoch # 7315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004488474070967641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,102 INFO epoch # 7316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004336977894126903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,125 INFO epoch # 7317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004281533358152956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,149 INFO epoch # 7318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004311692813644186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,172 INFO epoch # 7319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004269088774890406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,196 INFO epoch # 7320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004295361170079559
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:48,196 INFO *** epoch 7320, rolling-avg-loss (window=10)= 0.004355904165640823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,220 INFO epoch # 7321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004363920987088932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,243 INFO epoch # 7322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004478171613300219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,267 INFO epoch # 7323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004908805749437306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,291 INFO epoch # 7324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004834655323065817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,315 INFO epoch # 7325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00470634449084173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,339 INFO epoch # 7326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005011741563066607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,363 INFO epoch # 7327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004848430307902163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,387 INFO epoch # 7328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004954781059495872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,411 INFO epoch # 7329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004895285852398956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,435 INFO epoch # 7330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004591023804096039
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:48,435 INFO *** epoch 7330, rolling-avg-loss (window=10)= 0.004759316075069364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,459 INFO epoch # 7331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004627711605280638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,483 INFO epoch # 7332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004754380497615784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,507 INFO epoch # 7333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047731442682561465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,531 INFO epoch # 7334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0046619000131613575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,555 INFO epoch # 7335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004779205442901002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,579 INFO epoch # 7336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004537611825071508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,603 INFO epoch # 7337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004385256230307277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,626 INFO epoch # 7338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004910481293336488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,650 INFO epoch # 7339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048457819466420915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,674 INFO epoch # 7340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004842855472816154
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:48,674 INFO *** epoch 7340, rolling-avg-loss (window=10)= 0.004711832859538845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,698 INFO epoch # 7341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004382754956168355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,722 INFO epoch # 7342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004207214107736945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,746 INFO epoch # 7343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004977234417310683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,770 INFO epoch # 7344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045384917393676005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,794 INFO epoch # 7345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004270967710908735
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,817 INFO epoch # 7346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004948251171299489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,841 INFO epoch # 7347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00539566416409798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,865 INFO epoch # 7348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004972088921931572
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,890 INFO epoch # 7349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005178103961952729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,914 INFO epoch # 7350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005044024503149558
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:48,914 INFO *** epoch 7350, rolling-avg-loss (window=10)= 0.004791479565392364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,938 INFO epoch # 7351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004607506481988821
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,962 INFO epoch # 7352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004737938728794688
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:48,986 INFO epoch # 7353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004539952518825885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,010 INFO epoch # 7354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004366270495665958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,034 INFO epoch # 7355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004782933610840701
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,058 INFO epoch # 7356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004469479637918994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,081 INFO epoch # 7357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004580134802381508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,105 INFO epoch # 7358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047307922250183765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,129 INFO epoch # 7359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004470540901820641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,153 INFO epoch # 7360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004863689213379985
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:49,153 INFO *** epoch 7360, rolling-avg-loss (window=10)= 0.0046149238616635556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,177 INFO epoch # 7361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004527019711531466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,201 INFO epoch # 7362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004234154286677949
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,225 INFO epoch # 7363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004417282223585062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,248 INFO epoch # 7364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004377181765448768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,272 INFO epoch # 7365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004242224236804759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,296 INFO epoch # 7366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005020072632760275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,320 INFO epoch # 7367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005083430231024977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,344 INFO epoch # 7368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004588054223859217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,368 INFO epoch # 7369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004432463578268653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,392 INFO epoch # 7370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004646805486117955
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:49,392 INFO *** epoch 7370, rolling-avg-loss (window=10)= 0.004556868837607908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,416 INFO epoch # 7371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004409537126775831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,440 INFO epoch # 7372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004160291369771585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,463 INFO epoch # 7373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004180443451332394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,487 INFO epoch # 7374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041012007677636575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,511 INFO epoch # 7375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00414906011428684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,534 INFO epoch # 7376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004343340810009977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,558 INFO epoch # 7377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004295582264603581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,582 INFO epoch # 7378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004726165869215038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,606 INFO epoch # 7379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005121503127156757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,630 INFO epoch # 7380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004753989716846263
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:49,631 INFO *** epoch 7380, rolling-avg-loss (window=10)= 0.0044241114617761925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,654 INFO epoch # 7381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004659223595808726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,678 INFO epoch # 7382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004434105645486852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,702 INFO epoch # 7383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004346527632151265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,725 INFO epoch # 7384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004774792825628538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,749 INFO epoch # 7385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004779234463057946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,773 INFO epoch # 7386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004395357689645607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,798 INFO epoch # 7387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004635793164197821
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,822 INFO epoch # 7388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004895892518106848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,846 INFO epoch # 7389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004763089898915496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,870 INFO epoch # 7390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00490438353153877
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:49,871 INFO *** epoch 7390, rolling-avg-loss (window=10)= 0.004658840096453787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,895 INFO epoch # 7391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004893746394373011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,919 INFO epoch # 7392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045048299252812285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,943 INFO epoch # 7393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004186360631138086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,966 INFO epoch # 7394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004446944883966353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:49,990 INFO epoch # 7395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005235116543190088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,014 INFO epoch # 7396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004842041937081376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,038 INFO epoch # 7397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004485361696424661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,062 INFO epoch # 7398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0047306795640906785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,086 INFO epoch # 7399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004581774337566458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,110 INFO epoch # 7400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00459071599834715
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:50,110 INFO *** epoch 7400, rolling-avg-loss (window=10)= 0.004649757191145909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,133 INFO epoch # 7401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004369134141597897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,157 INFO epoch # 7402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004445235532330116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,181 INFO epoch # 7403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004205281584290788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,204 INFO epoch # 7404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004592409539327491
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,228 INFO epoch # 7405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045044237122056074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,252 INFO epoch # 7406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004482043266762048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,276 INFO epoch # 7407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004752280001412146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,299 INFO epoch # 7408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005169144882529508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,323 INFO epoch # 7409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004686412044975441
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,347 INFO epoch # 7410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004394817595311906
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:50,347 INFO *** epoch 7410, rolling-avg-loss (window=10)= 0.004560118230074295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,370 INFO epoch # 7411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00450129434466362
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,395 INFO epoch # 7412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004429661217727698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,419 INFO epoch # 7413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004430197783221956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,442 INFO epoch # 7414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0049051392343244515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,466 INFO epoch # 7415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004220770490064751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,490 INFO epoch # 7416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004151080811425345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,514 INFO epoch # 7417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004221040460834047
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,537 INFO epoch # 7418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042932868309435435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,561 INFO epoch # 7419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004500048751651775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,585 INFO epoch # 7420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004314359208365204
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:50,585 INFO *** epoch 7420, rolling-avg-loss (window=10)= 0.004396687913322239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,609 INFO epoch # 7421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044051214717910625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,633 INFO epoch # 7422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004269443357770797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,657 INFO epoch # 7423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004211969226162182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,681 INFO epoch # 7424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004380015798233217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,705 INFO epoch # 7425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004473314900678815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,728 INFO epoch # 7426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004277731597539969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,752 INFO epoch # 7427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004272150887118187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,776 INFO epoch # 7428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004128656881221104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,800 INFO epoch # 7429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004304121677705552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,824 INFO epoch # 7430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004221908726321999
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:50,824 INFO *** epoch 7430, rolling-avg-loss (window=10)= 0.004294443452454289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,849 INFO epoch # 7431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004514063548413105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,873 INFO epoch # 7432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00436055128739099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,898 INFO epoch # 7433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004221590988890966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,922 INFO epoch # 7434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004045387653604848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,946 INFO epoch # 7435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00411861615430098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,970 INFO epoch # 7436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004179884090262931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:50,993 INFO epoch # 7437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00435638195995125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,017 INFO epoch # 7438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004638071775843855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,041 INFO epoch # 7439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004270775985787623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,065 INFO epoch # 7440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045326775798457675
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:51,065 INFO *** epoch 7440, rolling-avg-loss (window=10)= 0.004323800102429232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,089 INFO epoch # 7441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00462032575160265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,113 INFO epoch # 7442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004431209214089904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,137 INFO epoch # 7443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004283669593860395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,161 INFO epoch # 7444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042157176103501115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,185 INFO epoch # 7445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004533245231868932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,208 INFO epoch # 7446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004722715944808442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,232 INFO epoch # 7447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004347895075625274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,255 INFO epoch # 7448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005215350742219016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,279 INFO epoch # 7449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004600873609888367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,303 INFO epoch # 7450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004779291732120328
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:51,303 INFO *** epoch 7450, rolling-avg-loss (window=10)= 0.004575029450643342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,327 INFO epoch # 7451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00424461920192698
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,351 INFO epoch # 7452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0046876963606337085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,375 INFO epoch # 7453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004914011697110254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,400 INFO epoch # 7454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004568478481814964
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,424 INFO epoch # 7455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004677167235058732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,447 INFO epoch # 7456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004336853868153412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,471 INFO epoch # 7457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004515344087849371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,495 INFO epoch # 7458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004350950686784927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,519 INFO epoch # 7459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004326074544223957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,543 INFO epoch # 7460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004436158847965999
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:51,543 INFO *** epoch 7460, rolling-avg-loss (window=10)= 0.00450573550115223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,567 INFO epoch # 7461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004181265416264068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,590 INFO epoch # 7462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004572008896502666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,614 INFO epoch # 7463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043368144033593126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,638 INFO epoch # 7464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039989225115277804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,661 INFO epoch # 7465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042203974262520205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,685 INFO epoch # 7466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004507900524913566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,709 INFO epoch # 7467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004599418898578733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,732 INFO epoch # 7468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043213860772084445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,756 INFO epoch # 7469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004246844404406147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,780 INFO epoch # 7470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004714723574579693
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:51,780 INFO *** epoch 7470, rolling-avg-loss (window=10)= 0.004369968213359244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,804 INFO epoch # 7471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005234656004176941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,828 INFO epoch # 7472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004742787852592301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,852 INFO epoch # 7473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042608853545971215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,875 INFO epoch # 7474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004373439973278437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,900 INFO epoch # 7475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043465513408591505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,924 INFO epoch # 7476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004388003955682507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,948 INFO epoch # 7477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004700817724369699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,972 INFO epoch # 7478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004361886236438295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:51,996 INFO epoch # 7479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005255138690699823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,019 INFO epoch # 7480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004704555296484614
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:52,020 INFO *** epoch 7480, rolling-avg-loss (window=10)= 0.004636872242917889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,044 INFO epoch # 7481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004231511793477694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,068 INFO epoch # 7482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041416635758650955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,092 INFO epoch # 7483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004041722626425326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,115 INFO epoch # 7484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004227381312375655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,139 INFO epoch # 7485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004428326530614868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,163 INFO epoch # 7486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004520823560596909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,187 INFO epoch # 7487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00454422265465837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,211 INFO epoch # 7488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004848460193898063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,234 INFO epoch # 7489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050447790999896824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,258 INFO epoch # 7490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004975125739292707
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:52,258 INFO *** epoch 7490, rolling-avg-loss (window=10)= 0.004500401708719437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,282 INFO epoch # 7491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044994943018537015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,306 INFO epoch # 7492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00433039744530106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,329 INFO epoch # 7493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004351570973085472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,354 INFO epoch # 7494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004814188978343736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,379 INFO epoch # 7495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004411210378748365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,404 INFO epoch # 7496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004606395559676457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,428 INFO epoch # 7497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004971998088876717
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,452 INFO epoch # 7498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004576953590003541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,475 INFO epoch # 7499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004469654522836208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,499 INFO epoch # 7500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004248493947670795
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:52,499 INFO *** epoch 7500, rolling-avg-loss (window=10)= 0.004528035778639606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,523 INFO epoch # 7501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042120628895645496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,547 INFO epoch # 7502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038649655289191287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,571 INFO epoch # 7503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004227574972901493
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,595 INFO epoch # 7504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004477237060200423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,620 INFO epoch # 7505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004314758007240016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,643 INFO epoch # 7506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004743542020150926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,667 INFO epoch # 7507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004109114128368674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,691 INFO epoch # 7508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004199503258860204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,714 INFO epoch # 7509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004693750262958929
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,738 INFO epoch # 7510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048104493180289865
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:52,738 INFO *** epoch 7510, rolling-avg-loss (window=10)= 0.004365295744719333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,762 INFO epoch # 7511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004300665445043705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,786 INFO epoch # 7512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004583942201861646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,810 INFO epoch # 7513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004913107946777018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,834 INFO epoch # 7514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004407653112139087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,858 INFO epoch # 7515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004712879115686519
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,883 INFO epoch # 7516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004592661920469254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,907 INFO epoch # 7517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004309945754357614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,930 INFO epoch # 7518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004077744553796947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,954 INFO epoch # 7519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004139524986385368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:52,978 INFO epoch # 7520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044905604590894654
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:52,978 INFO *** epoch 7520, rolling-avg-loss (window=10)= 0.004452868549560662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,002 INFO epoch # 7521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042811339008039795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,026 INFO epoch # 7522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004369625610706862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,050 INFO epoch # 7523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004293645688449033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,073 INFO epoch # 7524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004050882387673482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,097 INFO epoch # 7525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004239322843204718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,121 INFO epoch # 7526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004605980757332873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,144 INFO epoch # 7527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004195364817860536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,168 INFO epoch # 7528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005312695120665012
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,192 INFO epoch # 7529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005174399848328903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,216 INFO epoch # 7530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004311685355787631
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:53,216 INFO *** epoch 7530, rolling-avg-loss (window=10)= 0.004483473633081303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,240 INFO epoch # 7531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005056468136899639
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,264 INFO epoch # 7532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004784906093846075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,287 INFO epoch # 7533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004158291463681962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,311 INFO epoch # 7534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004211906216369243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,335 INFO epoch # 7535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041488738570478745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,359 INFO epoch # 7536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041306833663838916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,383 INFO epoch # 7537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004388952816952951
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,407 INFO epoch # 7538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004563978960504755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,431 INFO epoch # 7539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004543257331533823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,455 INFO epoch # 7540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00423945867078146
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:53,456 INFO *** epoch 7540, rolling-avg-loss (window=10)= 0.004422677691400168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,480 INFO epoch # 7541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004092590414074948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,504 INFO epoch # 7542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004351867413788568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,527 INFO epoch # 7543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004830310033867136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,551 INFO epoch # 7544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004505192817305215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,575 INFO epoch # 7545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004022739467473002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,598 INFO epoch # 7546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004164049751125276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,623 INFO epoch # 7547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004440082295332104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,647 INFO epoch # 7548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00456726499760407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,670 INFO epoch # 7549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004156475853960728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,695 INFO epoch # 7550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042712961658253334
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:53,695 INFO *** epoch 7550, rolling-avg-loss (window=10)= 0.004340186921035638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,718 INFO epoch # 7551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004792940882907715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,742 INFO epoch # 7552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005258642820990644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,766 INFO epoch # 7553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005210142437135801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,790 INFO epoch # 7554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004937417310429737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,813 INFO epoch # 7555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045839222220820375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,837 INFO epoch # 7556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041769443632801995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,862 INFO epoch # 7557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004291669625672512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,887 INFO epoch # 7558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004280513549019815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,911 INFO epoch # 7559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004595589998643845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,935 INFO epoch # 7560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004565103612549137
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:53,936 INFO *** epoch 7560, rolling-avg-loss (window=10)= 0.004669288682271145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,959 INFO epoch # 7561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004120280736970017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:53,983 INFO epoch # 7562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004031930275232298
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,006 INFO epoch # 7563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004176574231678387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,030 INFO epoch # 7564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004350992836407386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,054 INFO epoch # 7565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004232651725033065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,078 INFO epoch # 7566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041314555710414425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,103 INFO epoch # 7567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004470705600397196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,128 INFO epoch # 7568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004501708139287075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,152 INFO epoch # 7569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004496568566537462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,176 INFO epoch # 7570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005156868563062744
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:54,176 INFO *** epoch 7570, rolling-avg-loss (window=10)= 0.004366973624564707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,199 INFO epoch # 7571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006381749004503945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,223 INFO epoch # 7572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00783563447475899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,247 INFO epoch # 7573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004892986111372011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,271 INFO epoch # 7574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004246126860380173
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,295 INFO epoch # 7575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004278231797798071
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,318 INFO epoch # 7576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00429766132219811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,343 INFO epoch # 7577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004748309238493675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,367 INFO epoch # 7578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004419915137987118
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,393 INFO epoch # 7579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004301317811041372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,416 INFO epoch # 7580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004561214896966703
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:54,416 INFO *** epoch 7580, rolling-avg-loss (window=10)= 0.004996314665550017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,440 INFO epoch # 7581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004428580941748805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,464 INFO epoch # 7582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043066420330433175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,488 INFO epoch # 7583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043481073626026046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,512 INFO epoch # 7584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004443934580194764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,536 INFO epoch # 7585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004095698677701876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,560 INFO epoch # 7586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042743513331515715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,584 INFO epoch # 7587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004517272151133511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,608 INFO epoch # 7588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004138534037338104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,632 INFO epoch # 7589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004129004715650808
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,656 INFO epoch # 7590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041261850456066895
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:54,656 INFO *** epoch 7590, rolling-avg-loss (window=10)= 0.0042808310878172055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,680 INFO epoch # 7591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00446219288642169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,703 INFO epoch # 7592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004363307052699383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,727 INFO epoch # 7593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004582998411933659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,751 INFO epoch # 7594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004344915563706309
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,775 INFO epoch # 7595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004053387536259834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,799 INFO epoch # 7596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004102782510017278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,823 INFO epoch # 7597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003951680526370183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,847 INFO epoch # 7598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004354498705652077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,871 INFO epoch # 7599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043477611397975124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,896 INFO epoch # 7600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004131924484681804
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:54,896 INFO *** epoch 7600, rolling-avg-loss (window=10)= 0.004269544881753973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,920 INFO epoch # 7601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00428417186049046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,944 INFO epoch # 7602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004142537818552228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,968 INFO epoch # 7603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004445411446795333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:54,992 INFO epoch # 7604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004667055312893353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,016 INFO epoch # 7605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004800052229256835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,040 INFO epoch # 7606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004270816734788241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,064 INFO epoch # 7607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004324747660575667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,087 INFO epoch # 7608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004774973709572805
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,111 INFO epoch # 7609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004307685710955411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,135 INFO epoch # 7610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044095913544879295
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:55,136 INFO *** epoch 7610, rolling-avg-loss (window=10)= 0.0044427043838368265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,160 INFO epoch # 7611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004341003892477602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,184 INFO epoch # 7612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00436603466005181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,208 INFO epoch # 7613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004279685046640225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,232 INFO epoch # 7614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003919504466466606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,257 INFO epoch # 7615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004162032615568023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,280 INFO epoch # 7616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004675063188187778
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,304 INFO epoch # 7617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004702672558778431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,328 INFO epoch # 7618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004248614197422285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,352 INFO epoch # 7619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004177080045337789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,377 INFO epoch # 7620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004034218545712065
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:55,377 INFO *** epoch 7620, rolling-avg-loss (window=10)= 0.004290590921664261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,403 INFO epoch # 7621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004260953690391034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,427 INFO epoch # 7622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041637992253527045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,450 INFO epoch # 7623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003941253999073524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,474 INFO epoch # 7624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004535099185886793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,498 INFO epoch # 7625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00443678253213875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,522 INFO epoch # 7626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040397003140242305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,545 INFO epoch # 7627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004207042664347682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,569 INFO epoch # 7628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004026510388939641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,594 INFO epoch # 7629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041752522374736145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,618 INFO epoch # 7630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004082991308678174
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:55,618 INFO *** epoch 7630, rolling-avg-loss (window=10)= 0.004186938554630615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,643 INFO epoch # 7631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003985972187365405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,667 INFO epoch # 7632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003955779317038832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,691 INFO epoch # 7633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004329845618485706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,715 INFO epoch # 7634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004578526546538342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,739 INFO epoch # 7635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004496295649005333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,763 INFO epoch # 7636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004405099894938758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,786 INFO epoch # 7637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004351150411821436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,811 INFO epoch # 7638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005211013711232226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,835 INFO epoch # 7639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004736123835755279
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,860 INFO epoch # 7640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004261534362740349
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:55,860 INFO *** epoch 7640, rolling-avg-loss (window=10)= 0.004431134153492167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,884 INFO epoch # 7641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004304661524656694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,908 INFO epoch # 7642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004105422318389174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,931 INFO epoch # 7643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004375474352855235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,955 INFO epoch # 7644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00409916620264994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:55,979 INFO epoch # 7645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004235890010022558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,002 INFO epoch # 7646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004080108403286431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,026 INFO epoch # 7647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004047755013743881
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,050 INFO epoch # 7648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004612398857716471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,074 INFO epoch # 7649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004290793738618959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,099 INFO epoch # 7650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004525017066043802
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:56,099 INFO *** epoch 7650, rolling-avg-loss (window=10)= 0.004267668748798315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,122 INFO epoch # 7651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004332730375608662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,146 INFO epoch # 7652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004164862253674073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,170 INFO epoch # 7653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003987487274571322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,193 INFO epoch # 7654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004162846729741432
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,217 INFO epoch # 7655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004501238563534571
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,240 INFO epoch # 7656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004785593802807853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,264 INFO epoch # 7657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005151437686436111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,288 INFO epoch # 7658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004381847411423223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,312 INFO epoch # 7659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004339578332292149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,335 INFO epoch # 7660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004321509470173623
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:56,336 INFO *** epoch 7660, rolling-avg-loss (window=10)= 0.004412913190026302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,359 INFO epoch # 7661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004275048526324099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,383 INFO epoch # 7662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004097906763490755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,407 INFO epoch # 7663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004512415482167853
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,431 INFO epoch # 7664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0048554714157944545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,455 INFO epoch # 7665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005051744486991083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,479 INFO epoch # 7666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004224651645927224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,503 INFO epoch # 7667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004208268437650986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,527 INFO epoch # 7668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004332647047704086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,550 INFO epoch # 7669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004615440011548344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,574 INFO epoch # 7670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042631621909094974
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:56,574 INFO *** epoch 7670, rolling-avg-loss (window=10)= 0.004443675600850838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,598 INFO epoch # 7671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004188627055555116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,621 INFO epoch # 7672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00398763497651089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,645 INFO epoch # 7673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004102011414943263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,669 INFO epoch # 7674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042240666298312135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,693 INFO epoch # 7675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042636380494514015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,717 INFO epoch # 7676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041152912672259845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,741 INFO epoch # 7677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004272352576663252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,765 INFO epoch # 7678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004415380404680036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,789 INFO epoch # 7679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004104037489014445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,812 INFO epoch # 7680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004354341814178042
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:56,813 INFO *** epoch 7680, rolling-avg-loss (window=10)= 0.004202738167805364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,836 INFO epoch # 7681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004000281936896499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,860 INFO epoch # 7682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004148177336901426
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,884 INFO epoch # 7683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00405343283637194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,908 INFO epoch # 7684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003944279706047382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,932 INFO epoch # 7685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038373889692593366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,956 INFO epoch # 7686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003991423709521769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:56,980 INFO epoch # 7687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039229944050021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,005 INFO epoch # 7688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040296267325174995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,029 INFO epoch # 7689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003948934110667324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,053 INFO epoch # 7690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004292055087717017
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:57,053 INFO *** epoch 7690, rolling-avg-loss (window=10)= 0.0040168594830902295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,077 INFO epoch # 7691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004967436325387098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,101 INFO epoch # 7692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004573704551148694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,125 INFO epoch # 7693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042818219553737435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,149 INFO epoch # 7694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004105706089831074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,173 INFO epoch # 7695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044856104177597445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,197 INFO epoch # 7696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003970331894379342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,221 INFO epoch # 7697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004077848250744864
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,245 INFO epoch # 7698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004160619348112959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,268 INFO epoch # 7699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004229581933032023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,293 INFO epoch # 7700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004377160799776902
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:57,293 INFO *** epoch 7700, rolling-avg-loss (window=10)= 0.004322982156554645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,320 INFO epoch # 7701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003943117982998956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,344 INFO epoch # 7702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004013748512079474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,369 INFO epoch # 7703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004791910454514436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,394 INFO epoch # 7704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004444919686648063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,418 INFO epoch # 7705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003937405737815425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,442 INFO epoch # 7706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003989914810517803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,465 INFO epoch # 7707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041337372713314835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,489 INFO epoch # 7708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004177921138762031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,513 INFO epoch # 7709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004084610838617664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,537 INFO epoch # 7710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004080616461578757
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:57,537 INFO *** epoch 7710, rolling-avg-loss (window=10)= 0.004159790289486409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,561 INFO epoch # 7711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00400570479905582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,585 INFO epoch # 7712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003976629413955379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,609 INFO epoch # 7713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004220379980324651
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,637 INFO epoch # 7714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004419912431330886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,661 INFO epoch # 7715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00498095493458095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,685 INFO epoch # 7716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004394939696794609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,709 INFO epoch # 7717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004034180336020654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,733 INFO epoch # 7718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042400820457260124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,757 INFO epoch # 7719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004411315534525784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,781 INFO epoch # 7720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004182349912298378
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:57,781 INFO *** epoch 7720, rolling-avg-loss (window=10)= 0.004286644908461312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,805 INFO epoch # 7721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003910744442691794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,829 INFO epoch # 7722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004343515498476336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,853 INFO epoch # 7723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004443168298166711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,877 INFO epoch # 7724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004143312587984838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,902 INFO epoch # 7725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040509960999770556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,926 INFO epoch # 7726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004486231635382865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,950 INFO epoch # 7727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004501995870668907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,974 INFO epoch # 7728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004349274437117856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:57,998 INFO epoch # 7729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00404693947348278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,022 INFO epoch # 7730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00426710879401071
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:58,022 INFO *** epoch 7730, rolling-avg-loss (window=10)= 0.004254328713795985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,046 INFO epoch # 7731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004621507352567278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,070 INFO epoch # 7732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004017430132080335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,093 INFO epoch # 7733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004190126146568218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,117 INFO epoch # 7734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004786067562235985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,140 INFO epoch # 7735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045544823733507656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,164 INFO epoch # 7736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004229479858622653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,188 INFO epoch # 7737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004661440532800043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,212 INFO epoch # 7738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003939767018891871
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,236 INFO epoch # 7739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003995874256361276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,260 INFO epoch # 7740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004229700836731354
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:58,260 INFO *** epoch 7740, rolling-avg-loss (window=10)= 0.004322587607020978
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,284 INFO epoch # 7741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004028858835226856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,307 INFO epoch # 7742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004037399026856292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,331 INFO epoch # 7743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004495059652981581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,354 INFO epoch # 7744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004305335387471132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,379 INFO epoch # 7745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004102796792722074
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,403 INFO epoch # 7746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043628548010019585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,427 INFO epoch # 7747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004424614351592027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,451 INFO epoch # 7748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004425348903168924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,475 INFO epoch # 7749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004268487922672648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,499 INFO epoch # 7750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004547585613181582
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:58,499 INFO *** epoch 7750, rolling-avg-loss (window=10)= 0.0042998341286875075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,522 INFO epoch # 7751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004114060455322033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,546 INFO epoch # 7752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003982799038567464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,569 INFO epoch # 7753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004104837331396993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,593 INFO epoch # 7754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043450468219816685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,617 INFO epoch # 7755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003919037339073839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,642 INFO epoch # 7756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004095751206477871
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,666 INFO epoch # 7757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003816988373728236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,689 INFO epoch # 7758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00400016909770784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,713 INFO epoch # 7759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00435055958587327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,737 INFO epoch # 7760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004539772478892701
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:58,737 INFO *** epoch 7760, rolling-avg-loss (window=10)= 0.004126902172902192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,761 INFO epoch # 7761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005323252775269793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,784 INFO epoch # 7762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004146271545323543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,808 INFO epoch # 7763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004233365834807046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,832 INFO epoch # 7764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00407760639427579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,856 INFO epoch # 7765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003947980712837307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,881 INFO epoch # 7766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004567978521663463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,905 INFO epoch # 7767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004610229960235301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,929 INFO epoch # 7768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005327551167283673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,952 INFO epoch # 7769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004197976246359758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:58,976 INFO epoch # 7770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004310147749492899
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:58,976 INFO *** epoch 7770, rolling-avg-loss (window=10)= 0.004474236090754857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,000 INFO epoch # 7771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043814842429128475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,024 INFO epoch # 7772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004416191113705281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,048 INFO epoch # 7773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005131641337356996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,072 INFO epoch # 7774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004598665735102259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,096 INFO epoch # 7775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044778094088542275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,120 INFO epoch # 7776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004155810584052233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,143 INFO epoch # 7777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004179732532065827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,167 INFO epoch # 7778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004007829134934582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,191 INFO epoch # 7779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004053941396705341
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,214 INFO epoch # 7780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004410652072692756
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:59,215 INFO *** epoch 7780, rolling-avg-loss (window=10)= 0.004381375755838235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,238 INFO epoch # 7781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004330511590524111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,262 INFO epoch # 7782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039607907747267745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,286 INFO epoch # 7783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039037510905473027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,310 INFO epoch # 7784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004071705356182065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,334 INFO epoch # 7785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004148307863943046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,358 INFO epoch # 7786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038980185308901127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,382 INFO epoch # 7787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004087361619895091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,405 INFO epoch # 7788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004522279279626673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,429 INFO epoch # 7789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004493468881264562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,453 INFO epoch # 7790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004815927255549468
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:59,453 INFO *** epoch 7790, rolling-avg-loss (window=10)= 0.00422321222431492
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,477 INFO epoch # 7791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004561283967632335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,501 INFO epoch # 7792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004167893137491774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,525 INFO epoch # 7793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004201989209832391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,549 INFO epoch # 7794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00393788049404975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,572 INFO epoch # 7795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004046154626848875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,596 INFO epoch # 7796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042617412509571295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,620 INFO epoch # 7797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004516134868026711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,643 INFO epoch # 7798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004164934056461789
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,667 INFO epoch # 7799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004112561990041286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,691 INFO epoch # 7800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004257700893504079
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:59,691 INFO *** epoch 7800, rolling-avg-loss (window=10)= 0.004222827449484612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,715 INFO epoch # 7801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004266153315256815
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,739 INFO epoch # 7802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004354530217824504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,763 INFO epoch # 7803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00474440872494597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,786 INFO epoch # 7804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004674907842854736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,810 INFO epoch # 7805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004561990928777959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,833 INFO epoch # 7806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004186450565612176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,857 INFO epoch # 7807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004008057014289079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,882 INFO epoch # 7808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004286935865820851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,906 INFO epoch # 7809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004521011982433265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,930 INFO epoch # 7810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004252324921253603
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:40:59,930 INFO *** epoch 7810, rolling-avg-loss (window=10)= 0.004385677137906896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,954 INFO epoch # 7811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040077619269141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:40:59,978 INFO epoch # 7812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004269208191544749
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,001 INFO epoch # 7813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038625788074568845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,025 INFO epoch # 7814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004298233296140097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,049 INFO epoch # 7815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004504635100602172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,073 INFO epoch # 7816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042913076395052485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,096 INFO epoch # 7817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004075632183230482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,120 INFO epoch # 7818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00414112547878176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,144 INFO epoch # 7819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004222298906825017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,168 INFO epoch # 7820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004026977814646671
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:00,168 INFO *** epoch 7820, rolling-avg-loss (window=10)= 0.004169975934564718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,192 INFO epoch # 7821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004227465069561731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,216 INFO epoch # 7822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038423964215326123
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,239 INFO epoch # 7823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003970985289925011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,263 INFO epoch # 7824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00403232328244485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,286 INFO epoch # 7825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003970212284912122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,310 INFO epoch # 7826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004090217898919946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,334 INFO epoch # 7827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039947244367795065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,358 INFO epoch # 7828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004090292670298368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,382 INFO epoch # 7829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004117390933970455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,407 INFO epoch # 7830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004163426481682109
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:00,407 INFO *** epoch 7830, rolling-avg-loss (window=10)= 0.004049943477002671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,430 INFO epoch # 7831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003933714240702102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,454 INFO epoch # 7832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039684021030552685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,478 INFO epoch # 7833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004071389586897567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,501 INFO epoch # 7834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004061110088514397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,526 INFO epoch # 7835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004228600737405941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,549 INFO epoch # 7836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003981197107350454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,573 INFO epoch # 7837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003957687415095279
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,597 INFO epoch # 7838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003980088957177941
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,621 INFO epoch # 7839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041043912824534345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,645 INFO epoch # 7840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004176770496997051
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:00,645 INFO *** epoch 7840, rolling-avg-loss (window=10)= 0.004046335201564943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,669 INFO epoch # 7841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003976567102654371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,693 INFO epoch # 7842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004207535028399434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,716 INFO epoch # 7843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004234717878716765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,740 INFO epoch # 7844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004197586349619087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,764 INFO epoch # 7845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004114351697353413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,789 INFO epoch # 7846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003793269968809909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,812 INFO epoch # 7847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004122018566704355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,836 INFO epoch # 7848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042610901218722574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,862 INFO epoch # 7849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004582656285492703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,886 INFO epoch # 7850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004171799944742816
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:00,886 INFO *** epoch 7850, rolling-avg-loss (window=10)= 0.004166159294436511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,910 INFO epoch # 7851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004092059905815404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,934 INFO epoch # 7852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038815262996649835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,958 INFO epoch # 7853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003804038955422584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:00,982 INFO epoch # 7854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036860608524875715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,006 INFO epoch # 7855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038504264666698873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,030 INFO epoch # 7856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004382976017950568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,054 INFO epoch # 7857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0045188302246970125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,078 INFO epoch # 7858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004696580610470846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,101 INFO epoch # 7859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004449071228009416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,125 INFO epoch # 7860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004371033464849461
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:01,125 INFO *** epoch 7860, rolling-avg-loss (window=10)= 0.004173260402603773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,149 INFO epoch # 7861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004813883257156704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,173 INFO epoch # 7862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0046181617944967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,196 INFO epoch # 7863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004209012451610761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,220 INFO epoch # 7864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004818732391868252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,244 INFO epoch # 7865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004282904472347582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,268 INFO epoch # 7866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003803598174272338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,292 INFO epoch # 7867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004421620604261989
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,315 INFO epoch # 7868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004281676483515184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,339 INFO epoch # 7869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003911053885531146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,363 INFO epoch # 7870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003771615563891828
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:01,363 INFO *** epoch 7870, rolling-avg-loss (window=10)= 0.004293225907895248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,387 INFO epoch # 7871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037761088424304035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,411 INFO epoch # 7872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003908593709638808
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,435 INFO epoch # 7873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037727287963207345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,459 INFO epoch # 7874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036124059588473756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,483 INFO epoch # 7875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003839080651232507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,506 INFO epoch # 7876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004152790836087661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,530 INFO epoch # 7877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004348808546637883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,554 INFO epoch # 7878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004566163479466923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,578 INFO epoch # 7879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004118432996619958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,602 INFO epoch # 7880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003982447044108994
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:01,602 INFO *** epoch 7880, rolling-avg-loss (window=10)= 0.004007756086139125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,626 INFO epoch # 7881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004050176165037556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,650 INFO epoch # 7882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00453142894548364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,674 INFO epoch # 7883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003929439466446638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,698 INFO epoch # 7884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003837195585219888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,722 INFO epoch # 7885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004032446388009703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,745 INFO epoch # 7886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004357341131253634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,769 INFO epoch # 7887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039032734421198256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,798 INFO epoch # 7888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004497589252423495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,833 INFO epoch # 7889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042883601927314885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,869 INFO epoch # 7890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004528029148787027
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:01,869 INFO *** epoch 7890, rolling-avg-loss (window=10)= 0.00419552797175129
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,902 INFO epoch # 7891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004681408161559375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,929 INFO epoch # 7892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004702851074398495
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,954 INFO epoch # 7893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004491040366701782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:01,978 INFO epoch # 7894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005053621564002242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,002 INFO epoch # 7895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00438742380720214
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,025 INFO epoch # 7896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003925590666767675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,049 INFO epoch # 7897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038844098198751453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,073 INFO epoch # 7898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003709024069394218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,097 INFO epoch # 7899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037846917402930558
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,121 INFO epoch # 7900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036838467785855755
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:02,121 INFO *** epoch 7900, rolling-avg-loss (window=10)= 0.00423039080487797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,144 INFO epoch # 7901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040253643310279585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,168 INFO epoch # 7902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004061499548697611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,191 INFO epoch # 7903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003985295210441109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,215 INFO epoch # 7904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004191034742689226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,238 INFO epoch # 7905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039811882488720585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,262 INFO epoch # 7906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003978969678428257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,286 INFO epoch # 7907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044042209556209855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,310 INFO epoch # 7908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044292979619058315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,334 INFO epoch # 7909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004077723671798594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,358 INFO epoch # 7910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040722270096011925
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:02,358 INFO *** epoch 7910, rolling-avg-loss (window=10)= 0.0041206821359082825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,382 INFO epoch # 7911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004037099668494193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,406 INFO epoch # 7912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004662252809794154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,430 INFO epoch # 7913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005129952514835168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,454 INFO epoch # 7914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043479114974616095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,477 INFO epoch # 7915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040081443621602375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,501 INFO epoch # 7916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004170226897258544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,525 INFO epoch # 7917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004074138061696431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,549 INFO epoch # 7918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003549762343027396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,573 INFO epoch # 7919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003749625881027896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,597 INFO epoch # 7920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004079637226823252
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:02,597 INFO *** epoch 7920, rolling-avg-loss (window=10)= 0.004180875126257888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,621 INFO epoch # 7921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035526218671293464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,644 INFO epoch # 7922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00408223945487407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,668 INFO epoch # 7923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004300330110709183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,692 INFO epoch # 7924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004001205674285302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,715 INFO epoch # 7925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004087356424861355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,739 INFO epoch # 7926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037964004368404858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,763 INFO epoch # 7927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004175715948804282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,787 INFO epoch # 7928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004202532691124361
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,811 INFO epoch # 7929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004472455795621499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,835 INFO epoch # 7930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006063280226953793
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:02,835 INFO *** epoch 7930, rolling-avg-loss (window=10)= 0.004273413863120368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,858 INFO epoch # 7931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0052545512953656726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,882 INFO epoch # 7932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044370100877131335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,906 INFO epoch # 7933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004159027790592518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,930 INFO epoch # 7934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004008155854535289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,954 INFO epoch # 7935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004481184849282727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:02,978 INFO epoch # 7936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004607402966939844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,002 INFO epoch # 7937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004365598702861462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,026 INFO epoch # 7938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004395570351334754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,050 INFO epoch # 7939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003968264836657909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,074 INFO epoch # 7940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004004222711955663
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:03,074 INFO *** epoch 7940, rolling-avg-loss (window=10)= 0.004368098944723897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,098 INFO epoch # 7941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036028803879162297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,122 INFO epoch # 7942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038576757797272876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,146 INFO epoch # 7943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004638816331862472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,169 INFO epoch # 7944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003863982426992152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,193 INFO epoch # 7945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037407542622531764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,217 INFO epoch # 7946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003643494190328056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,241 INFO epoch # 7947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004024140922410879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,264 INFO epoch # 7948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004690060428401921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,288 INFO epoch # 7949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041117950531770475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,312 INFO epoch # 7950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004104856529011158
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:03,312 INFO *** epoch 7950, rolling-avg-loss (window=10)= 0.004027845631208038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,336 INFO epoch # 7951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003934520602342673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,360 INFO epoch # 7952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040968899265863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,385 INFO epoch # 7953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003981860554631567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,409 INFO epoch # 7954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004039006205857731
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,433 INFO epoch # 7955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038894832141522784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,457 INFO epoch # 7956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038606652669841424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,480 INFO epoch # 7957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004003033860499272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,504 INFO epoch # 7958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003668624452984659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,528 INFO epoch # 7959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038651764480164275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,552 INFO epoch # 7960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003959987676353194
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:03,552 INFO *** epoch 7960, rolling-avg-loss (window=10)= 0.003929924820840825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,576 INFO epoch # 7961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00433891356078675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,600 INFO epoch # 7962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004208099206152838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,624 INFO epoch # 7963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00427379422399099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,647 INFO epoch # 7964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003920691287930822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,671 INFO epoch # 7965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003937616631446872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,695 INFO epoch # 7966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004295380276744254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,718 INFO epoch # 7967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00409835566097172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,742 INFO epoch # 7968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004037302947835997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,766 INFO epoch # 7969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004203666241664905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,790 INFO epoch # 7970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038565717222809326
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:03,790 INFO *** epoch 7970, rolling-avg-loss (window=10)= 0.004117039175980608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,814 INFO epoch # 7971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004052723856148077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,838 INFO epoch # 7972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003674515006423462
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,862 INFO epoch # 7973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00395907492202241
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,886 INFO epoch # 7974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004146750801737653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,910 INFO epoch # 7975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003914833258022554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,933 INFO epoch # 7976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038354253993020393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,957 INFO epoch # 7977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00407738480498665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:03,981 INFO epoch # 7978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004076452729350422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,005 INFO epoch # 7979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004405889310874045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,029 INFO epoch # 7980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00429620790964691
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:04,029 INFO *** epoch 7980, rolling-avg-loss (window=10)= 0.004043925799851422
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,053 INFO epoch # 7981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004110684261831921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,077 INFO epoch # 7982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004173309986072127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,101 INFO epoch # 7983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004209038226690609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,125 INFO epoch # 7984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004097571541933576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,148 INFO epoch # 7985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038358918209269177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,172 INFO epoch # 7986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003702216892634169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,196 INFO epoch # 7987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004040115454699844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,220 INFO epoch # 7988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004413967672007857
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,243 INFO epoch # 7989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038694892209605314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,267 INFO epoch # 7990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004195805064227898
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:04,268 INFO *** epoch 7990, rolling-avg-loss (window=10)= 0.004064809014198545
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,291 INFO epoch # 7991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004194489723886363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,315 INFO epoch # 7992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004280724067939445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,339 INFO epoch # 7993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004076678294950398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,363 INFO epoch # 7994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004011284858279396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,387 INFO epoch # 7995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004100191144971177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,411 INFO epoch # 7996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004374195283162408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,435 INFO epoch # 7997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0051506998934200965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,459 INFO epoch # 7998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004714183553005569
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,483 INFO epoch # 7999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004139540567848599
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,508 INFO epoch # 8000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00411302339125541
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:04,508 INFO *** epoch 8000, rolling-avg-loss (window=10)= 0.004315501077871886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,532 INFO epoch # 8001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040170597640099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,555 INFO epoch # 8002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003841061672574142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,579 INFO epoch # 8003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037547901301877573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,603 INFO epoch # 8004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004081951083207969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,627 INFO epoch # 8005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004168919200310484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,651 INFO epoch # 8006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003732077580934856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,674 INFO epoch # 8007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003860360164253507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,698 INFO epoch # 8008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004164770674833562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,722 INFO epoch # 8009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003947290573705686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,746 INFO epoch # 8010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004021564342110651
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:04,746 INFO *** epoch 8010, rolling-avg-loss (window=10)= 0.003958984518612851
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,770 INFO epoch # 8011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004389608264318667
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,794 INFO epoch # 8012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003964413328503724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,817 INFO epoch # 8013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004099719459190965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,841 INFO epoch # 8014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003887547351041576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,865 INFO epoch # 8015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038737871873308904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,890 INFO epoch # 8016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004252489045029506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,915 INFO epoch # 8017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004768250077177072
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,939 INFO epoch # 8018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004408381588291377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,962 INFO epoch # 8019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004190029580058763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:04,986 INFO epoch # 8020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003932258718123194
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:04,986 INFO *** epoch 8020, rolling-avg-loss (window=10)= 0.004176648459906574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,010 INFO epoch # 8021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003994304217485478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,033 INFO epoch # 8022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044019593769917265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,057 INFO epoch # 8023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036483153562585358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,081 INFO epoch # 8024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036459685034060385
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,105 INFO epoch # 8025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003920389939594315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,129 INFO epoch # 8026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003950516787881497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,153 INFO epoch # 8027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003976296226028353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,177 INFO epoch # 8028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037858798968954943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,201 INFO epoch # 8029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004150725239014719
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,225 INFO epoch # 8030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004124927982047666
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:05,225 INFO *** epoch 8030, rolling-avg-loss (window=10)= 0.0039599283525603825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,248 INFO epoch # 8031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004137594001804246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,272 INFO epoch # 8032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004520514376054052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,296 INFO epoch # 8033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004498508784308797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,320 INFO epoch # 8034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004052822521771304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,344 INFO epoch # 8035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003779620190471178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,368 INFO epoch # 8036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003977713484346168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,393 INFO epoch # 8037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004077366818819428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,417 INFO epoch # 8038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003906637852196582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,440 INFO epoch # 8039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038239304449234623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,464 INFO epoch # 8040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004024468838906614
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:05,464 INFO *** epoch 8040, rolling-avg-loss (window=10)= 0.004079917731360183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,488 INFO epoch # 8041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003956083080993267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,514 INFO epoch # 8042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040814418462105095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,538 INFO epoch # 8043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036832416044489946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,562 INFO epoch # 8044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004174912093731109
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,586 INFO epoch # 8045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00479169608297525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,610 INFO epoch # 8046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039379177542286925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,634 INFO epoch # 8047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038655418684356846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,658 INFO epoch # 8048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004108352233743062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,681 INFO epoch # 8049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004057304409798235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,705 INFO epoch # 8050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038480003422591835
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:05,705 INFO *** epoch 8050, rolling-avg-loss (window=10)= 0.004050449131682399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,729 INFO epoch # 8051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004166788341535721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,753 INFO epoch # 8052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004112814363907091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,777 INFO epoch # 8053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004199030605377629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,800 INFO epoch # 8054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004069975610036636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,824 INFO epoch # 8055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00378673955128761
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,848 INFO epoch # 8056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041455600803601556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,872 INFO epoch # 8057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038371834161807783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,897 INFO epoch # 8058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038201229690457694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,921 INFO epoch # 8059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003936770164727932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,945 INFO epoch # 8060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004271624311513733
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:05,945 INFO *** epoch 8060, rolling-avg-loss (window=10)= 0.004034660941397305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,969 INFO epoch # 8061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042750272987177595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:05,993 INFO epoch # 8062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004113518443773501
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,017 INFO epoch # 8063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003918842960047186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,041 INFO epoch # 8064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036826406940235756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,064 INFO epoch # 8065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038893598975846544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,088 INFO epoch # 8066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036967121268389747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,111 INFO epoch # 8067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003729178039066028
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,135 INFO epoch # 8068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003649865568149835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,159 INFO epoch # 8069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038378780081984587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,183 INFO epoch # 8070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003910074825398624
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:06,183 INFO *** epoch 8070, rolling-avg-loss (window=10)= 0.0038703097861798597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,207 INFO epoch # 8071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003896366983099142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,231 INFO epoch # 8072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036692844550998416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,254 INFO epoch # 8073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004023217537906021
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,278 INFO epoch # 8074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044781075630453415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,302 INFO epoch # 8075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004171272539679194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,326 INFO epoch # 8076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004505917091591982
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,349 INFO epoch # 8077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037628847967425827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,373 INFO epoch # 8078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038933774121687748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,399 INFO epoch # 8079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036846398179477546
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,423 INFO epoch # 8080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003877878851199057
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:06,423 INFO *** epoch 8080, rolling-avg-loss (window=10)= 0.003996294704847969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,447 INFO epoch # 8081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004136913670663489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,471 INFO epoch # 8082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003927007277525263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,495 INFO epoch # 8083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004093405244930182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,518 INFO epoch # 8084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041852704307530075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,542 INFO epoch # 8085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00414813859606511
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,566 INFO epoch # 8086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004060020459291991
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,590 INFO epoch # 8087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004000739911134588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,614 INFO epoch # 8088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00395507454595645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,638 INFO epoch # 8089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041060753937927075
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,662 INFO epoch # 8090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039821936952648684
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:06,662 INFO *** epoch 8090, rolling-avg-loss (window=10)= 0.004059483922537766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,685 INFO epoch # 8091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004067714788106969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,709 INFO epoch # 8092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004132900394324679
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,733 INFO epoch # 8093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004150711672991747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,756 INFO epoch # 8094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003952225149987498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,780 INFO epoch # 8095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003826549062068807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,804 INFO epoch # 8096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003934624415705912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,827 INFO epoch # 8097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004176878446742194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,851 INFO epoch # 8098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004007994408311788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,875 INFO epoch # 8099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038703206919308286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,899 INFO epoch # 8100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00385157481287024
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:06,899 INFO *** epoch 8100, rolling-avg-loss (window=10)= 0.003997149384304066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,923 INFO epoch # 8101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004046646281494759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,947 INFO epoch # 8102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004141513240028871
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,970 INFO epoch # 8103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00381939571161638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:06,994 INFO epoch # 8104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037199600374151487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,018 INFO epoch # 8105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003750362422579201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,042 INFO epoch # 8106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038447173101303633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,066 INFO epoch # 8107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003975649160565808
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,090 INFO epoch # 8108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004181449112365954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,114 INFO epoch # 8109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004106932035938371
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,138 INFO epoch # 8110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003882274748320924
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:07,138 INFO *** epoch 8110, rolling-avg-loss (window=10)= 0.003946890006045578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,161 INFO epoch # 8111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041314098198199645
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,185 INFO epoch # 8112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036574258519976866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,209 INFO epoch # 8113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004026392969535664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,233 INFO epoch # 8114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038295286358334124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,257 INFO epoch # 8115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003729989668499911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,280 INFO epoch # 8116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003599030002078507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,304 INFO epoch # 8117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003713000944117084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,328 INFO epoch # 8118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003980741465056781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,352 INFO epoch # 8119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004346292163972976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,375 INFO epoch # 8120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004047560010803863
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:07,375 INFO *** epoch 8120, rolling-avg-loss (window=10)= 0.003906137153171585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,399 INFO epoch # 8121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004150587734329747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,423 INFO epoch # 8122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004407235348480754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,447 INFO epoch # 8123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004265554380253889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,471 INFO epoch # 8124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004665637829020852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,495 INFO epoch # 8125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004231001661537448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,519 INFO epoch # 8126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004084191088622902
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,543 INFO epoch # 8127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039309184830926824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,567 INFO epoch # 8128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00409411271175486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,590 INFO epoch # 8129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003728603394847596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,614 INFO epoch # 8130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037361364375101402
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:07,614 INFO *** epoch 8130, rolling-avg-loss (window=10)= 0.004129397906945087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,638 INFO epoch # 8131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003703542417497374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,662 INFO epoch # 8132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038644460328214336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,686 INFO epoch # 8133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004257741347828414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,710 INFO epoch # 8134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004075607415870763
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,734 INFO epoch # 8135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00443836854537949
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,757 INFO epoch # 8136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004164714220678434
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,781 INFO epoch # 8137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004732785171654541
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,805 INFO epoch # 8138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004291784352972172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,828 INFO epoch # 8139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004115491756238043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,852 INFO epoch # 8140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004225465909257764
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:07,852 INFO *** epoch 8140, rolling-avg-loss (window=10)= 0.004186994717019843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,876 INFO epoch # 8141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037083493880345486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,901 INFO epoch # 8142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036548266289173625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,925 INFO epoch # 8143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037761092935397755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,949 INFO epoch # 8144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003609906070778379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,973 INFO epoch # 8145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038475925066450145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:07,996 INFO epoch # 8146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038264316845015856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,020 INFO epoch # 8147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036617888108594343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,044 INFO epoch # 8148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037096695850777905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,068 INFO epoch # 8149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003592041572119342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,092 INFO epoch # 8150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003777941376029048
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:08,092 INFO *** epoch 8150, rolling-avg-loss (window=10)= 0.003716465691650228
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,116 INFO epoch # 8151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004040766245452687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,140 INFO epoch # 8152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004196158661216032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,163 INFO epoch # 8153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004089444402779918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,187 INFO epoch # 8154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003926800185581669
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,211 INFO epoch # 8155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039716827450320125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,235 INFO epoch # 8156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004208720500173513
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,259 INFO epoch # 8157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004447931984032039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,282 INFO epoch # 8158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003987956817582017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,306 INFO epoch # 8159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037989538941474166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,330 INFO epoch # 8160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038905649926164187
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:08,330 INFO *** epoch 8160, rolling-avg-loss (window=10)= 0.004055898042861372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,354 INFO epoch # 8161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004007549847301561
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,378 INFO epoch # 8162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0050154572309111245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,402 INFO epoch # 8163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00488650629267795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,425 INFO epoch # 8164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004856569714320358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,450 INFO epoch # 8165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00423324105940992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,476 INFO epoch # 8166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004020208300062222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,500 INFO epoch # 8167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037730303811258636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,526 INFO epoch # 8168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038500482551171444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,552 INFO epoch # 8169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039528250381408725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,578 INFO epoch # 8170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038844726550451014
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:08,578 INFO *** epoch 8170, rolling-avg-loss (window=10)= 0.0042479908774112115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,602 INFO epoch # 8171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003898563787515741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,626 INFO epoch # 8172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003828324599453481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,650 INFO epoch # 8173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004131485977268312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,674 INFO epoch # 8174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004075511995324632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,697 INFO epoch # 8175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003921146664652042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,723 INFO epoch # 8176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004435606053448282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,750 INFO epoch # 8177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004105362917471211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,776 INFO epoch # 8178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00463378572021611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,801 INFO epoch # 8179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038390712288673967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,825 INFO epoch # 8180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038572138364543207
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:08,825 INFO *** epoch 8180, rolling-avg-loss (window=10)= 0.004072607278067153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,849 INFO epoch # 8181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036547261152009014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,873 INFO epoch # 8182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038259091597865336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,897 INFO epoch # 8183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038129667336761486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,922 INFO epoch # 8184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004109633278858382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,949 INFO epoch # 8185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00396632792399032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:08,975 INFO epoch # 8186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003642344196123304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,001 INFO epoch # 8187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003879973246512236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,026 INFO epoch # 8188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004277276377251837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,050 INFO epoch # 8189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00415190798958065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,076 INFO epoch # 8190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004186795402347343
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:09,076 INFO *** epoch 8190, rolling-avg-loss (window=10)= 0.003950786042332765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,103 INFO epoch # 8191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004049614100949839
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,131 INFO epoch # 8192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003719888634805102
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,163 INFO epoch # 8193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003951092294300906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,194 INFO epoch # 8194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003922602132661268
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,224 INFO epoch # 8195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003771774769120384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,250 INFO epoch # 8196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003916550165740773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,275 INFO epoch # 8197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003677230273751775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,299 INFO epoch # 8198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004141638150031213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,323 INFO epoch # 8199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004624925277312286
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,348 INFO epoch # 8200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040260361129185185
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:09,349 INFO *** epoch 8200, rolling-avg-loss (window=10)= 0.003980135191159207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,375 INFO epoch # 8201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00414322057258687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,402 INFO epoch # 8202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003937658497306984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,427 INFO epoch # 8203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004077115710970247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,450 INFO epoch # 8204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004147652362007648
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,474 INFO epoch # 8205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003703706432133913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,498 INFO epoch # 8206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003803434457950061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,521 INFO epoch # 8207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003545652922184672
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,545 INFO epoch # 8208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003730584980075946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,571 INFO epoch # 8209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037576415234070737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,597 INFO epoch # 8210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003913340489816619
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:09,597 INFO *** epoch 8210, rolling-avg-loss (window=10)= 0.0038760007948440032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,623 INFO epoch # 8211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039679031615378335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,648 INFO epoch # 8212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004142741403484251
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,672 INFO epoch # 8213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004064630978973582
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,696 INFO epoch # 8214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003852633715723641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,719 INFO epoch # 8215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036409722633834463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,743 INFO epoch # 8216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035612250394478906
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,767 INFO epoch # 8217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037409490687423386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,793 INFO epoch # 8218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003808361288974993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,819 INFO epoch # 8219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040089939975587185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,845 INFO epoch # 8220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003653379000752466
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:09,845 INFO *** epoch 8220, rolling-avg-loss (window=10)= 0.0038441789918579163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,871 INFO epoch # 8221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037886171849095263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,896 INFO epoch # 8222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036787937933695503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,920 INFO epoch # 8223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003758572700462537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,944 INFO epoch # 8224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036883869688608684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,968 INFO epoch # 8225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037645361881004646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:09,991 INFO epoch # 8226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004218920454150066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,017 INFO epoch # 8227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041066380799748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,043 INFO epoch # 8228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00391530166962184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,069 INFO epoch # 8229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003947827659430914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,095 INFO epoch # 8230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004170151230937336
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:10,095 INFO *** epoch 8230, rolling-avg-loss (window=10)= 0.00390377459298179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,119 INFO epoch # 8231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004684727071435191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,143 INFO epoch # 8232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003911132505891146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,166 INFO epoch # 8233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038535242747457232
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,190 INFO epoch # 8234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003820867717877263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,213 INFO epoch # 8235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035554058995330706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,239 INFO epoch # 8236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038863106674398296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,265 INFO epoch # 8237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003597105613152962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,291 INFO epoch # 8238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004086592613020912
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,316 INFO epoch # 8239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00401482572851819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,340 INFO epoch # 8240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038010833741282113
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:10,340 INFO *** epoch 8240, rolling-avg-loss (window=10)= 0.00392115754657425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,364 INFO epoch # 8241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038446535436378326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,388 INFO epoch # 8242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038842930080136284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,412 INFO epoch # 8243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003865575319650816
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,436 INFO epoch # 8244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003835367657302413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,461 INFO epoch # 8245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003775537061301293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,487 INFO epoch # 8246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037985293092788197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,513 INFO epoch # 8247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038651024115097243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,538 INFO epoch # 8248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039055720990290865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,562 INFO epoch # 8249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003952342827687971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,586 INFO epoch # 8250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.006060081359464675
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:10,586 INFO *** epoch 8250, rolling-avg-loss (window=10)= 0.004078705459687626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,610 INFO epoch # 8251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00537944225652609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,634 INFO epoch # 8252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004753822144266451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,657 INFO epoch # 8253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004092147893970832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,681 INFO epoch # 8254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003757802245672792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,705 INFO epoch # 8255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003378339890332427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,728 INFO epoch # 8256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034927122615044937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,752 INFO epoch # 8257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035697217754204758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,776 INFO epoch # 8258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036990815824538004
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,800 INFO epoch # 8259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035716995407710783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,823 INFO epoch # 8260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035798164026346058
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:10,824 INFO *** epoch 8260, rolling-avg-loss (window=10)= 0.003927458599355305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,848 INFO epoch # 8261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034964672704518307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,872 INFO epoch # 8262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003683535658637993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,896 INFO epoch # 8263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037212741663097404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,920 INFO epoch # 8264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003768476326513337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,944 INFO epoch # 8265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003824262850685045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,968 INFO epoch # 8266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038236937689362094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:10,993 INFO epoch # 8267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003887559814756969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,016 INFO epoch # 8268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039696289313724265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,040 INFO epoch # 8269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003840460234641796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,064 INFO epoch # 8270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037538377655437216
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:11,064 INFO *** epoch 8270, rolling-avg-loss (window=10)= 0.003776919678784907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,088 INFO epoch # 8271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039578477226314135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,112 INFO epoch # 8272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003689832414238481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,136 INFO epoch # 8273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003772204894630704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,160 INFO epoch # 8274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036944340135960374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,183 INFO epoch # 8275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036661785634350963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,207 INFO epoch # 8276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036025931331096217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,231 INFO epoch # 8277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003805330368777504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,255 INFO epoch # 8278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003720701151905814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,278 INFO epoch # 8279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037383981289167423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,302 INFO epoch # 8280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004429253203852568
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:11,302 INFO *** epoch 8280, rolling-avg-loss (window=10)= 0.003807677359509398
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,326 INFO epoch # 8281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003961661146604456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,349 INFO epoch # 8282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003804284642683342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,373 INFO epoch # 8283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00399671067862073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,398 INFO epoch # 8284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037481556573766284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,422 INFO epoch # 8285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003860164862999227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,446 INFO epoch # 8286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037706423536292277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,470 INFO epoch # 8287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00417517969981418
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,494 INFO epoch # 8288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004284167353034718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,517 INFO epoch # 8289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003481477622699458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,541 INFO epoch # 8290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003670216003229143
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:11,541 INFO *** epoch 8290, rolling-avg-loss (window=10)= 0.003875266002069111
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,565 INFO epoch # 8291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004086289027327439
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,589 INFO epoch # 8292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004177958704531193
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,613 INFO epoch # 8293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041203051259799395
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,637 INFO epoch # 8294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036911313654854894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,661 INFO epoch # 8295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003769450919207884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,684 INFO epoch # 8296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003691396315844031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,708 INFO epoch # 8297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003700117340486031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,732 INFO epoch # 8298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003932721177989151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,755 INFO epoch # 8299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035817400967061985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,779 INFO epoch # 8300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038284284455585293
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:11,779 INFO *** epoch 8300, rolling-avg-loss (window=10)= 0.0038579538519115885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,803 INFO epoch # 8301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003779360409680521
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,827 INFO epoch # 8302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038608687900705263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,851 INFO epoch # 8303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003786392517213244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,875 INFO epoch # 8304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004138950582273537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,899 INFO epoch # 8305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037992030411260203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,923 INFO epoch # 8306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004156199171120534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,947 INFO epoch # 8307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004094674954103539
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,971 INFO epoch # 8308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037571128050331026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:11,995 INFO epoch # 8309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003776658388233045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,019 INFO epoch # 8310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003861460223561153
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:12,019 INFO *** epoch 8310, rolling-avg-loss (window=10)= 0.003901088088241522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,043 INFO epoch # 8311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039308566447289195
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,067 INFO epoch # 8312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038314590019581374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,090 INFO epoch # 8313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036841620931227226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,114 INFO epoch # 8314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037600496762024704
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,137 INFO epoch # 8315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003729533404111862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,161 INFO epoch # 8316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037055758948554285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,185 INFO epoch # 8317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003783928324992303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,209 INFO epoch # 8318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037070199687150307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,232 INFO epoch # 8319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037330414597818162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,256 INFO epoch # 8320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003647091129096225
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:12,256 INFO *** epoch 8320, rolling-avg-loss (window=10)= 0.0037512717597564917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,280 INFO epoch # 8321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037135122547624633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,304 INFO epoch # 8322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003471834017545916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,327 INFO epoch # 8323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003769926799577661
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,351 INFO epoch # 8324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004415805036842357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,374 INFO epoch # 8325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004172292734438088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,399 INFO epoch # 8326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003955022450099932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,423 INFO epoch # 8327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004068313981406391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,448 INFO epoch # 8328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037985521557857282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,472 INFO epoch # 8329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003669610905490117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,498 INFO epoch # 8330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004035702266264707
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:12,498 INFO *** epoch 8330, rolling-avg-loss (window=10)= 0.003907057260221336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,522 INFO epoch # 8331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003519569378113374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,545 INFO epoch # 8332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038597960337938275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,569 INFO epoch # 8333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040900957901612855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,593 INFO epoch # 8334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037576392460323405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,617 INFO epoch # 8335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003797536111960653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,641 INFO epoch # 8336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003762692886084551
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,665 INFO epoch # 8337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041262262529926375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,688 INFO epoch # 8338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004046649126394186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,712 INFO epoch # 8339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003838103228190448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,736 INFO epoch # 8340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003749884468561504
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:12,736 INFO *** epoch 8340, rolling-avg-loss (window=10)= 0.003854819252228481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,760 INFO epoch # 8341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003925841330783442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,783 INFO epoch # 8342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00405140773000312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,807 INFO epoch # 8343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004119945573620498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,831 INFO epoch # 8344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003704934410052374
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,855 INFO epoch # 8345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036769604666915257
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,879 INFO epoch # 8346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003866178321914049
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,904 INFO epoch # 8347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003608405764680356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,928 INFO epoch # 8348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035576273512560874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,952 INFO epoch # 8349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00397785650784499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:12,976 INFO epoch # 8350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005168225063243881
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:12,976 INFO *** epoch 8350, rolling-avg-loss (window=10)= 0.0039657382520090325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,000 INFO epoch # 8351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00403494014244643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,023 INFO epoch # 8352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003703477341332473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,047 INFO epoch # 8353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003576846902433317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,071 INFO epoch # 8354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003937059947929811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,095 INFO epoch # 8355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038542336187674664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,119 INFO epoch # 8356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038112037582322955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,143 INFO epoch # 8357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035708394316316117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,166 INFO epoch # 8358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034685348364291713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,190 INFO epoch # 8359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003338553124194732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,213 INFO epoch # 8360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035204716841690242
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:13,213 INFO *** epoch 8360, rolling-avg-loss (window=10)= 0.003681616078756633
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,237 INFO epoch # 8361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003644436575996224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,261 INFO epoch # 8362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036902598112646956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,285 INFO epoch # 8363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003653121370007284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,309 INFO epoch # 8364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038089128356659785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,333 INFO epoch # 8365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003826421165285865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,357 INFO epoch # 8366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003823653685685713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,382 INFO epoch # 8367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003994227165094344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,405 INFO epoch # 8368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0043242764477327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,429 INFO epoch # 8369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004066375109687215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,453 INFO epoch # 8370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037795262833242305
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:13,453 INFO *** epoch 8370, rolling-avg-loss (window=10)= 0.0038611210449744247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,477 INFO epoch # 8371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036667684980784543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,501 INFO epoch # 8372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003653150790341897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,525 INFO epoch # 8373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035931254642491695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,549 INFO epoch # 8374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034946176965604536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,573 INFO epoch # 8375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003764412485907087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,597 INFO epoch # 8376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003985587380157085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,620 INFO epoch # 8377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003810575697571039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,643 INFO epoch # 8378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003819879570073681
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,673 INFO epoch # 8379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003813337731116917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,711 INFO epoch # 8380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004042979842779459
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:13,711 INFO *** epoch 8380, rolling-avg-loss (window=10)= 0.0037644435156835242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,739 INFO epoch # 8381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003953070157876937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,766 INFO epoch # 8382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041392281200387515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,792 INFO epoch # 8383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036154330846329685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,816 INFO epoch # 8384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003979621433245484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,840 INFO epoch # 8385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004073092779435683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,863 INFO epoch # 8386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036108038329984993
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,888 INFO epoch # 8387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003330011837533675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,911 INFO epoch # 8388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037486602195713203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,935 INFO epoch # 8389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036569864823832177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,959 INFO epoch # 8390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038422306715801824
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:13,959 INFO *** epoch 8390, rolling-avg-loss (window=10)= 0.0037949138619296718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:13,983 INFO epoch # 8391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004029863623145502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,007 INFO epoch # 8392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004360584600362927
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,031 INFO epoch # 8393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003992683545220643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,054 INFO epoch # 8394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004400586796691641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,078 INFO epoch # 8395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004011999233625829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,102 INFO epoch # 8396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036099133285460994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,126 INFO epoch # 8397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003958363329729764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,149 INFO epoch # 8398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.005433470545540331
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,173 INFO epoch # 8399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003847969142952934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,197 INFO epoch # 8400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004101031103346031
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:14,197 INFO *** epoch 8400, rolling-avg-loss (window=10)= 0.00417464652491617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,221 INFO epoch # 8401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004108570974494796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,244 INFO epoch # 8402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004138921172852861
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,268 INFO epoch # 8403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003797750498051755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,292 INFO epoch # 8404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037200780352577567
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,316 INFO epoch # 8405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037368953162513208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,339 INFO epoch # 8406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037758656435471494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,363 INFO epoch # 8407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003535608480888186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,388 INFO epoch # 8408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004094910960702691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,412 INFO epoch # 8409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037098722023074515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,436 INFO epoch # 8410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003807648055953905
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:14,436 INFO *** epoch 8410, rolling-avg-loss (window=10)= 0.0038426121340307874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,460 INFO epoch # 8411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037445612069859635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,484 INFO epoch # 8412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00369356677038013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,508 INFO epoch # 8413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004290158074581996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,532 INFO epoch # 8414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004188211463770131
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,555 INFO epoch # 8415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004006164603197249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,579 INFO epoch # 8416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004055555746163009
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,603 INFO epoch # 8417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038391936868720222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,627 INFO epoch # 8418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003586061557143694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,651 INFO epoch # 8419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003825954529020237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,675 INFO epoch # 8420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037844064536329824
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:14,675 INFO *** epoch 8420, rolling-avg-loss (window=10)= 0.0039013834091747414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,699 INFO epoch # 8421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032895814583753236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,723 INFO epoch # 8422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035836609058605973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,746 INFO epoch # 8423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034248182018927764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,770 INFO epoch # 8424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003354378910444211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,794 INFO epoch # 8425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036029840739502106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,818 INFO epoch # 8426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035508585679053795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,841 INFO epoch # 8427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003466090278379852
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,865 INFO epoch # 8428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036083211525692604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,889 INFO epoch # 8429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036973349560867064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,913 INFO epoch # 8430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035190675262128934
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:14,913 INFO *** epoch 8430, rolling-avg-loss (window=10)= 0.0035097096031677212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,937 INFO epoch # 8431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003786078781558899
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,961 INFO epoch # 8432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036959372264391277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:14,984 INFO epoch # 8433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034701969671004917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,008 INFO epoch # 8434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003505540193145862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,032 INFO epoch # 8435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004081011360540288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,056 INFO epoch # 8436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003594311921915505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,079 INFO epoch # 8437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00417927714806865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,103 INFO epoch # 8438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037349825870478526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,126 INFO epoch # 8439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038264431241259445
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,150 INFO epoch # 8440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036972530288039707
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:15,150 INFO *** epoch 8440, rolling-avg-loss (window=10)= 0.003757103233874659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,174 INFO epoch # 8441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003894898654834833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,197 INFO epoch # 8442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036734797176904976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,221 INFO epoch # 8443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035285883204778656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,244 INFO epoch # 8444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037834142858628184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,268 INFO epoch # 8445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038864980888320133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,292 INFO epoch # 8446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004014323290903121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,316 INFO epoch # 8447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034934877076011617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,339 INFO epoch # 8448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034154567401856184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,363 INFO epoch # 8449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033711229734763037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,387 INFO epoch # 8450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036213093626429327
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:15,387 INFO *** epoch 8450, rolling-avg-loss (window=10)= 0.0036682579142507167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,411 INFO epoch # 8451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003569853532098932
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,435 INFO epoch # 8452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00330461507110158
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,458 INFO epoch # 8453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036554264261212666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,482 INFO epoch # 8454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003902704323991202
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,506 INFO epoch # 8455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003634136388427578
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,530 INFO epoch # 8456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003520312428008765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,553 INFO epoch # 8457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036633152813010383
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,577 INFO epoch # 8458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003816514181380626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,600 INFO epoch # 8459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036560137232299894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,624 INFO epoch # 8460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003586487644497538
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:15,624 INFO *** epoch 8460, rolling-avg-loss (window=10)= 0.0036309379000158517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,648 INFO epoch # 8461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035896667613997124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,672 INFO epoch # 8462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036287257680669427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,695 INFO epoch # 8463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003909425977326464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,719 INFO epoch # 8464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035673571728693787
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,743 INFO epoch # 8465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036867567141598556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,767 INFO epoch # 8466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003864921469357796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,790 INFO epoch # 8467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004234839070704766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,814 INFO epoch # 8468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004431057572219288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,837 INFO epoch # 8469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038687651613145135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,862 INFO epoch # 8470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004430521395988762
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:15,862 INFO *** epoch 8470, rolling-avg-loss (window=10)= 0.003921203706340748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,886 INFO epoch # 8471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037897038273513317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,910 INFO epoch # 8472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00388437625952065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,934 INFO epoch # 8473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003773550401092507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,958 INFO epoch # 8474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035943307702837046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:15,982 INFO epoch # 8475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033956496881728526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,006 INFO epoch # 8476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003587250663258601
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,030 INFO epoch # 8477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003612603530200431
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,053 INFO epoch # 8478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036626795736083295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,077 INFO epoch # 8479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004070454342581797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,101 INFO epoch # 8480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003901391042745672
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:16,101 INFO *** epoch 8480, rolling-avg-loss (window=10)= 0.003727199009881588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,125 INFO epoch # 8481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004121619123907294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,149 INFO epoch # 8482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003990746059571393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,173 INFO epoch # 8483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004879668937064707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,197 INFO epoch # 8484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00531799503005459
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,220 INFO epoch # 8485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003792922248976538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,244 INFO epoch # 8486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003514465770422248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,268 INFO epoch # 8487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033219714860024396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,291 INFO epoch # 8488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037966742856951896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,315 INFO epoch # 8489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035365822186577134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,339 INFO epoch # 8490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004018978037493071
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:16,339 INFO *** epoch 8490, rolling-avg-loss (window=10)= 0.0040291623197845185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,363 INFO epoch # 8491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003847149298962904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,387 INFO epoch # 8492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003453645571426023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,411 INFO epoch # 8493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038512638820975553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,435 INFO epoch # 8494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00371113187429728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,458 INFO epoch # 8495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034956113013322465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,482 INFO epoch # 8496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003526879590936005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,505 INFO epoch # 8497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003529775898641674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,529 INFO epoch # 8498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003920023114915239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,553 INFO epoch # 8499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038113834852993023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,577 INFO epoch # 8500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003996869909315137
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:16,577 INFO *** epoch 8500, rolling-avg-loss (window=10)= 0.0037143733927223364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,601 INFO epoch # 8501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003651616589195328
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,635 INFO epoch # 8502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00402525124445674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,663 INFO epoch # 8503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003960519912652671
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,688 INFO epoch # 8504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036521506481221877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,712 INFO epoch # 8505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034906661639979575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,737 INFO epoch # 8506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003505216365738306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,761 INFO epoch # 8507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003993466631072806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,785 INFO epoch # 8508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004902798926195828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,809 INFO epoch # 8509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004275821327610174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,833 INFO epoch # 8510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004179745930741774
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:16,833 INFO *** epoch 8510, rolling-avg-loss (window=10)= 0.003963725373978377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,856 INFO epoch # 8511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003371137510839617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,880 INFO epoch # 8512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035309199411130976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,905 INFO epoch # 8513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003729206095158588
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,929 INFO epoch # 8514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038934593612793833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,953 INFO epoch # 8515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036805151285079774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:16,977 INFO epoch # 8516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003822744529315969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,001 INFO epoch # 8517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004177211761998478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,025 INFO epoch # 8518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003758778064366197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,049 INFO epoch # 8519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037903988813923206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,073 INFO epoch # 8520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003529997728037415
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:17,073 INFO *** epoch 8520, rolling-avg-loss (window=10)= 0.003728436900200904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,096 INFO epoch # 8521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003971280308178393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,120 INFO epoch # 8522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038237494700297248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,144 INFO epoch # 8523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003450839405559236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,168 INFO epoch # 8524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034025607565126847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,191 INFO epoch # 8525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035211085487389937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,215 INFO epoch # 8526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037510401525651105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,239 INFO epoch # 8527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041596073933760636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,263 INFO epoch # 8528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004047085170896025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,287 INFO epoch # 8529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038313092045427766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,310 INFO epoch # 8530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035658532251545694
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:17,310 INFO *** epoch 8530, rolling-avg-loss (window=10)= 0.003752443363555358
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,334 INFO epoch # 8531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036513976483547594
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,358 INFO epoch # 8532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034461329742043745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,382 INFO epoch # 8533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003578864492737921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,406 INFO epoch # 8534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041209964438166935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,430 INFO epoch # 8535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004261101101292297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,454 INFO epoch # 8536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036893457217956893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,478 INFO epoch # 8537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034186942502856255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,501 INFO epoch # 8538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003538536369887879
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,525 INFO epoch # 8539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036253141806810163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,548 INFO epoch # 8540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003493567253826768
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:17,548 INFO *** epoch 8540, rolling-avg-loss (window=10)= 0.0036823950436883023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,572 INFO epoch # 8541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035848040533892345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,596 INFO epoch # 8542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037045625686005224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,620 INFO epoch # 8543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035358444292796776
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,644 INFO epoch # 8544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034225621220684843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,668 INFO epoch # 8545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036482664072536863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,691 INFO epoch # 8546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033393585108569823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,715 INFO epoch # 8547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003429453634453239
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,738 INFO epoch # 8548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036924406504112994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,762 INFO epoch # 8549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036016992198710795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,786 INFO epoch # 8550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00463149675852037
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:17,786 INFO *** epoch 8550, rolling-avg-loss (window=10)= 0.0036590488354704574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,810 INFO epoch # 8551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004518991019722307
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,833 INFO epoch # 8552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003911192918167217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,857 INFO epoch # 8553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0040254117193399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,881 INFO epoch # 8554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004738895298942225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,905 INFO epoch # 8555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00422163931580144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,929 INFO epoch # 8556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034622417333594058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,952 INFO epoch # 8557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037215297124930657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:17,976 INFO epoch # 8558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033793063485063612
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,000 INFO epoch # 8559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003453989280387759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,024 INFO epoch # 8560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037741031592304353
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:18,024 INFO *** epoch 8560, rolling-avg-loss (window=10)= 0.0039207300505950116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,048 INFO epoch # 8561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034826371629606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,072 INFO epoch # 8562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033209772846021224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,096 INFO epoch # 8563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036580216692527756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,119 INFO epoch # 8564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034543710498837754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,143 INFO epoch # 8565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034596667901496403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,167 INFO epoch # 8566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003635547778685577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,190 INFO epoch # 8567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036239543442206923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,214 INFO epoch # 8568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003564629045285983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,238 INFO epoch # 8569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036742964512086473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,262 INFO epoch # 8570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038769917555327993
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:18,262 INFO *** epoch 8570, rolling-avg-loss (window=10)= 0.0035751093331782614
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,286 INFO epoch # 8571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035219534911448136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,310 INFO epoch # 8572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003812258866673801
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,333 INFO epoch # 8573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035189967493352015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,357 INFO epoch # 8574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038796182816440705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,380 INFO epoch # 8575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036754037828359287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,405 INFO epoch # 8576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034014400334854145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,428 INFO epoch # 8577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033728304697433487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,452 INFO epoch # 8578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034365459287073463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,476 INFO epoch # 8579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035809250694001094
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,500 INFO epoch # 8580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003685615083668381
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:18,500 INFO *** epoch 8580, rolling-avg-loss (window=10)= 0.0035885587756638417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,524 INFO epoch # 8581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033120687512564473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,548 INFO epoch # 8582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003407416334084701
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,572 INFO epoch # 8583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003555838851752924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,595 INFO epoch # 8584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034849377188947983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,619 INFO epoch # 8585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033500955323688686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,643 INFO epoch # 8586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034186675475211814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,667 INFO epoch # 8587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036719176241604146
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,691 INFO epoch # 8588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033245006488868967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,715 INFO epoch # 8589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003472398304438684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,739 INFO epoch # 8590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003361212002346292
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:18,739 INFO *** epoch 8590, rolling-avg-loss (window=10)= 0.0034359053315711208
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,763 INFO epoch # 8591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003740711141290376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,787 INFO epoch # 8592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00393147017530282
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,810 INFO epoch # 8593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004030557494843379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,834 INFO epoch # 8594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003755576071853284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,857 INFO epoch # 8595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003410822813748382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,882 INFO epoch # 8596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003654388990980806
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,906 INFO epoch # 8597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035607769750640728
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,930 INFO epoch # 8598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003584272322768811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,954 INFO epoch # 8599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004293898127798457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:18,977 INFO epoch # 8600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003888357761752559
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:18,978 INFO *** epoch 8600, rolling-avg-loss (window=10)= 0.0037850831875402948
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,001 INFO epoch # 8601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003630728660937166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,025 INFO epoch # 8602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003340093055157922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,049 INFO epoch # 8603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003237400724174222
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,073 INFO epoch # 8604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003489809481834527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,097 INFO epoch # 8605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039771000629116315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,121 INFO epoch # 8606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037354610321926884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,145 INFO epoch # 8607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035240491197328083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,169 INFO epoch # 8608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004090372538485099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,193 INFO epoch # 8609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036138737268629484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,216 INFO epoch # 8610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003458151619270211
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:19,217 INFO *** epoch 8610, rolling-avg-loss (window=10)= 0.003609704002155922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,240 INFO epoch # 8611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037798540106450673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,264 INFO epoch # 8612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003531054961058544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,288 INFO epoch # 8613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004289475353289163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,312 INFO epoch # 8614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036699635311379097
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,336 INFO epoch # 8615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036830922836088575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,360 INFO epoch # 8616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037272059998940676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,384 INFO epoch # 8617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003807447661529295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,408 INFO epoch # 8618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003831353529676562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,432 INFO epoch # 8619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036366407039167825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,455 INFO epoch # 8620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037391838704934344
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:19,455 INFO *** epoch 8620, rolling-avg-loss (window=10)= 0.003769527190524968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,479 INFO epoch # 8621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037879617084399797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,503 INFO epoch # 8622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00400883296970278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,527 INFO epoch # 8623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003772185034904396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,551 INFO epoch # 8624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038998122108750977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,575 INFO epoch # 8625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003883751607645536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,599 INFO epoch # 8626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004216148601699388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,623 INFO epoch # 8627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038129650783957914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,646 INFO epoch # 8628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037543207363341935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,670 INFO epoch # 8629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035825704435410444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,694 INFO epoch # 8630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032498034597665537
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:19,694 INFO *** epoch 8630, rolling-avg-loss (window=10)= 0.003796835185130476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,718 INFO epoch # 8631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003337720689160051
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,742 INFO epoch # 8632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034116939168598037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,765 INFO epoch # 8633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004338638060289668
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,789 INFO epoch # 8634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004425089311553165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,813 INFO epoch # 8635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038160798358148895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,837 INFO epoch # 8636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035547556726669427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,861 INFO epoch # 8637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003599627201765543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,885 INFO epoch # 8638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037979275184625294
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,909 INFO epoch # 8639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036535066938085947
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,932 INFO epoch # 8640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003576462924684165
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:19,933 INFO *** epoch 8640, rolling-avg-loss (window=10)= 0.003751150182506535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,957 INFO epoch # 8641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036948946544725914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:19,981 INFO epoch # 8642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003630245690146694
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,005 INFO epoch # 8643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003529844936565496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,029 INFO epoch # 8644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037170463729125913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,053 INFO epoch # 8645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003435277427342953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,076 INFO epoch # 8646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003255480416555656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,100 INFO epoch # 8647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037122171379451174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,124 INFO epoch # 8648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003710504468472209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,148 INFO epoch # 8649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003926526263967389
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,172 INFO epoch # 8650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036592945507436525
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:20,172 INFO *** epoch 8650, rolling-avg-loss (window=10)= 0.003627133191912435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,196 INFO epoch # 8651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039030081279634032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,220 INFO epoch # 8652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037347329962358344
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,244 INFO epoch # 8653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004056460446008714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,267 INFO epoch # 8654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003749366576812463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,291 INFO epoch # 8655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004182602606306318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,315 INFO epoch # 8656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003650343445769977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,339 INFO epoch # 8657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035876699184882455
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,363 INFO epoch # 8658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037095212101121433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,388 INFO epoch # 8659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003549575609213207
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,412 INFO epoch # 8660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003557531064871
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:20,412 INFO *** epoch 8660, rolling-avg-loss (window=10)= 0.0037680812001781305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,437 INFO epoch # 8661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038124591774248984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,460 INFO epoch # 8662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003430886983551318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,484 INFO epoch # 8663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034788157063303515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,508 INFO epoch # 8664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039038257018546574
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,532 INFO epoch # 8665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003416926192585379
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,555 INFO epoch # 8666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003488599537377013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,579 INFO epoch # 8667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003613093740568729
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,603 INFO epoch # 8668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036239582041162066
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,627 INFO epoch # 8669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037490586946660187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,651 INFO epoch # 8670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033533865462231915
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:20,652 INFO *** epoch 8670, rolling-avg-loss (window=10)= 0.0035871010484697765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,675 INFO epoch # 8671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035320928764122073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,699 INFO epoch # 8672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034844463407353032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,723 INFO epoch # 8673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034797471162164584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,747 INFO epoch # 8674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035618964211607818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,771 INFO epoch # 8675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035265566075395327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,795 INFO epoch # 8676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003436159691773355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,819 INFO epoch # 8677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036605757122742943
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,843 INFO epoch # 8678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003584948801290011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,870 INFO epoch # 8679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003912800832040375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,895 INFO epoch # 8680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00365167751442641
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:20,895 INFO *** epoch 8680, rolling-avg-loss (window=10)= 0.003583090191386873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,918 INFO epoch # 8681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003471194264420774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,942 INFO epoch # 8682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035804575200018007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,966 INFO epoch # 8683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00397831252485048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:20,990 INFO epoch # 8684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003908772530849092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,013 INFO epoch # 8685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038272065212368034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,037 INFO epoch # 8686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036864772555418313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,061 INFO epoch # 8687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039909474762680475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,085 INFO epoch # 8688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003727003000676632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,110 INFO epoch # 8689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003533418901497498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,133 INFO epoch # 8690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00339241722758743
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:21,133 INFO *** epoch 8690, rolling-avg-loss (window=10)= 0.003709620722293039
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,157 INFO epoch # 8691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003322698103147559
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,180 INFO epoch # 8692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003352895255375188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,204 INFO epoch # 8693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035173001706425566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,228 INFO epoch # 8694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003964621486375108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,252 INFO epoch # 8695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041343978300574236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,276 INFO epoch # 8696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003987648487964179
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,300 INFO epoch # 8697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003685227708047023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,323 INFO epoch # 8698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034601455445226748
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,347 INFO epoch # 8699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003368355319253169
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,371 INFO epoch # 8700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003659308757050894
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:21,371 INFO *** epoch 8700, rolling-avg-loss (window=10)= 0.0036452598662435774
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,395 INFO epoch # 8701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035052009116043337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,418 INFO epoch # 8702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003553104612365132
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,443 INFO epoch # 8703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003740287007531151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,467 INFO epoch # 8704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003524877360177925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,491 INFO epoch # 8705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044540615563164465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,515 INFO epoch # 8706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004933944044751115
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,539 INFO epoch # 8707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004152675690420438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,563 INFO epoch # 8708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033983892571995966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,587 INFO epoch # 8709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00345017392464797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,611 INFO epoch # 8710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003579609860025812
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:21,612 INFO *** epoch 8710, rolling-avg-loss (window=10)= 0.003829232422503992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,636 INFO epoch # 8711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033711636988300597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,660 INFO epoch # 8712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034058728342643008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,684 INFO epoch # 8713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003566377556126099
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,709 INFO epoch # 8714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035930291805925663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,733 INFO epoch # 8715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00339041908227955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,757 INFO epoch # 8716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035165703047823627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,782 INFO epoch # 8717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003456520884355996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,806 INFO epoch # 8718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037703163288824726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,830 INFO epoch # 8719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003617196995037375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,854 INFO epoch # 8720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003603996643505525
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:21,854 INFO *** epoch 8720, rolling-avg-loss (window=10)= 0.0035291463508656308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,879 INFO epoch # 8721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003424929494940443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,906 INFO epoch # 8722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032400932650489267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,930 INFO epoch # 8723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035314240558363963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,955 INFO epoch # 8724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036564576003002003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:21,979 INFO epoch # 8725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032914155708567705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,003 INFO epoch # 8726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037452640135597903
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,027 INFO epoch # 8727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003420613782509463
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,051 INFO epoch # 8728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033383544359821826
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,075 INFO epoch # 8729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003407806456380058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,104 INFO epoch # 8730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003765382800338557
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:22,104 INFO *** epoch 8730, rolling-avg-loss (window=10)= 0.0034821741475752786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,128 INFO epoch # 8731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003412580947042443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,153 INFO epoch # 8732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033531672379467636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,177 INFO epoch # 8733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003547687145328382
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,201 INFO epoch # 8734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032659811804478522
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,225 INFO epoch # 8735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003734208396053873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,250 INFO epoch # 8736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003406916141102556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,274 INFO epoch # 8737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034011802708846517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,298 INFO epoch # 8738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003492071009532083
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,322 INFO epoch # 8739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004078246143762954
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,346 INFO epoch # 8740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004001062541647116
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:22,346 INFO *** epoch 8740, rolling-avg-loss (window=10)= 0.0035693101013748673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,371 INFO epoch # 8741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038850461969559547
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,399 INFO epoch # 8742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003554141101631103
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,424 INFO epoch # 8743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037054806634841952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,448 INFO epoch # 8744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003697054067743011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,472 INFO epoch # 8745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003559988941560732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,496 INFO epoch # 8746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034434254885127302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,520 INFO epoch # 8747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003387398006452713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,545 INFO epoch # 8748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034373725502518937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,569 INFO epoch # 8749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00331376042595366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,594 INFO epoch # 8750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032937443211267237
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:22,594 INFO *** epoch 8750, rolling-avg-loss (window=10)= 0.0035277411763672715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,618 INFO epoch # 8751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032310369388142135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,643 INFO epoch # 8752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003536099913617363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,667 INFO epoch # 8753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034815453946066555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,691 INFO epoch # 8754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003354077198309824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,715 INFO epoch # 8755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034527874740888365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,740 INFO epoch # 8756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003395266303414246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,764 INFO epoch # 8757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003961435148085002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,789 INFO epoch # 8758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003954238327423809
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,813 INFO epoch # 8759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034740418595902156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,837 INFO epoch # 8760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003548436910932651
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:22,838 INFO *** epoch 8760, rolling-avg-loss (window=10)= 0.0035388965468882818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,862 INFO epoch # 8761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035314725355419796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,888 INFO epoch # 8762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034077874443028122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,913 INFO epoch # 8763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034115530834242236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,937 INFO epoch # 8764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035818704309349414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,961 INFO epoch # 8765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003868384701490868
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:22,985 INFO epoch # 8766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0039814131923776586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,009 INFO epoch # 8767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038369374015019275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,033 INFO epoch # 8768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036814524028159212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,058 INFO epoch # 8769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003549567143636523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,083 INFO epoch # 8770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031516399030806497
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:23,083 INFO *** epoch 8770, rolling-avg-loss (window=10)= 0.0036002078239107506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,111 INFO epoch # 8771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003374181658728048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,136 INFO epoch # 8772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003290100703452481
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,160 INFO epoch # 8773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003382515329576563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,184 INFO epoch # 8774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033867714191728737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,208 INFO epoch # 8775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032331367292499635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,232 INFO epoch # 8776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003536114585585892
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,257 INFO epoch # 8777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00358919977225014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,281 INFO epoch # 8778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037701710825785995
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,305 INFO epoch # 8779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003683033319248352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,329 INFO epoch # 8780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003943820076528937
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:23,329 INFO *** epoch 8780, rolling-avg-loss (window=10)= 0.003518904467637185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,354 INFO epoch # 8781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035997746090288274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,381 INFO epoch # 8782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034140927855332848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,414 INFO epoch # 8783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003526419011905091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,443 INFO epoch # 8784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003428246403927915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,471 INFO epoch # 8785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003390159246919211
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,498 INFO epoch # 8786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035336560831638053
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,523 INFO epoch # 8787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034103592406609096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,549 INFO epoch # 8788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003702913501911098
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,574 INFO epoch # 8789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003988138007116504
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,600 INFO epoch # 8790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00352107239086763
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:23,600 INFO *** epoch 8790, rolling-avg-loss (window=10)= 0.0035514831281034274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,624 INFO epoch # 8791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003667411092465045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,649 INFO epoch # 8792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003605917216191301
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,673 INFO epoch # 8793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037703456400777213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,699 INFO epoch # 8794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034009705996140838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,723 INFO epoch # 8795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033910042147908825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,747 INFO epoch # 8796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037424359943543095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,772 INFO epoch # 8797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003706463769049151
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,797 INFO epoch # 8798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035178842044842895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,822 INFO epoch # 8799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032519603373657446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,847 INFO epoch # 8800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003375289190444164
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:23,847 INFO *** epoch 8800, rolling-avg-loss (window=10)= 0.003542968225883669
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,871 INFO epoch # 8801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003543099301168695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,901 INFO epoch # 8802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033022873212757986
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,926 INFO epoch # 8803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036102340927754994
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,951 INFO epoch # 8804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003346515619341517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:23,976 INFO epoch # 8805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034117877476091962
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,000 INFO epoch # 8806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003554654111212585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,024 INFO epoch # 8807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003374568204890238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,048 INFO epoch # 8808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00347838262678124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,073 INFO epoch # 8809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036933822484570555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,097 INFO epoch # 8810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035519518532964867
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:24,098 INFO *** epoch 8810, rolling-avg-loss (window=10)= 0.003486686312680831
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,127 INFO epoch # 8811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036676955423899926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,151 INFO epoch # 8812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003564764352631755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,175 INFO epoch # 8813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003334064498631051
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,201 INFO epoch # 8814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033977066523220856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,226 INFO epoch # 8815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003447730421612505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,249 INFO epoch # 8816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032732644940551836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,275 INFO epoch # 8817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032809787207952468
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,300 INFO epoch # 8818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003320371801237343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,325 INFO epoch # 8819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003345372824696824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,349 INFO epoch # 8820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033574307126400527
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:24,349 INFO *** epoch 8820, rolling-avg-loss (window=10)= 0.003398938002101204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,374 INFO epoch # 8821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003764709319511894
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,401 INFO epoch # 8822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031392866185342427
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,425 INFO epoch # 8823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003807629571383586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,449 INFO epoch # 8824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035792413909803145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,473 INFO epoch # 8825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034233648839290254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,500 INFO epoch # 8826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034261181281181052
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,525 INFO epoch # 8827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034130397107219324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,550 INFO epoch # 8828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036261782406654675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,575 INFO epoch # 8829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003724207079358166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,600 INFO epoch # 8830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034137254988308996
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:24,600 INFO *** epoch 8830, rolling-avg-loss (window=10)= 0.0035317500442033634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,624 INFO epoch # 8831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033198404125869274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,648 INFO epoch # 8832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003504588148643961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,673 INFO epoch # 8833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035467396955937147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,697 INFO epoch # 8834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035676286788657308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,721 INFO epoch # 8835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035297259128128644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,746 INFO epoch # 8836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003465827030595392
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,770 INFO epoch # 8837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032984494100674056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,794 INFO epoch # 8838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034492192935431376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,819 INFO epoch # 8839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033919396628334653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,843 INFO epoch # 8840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003829667930403957
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:24,843 INFO *** epoch 8840, rolling-avg-loss (window=10)= 0.0034903626175946554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,868 INFO epoch # 8841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004235110529407393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,907 INFO epoch # 8842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004471574302442605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,944 INFO epoch # 8843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003848672247841023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:24,979 INFO epoch # 8844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038709080654371064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,008 INFO epoch # 8845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035017748195969034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,036 INFO epoch # 8846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003373995430592913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,062 INFO epoch # 8847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033467812536400743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,088 INFO epoch # 8848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003465764624706935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,113 INFO epoch # 8849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034534139595052693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,138 INFO epoch # 8850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034142744698328897
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:25,138 INFO *** epoch 8850, rolling-avg-loss (window=10)= 0.0036982269703003112
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,164 INFO epoch # 8851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003621598272729898
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,190 INFO epoch # 8852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003634583143139025
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,215 INFO epoch # 8853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003315787071187515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,245 INFO epoch # 8854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003542500297044171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,286 INFO epoch # 8855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033108102179539856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,321 INFO epoch # 8856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034898022095148917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,346 INFO epoch # 8857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037981446366757154
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,371 INFO epoch # 8858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003956918157200562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,400 INFO epoch # 8859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034159072674810886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,426 INFO epoch # 8860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032630559762765188
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:25,427 INFO *** epoch 8860, rolling-avg-loss (window=10)= 0.003534910724920337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,451 INFO epoch # 8861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003251133051890065
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,478 INFO epoch # 8862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033016955567291006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,503 INFO epoch # 8863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035287387800053693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,528 INFO epoch # 8864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034384612372377887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,553 INFO epoch # 8865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036913801450282335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,579 INFO epoch # 8866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003509750804369105
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,610 INFO epoch # 8867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037062954434077255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,635 INFO epoch # 8868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033524174978083465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,661 INFO epoch # 8869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033512655063532293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,687 INFO epoch # 8870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033970127187785693
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:25,687 INFO *** epoch 8870, rolling-avg-loss (window=10)= 0.003452815074160753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,713 INFO epoch # 8871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003626965328294318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,741 INFO epoch # 8872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036547887830238324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,766 INFO epoch # 8873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036159643168502953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,794 INFO epoch # 8874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003538293964084005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,823 INFO epoch # 8875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033031419188773725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,852 INFO epoch # 8876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003607472128351219
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,880 INFO epoch # 8877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031594413703714963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,909 INFO epoch # 8878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030897506403562147
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,939 INFO epoch # 8879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033295309403911233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:25,974 INFO epoch # 8880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003359624221047852
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:25,975 INFO *** epoch 8880, rolling-avg-loss (window=10)= 0.003428497361164773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,000 INFO epoch # 8881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033696384889481124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,024 INFO epoch # 8882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036488052101049107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,048 INFO epoch # 8883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034911391012428794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,072 INFO epoch # 8884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032724397569836583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,096 INFO epoch # 8885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035424530324235093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,120 INFO epoch # 8886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003823928731435444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,144 INFO epoch # 8887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035140162835887168
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,168 INFO epoch # 8888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032658196359989233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,192 INFO epoch # 8889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035947215583291836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,216 INFO epoch # 8890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036530494107864797
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:26,216 INFO *** epoch 8890, rolling-avg-loss (window=10)= 0.003517601120984182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,240 INFO epoch # 8891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003504377678837045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,265 INFO epoch # 8892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00357300220275647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,289 INFO epoch # 8893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033720868268574122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,313 INFO epoch # 8894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036371583373693284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,337 INFO epoch # 8895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038559979730052873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,360 INFO epoch # 8896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038216048378671985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,385 INFO epoch # 8897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038889806855877396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,409 INFO epoch # 8898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035540647513698786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,433 INFO epoch # 8899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003171428408677457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,457 INFO epoch # 8900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032420414572698064
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:26,457 INFO *** epoch 8900, rolling-avg-loss (window=10)= 0.0035620743159597624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,481 INFO epoch # 8901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00342221492974204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,505 INFO epoch # 8902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033801972822402604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,531 INFO epoch # 8903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00365537176548969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,562 INFO epoch # 8904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035820398588839453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,589 INFO epoch # 8905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034159432871092577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,616 INFO epoch # 8906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003255679977883119
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,642 INFO epoch # 8907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032412300424766727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,668 INFO epoch # 8908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037823244456376415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,693 INFO epoch # 8909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037725871079601347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,717 INFO epoch # 8910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003702468799019698
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:26,717 INFO *** epoch 8910, rolling-avg-loss (window=10)= 0.0035210057496442458
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,741 INFO epoch # 8911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003465415287791984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,765 INFO epoch # 8912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035666752082761377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,789 INFO epoch # 8913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003512771821988281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,812 INFO epoch # 8914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003517873235978186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,836 INFO epoch # 8915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034077894779329654
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,862 INFO epoch # 8916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034066974731103983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,889 INFO epoch # 8917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003649829945061356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,917 INFO epoch # 8918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035160938314220402
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,944 INFO epoch # 8919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003316708120109979
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,971 INFO epoch # 8920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036134706460870802
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:26,971 INFO *** epoch 8920, rolling-avg-loss (window=10)= 0.0034973325047758406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:26,998 INFO epoch # 8921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033837348673841916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,023 INFO epoch # 8922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034034744458040223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,048 INFO epoch # 8923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003819501376710832
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,072 INFO epoch # 8924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003882972767314641
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,096 INFO epoch # 8925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003487847698124824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,120 INFO epoch # 8926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033462139217590448
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,144 INFO epoch # 8927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00364927178088692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,168 INFO epoch # 8928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003122280540992506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,192 INFO epoch # 8929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032148481768672355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,216 INFO epoch # 8930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033682343891996425
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:27,216 INFO *** epoch 8930, rolling-avg-loss (window=10)= 0.003467837996504386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,240 INFO epoch # 8931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003532276656187605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,263 INFO epoch # 8932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029937046638224274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,288 INFO epoch # 8933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032974142050079536
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,312 INFO epoch # 8934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003672938939416781
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,336 INFO epoch # 8935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003576668386813253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,361 INFO epoch # 8936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036125165861449204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,386 INFO epoch # 8937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00355717684578849
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,412 INFO epoch # 8938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038248543569352478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,437 INFO epoch # 8939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004308471594413277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,462 INFO epoch # 8940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00417607298004441
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:27,462 INFO *** epoch 8940, rolling-avg-loss (window=10)= 0.0036552095214574367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,486 INFO epoch # 8941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036870246003672946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,513 INFO epoch # 8942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036465295415837318
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,539 INFO epoch # 8943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003274157443229342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,565 INFO epoch # 8944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032019881473388523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,590 INFO epoch # 8945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034055563592119142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,615 INFO epoch # 8946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003284157264715759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,640 INFO epoch # 8947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003406415422432474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,665 INFO epoch # 8948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032035617223300505
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,689 INFO epoch # 8949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035176287856302224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,713 INFO epoch # 8950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004367702596937306
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:27,713 INFO *** epoch 8950, rolling-avg-loss (window=10)= 0.0034994721883776945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,737 INFO epoch # 8951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004078412366652628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,762 INFO epoch # 8952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033303969248663634
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,786 INFO epoch # 8953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033446432535129134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,809 INFO epoch # 8954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003229454614483984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,833 INFO epoch # 8955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003479237959254533
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,857 INFO epoch # 8956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00343377251556376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,881 INFO epoch # 8957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036095115829084534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,906 INFO epoch # 8958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003910109029675368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,930 INFO epoch # 8959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034729274739220273
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,954 INFO epoch # 8960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003512335151754087
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:27,954 INFO *** epoch 8960, rolling-avg-loss (window=10)= 0.0035400800872594116
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:27,978 INFO epoch # 8961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003216089986381121
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,003 INFO epoch # 8962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003287109822849743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,027 INFO epoch # 8963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00311346938542556
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,051 INFO epoch # 8964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036709666783281136
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,075 INFO epoch # 8965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003164514033414889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,099 INFO epoch # 8966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003388648732652655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,123 INFO epoch # 8967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003555903174856212
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,147 INFO epoch # 8968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029738682496827096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,171 INFO epoch # 8969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031707858142908663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,195 INFO epoch # 8970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003352245421410771
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:28,196 INFO *** epoch 8970, rolling-avg-loss (window=10)= 0.0032893601299292642
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,219 INFO epoch # 8971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034058490964525845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,243 INFO epoch # 8972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033405242538719904
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,267 INFO epoch # 8973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033770110276236665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,291 INFO epoch # 8974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003544717801560182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,315 INFO epoch # 8975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003274284641520353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,338 INFO epoch # 8976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003395206902496284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,362 INFO epoch # 8977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034543932852102444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,389 INFO epoch # 8978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034904519670817535
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,413 INFO epoch # 8979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035659590430441312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,437 INFO epoch # 8980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003756973452254897
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:28,437 INFO *** epoch 8980, rolling-avg-loss (window=10)= 0.0034605371471116086
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,461 INFO epoch # 8981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0042891693010460585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,485 INFO epoch # 8982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035284620789752807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,508 INFO epoch # 8983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034964806691277772
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,532 INFO epoch # 8984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035939718509325758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,556 INFO epoch # 8985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031601860900991596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,580 INFO epoch # 8986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031355502869701013
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,604 INFO epoch # 8987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034437363647157326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,628 INFO epoch # 8988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034734884720819537
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,653 INFO epoch # 8989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032842166838236153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,677 INFO epoch # 8990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003275255654443754
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:28,677 INFO *** epoch 8990, rolling-avg-loss (window=10)= 0.0034680517452216008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,702 INFO epoch # 8991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003379858717380557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,726 INFO epoch # 8992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034992836008314043
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,749 INFO epoch # 8993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033352286300214473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,773 INFO epoch # 8994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032953726695268415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,797 INFO epoch # 8995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033610523169045337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,821 INFO epoch # 8996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003583523717679782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,845 INFO epoch # 8997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003178358881996246
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,868 INFO epoch # 8998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033295610301138368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,893 INFO epoch # 8999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003576371746021323
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,917 INFO epoch # 9000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033674245678412262
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:28,917 INFO *** epoch 9000, rolling-avg-loss (window=10)= 0.0033906035878317198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,941 INFO epoch # 9001 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003328847171360394
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,965 INFO epoch # 9002 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003394752595340833
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:28,988 INFO epoch # 9003 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003698761272971751
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,012 INFO epoch # 9004 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035027936501137447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,036 INFO epoch # 9005 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003640106409875443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,060 INFO epoch # 9006 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003439397449255921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,084 INFO epoch # 9007 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003301385861050221
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,108 INFO epoch # 9008 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003245809446525527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,131 INFO epoch # 9009 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036574391488102265
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,155 INFO epoch # 9010 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038074766926001757
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:29,155 INFO *** epoch 9010, rolling-avg-loss (window=10)= 0.0035016769697904238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,179 INFO epoch # 9011 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003314910973131191
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,203 INFO epoch # 9012 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003150656764773885
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,227 INFO epoch # 9013 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033533234418428037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,251 INFO epoch # 9014 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003469905390375061
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,275 INFO epoch # 9015 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032060686135082506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,299 INFO epoch # 9016 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003072859937674366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,322 INFO epoch # 9017 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033887403333210386
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,346 INFO epoch # 9018 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003616855450673029
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,370 INFO epoch # 9019 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003791996630752692
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,394 INFO epoch # 9020 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034705451034824364
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:29,394 INFO *** epoch 9020, rolling-avg-loss (window=10)= 0.0033835862639534755
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,418 INFO epoch # 9021 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037137759536562953
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,442 INFO epoch # 9022 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004319097621191759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,466 INFO epoch # 9023 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033171712420880795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,490 INFO epoch # 9024 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032134833199961577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,514 INFO epoch # 9025 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030303282037493773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,537 INFO epoch # 9026 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033541033262736164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,561 INFO epoch # 9027 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003266736450314056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,585 INFO epoch # 9028 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031027584554976784
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,609 INFO epoch # 9029 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032903838400670793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,633 INFO epoch # 9030 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003387024760741042
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:29,633 INFO *** epoch 9030, rolling-avg-loss (window=10)= 0.003399486317357514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,656 INFO epoch # 9031 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034805990362656303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,681 INFO epoch # 9032 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031841116542636883
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,705 INFO epoch # 9033 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003363951047504088
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,729 INFO epoch # 9034 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003431830402405467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,753 INFO epoch # 9035 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035396571256569587
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,777 INFO epoch # 9036 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033193102390214335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,800 INFO epoch # 9037 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003576228416932281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,824 INFO epoch # 9038 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038332939766405616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,848 INFO epoch # 9039 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035409942211117595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,872 INFO epoch # 9040 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003537027172569651
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:29,872 INFO *** epoch 9040, rolling-avg-loss (window=10)= 0.003480700329237152
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,897 INFO epoch # 9041 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032484725743415765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,921 INFO epoch # 9042 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034764176052703988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,945 INFO epoch # 9043 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003438575269683497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,969 INFO epoch # 9044 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003532597929734038
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:29,992 INFO epoch # 9045 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003429808475630125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,016 INFO epoch # 9046 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003425288301514229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,040 INFO epoch # 9047 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003365256761753699
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,063 INFO epoch # 9048 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031787158113729674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,087 INFO epoch # 9049 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003421454253839329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,111 INFO epoch # 9050 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036076538744964637
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:30,111 INFO *** epoch 9050, rolling-avg-loss (window=10)= 0.003412424085763632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,135 INFO epoch # 9051 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003130707711534342
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,159 INFO epoch # 9052 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034217457723570988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,183 INFO epoch # 9053 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032143229982466437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,207 INFO epoch # 9054 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031680454449087847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,230 INFO epoch # 9055 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00322615570621565
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,254 INFO epoch # 9056 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033701150459819473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,277 INFO epoch # 9057 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034915340183943044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,302 INFO epoch # 9058 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003292671790404711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,325 INFO epoch # 9059 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032710419509385247
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,349 INFO epoch # 9060 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003243126586312428
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:30,350 INFO *** epoch 9060, rolling-avg-loss (window=10)= 0.0032829467025294436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,374 INFO epoch # 9061 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003130741824861616
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,398 INFO epoch # 9062 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003409954002563609
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,422 INFO epoch # 9063 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003475293648079969
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,445 INFO epoch # 9064 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033225154438696336
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,469 INFO epoch # 9065 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003552474245225312
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,493 INFO epoch # 9066 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034427991522534285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,517 INFO epoch # 9067 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003915877237886889
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,541 INFO epoch # 9068 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003892274762620218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,566 INFO epoch # 9069 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033985013396886643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,591 INFO epoch # 9070 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003212574596545892
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:30,591 INFO *** epoch 9070, rolling-avg-loss (window=10)= 0.003475300625359523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,616 INFO epoch # 9071 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003165007357893046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,640 INFO epoch # 9072 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033754500800569076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,664 INFO epoch # 9073 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003920333223504713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,688 INFO epoch # 9074 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035906279772461858
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,712 INFO epoch # 9075 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036888136601191945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,736 INFO epoch # 9076 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035543927479011472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,760 INFO epoch # 9077 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032372581263189204
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,785 INFO epoch # 9078 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003372357619809918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,809 INFO epoch # 9079 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029865733959013596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,835 INFO epoch # 9080 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035726934620470274
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:30,835 INFO *** epoch 9080, rolling-avg-loss (window=10)= 0.003446350765079842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,861 INFO epoch # 9081 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004742216748127248
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,886 INFO epoch # 9082 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003382544466148829
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,911 INFO epoch # 9083 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003379723955731606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,935 INFO epoch # 9084 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003290830172772985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,961 INFO epoch # 9085 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033790506604418624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:30,986 INFO epoch # 9086 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003185143283189973
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,011 INFO epoch # 9087 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003182416177878622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,036 INFO epoch # 9088 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036052558025403414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,062 INFO epoch # 9089 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003504820939269848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,087 INFO epoch # 9090 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003382281920494279
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:31,087 INFO *** epoch 9090, rolling-avg-loss (window=10)= 0.0035034284126595596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,111 INFO epoch # 9091 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003211089722753968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,137 INFO epoch # 9092 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031452532566618174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,162 INFO epoch # 9093 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030997719077276997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,186 INFO epoch # 9094 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031623357244825456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,210 INFO epoch # 9095 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003324399651319254
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,234 INFO epoch # 9096 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034095008741132915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,258 INFO epoch # 9097 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034904254898719955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,282 INFO epoch # 9098 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033846608021121938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,305 INFO epoch # 9099 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003362047915288713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,329 INFO epoch # 9100 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031000949893496
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:31,329 INFO *** epoch 9100, rolling-avg-loss (window=10)= 0.003268958033368108
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,354 INFO epoch # 9101 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031280550065275747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,379 INFO epoch # 9102 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003063763775571715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,404 INFO epoch # 9103 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032080977216537576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,429 INFO epoch # 9104 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031964644840627443
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,458 INFO epoch # 9105 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003348742309754016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,487 INFO epoch # 9106 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035435797581158113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,517 INFO epoch # 9107 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003330168816319201
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,545 INFO epoch # 9108 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037496739932976197
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,573 INFO epoch # 9109 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003518017903843429
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,601 INFO epoch # 9110 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035293973160150927
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:31,601 INFO *** epoch 9110, rolling-avg-loss (window=10)= 0.003361596108516096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,629 INFO epoch # 9111 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003268117918196367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,657 INFO epoch # 9112 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031172247508948203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,684 INFO epoch # 9113 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003006584312970517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,710 INFO epoch # 9114 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00304093477097922
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,735 INFO epoch # 9115 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033328717508993577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,760 INFO epoch # 9116 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003366683671629289
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,788 INFO epoch # 9117 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032727545221860055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,816 INFO epoch # 9118 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031776344367244747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,842 INFO epoch # 9119 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031195346018648706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,866 INFO epoch # 9120 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003116406711342279
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:31,866 INFO *** epoch 9120, rolling-avg-loss (window=10)= 0.00318187474476872
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,892 INFO epoch # 9121 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033820942626334727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,916 INFO epoch # 9122 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003488265025225701
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,940 INFO epoch # 9123 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033339227484248113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,965 INFO epoch # 9124 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031131235336943064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:31,989 INFO epoch # 9125 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003461467436864041
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,013 INFO epoch # 9126 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002932396544565563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,038 INFO epoch # 9127 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032366144405386876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,062 INFO epoch # 9128 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030922473561076913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,086 INFO epoch # 9129 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00336274764049449
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,111 INFO epoch # 9130 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031994873643270694
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:32,111 INFO *** epoch 9130, rolling-avg-loss (window=10)= 0.0032602366352875835
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,135 INFO epoch # 9131 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035442628286546096
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,159 INFO epoch # 9132 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003703068028698908
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,183 INFO epoch # 9133 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003145987917378079
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,207 INFO epoch # 9134 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033535561378812417
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,231 INFO epoch # 9135 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033562210919626523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,255 INFO epoch # 9136 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003431729317526333
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,279 INFO epoch # 9137 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00332087183414842
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,303 INFO epoch # 9138 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032717333888285793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,327 INFO epoch # 9139 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033817520816228352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,351 INFO epoch # 9140 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035484993895806838
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:32,351 INFO *** epoch 9140, rolling-avg-loss (window=10)= 0.003405768201628234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,375 INFO epoch # 9141 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003406081763387192
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,400 INFO epoch # 9142 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035041926930716727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,424 INFO epoch # 9143 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003065990320465062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,448 INFO epoch # 9144 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028749486737069674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,472 INFO epoch # 9145 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030036713906156365
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,497 INFO epoch # 9146 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003284320730017498
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,521 INFO epoch # 9147 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034596898840391077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,544 INFO epoch # 9148 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033653919599601068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,569 INFO epoch # 9149 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003275055300036911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,592 INFO epoch # 9150 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003410096094739856
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:32,593 INFO *** epoch 9150, rolling-avg-loss (window=10)= 0.003264943881004001
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,616 INFO epoch # 9151 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035494122421368957
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,640 INFO epoch # 9152 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033881720664794557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,664 INFO epoch # 9153 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032243049499811605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,688 INFO epoch # 9154 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035560898832045496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,713 INFO epoch # 9155 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035177297686459497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,737 INFO epoch # 9156 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033178174890053924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,760 INFO epoch # 9157 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003325866013256018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,784 INFO epoch # 9158 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032106719954754226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,808 INFO epoch # 9159 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031842515600146726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,832 INFO epoch # 9160 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003303463810880203
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:32,832 INFO *** epoch 9160, rolling-avg-loss (window=10)= 0.0033577779779079718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,855 INFO epoch # 9161 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003272270503657637
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,879 INFO epoch # 9162 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031546172067464795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,905 INFO epoch # 9163 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034115142407245003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,929 INFO epoch # 9164 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003975233164965175
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,953 INFO epoch # 9165 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038657941222481895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:32,977 INFO epoch # 9166 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033987669048656244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,001 INFO epoch # 9167 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033837584323919145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,025 INFO epoch # 9168 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031430804010597058
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,049 INFO epoch # 9169 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003187294099916471
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,073 INFO epoch # 9170 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034254022721142974
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:33,073 INFO *** epoch 9170, rolling-avg-loss (window=10)= 0.0034217731348689996
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,097 INFO epoch # 9171 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037091995545779355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,121 INFO epoch # 9172 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032679937103239354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,146 INFO epoch # 9173 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032710802552173845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,170 INFO epoch # 9174 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030161439972289372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,194 INFO epoch # 9175 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030340454213728663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,218 INFO epoch # 9176 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034339303529122844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,242 INFO epoch # 9177 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031962014763848856
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,266 INFO epoch # 9178 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030422498712141532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,290 INFO epoch # 9179 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032795065053505823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,314 INFO epoch # 9180 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031131444884522352
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:33,314 INFO *** epoch 9180, rolling-avg-loss (window=10)= 0.00323634956330352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,339 INFO epoch # 9181 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031699203937023412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,363 INFO epoch # 9182 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032164327094506007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,387 INFO epoch # 9183 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033366909774485976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,412 INFO epoch # 9184 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003391322312381817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,436 INFO epoch # 9185 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00400768888721359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,460 INFO epoch # 9186 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032721079587645363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,484 INFO epoch # 9187 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003532884566084249
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,508 INFO epoch # 9188 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034949536675412674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,532 INFO epoch # 9189 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003067485900828615
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,555 INFO epoch # 9190 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003338737689773552
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:33,556 INFO *** epoch 9190, rolling-avg-loss (window=10)= 0.0033828225063189167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,579 INFO epoch # 9191 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034073200331476983
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,603 INFO epoch # 9192 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036902287138218526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,627 INFO epoch # 9193 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032800749577290844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,651 INFO epoch # 9194 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003273543690738734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,674 INFO epoch # 9195 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033350526537105907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,698 INFO epoch # 9196 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033663558861007914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,722 INFO epoch # 9197 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003106114560068818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,746 INFO epoch # 9198 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030638438838650472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,770 INFO epoch # 9199 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030603735758631956
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,794 INFO epoch # 9200 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031666870145272696
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:33,794 INFO *** epoch 9200, rolling-avg-loss (window=10)= 0.0032749594969573082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,818 INFO epoch # 9201 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029781531884509604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,842 INFO epoch # 9202 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003293648987892084
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,866 INFO epoch # 9203 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003429961234360235
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,890 INFO epoch # 9204 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003391659185581375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,914 INFO epoch # 9205 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00355464525273419
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,938 INFO epoch # 9206 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036312602314865217
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,963 INFO epoch # 9207 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003644915090262657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:33,987 INFO epoch # 9208 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035852988803526387
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,013 INFO epoch # 9209 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003432288493058877
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,037 INFO epoch # 9210 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031568751255690586
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:34,037 INFO *** epoch 9210, rolling-avg-loss (window=10)= 0.0034098705669748596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,061 INFO epoch # 9211 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033759455109247938
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,085 INFO epoch # 9212 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032374659131164663
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,109 INFO epoch # 9213 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035656732179631945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,133 INFO epoch # 9214 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034023148982669227
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,157 INFO epoch # 9215 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003376133226993261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,182 INFO epoch # 9216 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033955434555537067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,205 INFO epoch # 9217 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029989628092153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,230 INFO epoch # 9218 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029938727093394846
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,254 INFO epoch # 9219 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003527743203449063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,277 INFO epoch # 9220 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034182748313469347
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:34,277 INFO *** epoch 9220, rolling-avg-loss (window=10)= 0.0033291929776169127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,301 INFO epoch # 9221 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036789213081647176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,325 INFO epoch # 9222 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032560557356191566
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,349 INFO epoch # 9223 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003188062139088288
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,372 INFO epoch # 9224 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036348061403259635
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,398 INFO epoch # 9225 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031421965795743745
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,422 INFO epoch # 9226 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003387845204997575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,446 INFO epoch # 9227 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003731209595571272
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,471 INFO epoch # 9228 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035287399514345452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,494 INFO epoch # 9229 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030453796935034916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,518 INFO epoch # 9230 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031232065448421054
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:34,518 INFO *** epoch 9230, rolling-avg-loss (window=10)= 0.003371642289312149
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,542 INFO epoch # 9231 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034337850665906444
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,566 INFO epoch # 9232 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003751840806216933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,590 INFO epoch # 9233 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033838998097053263
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,614 INFO epoch # 9234 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030799721607763786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,638 INFO epoch # 9235 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003328039667394478
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,662 INFO epoch # 9236 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034288354909222107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,685 INFO epoch # 9237 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033401811651856406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,709 INFO epoch # 9238 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032446521472593304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,733 INFO epoch # 9239 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030186332915036473
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,757 INFO epoch # 9240 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030563100990548264
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:34,757 INFO *** epoch 9240, rolling-avg-loss (window=10)= 0.0033066149704609415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,781 INFO epoch # 9241 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003233329942304408
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,805 INFO epoch # 9242 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034086552950611804
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,829 INFO epoch # 9243 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033385674651071895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,853 INFO epoch # 9244 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034996201829926576
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,877 INFO epoch # 9245 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003312455530249281
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,901 INFO epoch # 9246 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003020760927029187
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,925 INFO epoch # 9247 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033133614706457593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,948 INFO epoch # 9248 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035342420887900516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,972 INFO epoch # 9249 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031501013472734485
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:34,996 INFO epoch # 9250 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003128943335468648
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:34,996 INFO *** epoch 9250, rolling-avg-loss (window=10)= 0.003294003758492181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,020 INFO epoch # 9251 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033834346759249456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,044 INFO epoch # 9252 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030892628710716963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,067 INFO epoch # 9253 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003001503431732999
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,091 INFO epoch # 9254 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034904045423900243
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,115 INFO epoch # 9255 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033783236649469472
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,139 INFO epoch # 9256 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003283901936811162
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,163 INFO epoch # 9257 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034004960652964655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,186 INFO epoch # 9258 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032131290317920502
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,210 INFO epoch # 9259 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003440454511292046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,234 INFO epoch # 9260 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034110368687834125
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:35,234 INFO *** epoch 9260, rolling-avg-loss (window=10)= 0.0033091947600041747
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,258 INFO epoch # 9261 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030913695773051586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,281 INFO epoch # 9262 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034277279846719466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,305 INFO epoch # 9263 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003332943801069632
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,329 INFO epoch # 9264 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003231961432902608
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,353 INFO epoch # 9265 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033284549608652014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,377 INFO epoch # 9266 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033327168857795186
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,401 INFO epoch # 9267 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030536137746821623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,424 INFO epoch # 9268 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029949739000585396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,448 INFO epoch # 9269 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031794380702194758
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,473 INFO epoch # 9270 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029553544482041616
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:35,473 INFO *** epoch 9270, rolling-avg-loss (window=10)= 0.0031928554835758405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,497 INFO epoch # 9271 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003681732778204605
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,521 INFO epoch # 9272 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032542240114707965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,545 INFO epoch # 9273 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034675339120440185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,569 INFO epoch # 9274 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036545223629218526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,592 INFO epoch # 9275 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036614060700230766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,616 INFO epoch # 9276 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035168900176358875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,639 INFO epoch # 9277 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034701557269727346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,663 INFO epoch # 9278 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034707436207099818
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,687 INFO epoch # 9279 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033326891898468602
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,712 INFO epoch # 9280 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033446954366809223
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:35,712 INFO *** epoch 9280, rolling-avg-loss (window=10)= 0.0034854593126510736
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,736 INFO epoch # 9281 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032820088890730403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,760 INFO epoch # 9282 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003318741077237064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,784 INFO epoch # 9283 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003623837743361946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,807 INFO epoch # 9284 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003656525044789305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,831 INFO epoch # 9285 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003445388763793744
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,855 INFO epoch # 9286 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030368568041012622
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,879 INFO epoch # 9287 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003106769665464526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,904 INFO epoch # 9288 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030821804502920713
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,928 INFO epoch # 9289 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032172363135032356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,952 INFO epoch # 9290 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034357039221504238
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:35,953 INFO *** epoch 9290, rolling-avg-loss (window=10)= 0.003320524867376662
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:35,977 INFO epoch # 9291 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003326692745758919
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,000 INFO epoch # 9292 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035310669045429677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,024 INFO epoch # 9293 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003189554849086562
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,048 INFO epoch # 9294 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034447159414412454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,072 INFO epoch # 9295 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031630651610612404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,095 INFO epoch # 9296 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003020595664565917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,120 INFO epoch # 9297 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031251388900273014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,144 INFO epoch # 9298 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029611543177452404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,168 INFO epoch # 9299 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00305739613759215
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,191 INFO epoch # 9300 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003209938749932917
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:36,192 INFO *** epoch 9300, rolling-avg-loss (window=10)= 0.003202931936175446
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,215 INFO epoch # 9301 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003925716351659503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,239 INFO epoch # 9302 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029156874425098067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,263 INFO epoch # 9303 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032242269735434093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,287 INFO epoch # 9304 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029042528476566076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,311 INFO epoch # 9305 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002825831816153368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,334 INFO epoch # 9306 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003121445366559783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,358 INFO epoch # 9307 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003341329893373768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,382 INFO epoch # 9308 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030236229467845988
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,408 INFO epoch # 9309 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003174776200467022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,431 INFO epoch # 9310 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003423272275540512
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:36,432 INFO *** epoch 9310, rolling-avg-loss (window=10)= 0.003188016211424838
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,455 INFO epoch # 9311 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003572819237888325
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,479 INFO epoch # 9312 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003212710584193701
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,503 INFO epoch # 9313 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033447601708758157
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,527 INFO epoch # 9314 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003229354912036797
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,551 INFO epoch # 9315 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003128024807665497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,575 INFO epoch # 9316 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030676573915116023
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,599 INFO epoch # 9317 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003179437164362753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,623 INFO epoch # 9318 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004054152766912011
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,646 INFO epoch # 9319 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003477324418781791
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,670 INFO epoch # 9320 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003920013812603429
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:36,670 INFO *** epoch 9320, rolling-avg-loss (window=10)= 0.003418625526683172
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,694 INFO epoch # 9321 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003479251179669518
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,717 INFO epoch # 9322 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032990645777317695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,741 INFO epoch # 9323 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003148125939333113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,765 INFO epoch # 9324 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030507331575790886
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,789 INFO epoch # 9325 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003193620930687757
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,813 INFO epoch # 9326 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029078745501465164
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,837 INFO epoch # 9327 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002985812447150238
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,862 INFO epoch # 9328 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003216939116100548
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,886 INFO epoch # 9329 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031364965725515503
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,913 INFO epoch # 9330 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003102087681327248
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:36,913 INFO *** epoch 9330, rolling-avg-loss (window=10)= 0.0031520006152277345
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,937 INFO epoch # 9331 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030363202713488135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,961 INFO epoch # 9332 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003213987387425732
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:36,985 INFO epoch # 9333 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032129992760019377
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,009 INFO epoch # 9334 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032593974865449127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,033 INFO epoch # 9335 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034710939289652742
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,057 INFO epoch # 9336 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031694859972049017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,080 INFO epoch # 9337 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031589027312293183
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,104 INFO epoch # 9338 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003095677402598085
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,128 INFO epoch # 9339 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030091248845565133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,152 INFO epoch # 9340 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031473333001486026
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:37,152 INFO *** epoch 9340, rolling-avg-loss (window=10)= 0.003177432266602409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,176 INFO epoch # 9341 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033330181940982584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,201 INFO epoch # 9342 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003239098488847958
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,225 INFO epoch # 9343 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031717589372419752
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,250 INFO epoch # 9344 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031905175601423252
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,274 INFO epoch # 9345 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031032752376631834
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,298 INFO epoch # 9346 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002962514743558131
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,322 INFO epoch # 9347 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030018951983947773
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,345 INFO epoch # 9348 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003011756176420022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,369 INFO epoch # 9349 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032080075980047695
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,395 INFO epoch # 9350 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030574468510167208
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:37,395 INFO *** epoch 9350, rolling-avg-loss (window=10)= 0.0031279288985388122
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,419 INFO epoch # 9351 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003021248943696264
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,444 INFO epoch # 9352 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031377657505800016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,468 INFO epoch # 9353 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003119878572761081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,492 INFO epoch # 9354 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032972108128888067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,516 INFO epoch # 9355 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003709411161253229
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,540 INFO epoch # 9356 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00378705854745931
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,564 INFO epoch # 9357 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032761178808868863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,587 INFO epoch # 9358 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034726149096968584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,612 INFO epoch # 9359 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00369874450370844
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,636 INFO epoch # 9360 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003077573785049026
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:37,636 INFO *** epoch 9360, rolling-avg-loss (window=10)= 0.0033597624867979905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,660 INFO epoch # 9361 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033030791382770985
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,683 INFO epoch # 9362 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003372010301973205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,707 INFO epoch # 9363 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032776675398054067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,731 INFO epoch # 9364 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003175407535309205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,755 INFO epoch # 9365 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002986728366522584
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,779 INFO epoch # 9366 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003108351978880819
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,803 INFO epoch # 9367 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029462889178830665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,826 INFO epoch # 9368 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002954706738819368
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,851 INFO epoch # 9369 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003084615254920209
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,875 INFO epoch # 9370 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003365578630109667
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:37,875 INFO *** epoch 9370, rolling-avg-loss (window=10)= 0.0031574434402500628
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,900 INFO epoch # 9371 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035122395674989093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,925 INFO epoch # 9372 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003442785411607474
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,949 INFO epoch # 9373 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0041475728976365644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,972 INFO epoch # 9374 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037609434803016484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:37,996 INFO epoch # 9375 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032924821207416244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,020 INFO epoch # 9376 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031707895468571223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,044 INFO epoch # 9377 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030339384102262557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,069 INFO epoch # 9378 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032891165210457984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,094 INFO epoch # 9379 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003781167517445283
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,118 INFO epoch # 9380 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003555484930984676
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:38,118 INFO *** epoch 9380, rolling-avg-loss (window=10)= 0.0034986520404345356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,142 INFO epoch # 9381 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033314477768726647
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,166 INFO epoch # 9382 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031728713584016077
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,190 INFO epoch # 9383 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029760610923403874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,213 INFO epoch # 9384 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029012001505179796
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,237 INFO epoch # 9385 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027742566999222618
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,261 INFO epoch # 9386 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030182246191543527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,285 INFO epoch # 9387 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030392621156352106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,309 INFO epoch # 9388 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036330746806925163
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,333 INFO epoch # 9389 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037956677879265044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,357 INFO epoch # 9390 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003308109222416533
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:38,357 INFO *** epoch 9390, rolling-avg-loss (window=10)= 0.0031950175503880018
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,381 INFO epoch # 9391 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003311890654003946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,406 INFO epoch # 9392 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002995174829266034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,430 INFO epoch # 9393 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003071049399295589
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,454 INFO epoch # 9394 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030106591184448916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,479 INFO epoch # 9395 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028795533398806583
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,503 INFO epoch # 9396 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003115388441074174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,528 INFO epoch # 9397 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035422949040366802
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,552 INFO epoch # 9398 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031323451003117952
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,576 INFO epoch # 9399 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035756682191276923
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,600 INFO epoch # 9400 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003191324256476946
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:38,600 INFO *** epoch 9400, rolling-avg-loss (window=10)= 0.0031825348261918405
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,623 INFO epoch # 9401 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028968013466510456
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,647 INFO epoch # 9402 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029904071598139126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,671 INFO epoch # 9403 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003087837772909552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,695 INFO epoch # 9404 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033166206412715837
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,719 INFO epoch # 9405 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031549603991152253
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,743 INFO epoch # 9406 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034078855387633666
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,767 INFO epoch # 9407 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033364276714564767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,791 INFO epoch # 9408 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003290978136647027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,815 INFO epoch # 9409 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034144136152463034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,838 INFO epoch # 9410 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003201753705070587
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:38,838 INFO *** epoch 9410, rolling-avg-loss (window=10)= 0.003209808598694508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,862 INFO epoch # 9411 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003419530603423482
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,886 INFO epoch # 9412 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034498684872232843
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,910 INFO epoch # 9413 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030879799778631423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,934 INFO epoch # 9414 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033878610738611314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,958 INFO epoch # 9415 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003004785589837411
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:38,982 INFO epoch # 9416 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028968753995286534
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,006 INFO epoch # 9417 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028634043228521477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,030 INFO epoch # 9418 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029147414097678848
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,054 INFO epoch # 9419 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003549818127794424
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,077 INFO epoch # 9420 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003633012787759071
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:39,077 INFO *** epoch 9420, rolling-avg-loss (window=10)= 0.003220787777991063
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,101 INFO epoch # 9421 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030913341797713656
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,128 INFO epoch # 9422 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032378378200519364
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,156 INFO epoch # 9423 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033012738094839733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,185 INFO epoch # 9424 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037203170722932555
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,214 INFO epoch # 9425 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004834583596675657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,244 INFO epoch # 9426 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004135149018111406
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,271 INFO epoch # 9427 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037352977960836142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,298 INFO epoch # 9428 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003184479777701199
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,325 INFO epoch # 9429 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031241129836416803
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,352 INFO epoch # 9430 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002979773769766325
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:39,353 INFO *** epoch 9430, rolling-avg-loss (window=10)= 0.0035344159823580412
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,381 INFO epoch # 9431 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002916076329711359
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,410 INFO epoch # 9432 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002991969493450597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,437 INFO epoch # 9433 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032901955382840242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,464 INFO epoch # 9434 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029092604709148873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,490 INFO epoch # 9435 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031704128086857963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,517 INFO epoch # 9436 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034081753983628005
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,543 INFO epoch # 9437 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003330599112814525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,567 INFO epoch # 9438 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002903621134464629
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,591 INFO epoch # 9439 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032789602846605703
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,615 INFO epoch # 9440 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003137102728942409
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:39,616 INFO *** epoch 9440, rolling-avg-loss (window=10)= 0.0031336373300291597
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,640 INFO epoch # 9441 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032224212591245305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,664 INFO epoch # 9442 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030340272933244705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,689 INFO epoch # 9443 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003102803808360477
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,728 INFO epoch # 9444 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031046047442941926
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,763 INFO epoch # 9445 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002979578985105036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,804 INFO epoch # 9446 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029844690252502915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,843 INFO epoch # 9447 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029496076094801538
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,872 INFO epoch # 9448 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030975533882156014
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,900 INFO epoch # 9449 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029586020718852524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,935 INFO epoch # 9450 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003153912755806232
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:39,935 INFO *** epoch 9450, rolling-avg-loss (window=10)= 0.0030587580940846237
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,964 INFO epoch # 9451 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003508403926389292
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:39,993 INFO epoch # 9452 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004213003659970127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,021 INFO epoch # 9453 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032816603379615117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,051 INFO epoch # 9454 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029834095148544293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,079 INFO epoch # 9455 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031217695141094737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,107 INFO epoch # 9456 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003122265730780782
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,136 INFO epoch # 9457 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030109542240097653
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,164 INFO epoch # 9458 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002855037648259895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,192 INFO epoch # 9459 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002680258250620682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,218 INFO epoch # 9460 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028207531540829223
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:40,218 INFO *** epoch 9460, rolling-avg-loss (window=10)= 0.003159751596103888
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,243 INFO epoch # 9461 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030617566881119274
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,267 INFO epoch # 9462 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031162529849098064
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,292 INFO epoch # 9463 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029827312973793596
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,318 INFO epoch # 9464 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030791348526690854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,342 INFO epoch # 9465 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030872855095367413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,366 INFO epoch # 9466 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031192567803373095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,390 INFO epoch # 9467 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003117909604043234
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,414 INFO epoch # 9468 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030208941534510814
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,438 INFO epoch # 9469 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032686027661839034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,462 INFO epoch # 9470 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029492520734493155
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:40,462 INFO *** epoch 9470, rolling-avg-loss (window=10)= 0.0030803076710071765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,487 INFO epoch # 9471 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002987969939567847
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,512 INFO epoch # 9472 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003244600291509414
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,537 INFO epoch # 9473 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003230386348150205
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,561 INFO epoch # 9474 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003401978905458236
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,585 INFO epoch # 9475 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003597722086851718
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,611 INFO epoch # 9476 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003315603476949036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,635 INFO epoch # 9477 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032359714241465554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,659 INFO epoch # 9478 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003566035331459716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,683 INFO epoch # 9479 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035008057639061008
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,706 INFO epoch # 9480 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003045051358640194
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:40,707 INFO *** epoch 9480, rolling-avg-loss (window=10)= 0.0033126124926639022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,731 INFO epoch # 9481 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003109791607130319
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,757 INFO epoch # 9482 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031969537667464465
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,781 INFO epoch # 9483 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030865925291436724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,805 INFO epoch # 9484 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00383991957642138
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,829 INFO epoch # 9485 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0044052267403458245
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,853 INFO epoch # 9486 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003194786799213034
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,877 INFO epoch # 9487 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003122987171082059
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,900 INFO epoch # 9488 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032123100354510825
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,924 INFO epoch # 9489 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032841535030456726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,948 INFO epoch # 9490 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030471080826828256
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:40,948 INFO *** epoch 9490, rolling-avg-loss (window=10)= 0.0033499829811262315
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,972 INFO epoch # 9491 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031416047568200156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:40,996 INFO epoch # 9492 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029081497305014636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,021 INFO epoch # 9493 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031353062950074673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,049 INFO epoch # 9494 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030333002723637037
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,075 INFO epoch # 9495 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003027891034435015
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,101 INFO epoch # 9496 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030233454090193845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,125 INFO epoch # 9497 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002838161773979664
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,151 INFO epoch # 9498 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002889899733418133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,175 INFO epoch # 9499 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002973935777845327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,200 INFO epoch # 9500 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030901218524377327
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:41,200 INFO *** epoch 9500, rolling-avg-loss (window=10)= 0.0030061716635827905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,224 INFO epoch # 9501 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002892705571866827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,248 INFO epoch # 9502 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031407522328663617
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,271 INFO epoch # 9503 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033807038780651055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,295 INFO epoch # 9504 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034788728335115593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,319 INFO epoch # 9505 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003474348603049293
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,343 INFO epoch # 9506 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038393589602492284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,367 INFO epoch # 9507 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031732110037410166
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,396 INFO epoch # 9508 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003072579562285682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,426 INFO epoch # 9509 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029541531730501447
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,453 INFO epoch # 9510 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032290521266986616
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:41,454 INFO *** epoch 9510, rolling-avg-loss (window=10)= 0.003263573794538388
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,481 INFO epoch # 9511 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030993879299785476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,508 INFO epoch # 9512 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030052667170821223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,532 INFO epoch # 9513 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002899397699366091
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,557 INFO epoch # 9514 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002952697697764961
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,584 INFO epoch # 9515 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027717225202650297
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,609 INFO epoch # 9516 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031572452680848073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,633 INFO epoch # 9517 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033673963080218527
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,657 INFO epoch # 9518 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003976374344347278
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,681 INFO epoch # 9519 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00484339564354741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,705 INFO epoch # 9520 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00353042872302467
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:41,705 INFO *** epoch 9520, rolling-avg-loss (window=10)= 0.0033603312851482768
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,728 INFO epoch # 9521 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032914133116719313
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,754 INFO epoch # 9522 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003223357522074366
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,778 INFO epoch # 9523 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029469674118445255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,804 INFO epoch # 9524 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028610905283130705
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,832 INFO epoch # 9525 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002855367085430771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,860 INFO epoch # 9526 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032202028050960507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,885 INFO epoch # 9527 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003399766232178081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,910 INFO epoch # 9528 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030709930761076976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,934 INFO epoch # 9529 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029138084209989756
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,958 INFO epoch # 9530 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003375993042936898
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:41,958 INFO *** epoch 9530, rolling-avg-loss (window=10)= 0.0031158959436652367
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:41,983 INFO epoch # 9531 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034895072458311915
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,008 INFO epoch # 9532 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003092669699981343
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,032 INFO epoch # 9533 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003206465240509715
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,058 INFO epoch # 9534 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003487598201900255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,082 INFO epoch # 9535 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032797079293231945
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,106 INFO epoch # 9536 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031637030660931487
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,130 INFO epoch # 9537 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032539799249207135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,154 INFO epoch # 9538 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00322129422420403
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,179 INFO epoch # 9539 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002805887794238515
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,203 INFO epoch # 9540 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002971463527501328
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:42,203 INFO *** epoch 9540, rolling-avg-loss (window=10)= 0.0031972276854503436
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,227 INFO epoch # 9541 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031901923539408017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,256 INFO epoch # 9542 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003441515491431346
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,284 INFO epoch # 9543 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033070643730752636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,310 INFO epoch # 9544 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031081890629138798
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,335 INFO epoch # 9545 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028113229273003526
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,359 INFO epoch # 9546 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003025987334694946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,382 INFO epoch # 9547 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032938959666353185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,406 INFO epoch # 9548 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002871262389817275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,430 INFO epoch # 9549 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002818276123434771
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,454 INFO epoch # 9550 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030691696647409117
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:42,454 INFO *** epoch 9550, rolling-avg-loss (window=10)= 0.0030936875687984865
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,478 INFO epoch # 9551 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030360773707798216
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,502 INFO epoch # 9552 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00322227011929499
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,526 INFO epoch # 9553 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028407995705492795
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,559 INFO epoch # 9554 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00319571037471178
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,587 INFO epoch # 9555 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031766608626639936
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,614 INFO epoch # 9556 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033235967785003595
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,640 INFO epoch # 9557 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031785888459126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,665 INFO epoch # 9558 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003081933373323409
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,689 INFO epoch # 9559 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029844530472473707
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,713 INFO epoch # 9560 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031259923125617206
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:42,713 INFO *** epoch 9560, rolling-avg-loss (window=10)= 0.0031166082655545322
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,737 INFO epoch # 9561 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029591056554636452
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,761 INFO epoch # 9562 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003094940610026242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,785 INFO epoch # 9563 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030148689074849244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,808 INFO epoch # 9564 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029517886687244754
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,832 INFO epoch # 9565 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003227846395020606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,856 INFO epoch # 9566 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002906607882323442
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,880 INFO epoch # 9567 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002887230679334607
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,904 INFO epoch # 9568 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029707672038057353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,928 INFO epoch # 9569 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031252343687810935
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,952 INFO epoch # 9570 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003219463105779141
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:42,952 INFO *** epoch 9570, rolling-avg-loss (window=10)= 0.003035785347674391
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,975 INFO epoch # 9571 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003204771153832553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:42,999 INFO epoch # 9572 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035550061984395143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,023 INFO epoch # 9573 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003177525348291965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,047 INFO epoch # 9574 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003201206876838114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,071 INFO epoch # 9575 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030922426922188606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,095 INFO epoch # 9576 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029466970554494765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,119 INFO epoch # 9577 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002714407692110399
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,143 INFO epoch # 9578 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002896361194871133
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,167 INFO epoch # 9579 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029596101958304644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,191 INFO epoch # 9580 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030239808329497464
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:43,191 INFO *** epoch 9580, rolling-avg-loss (window=10)= 0.003077180924083223
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,214 INFO epoch # 9581 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003825713774858741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,238 INFO epoch # 9582 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035913068350055255
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,262 INFO epoch # 9583 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003243836839828873
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,286 INFO epoch # 9584 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030611465299443807
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,310 INFO epoch # 9585 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003026637534276233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,334 INFO epoch # 9586 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002752579348452855
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,358 INFO epoch # 9587 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002755293382506352
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,381 INFO epoch # 9588 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029156486598367337
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,405 INFO epoch # 9589 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029872298291593324
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,429 INFO epoch # 9590 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029238445313239936
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:43,429 INFO *** epoch 9590, rolling-avg-loss (window=10)= 0.003108323726519302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,453 INFO epoch # 9591 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029535227295127697
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,477 INFO epoch # 9592 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029719980775553267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,501 INFO epoch # 9593 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003137538438750198
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,525 INFO epoch # 9594 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029803754478052724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,549 INFO epoch # 9595 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003042068459762959
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,573 INFO epoch # 9596 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003136350500426488
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,596 INFO epoch # 9597 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029522507356887218
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,620 INFO epoch # 9598 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028180501503811684
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,644 INFO epoch # 9599 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030162910370563623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,668 INFO epoch # 9600 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002956422140414361
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:43,668 INFO *** epoch 9600, rolling-avg-loss (window=10)= 0.0029964867717353627
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,692 INFO epoch # 9601 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030675097768835258
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,716 INFO epoch # 9602 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030586267384933308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,739 INFO epoch # 9603 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034179417343693785
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,763 INFO epoch # 9604 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004025579946755897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,787 INFO epoch # 9605 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002993895592226181
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,811 INFO epoch # 9606 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028795591788366437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,835 INFO epoch # 9607 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031561500636598794
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,858 INFO epoch # 9608 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003062156305531971
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,882 INFO epoch # 9609 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032173521212826017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,906 INFO epoch # 9610 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031669581221649423
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:43,906 INFO *** epoch 9610, rolling-avg-loss (window=10)= 0.0032045729580204353
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,930 INFO epoch # 9611 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029914135520812124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,954 INFO epoch # 9612 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030578617042920087
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:43,978 INFO epoch # 9613 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003402127043955261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,002 INFO epoch # 9614 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003393825034436304
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,026 INFO epoch # 9615 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003544867526215967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,049 INFO epoch # 9616 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003980850375228329
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,073 INFO epoch # 9617 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00404275279288413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,097 INFO epoch # 9618 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004903499821011792
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,120 INFO epoch # 9619 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003286439517978579
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,144 INFO epoch # 9620 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029002245610172395
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:44,144 INFO *** epoch 9620, rolling-avg-loss (window=10)= 0.003550386192910082
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,168 INFO epoch # 9621 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002863413599698106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,192 INFO epoch # 9622 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002738783849054016
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,216 INFO epoch # 9623 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003203063450200716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,239 INFO epoch # 9624 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002953119696030626
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,263 INFO epoch # 9625 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003098544431850314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,286 INFO epoch # 9626 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003317377715575276
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,310 INFO epoch # 9627 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030801431385043543
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,334 INFO epoch # 9628 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003194537555827992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,358 INFO epoch # 9629 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002865247541194549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,381 INFO epoch # 9630 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003147709670884069
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:44,382 INFO *** epoch 9630, rolling-avg-loss (window=10)= 0.003046194064882002
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,407 INFO epoch # 9631 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003299533334939042
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,431 INFO epoch # 9632 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032866623769223224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,455 INFO epoch # 9633 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002913052092480939
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,479 INFO epoch # 9634 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029440292091749143
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,503 INFO epoch # 9635 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028721405869873706
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,528 INFO epoch # 9636 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027394121352699585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,552 INFO epoch # 9637 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00270327702673967
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,576 INFO epoch # 9638 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027709661408152897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,600 INFO epoch # 9639 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00293344731835532
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,624 INFO epoch # 9640 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029125468172424007
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:44,624 INFO *** epoch 9640, rolling-avg-loss (window=10)= 0.0029375067038927226
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,648 INFO epoch # 9641 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002885247435187921
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,673 INFO epoch # 9642 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00268885610421421
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,696 INFO epoch # 9643 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026247044261253905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,720 INFO epoch # 9644 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002655431930179475
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,744 INFO epoch # 9645 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028486289556894917
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,768 INFO epoch # 9646 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029428496345644817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,793 INFO epoch # 9647 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032055998217401793
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,817 INFO epoch # 9648 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003061344221350737
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,841 INFO epoch # 9649 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034541981331130955
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,864 INFO epoch # 9650 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029439812460623216
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:44,865 INFO *** epoch 9650, rolling-avg-loss (window=10)= 0.0029310841908227305
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,889 INFO epoch # 9651 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003053590884519508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,912 INFO epoch # 9652 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028935024383827113
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,936 INFO epoch # 9653 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004081622340891045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,960 INFO epoch # 9654 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035027314552280586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:44,984 INFO epoch # 9655 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003051339852390811
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,009 INFO epoch # 9656 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003029711313502048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,033 INFO epoch # 9657 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003109875055088196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,057 INFO epoch # 9658 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031270539948309306
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,081 INFO epoch # 9659 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032194806153711397
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,105 INFO epoch # 9660 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030238128747441806
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:45,105 INFO *** epoch 9660, rolling-avg-loss (window=10)= 0.003209272082494863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,129 INFO epoch # 9661 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032005002431105822
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,153 INFO epoch # 9662 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030134693697618786
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,177 INFO epoch # 9663 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003599236904847203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,201 INFO epoch # 9664 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003731956341653131
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,225 INFO epoch # 9665 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036508623816189356
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,249 INFO epoch # 9666 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003168070012179669
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,273 INFO epoch # 9667 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030223384892451577
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,297 INFO epoch # 9668 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002743889372140984
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,321 INFO epoch # 9669 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026249494912917726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,345 INFO epoch # 9670 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003021630251168972
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:45,345 INFO *** epoch 9670, rolling-avg-loss (window=10)= 0.0031776902857018285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,368 INFO epoch # 9671 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030172506376402453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,394 INFO epoch # 9672 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003015096710441867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,419 INFO epoch # 9673 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028751118879881687
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,444 INFO epoch # 9674 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031357432271761354
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,468 INFO epoch # 9675 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029792183049721643
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,492 INFO epoch # 9676 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027006885247828905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,516 INFO epoch # 9677 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002893478475016309
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,540 INFO epoch # 9678 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027836132685479242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,563 INFO epoch # 9679 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030228950527089182
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,587 INFO epoch # 9680 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028581008828041377
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:45,587 INFO *** epoch 9680, rolling-avg-loss (window=10)= 0.002928119697207876
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,612 INFO epoch # 9681 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030899979719833937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,639 INFO epoch # 9682 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002978049400553573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,667 INFO epoch # 9683 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029601334463222884
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,695 INFO epoch # 9684 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003117455304163741
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,722 INFO epoch # 9685 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003275887600466376
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,750 INFO epoch # 9686 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029014834253757726
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,776 INFO epoch # 9687 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002816601172526134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,800 INFO epoch # 9688 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027332126192050055
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,824 INFO epoch # 9689 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002913445609010523
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,848 INFO epoch # 9690 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030737454580958
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:45,848 INFO *** epoch 9690, rolling-avg-loss (window=10)= 0.002986001200770261
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,875 INFO epoch # 9691 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029345721759455046
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,900 INFO epoch # 9692 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00296133970914525
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,924 INFO epoch # 9693 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003014822676050244
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,950 INFO epoch # 9694 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034131946849811357
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,974 INFO epoch # 9695 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036837164625467267
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:45,998 INFO epoch # 9696 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0038678384480590466
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,022 INFO epoch # 9697 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033733523996488657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,046 INFO epoch # 9698 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003640139249910135
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,071 INFO epoch # 9699 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00314368928593467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,096 INFO epoch # 9700 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032442933625134174
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:46,096 INFO *** epoch 9700, rolling-avg-loss (window=10)= 0.0033276958454734997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,121 INFO epoch # 9701 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003456010246736696
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,145 INFO epoch # 9702 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029183388578530867
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,170 INFO epoch # 9703 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002941251888842089
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,194 INFO epoch # 9704 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031374731624964625
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,218 INFO epoch # 9705 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035636922730191145
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,242 INFO epoch # 9706 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036586269779945724
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,266 INFO epoch # 9707 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002974845963763073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,296 INFO epoch # 9708 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002915137847594451
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,325 INFO epoch # 9709 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031274578723241575
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,353 INFO epoch # 9710 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003096150998317171
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:46,353 INFO *** epoch 9710, rolling-avg-loss (window=10)= 0.0031788986088940874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,382 INFO epoch # 9711 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002761312895017909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,409 INFO epoch # 9712 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002814884137478657
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,437 INFO epoch # 9713 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002704985272430349
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,465 INFO epoch # 9714 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026536058358033188
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,492 INFO epoch # 9715 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029806394340994302
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,520 INFO epoch # 9716 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032626048632664606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,548 INFO epoch # 9717 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033465201486251317
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,576 INFO epoch # 9718 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003152765602862928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,602 INFO epoch # 9719 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032228640193352476
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,626 INFO epoch # 9720 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003142861791275209
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:46,626 INFO *** epoch 9720, rolling-avg-loss (window=10)= 0.003004304400019464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,651 INFO epoch # 9721 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030028901783225592
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,675 INFO epoch # 9722 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00289325154517428
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,699 INFO epoch # 9723 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030835218021820765
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,723 INFO epoch # 9724 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029205007194832433
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,747 INFO epoch # 9725 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002836231837136438
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,771 INFO epoch # 9726 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002905480805566185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,795 INFO epoch # 9727 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002679249082575552
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,819 INFO epoch # 9728 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027718971577996854
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,844 INFO epoch # 9729 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029501520621124655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,873 INFO epoch # 9730 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00296217391587561
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:46,873 INFO *** epoch 9730, rolling-avg-loss (window=10)= 0.0029005349106228095
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,900 INFO epoch # 9731 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032801057386677712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,925 INFO epoch # 9732 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002999179134349106
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,949 INFO epoch # 9733 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027295430154481437
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,973 INFO epoch # 9734 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003220116270313156
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:46,997 INFO epoch # 9735 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003003863552294206
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,021 INFO epoch # 9736 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002911172810854623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,045 INFO epoch # 9737 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003379508176294621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,069 INFO epoch # 9738 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032037461514846655
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,093 INFO epoch # 9739 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002835856161254924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,117 INFO epoch # 9740 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031553065055049956
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:47,117 INFO *** epoch 9740, rolling-avg-loss (window=10)= 0.003071839751646621
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,141 INFO epoch # 9741 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002839463410055032
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,183 INFO epoch # 9742 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029119197115505813
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,226 INFO epoch # 9743 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030524445210176054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,254 INFO epoch # 9744 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034463415358914062
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,279 INFO epoch # 9745 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003704507147631375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,303 INFO epoch # 9746 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031302332718041725
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,327 INFO epoch # 9747 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028669107632595114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,351 INFO epoch # 9748 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002999274711328326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,375 INFO epoch # 9749 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035047380915784743
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,400 INFO epoch # 9750 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036021788200741867
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:47,400 INFO *** epoch 9750, rolling-avg-loss (window=10)= 0.003205801198419067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,425 INFO epoch # 9751 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034858756989706308
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,450 INFO epoch # 9752 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033014720138453413
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,474 INFO epoch # 9753 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0033750387810869142
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,497 INFO epoch # 9754 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00301547723938711
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,521 INFO epoch # 9755 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002909022936364636
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,545 INFO epoch # 9756 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032528641968383454
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,569 INFO epoch # 9757 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003490833711111918
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,593 INFO epoch # 9758 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028275043514440767
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,618 INFO epoch # 9759 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032846966460056137
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,642 INFO epoch # 9760 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002912663039751351
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:47,642 INFO *** epoch 9760, rolling-avg-loss (window=10)= 0.0031855448614805937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,666 INFO epoch # 9761 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030774675760767423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,690 INFO epoch # 9762 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030306833468785044
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,713 INFO epoch # 9763 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029338325366552453
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,744 INFO epoch # 9764 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029094796955178026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,770 INFO epoch # 9765 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029000694667047355
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,794 INFO epoch # 9766 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028845858832937665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,819 INFO epoch # 9767 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029379274492384866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,843 INFO epoch # 9768 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002772483123408165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,869 INFO epoch # 9769 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002727902736296528
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,895 INFO epoch # 9770 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028397246496751904
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:47,896 INFO *** epoch 9770, rolling-avg-loss (window=10)= 0.0029014156463745165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,921 INFO epoch # 9771 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028587792039616033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,945 INFO epoch # 9772 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031715396216895897
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,969 INFO epoch # 9773 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028823589636886027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:47,994 INFO epoch # 9774 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003180800991685828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,020 INFO epoch # 9775 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028666188482020516
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,046 INFO epoch # 9776 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002801751110382611
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,072 INFO epoch # 9777 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002695134411624167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,096 INFO epoch # 9778 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002744858942605788
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,119 INFO epoch # 9779 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00291561067206203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,143 INFO epoch # 9780 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003774920631258283
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:48,143 INFO *** epoch 9780, rolling-avg-loss (window=10)= 0.0029892373397160553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,167 INFO epoch # 9781 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032834707526490092
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,190 INFO epoch # 9782 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002967851181892911
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,214 INFO epoch # 9783 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003083672247157665
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,238 INFO epoch # 9784 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030143919357215054
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,261 INFO epoch # 9785 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026635880039975746
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,285 INFO epoch # 9786 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002972513972054003
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,309 INFO epoch # 9787 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002957486420200439
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,335 INFO epoch # 9788 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030106022859399673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,363 INFO epoch # 9789 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002646017946972279
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,391 INFO epoch # 9790 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002799776913889218
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:48,392 INFO *** epoch 9790, rolling-avg-loss (window=10)= 0.002939937166047457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,420 INFO epoch # 9791 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026514796882111114
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,449 INFO epoch # 9792 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029180759120208677
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,478 INFO epoch # 9793 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002711893030209467
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,506 INFO epoch # 9794 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030997788162494544
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,534 INFO epoch # 9795 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003273081405495759
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,562 INFO epoch # 9796 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032178932924580295
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,588 INFO epoch # 9797 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036622688403440407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,612 INFO epoch # 9798 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003127683376078494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,636 INFO epoch # 9799 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003201904577508685
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,660 INFO epoch # 9800 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003181735908583505
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:48,660 INFO *** epoch 9800, rolling-avg-loss (window=10)= 0.0031045794847159415
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,684 INFO epoch # 9801 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029644015412486624
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,708 INFO epoch # 9802 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002772239466139581
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,732 INFO epoch # 9803 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002978604428790277
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,756 INFO epoch # 9804 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002789383459457895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,779 INFO epoch # 9805 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030264985653047916
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,803 INFO epoch # 9806 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029159414443711285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,832 INFO epoch # 9807 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030082786433922593
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,862 INFO epoch # 9808 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030279641287052073
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,901 INFO epoch # 9809 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031315208489104407
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,933 INFO epoch # 9810 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002800037163979141
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:48,933 INFO *** epoch 9810, rolling-avg-loss (window=10)= 0.0029414869690299384
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,961 INFO epoch # 9811 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030982762327766977
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:48,989 INFO epoch # 9812 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026440133751748363
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,016 INFO epoch # 9813 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026548554706096184
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,046 INFO epoch # 9814 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002987356077937875
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,075 INFO epoch # 9815 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002972624046378769
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,102 INFO epoch # 9816 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036110001674387604
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,128 INFO epoch # 9817 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032909962537814863
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,153 INFO epoch # 9818 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032722969153837766
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,178 INFO epoch # 9819 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029793063877150416
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,202 INFO epoch # 9820 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003078409965382889
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:49,202 INFO *** epoch 9820, rolling-avg-loss (window=10)= 0.003058913489257975
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,225 INFO epoch # 9821 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002955119405669393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,249 INFO epoch # 9822 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032253149038297124
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,273 INFO epoch # 9823 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030681654534419067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,297 INFO epoch # 9824 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004353036776592489
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,321 INFO epoch # 9825 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035697377752512693
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,345 INFO epoch # 9826 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004099724632396828
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,369 INFO epoch # 9827 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031507885942119174
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,394 INFO epoch # 9828 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029621343237522524
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,417 INFO epoch # 9829 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002868938317988068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,445 INFO epoch # 9830 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027254312408331316
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:49,445 INFO *** epoch 9830, rolling-avg-loss (window=10)= 0.0032978391423966968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,474 INFO epoch # 9831 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028599697398021817
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,507 INFO epoch # 9832 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027366399772290606
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,537 INFO epoch # 9833 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029086884460411966
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,566 INFO epoch # 9834 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003033198354387423
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,592 INFO epoch # 9835 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002950153411802603
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,616 INFO epoch # 9836 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002849681100997259
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,641 INFO epoch # 9837 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002966762287542224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,665 INFO epoch # 9838 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036102203266636934
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,696 INFO epoch # 9839 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036373258008097764
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,720 INFO epoch # 9840 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035508771798049565
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:49,720 INFO *** epoch 9840, rolling-avg-loss (window=10)= 0.0031103516625080375
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,747 INFO epoch # 9841 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003620756309828721
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,771 INFO epoch # 9842 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030206328447093256
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,794 INFO epoch # 9843 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00361066839468549
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,818 INFO epoch # 9844 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030368388834176585
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,842 INFO epoch # 9845 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034908242741948925
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,865 INFO epoch # 9846 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034556805476313457
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,889 INFO epoch # 9847 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031644098307879176
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,912 INFO epoch # 9848 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003539109973644372
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,936 INFO epoch # 9849 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032818477775435895
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,960 INFO epoch # 9850 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002960105695819948
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:49,961 INFO *** epoch 9850, rolling-avg-loss (window=10)= 0.003318087453226326
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:49,985 INFO epoch # 9851 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027337072788213845
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,008 INFO epoch # 9852 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002800275638946914
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,032 INFO epoch # 9853 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002724863323237514
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,056 INFO epoch # 9854 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027663189684972167
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,079 INFO epoch # 9855 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031561253454128746
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,103 INFO epoch # 9856 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026878061034949496
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,126 INFO epoch # 9857 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026407693803776056
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,150 INFO epoch # 9858 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028867973924207035
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,174 INFO epoch # 9859 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031210812121571507
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,198 INFO epoch # 9860 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031682013141107745
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:50,198 INFO *** epoch 9860, rolling-avg-loss (window=10)= 0.002868594595747709
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,222 INFO epoch # 9861 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030093739296717104
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,246 INFO epoch # 9862 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002901190231568762
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,269 INFO epoch # 9863 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002753137618128676
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,293 INFO epoch # 9864 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003281848970800638
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,316 INFO epoch # 9865 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00290943682557554
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,340 INFO epoch # 9866 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002842418380168965
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,364 INFO epoch # 9867 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002880179743442568
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,388 INFO epoch # 9868 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002945537031337153
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,412 INFO epoch # 9869 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002925651424448006
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,436 INFO epoch # 9870 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002705097725993255
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:50,436 INFO *** epoch 9870, rolling-avg-loss (window=10)= 0.0029153871881135275
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,460 INFO epoch # 9871 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002792183764540823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,484 INFO epoch # 9872 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031234292255248874
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,510 INFO epoch # 9873 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002924798283856944
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,535 INFO epoch # 9874 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002822360322170425
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,560 INFO epoch # 9875 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028658319351961836
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,584 INFO epoch # 9876 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029453853603627067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,608 INFO epoch # 9877 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003700267094245646
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,632 INFO epoch # 9878 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003300622400274733
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,656 INFO epoch # 9879 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032533980083826464
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,680 INFO epoch # 9880 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032355847124563297
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:50,680 INFO *** epoch 9880, rolling-avg-loss (window=10)= 0.0030963861107011327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,704 INFO epoch # 9881 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030689043287566165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,727 INFO epoch # 9882 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028987876321480144
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,751 INFO epoch # 9883 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026723330411186907
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,774 INFO epoch # 9884 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029741108846792486
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,798 INFO epoch # 9885 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029406827816274017
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,840 INFO epoch # 9886 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028919796786794905
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,879 INFO epoch # 9887 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002962493363156682
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,910 INFO epoch # 9888 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031063527349033393
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,940 INFO epoch # 9889 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032413269345852314
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,964 INFO epoch # 9890 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002804480285703903
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:50,964 INFO *** epoch 9890, rolling-avg-loss (window=10)= 0.002956145166535862
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:50,988 INFO epoch # 9891 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027962216699961573
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,012 INFO epoch # 9892 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029692093630728777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,036 INFO epoch # 9893 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030605323627241887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,060 INFO epoch # 9894 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027931665135838557
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,084 INFO epoch # 9895 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002627603496875963
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,112 INFO epoch # 9896 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029766761399514508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,136 INFO epoch # 9897 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028638440380746033
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,160 INFO epoch # 9898 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031952538665791508
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,183 INFO epoch # 9899 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030680896816193126
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,208 INFO epoch # 9900 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003039155842998298
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:51,208 INFO *** epoch 9900, rolling-avg-loss (window=10)= 0.002938975297547586
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,232 INFO epoch # 9901 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003077588637097506
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,257 INFO epoch # 9902 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028325541061349213
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,282 INFO epoch # 9903 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002980816690978827
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,307 INFO epoch # 9904 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003006647886650171
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,331 INFO epoch # 9905 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002829965902492404
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,355 INFO epoch # 9906 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002813534505548887
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,379 INFO epoch # 9907 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002772155978163937
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,403 INFO epoch # 9908 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002885588455683319
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,427 INFO epoch # 9909 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002792935418256093
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,451 INFO epoch # 9910 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002698001087992452
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:51,452 INFO *** epoch 9910, rolling-avg-loss (window=10)= 0.0028689788668998517
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,475 INFO epoch # 9911 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028160164511064067
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,499 INFO epoch # 9912 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027504885001690127
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,523 INFO epoch # 9913 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.004116682193853194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,547 INFO epoch # 9914 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003353641677676933
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,579 INFO epoch # 9915 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029822805736330338
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,604 INFO epoch # 9916 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003080141486861976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,627 INFO epoch # 9917 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028656588765443303
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,652 INFO epoch # 9918 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028612642927328125
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,676 INFO epoch # 9919 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00267969037668081
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,700 INFO epoch # 9920 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026381890529592056
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:51,700 INFO *** epoch 9920, rolling-avg-loss (window=10)= 0.0030144053482217712
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,723 INFO epoch # 9921 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027853521751239896
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,747 INFO epoch # 9922 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028595173680514563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,771 INFO epoch # 9923 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002845353759767022
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,794 INFO epoch # 9924 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027658628750941716
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,818 INFO epoch # 9925 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002808028810250107
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,841 INFO epoch # 9926 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027158327866345644
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,865 INFO epoch # 9927 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030679190858791117
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,890 INFO epoch # 9928 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027480524076963775
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,914 INFO epoch # 9929 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002516175587516045
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,938 INFO epoch # 9930 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028662890363193583
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:51,938 INFO *** epoch 9930, rolling-avg-loss (window=10)= 0.0027978383892332203
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,963 INFO epoch # 9931 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002735527090408141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:51,987 INFO epoch # 9932 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003132018173346296
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,011 INFO epoch # 9933 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003333445347379893
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,035 INFO epoch # 9934 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00306374488354777
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,059 INFO epoch # 9935 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002785598157061031
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,083 INFO epoch # 9936 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031153423769865185
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,107 INFO epoch # 9937 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0035279033254482783
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,131 INFO epoch # 9938 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034116960850951727
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,154 INFO epoch # 9939 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0036507617005554494
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,178 INFO epoch # 9940 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028337341391306836
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:52,178 INFO *** epoch 9940, rolling-avg-loss (window=10)= 0.0031589771278959233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,202 INFO epoch # 9941 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028619501754292287
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,226 INFO epoch # 9942 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029271471867104992
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,250 INFO epoch # 9943 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029769115208182484
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,275 INFO epoch # 9944 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029883725110266823
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,300 INFO epoch # 9945 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002794052783428924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,325 INFO epoch # 9946 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003039330644242
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,350 INFO epoch # 9947 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002731666358158691
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,375 INFO epoch # 9948 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028096919868403347
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,399 INFO epoch # 9949 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003117175539955497
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,423 INFO epoch # 9950 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003208755613741232
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:52,423 INFO *** epoch 9950, rolling-avg-loss (window=10)= 0.0029455054320351335
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,448 INFO epoch # 9951 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028808240640501026
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,472 INFO epoch # 9952 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031002853720565327
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,498 INFO epoch # 9953 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028235244790266734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,523 INFO epoch # 9954 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027819082206406165
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,547 INFO epoch # 9955 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002896424375649076
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,575 INFO epoch # 9956 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00274133104539942
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,602 INFO epoch # 9957 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002767504811345134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,627 INFO epoch # 9958 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027796637878054753
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,652 INFO epoch # 9959 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0037221451129880734
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,677 INFO epoch # 9960 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003781891622566036
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:52,677 INFO *** epoch 9960, rolling-avg-loss (window=10)= 0.003027550289152714
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,702 INFO epoch # 9961 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030848829228489194
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,727 INFO epoch # 9962 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027265828066447284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,752 INFO epoch # 9963 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0025509100596536882
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,776 INFO epoch # 9964 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0026455962615727913
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,800 INFO epoch # 9965 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031426588211616036
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,826 INFO epoch # 9966 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027543375799723435
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,851 INFO epoch # 9967 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031367969895654824
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,876 INFO epoch # 9968 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.00313692923009512
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,900 INFO epoch # 9969 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002774618957118946
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,924 INFO epoch # 9970 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027289543140796013
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:52,924 INFO *** epoch 9970, rolling-avg-loss (window=10)= 0.0028682267942713225
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,948 INFO epoch # 9971 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027608047021203674
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,972 INFO epoch # 9972 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0025665528046374675
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:52,996 INFO epoch # 9973 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0025644005872891285
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,020 INFO epoch # 9974 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002546818108385196
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,044 INFO epoch # 9975 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002641281669639284
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,068 INFO epoch # 9976 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028266700101085007
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,092 INFO epoch # 9977 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029159949117456563
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,124 INFO epoch # 9978 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003163283265166683
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,157 INFO epoch # 9979 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030530552721756976
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,182 INFO epoch # 9980 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0032341554324375466
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:53,182 INFO *** epoch 9980, rolling-avg-loss (window=10)= 0.002827301676370553
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,207 INFO epoch # 9981 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029768409258394968
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,231 INFO epoch # 9982 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027958398968621623
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,256 INFO epoch # 9983 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0029535035682783928
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,281 INFO epoch # 9984 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.003352166590048
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,306 INFO epoch # 9985 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0034606912267918233
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,331 INFO epoch # 9986 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030626798470620997
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,355 INFO epoch # 9987 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002988576583447866
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,380 INFO epoch # 9988 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002666330179636134
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,412 INFO epoch # 9989 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002807599350489909
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,440 INFO epoch # 9990 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028699164831778035
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:53,440 INFO *** epoch 9990, rolling-avg-loss (window=10)= 0.0029934144651633686
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,467 INFO epoch # 9991 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0030866175366099924
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,491 INFO epoch # 9992 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002942816645372659
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,514 INFO epoch # 9993 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027408974347054027
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,538 INFO epoch # 9994 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002895268748034141
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,565 INFO epoch # 9995 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002908260175900068
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,589 INFO epoch # 9996 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0028335211536614224
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,613 INFO epoch # 9997 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0031633811704523396
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,638 INFO epoch # 9998 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027923336419917177
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,662 INFO epoch # 9999 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.0027574784253374673
[experiments_sandbox.py:899 -   <module>()] 2023-04-25 14:41:53,686 INFO epoch # 10000 - for optimizer <class 'torch.optim.adam.Adam'> -lr : 0.001-> 0.001 -loss = 0.002966959960758686
[experiments_sandbox.py:911 -   <module>()] 2023-04-25 14:41:53,686 INFO *** epoch 10000, rolling-avg-loss (window=10)= 0.00290875348928239
[experiments_sandbox.py:919 -   <module>()] 2023-04-25 14:41:53,686 INFO training time in seconds = 245
[experiments_sandbox.py:923 -   <module>()] 2023-04-25 14:41:53,687 INFO train-epochs-loss curve df :
[experiments_sandbox.py:924 -   <module>()] 2023-04-25 14:41:53,707 INFO 
     epochs  rolling-avg-loss
0        10         23.170703
1        20         16.630950
2        30         11.665174
3        40         11.568200
4        50          8.163261
5        60          5.616661
6        70          5.045490
7        80          3.657616
8        90          3.108688
9       100          3.240002
10      110          2.205344
11      120          1.782612
12      130          1.841136
13      140          2.377877
14      150          1.433974
15      160          1.176470
16      170          0.753494
17      180          0.769869
18      190          0.517752
19      200          0.403355
20      210          0.288858
21      220          0.322486
22      230          0.332154
23      240          0.223147
24      250          0.237845
25      260          0.191094
26      270          0.180724
27      280          0.143021
28      290          0.155648
29      300          0.110820
30      310          0.139340
31      320          0.099109
32      330          0.085119
33      340          0.082347
34      350          0.081419
35      360          0.071729
36      370          0.074443
37      380          0.076978
38      390          0.062628
39      400          0.062531
40      410          0.059587
41      420          0.056686
42      430          0.052252
43      440          0.052878
44      450          0.049552
45      460          0.048928
46      470          0.048355
47      480          0.048816
48      490          0.046290
49      500          0.045211
50      510          0.044633
51      520          0.043050
52      530          0.041374
53      540          0.041045
54      550          0.040013
55      560          0.039384
56      570          0.038652
57      580          0.038623
58      590          0.037571
59      600          0.037387
60      610          0.037197
61      620          0.037219
62      630          0.036679
63      640          0.036792
64      650          0.036597
65      660          0.036050
66      670          0.036158
67      680          0.035740
68      690          0.035157
69      700          0.035094
70      710          0.034640
71      720          0.034651
72      730          0.034511
73      740          0.034669
74      750          0.033838
75      760          0.033668
76      770          0.033476
77      780          0.033669
78      790          0.033500
79      800          0.032755
80      810          0.032740
81      820          0.032639
82      830          0.032480
83      840          0.032428
84      850          0.031615
85      860          0.031648
86      870          0.031463
87      880          0.031295
88      890          0.030749
89      900          0.030793
90      910          0.030601
91      920          0.030311
92      930          0.030169
93      940          0.029843
94      950          0.029581
95      960          0.029547
96      970          0.029137
97      980          0.029523
98      990          0.028768
99     1000          0.028961
100    1010          0.028438
101    1020          0.028291
102    1030          0.028202
103    1040          0.028015
104    1050          0.027795
105    1060          0.027852
106    1070          0.027802
107    1080          0.028148
108    1090          0.027887
109    1100          0.027146
110    1110          0.027430
111    1120          0.027056
112    1130          0.026639
113    1140          0.026353
114    1150          0.026928
115    1160          0.025949
116    1170          0.026410
117    1180          0.026155
118    1190          0.026164
119    1200          0.025748
120    1210          0.025901
121    1220          0.024962
122    1230          0.025357
123    1240          0.025859
124    1250          0.025264
125    1260          0.025120
126    1270          0.025095
127    1280          0.024720
128    1290          0.024767
129    1300          0.024398
130    1310          0.024346
131    1320          0.024183
132    1330          0.024200
133    1340          0.023933
134    1350          0.024043
135    1360          0.023737
136    1370          0.024079
137    1380          0.023321
138    1390          0.022992
139    1400          0.023118
140    1410          0.022766
141    1420          0.023037
142    1430          0.022958
143    1440          0.022741
144    1450          0.022423
145    1460          0.022412
146    1470          0.022151
147    1480          0.022439
148    1490          0.022572
149    1500          0.022168
150    1510          0.022018
151    1520          0.021513
152    1530          0.021478
153    1540          0.021515
154    1550          0.021400
155    1560          0.021282
156    1570          0.021803
157    1580          0.020924
158    1590          0.020889
159    1600          0.021144
160    1610          0.020650
161    1620          0.020946
162    1630          0.020324
163    1640          0.020769
164    1650          0.020499
165    1660          0.020532
166    1670          0.020274
167    1680          0.020155
168    1690          0.019979
169    1700          0.020000
170    1710          0.019542
171    1720          0.020139
172    1730          0.020043
173    1740          0.019485
174    1750          0.019615
175    1760          0.019650
176    1770          0.019364
177    1780          0.019531
178    1790          0.019140
179    1800          0.019058
180    1810          0.018878
181    1820          0.019228
182    1830          0.018923
183    1840          0.018579
184    1850          0.018737
185    1860          0.018360
186    1870          0.018814
187    1880          0.018319
188    1890          0.018569
189    1900          0.018681
190    1910          0.018201
191    1920          0.018775
192    1930          0.018160
193    1940          0.017812
194    1950          0.018143
195    1960          0.017932
196    1970          0.017770
197    1980          0.017569
198    1990          0.017671
199    2000          0.017464
200    2010          0.017312
201    2020          0.017552
202    2030          0.017537
203    2040          0.017364
204    2050          0.017466
205    2060          0.017026
206    2070          0.017048
207    2080          0.017151
208    2090          0.016791
209    2100          0.017416
210    2110          0.016861
211    2120          0.016884
212    2130          0.016662
213    2140          0.016718
214    2150          0.016776
215    2160          0.016391
216    2170          0.016921
217    2180          0.016404
218    2190          0.016061
219    2200          0.016135
220    2210          0.016300
221    2220          0.016165
222    2230          0.015993
223    2240          0.015862
224    2250          0.016198
225    2260          0.015840
226    2270          0.015829
227    2280          0.015850
228    2290          0.015684
229    2300          0.015779
230    2310          0.015385
231    2320          0.015503
232    2330          0.015816
233    2340          0.015296
234    2350          0.015436
235    2360          0.015325
236    2370          0.014994
237    2380          0.015145
238    2390          0.015226
239    2400          0.015530
240    2410          0.014807
241    2420          0.015285
242    2430          0.014946
243    2440          0.014546
244    2450          0.014697
245    2460          0.014653
246    2470          0.014756
247    2480          0.014378
248    2490          0.014448
249    2500          0.014772
250    2510          0.014385
251    2520          0.014750
252    2530          0.014583
253    2540          0.014248
254    2550          0.014182
255    2560          0.014314
256    2570          0.013969
257    2580          0.014139
258    2590          0.014100
259    2600          0.014028
260    2610          0.013879
261    2620          0.014269
262    2630          0.013761
263    2640          0.013626
264    2650          0.014123
265    2660          0.013540
266    2670          0.013669
267    2680          0.013777
268    2690          0.013843
269    2700          0.013588
270    2710          0.013352
271    2720          0.013431
272    2730          0.013505
273    2740          0.013471
274    2750          0.013069
275    2760          0.013363
276    2770          0.013293
277    2780          0.013252
278    2790          0.013256
279    2800          0.013085
280    2810          0.013147
281    2820          0.012976
282    2830          0.013048
283    2840          0.012920
284    2850          0.012817
285    2860          0.012874
286    2870          0.012764
287    2880          0.012754
288    2890          0.012392
289    2900          0.012806
290    2910          0.012554
291    2920          0.012305
292    2930          0.012240
293    2940          0.012775
294    2950          0.012582
295    2960          0.012246
296    2970          0.012381
297    2980          0.012387
298    2990          0.012250
299    3000          0.012154
300    3010          0.012535
301    3020          0.011898
302    3030          0.012030
303    3040          0.011947
304    3050          0.011966
305    3060          0.012026
306    3070          0.011791
307    3080          0.011657
308    3090          0.011840
309    3100          0.011759
310    3110          0.011504
311    3120          0.011638
312    3130          0.011788
313    3140          0.011545
314    3150          0.011627
315    3160          0.011486
316    3170          0.011611
317    3180          0.011462
318    3190          0.011622
319    3200          0.011447
320    3210          0.011262
321    3220          0.011562
322    3230          0.011326
323    3240          0.011315
324    3250          0.010978
325    3260          0.011221
326    3270          0.011013
327    3280          0.011444
328    3290          0.011087
329    3300          0.011220
330    3310          0.011001
331    3320          0.011129
332    3330          0.010585
333    3340          0.011035
334    3350          0.010759
335    3360          0.010595
336    3370          0.010627
337    3380          0.010677
338    3390          0.010728
339    3400          0.010901
340    3410          0.010535
341    3420          0.010462
342    3430          0.010828
343    3440          0.010730
344    3450          0.010489
345    3460          0.010286
346    3470          0.010247
347    3480          0.010495
348    3490          0.010656
349    3500          0.010305
350    3510          0.010362
351    3520          0.010239
352    3530          0.010015
353    3540          0.010298
354    3550          0.010462
355    3560          0.010176
356    3570          0.010047
357    3580          0.010015
358    3590          0.009927
359    3600          0.010103
360    3610          0.010120
361    3620          0.009833
362    3630          0.009643
363    3640          0.010022
364    3650          0.010059
365    3660          0.009881
366    3670          0.009712
367    3680          0.009848
368    3690          0.009827
369    3700          0.009886
370    3710          0.009417
371    3720          0.009795
372    3730          0.009502
373    3740          0.009572
374    3750          0.009222
375    3760          0.009760
376    3770          0.009472
377    3780          0.009368
378    3790          0.009477
379    3800          0.009627
380    3810          0.009576
381    3820          0.009493
382    3830          0.009257
383    3840          0.009152
384    3850          0.009136
385    3860          0.009127
386    3870          0.009519
387    3880          0.009236
388    3890          0.009016
389    3900          0.008994
390    3910          0.009193
391    3920          0.009165
392    3930          0.008829
393    3940          0.008886
394    3950          0.009058
395    3960          0.008911
396    3970          0.008774
397    3980          0.008912
398    3990          0.009474
399    4000          0.009090
400    4010          0.008992
401    4020          0.009367
402    4030          0.008964
403    4040          0.008771
404    4050          0.008747
405    4060          0.008881
406    4070          0.008936
407    4080          0.008732
408    4090          0.008712
409    4100          0.008660
410    4110          0.008510
411    4120          0.008678
412    4130          0.008763
413    4140          0.008612
414    4150          0.008382
415    4160          0.008500
416    4170          0.008683
417    4180          0.008268
418    4190          0.008524
419    4200          0.008455
420    4210          0.008282
421    4220          0.008486
422    4230          0.008279
423    4240          0.008282
424    4250          0.008312
425    4260          0.008590
426    4270          0.008451
427    4280          0.008095
428    4290          0.008205
429    4300          0.008190
430    4310          0.008455
431    4320          0.008027
432    4330          0.008018
433    4340          0.008640
434    4350          0.008214
435    4360          0.008234
436    4370          0.008085
437    4380          0.007972
438    4390          0.007827
439    4400          0.008026
440    4410          0.007865
441    4420          0.008065
442    4430          0.008291
443    4440          0.007858
444    4450          0.007857
445    4460          0.007988
446    4470          0.007863
447    4480          0.007941
448    4490          0.007910
449    4500          0.007825
450    4510          0.007635
451    4520          0.007845
452    4530          0.007927
453    4540          0.007796
454    4550          0.007792
455    4560          0.007648
456    4570          0.007757
457    4580          0.007542
458    4590          0.007939
459    4600          0.007942
460    4610          0.007499
461    4620          0.007649
462    4630          0.007476
463    4640          0.007577
464    4650          0.007574
465    4660          0.007624
466    4670          0.007593
467    4680          0.007254
468    4690          0.007545
469    4700          0.007532
470    4710          0.007491
471    4720          0.007322
472    4730          0.007549
473    4740          0.007483
474    4750          0.007362
475    4760          0.007402
476    4770          0.007155
477    4780          0.007292
478    4790          0.007463
479    4800          0.007101
480    4810          0.007299
481    4820          0.007244
482    4830          0.007263
483    4840          0.007123
484    4850          0.007205
485    4860          0.007184
486    4870          0.007292
487    4880          0.006954
488    4890          0.007332
489    4900          0.007147
490    4910          0.007115
491    4920          0.007162
492    4930          0.006859
493    4940          0.007256
494    4950          0.007193
495    4960          0.006813
496    4970          0.007051
497    4980          0.007602
498    4990          0.007034
499    5000          0.006895
500    5010          0.006766
501    5020          0.007408
502    5030          0.007187
503    5040          0.006858
504    5050          0.006785
505    5060          0.006747
506    5070          0.006917
507    5080          0.006765
508    5090          0.006942
509    5100          0.006877
510    5110          0.006696
511    5120          0.006758
512    5130          0.006691
513    5140          0.006664
514    5150          0.006773
515    5160          0.006689
516    5170          0.006688
517    5180          0.006689
518    5190          0.006751
519    5200          0.006540
520    5210          0.006452
521    5220          0.006769
522    5230          0.007197
523    5240          0.006933
524    5250          0.006663
525    5260          0.006433
526    5270          0.006491
527    5280          0.006387
528    5290          0.006400
529    5300          0.006684
530    5310          0.006500
531    5320          0.006607
532    5330          0.006484
533    5340          0.006543
534    5350          0.006492
535    5360          0.006507
536    5370          0.006786
537    5380          0.006683
538    5390          0.006435
539    5400          0.006214
540    5410          0.006344
541    5420          0.006599
542    5430          0.006677
543    5440          0.006376
544    5450          0.006514
545    5460          0.006421
546    5470          0.006229
547    5480          0.006362
548    5490          0.006256
549    5500          0.006393
550    5510          0.006124
551    5520          0.006157
552    5530          0.006457
553    5540          0.006089
554    5550          0.006054
555    5560          0.006371
556    5570          0.006329
557    5580          0.006310
558    5590          0.006422
559    5600          0.005979
560    5610          0.006109
561    5620          0.006491
562    5630          0.006273
563    5640          0.006111
564    5650          0.006091
565    5660          0.006093
566    5670          0.006518
567    5680          0.005871
568    5690          0.006203
569    5700          0.005980
570    5710          0.006076
571    5720          0.005847
572    5730          0.005915
573    5740          0.006225
574    5750          0.006264
575    5760          0.006046
576    5770          0.006146
577    5780          0.006257
578    5790          0.005964
579    5800          0.005904
580    5810          0.005792
581    5820          0.005947
582    5830          0.005763
583    5840          0.006146
584    5850          0.005937
585    5860          0.005958
586    5870          0.005907
587    5880          0.005789
588    5890          0.005801
589    5900          0.005813
590    5910          0.005904
591    5920          0.005795
592    5930          0.005780
593    5940          0.005840
594    5950          0.005991
595    5960          0.005603
596    5970          0.005789
597    5980          0.005807
598    5990          0.005619
599    6000          0.006055
600    6010          0.005625
601    6020          0.005822
602    6030          0.005784
603    6040          0.005563
604    6050          0.005878
605    6060          0.005672
606    6070          0.005558
607    6080          0.005637
608    6090          0.005820
609    6100          0.005859
610    6110          0.005541
611    6120          0.005626
612    6130          0.005528
613    6140          0.005626
614    6150          0.005753
615    6160          0.005506
616    6170          0.005405
617    6180          0.005679
618    6190          0.005447
619    6200          0.006088
620    6210          0.005318
621    6220          0.005457
622    6230          0.005440
623    6240          0.005437
624    6250          0.005547
625    6260          0.005449
626    6270          0.005635
627    6280          0.005329
628    6290          0.005353
629    6300          0.005297
630    6310          0.005634
631    6320          0.005394
632    6330          0.005318
633    6340          0.005401
634    6350          0.005618
635    6360          0.005221
636    6370          0.005738
637    6380          0.005250
638    6390          0.005494
639    6400          0.005636
640    6410          0.005125
641    6420          0.005508
642    6430          0.005097
643    6440          0.005296
644    6450          0.005395
645    6460          0.005339
646    6470          0.005285
647    6480          0.005428
648    6490          0.005120
649    6500          0.005385
650    6510          0.005444
651    6520          0.005212
652    6530          0.005295
653    6540          0.005031
654    6550          0.005159
655    6560          0.005008
656    6570          0.005181
657    6580          0.005136
658    6590          0.005360
659    6600          0.005319
660    6610          0.005149
661    6620          0.004968
662    6630          0.005222
663    6640          0.005238
664    6650          0.005150
665    6660          0.004875
666    6670          0.005427
667    6680          0.004959
668    6690          0.005140
669    6700          0.004920
670    6710          0.005072
671    6720          0.005069
672    6730          0.005254
673    6740          0.005077
674    6750          0.005057
675    6760          0.004951
676    6770          0.004991
677    6780          0.004825
678    6790          0.005017
679    6800          0.004976
680    6810          0.005089
681    6820          0.005152
682    6830          0.004858
683    6840          0.005136
684    6850          0.004956
685    6860          0.005051
686    6870          0.004828
687    6880          0.004891
688    6890          0.005028
689    6900          0.005015
690    6910          0.005061
691    6920          0.004773
692    6930          0.004729
693    6940          0.004802
694    6950          0.004832
695    6960          0.004995
696    6970          0.004788
697    6980          0.004742
698    6990          0.004650
699    7000          0.004711
700    7010          0.005093
701    7020          0.004772
702    7030          0.004947
703    7040          0.004679
704    7050          0.004794
705    7060          0.004806
706    7070          0.004840
707    7080          0.005022
708    7090          0.004669
709    7100          0.004739
710    7110          0.004602
711    7120          0.004754
712    7130          0.004707
713    7140          0.004835
714    7150          0.004727
715    7160          0.004790
716    7170          0.004647
717    7180          0.004763
718    7190          0.004670
719    7200          0.004585
720    7210          0.004852
721    7220          0.004821
722    7230          0.004489
723    7240          0.004576
724    7250          0.005023
725    7260          0.004470
726    7270          0.004554
727    7280          0.004715
728    7290          0.004630
729    7300          0.004513
730    7310          0.004653
731    7320          0.004356
732    7330          0.004759
733    7340          0.004712
734    7350          0.004791
735    7360          0.004615
736    7370          0.004557
737    7380          0.004424
738    7390          0.004659
739    7400          0.004650
740    7410          0.004560
741    7420          0.004397
742    7430          0.004294
743    7440          0.004324
744    7450          0.004575
745    7460          0.004506
746    7470          0.004370
747    7480          0.004637
748    7490          0.004500
749    7500          0.004528
750    7510          0.004365
751    7520          0.004453
752    7530          0.004483
753    7540          0.004423
754    7550          0.004340
755    7560          0.004669
756    7570          0.004367
757    7580          0.004996
758    7590          0.004281
759    7600          0.004270
760    7610          0.004443
761    7620          0.004291
762    7630          0.004187
763    7640          0.004431
764    7650          0.004268
765    7660          0.004413
766    7670          0.004444
767    7680          0.004203
768    7690          0.004017
769    7700          0.004323
770    7710          0.004160
771    7720          0.004287
772    7730          0.004254
773    7740          0.004323
774    7750          0.004300
775    7760          0.004127
776    7770          0.004474
777    7780          0.004381
778    7790          0.004223
779    7800          0.004223
780    7810          0.004386
781    7820          0.004170
782    7830          0.004050
783    7840          0.004046
784    7850          0.004166
785    7860          0.004173
786    7870          0.004293
787    7880          0.004008
788    7890          0.004196
789    7900          0.004230
790    7910          0.004121
791    7920          0.004181
792    7930          0.004273
793    7940          0.004368
794    7950          0.004028
795    7960          0.003930
796    7970          0.004117
797    7980          0.004044
798    7990          0.004065
799    8000          0.004316
800    8010          0.003959
801    8020          0.004177
802    8030          0.003960
803    8040          0.004080
804    8050          0.004050
805    8060          0.004035
806    8070          0.003870
807    8080          0.003996
808    8090          0.004059
809    8100          0.003997
810    8110          0.003947
811    8120          0.003906
812    8130          0.004129
813    8140          0.004187
814    8150          0.003716
815    8160          0.004056
816    8170          0.004248
817    8180          0.004073
818    8190          0.003951
819    8200          0.003980
820    8210          0.003876
821    8220          0.003844
822    8230          0.003904
823    8240          0.003921
824    8250          0.004079
825    8260          0.003927
826    8270          0.003777
827    8280          0.003808
828    8290          0.003875
829    8300          0.003858
830    8310          0.003901
831    8320          0.003751
832    8330          0.003907
833    8340          0.003855
834    8350          0.003966
835    8360          0.003682
836    8370          0.003861
837    8380          0.003764
838    8390          0.003795
839    8400          0.004175
840    8410          0.003843
841    8420          0.003901
842    8430          0.003510
843    8440          0.003757
844    8450          0.003668
845    8460          0.003631
846    8470          0.003921
847    8480          0.003727
848    8490          0.004029
849    8500          0.003714
850    8510          0.003964
851    8520          0.003728
852    8530          0.003752
853    8540          0.003682
854    8550          0.003659
855    8560          0.003921
856    8570          0.003575
857    8580          0.003589
858    8590          0.003436
859    8600          0.003785
860    8610          0.003610
861    8620          0.003770
862    8630          0.003797
863    8640          0.003751
864    8650          0.003627
865    8660          0.003768
866    8670          0.003587
867    8680          0.003583
868    8690          0.003710
869    8700          0.003645
870    8710          0.003829
871    8720          0.003529
872    8730          0.003482
873    8740          0.003569
874    8750          0.003528
875    8760          0.003539
876    8770          0.003600
877    8780          0.003519
878    8790          0.003551
879    8800          0.003543
880    8810          0.003487
881    8820          0.003399
882    8830          0.003532
883    8840          0.003490
884    8850          0.003698
885    8860          0.003535
886    8870          0.003453
887    8880          0.003428
888    8890          0.003518
889    8900          0.003562
890    8910          0.003521
891    8920          0.003497
892    8930          0.003468
893    8940          0.003655
894    8950          0.003499
895    8960          0.003540
896    8970          0.003289
897    8980          0.003461
898    8990          0.003468
899    9000          0.003391
900    9010          0.003502
901    9020          0.003384
902    9030          0.003399
903    9040          0.003481
904    9050          0.003412
905    9060          0.003283
906    9070          0.003475
907    9080          0.003446
908    9090          0.003503
909    9100          0.003269
910    9110          0.003362
911    9120          0.003182
912    9130          0.003260
913    9140          0.003406
914    9150          0.003265
915    9160          0.003358
916    9170          0.003422
917    9180          0.003236
918    9190          0.003383
919    9200          0.003275
920    9210          0.003410
921    9220          0.003329
922    9230          0.003372
923    9240          0.003307
924    9250          0.003294
925    9260          0.003309
926    9270          0.003193
927    9280          0.003485
928    9290          0.003321
929    9300          0.003203
930    9310          0.003188
931    9320          0.003419
932    9330          0.003152
933    9340          0.003177
934    9350          0.003128
935    9360          0.003360
936    9370          0.003157
937    9380          0.003499
938    9390          0.003195
939    9400          0.003183
940    9410          0.003210
941    9420          0.003221
942    9430          0.003534
943    9440          0.003134
944    9450          0.003059
945    9460          0.003160
946    9470          0.003080
947    9480          0.003313
948    9490          0.003350
949    9500          0.003006
950    9510          0.003264
951    9520          0.003360
952    9530          0.003116
953    9540          0.003197
954    9550          0.003094
955    9560          0.003117
956    9570          0.003036
957    9580          0.003077
958    9590          0.003108
959    9600          0.002996
960    9610          0.003205
961    9620          0.003550
962    9630          0.003046
963    9640          0.002938
964    9650          0.002931
965    9660          0.003209
966    9670          0.003178
967    9680          0.002928
968    9690          0.002986
969    9700          0.003328
970    9710          0.003179
971    9720          0.003004
972    9730          0.002901
973    9740          0.003072
974    9750          0.003206
975    9760          0.003186
976    9770          0.002901
977    9780          0.002989
978    9790          0.002940
979    9800          0.003105
980    9810          0.002941
981    9820          0.003059
982    9830          0.003298
983    9840          0.003110
984    9850          0.003318
985    9860          0.002869
986    9870          0.002915
987    9880          0.003096
988    9890          0.002956
989    9900          0.002939
990    9910          0.002869
991    9920          0.003014
992    9930          0.002798
993    9940          0.003159
994    9950          0.002946
995    9960          0.003028
996    9970          0.002868
997    9980          0.002827
998    9990          0.002993
999   10000          0.002909
[experiments_sandbox.py:927 -   <module>()] 2023-04-25 14:41:53,707 INFO Out-of sample batch-test
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,707 INFO test-batch  # 0 => test-loss = 0.0161165539175272
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,708 INFO test-batch  # 1 => test-loss = 0.020089762285351753
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,708 INFO test-batch  # 2 => test-loss = 0.0030735062900930643
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,708 INFO test-batch  # 3 => test-loss = 53.95866775512695
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,709 INFO test-batch  # 4 => test-loss = 0.005060364492237568
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,709 INFO test-batch  # 5 => test-loss = 0.013207127340137959
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,709 INFO test-batch  # 6 => test-loss = 0.008152741007506847
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,710 INFO test-batch  # 7 => test-loss = 0.06101980432868004
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,710 INFO test-batch  # 8 => test-loss = 1.1428718566894531
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,710 INFO test-batch  # 9 => test-loss = 0.04768948629498482
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,711 INFO test-batch  # 10 => test-loss = 0.20530442893505096
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,711 INFO test-batch  # 11 => test-loss = 0.027965551242232323
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,711 INFO test-batch  # 12 => test-loss = 0.2095532864332199
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,712 INFO test-batch  # 13 => test-loss = 0.02129216492176056
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,712 INFO test-batch  # 14 => test-loss = 0.011819430626928806
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,712 INFO test-batch  # 15 => test-loss = 0.011567641980946064
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,713 INFO test-batch  # 16 => test-loss = 0.02764596790075302
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,713 INFO test-batch  # 17 => test-loss = 0.006184025201946497
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,713 INFO test-batch  # 18 => test-loss = 0.11932897567749023
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,714 INFO test-batch  # 19 => test-loss = 0.005985224153846502
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,714 INFO test-batch  # 20 => test-loss = 0.0023587974719703197
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,714 INFO test-batch  # 21 => test-loss = 0.009398759342730045
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,715 INFO test-batch  # 22 => test-loss = 0.006130685564130545
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,715 INFO test-batch  # 23 => test-loss = 0.026294352486729622
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,715 INFO test-batch  # 24 => test-loss = 0.005163684021681547
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,716 INFO test-batch  # 25 => test-loss = 0.0033556248527020216
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,716 INFO test-batch  # 26 => test-loss = 1.157479166984558
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,716 INFO test-batch  # 27 => test-loss = 0.0064062452875077724
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,717 INFO test-batch  # 28 => test-loss = 0.38560089468955994
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,717 INFO test-batch  # 29 => test-loss = 0.0020828871056437492
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,717 INFO test-batch  # 30 => test-loss = 0.024701319634914398
[experiments_sandbox.py:932 -   <module>()] 2023-04-25 14:41:53,718 INFO test-batch  # 31 => test-loss = 0.0013517094776034355
